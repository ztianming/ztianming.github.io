[{"title":"Spring Cloud Bus自动刷新配置","date":"2020-06-17T05:25:00.000Z","path":"2020/06/17/SpringCloudBus自动刷新配置/","text":"在之前的文章中，我讲述了统一配置中心（服务端和客户端）的基本使用，并且演示了从配置Git仓库到拉取配置的整个流程，请见《统一配置中心》这篇博客。在该博客中我们说到了，统一配置中心还有个好处就是动态更新配置文件，而无需手动重启服务，但是并没有演示或者实现。本文将记述如何通过Spring Cloud Bus自动刷新配置，以及自动刷新的原理。 自动刷新配置原理实现配置的自动刷新是很有必要的，先看看使用Spring Cloud Bus实现配置的自动刷新的原理，如下图: Spring Cloud Bus提供了批量刷新配置的机制，它使用轻量级的消息代理（例如RabbitMQ、Kafka等）连接分布式系统的节点，这样就可以通过Spring Cloud Bus广播配置的变化或者其他的管理指令。shop服务的所有实例通过消息总线连接到了一起，每个实例都会订阅配置更新事件。当其中一个微服务节点的/bus/refresh端点被请求时，该实例就会向消息总线发送一个配置更新事件，其他实例获得该事件后也会更新配置。 实现配置自动刷新1、引入相关依赖和配置1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-bus-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 因为Spring Cloud Bus是需要通过消息队列来完成自动刷新配置的功能的，所以需要开启一个消息队列，并且在配置文件中配置一下消息队列，我这里采用的是RabbitMQ： 1234567891011121314151617181920212223spring: application: name: config cloud: config: server: git: uri: https://gitee.com/zouchanglin/config-repo username: zouchanglin password: 00001010101 basedir: /root/config rabbitmq: host: 192.168.79.128 port: 5672 username: guest password: guest eureka: instance: appname: config client: service-url: defaultZone: http://localhost:8762/eureka 统一配置中心和Client启动后可以看到对应的消息队列： 2、暴露更新接口1POST http://localhost:8080/actuator/bus-refresh 使用POST方式请求该接口，统一配置中心才知道Git仓库中的配置发生了变化，才会主动去拉取最新的配置，然后把配置更新的消息发送到消息队列，Config Client消费消息从而主动去配置中心拉取最新的配置，才完成了配置自动更新。但是actuator/bus-refresh 这个接口需要暴露出去，Git的WebHook才能访问到这个接口，所以还需要在统一配置中心的配置文件中加入以下内容： 123456789101112131415161718192021222324252627282930spring: application: name: config cloud: config: server: git: uri: https://gitee.com/zouchanglin/config-repo username: zouchanglin password: 00101010101 basedir: /root/config rabbitmq: host: 192.168.79.128 port: 5672 username: guest password: guest# 把actuator/bus-refresh接口暴露出去management: endpoints: web: exposure: include: \"*\"eureka: instance: appname: config client: service-url: defaultZone: http://localhost:8762/eureka 更新完配置文件后，重启一下统一配置中心。 3、声明配置自动更新范围shop-dev.yml配置如下，它的env属性是dev 1234567891011121314151617server: port: 8090eureka: client: service-url: defaultZone: http://localhost:8762/eureka/spring: application: name: shop rabbitmq: host: 192.168.79.128 port: 5672 username: guest password: guestenv: dev 我们在测试拿到的配置文件的内容的时候，写了这样的Controller，现在需要新加一个@RefreshScope注解，其实就是声明了配置自动更新的生效范围，所以需要加上@RefreshScope注解： 12345678910111213@RestController@RequestMapping(\"/env\")@RefreshScopepublic class EnvController &#123; @Value(\"$&#123;env&#125;\") private String env; @GetMapping(\"print\") public String printEnv()&#123; return env; &#125;&#125; 但是通常不这样使用，我们往往是把配置集中到一起，然后在配置类上面声明，假设现在Git仓库的shop-dev.yml配置如下： 123456789101112131415161718192021server: port: 8090eureka: client: service-url: defaultZone: http://localhost:8762/eureka/spring: application: name: shop rabbitmq: host: 192.168.79.128 port: 5672 username: guest password: guestenv: devboy: name: Tim age: 18 于是在config目录下新建一个JavaBean，叫做Boy，这时对这个Boy加上@RefreshScope： 12345678910package xpu.edu.shop_service.config;@Data@Component@ConfigurationProperties(prefix = \"boy\")@RefreshScopepublic class Boy &#123; private String name; private int age;&#125; 此时Controller修改为如下： 123456789101112@RestController@RequestMapping(\"/env\")public class EnvController &#123; @Autowired private Boy boy; @GetMapping(\"print\") public String printEnv()&#123; return \"name:\" + boy.getName() + \" age:\" + boy.getAge(); &#125;&#125; 通过手动访问/actuator/bus-refresh 接口，已经实现了刷新，如下图所示： 现在自动刷新就只差一步了，那就是配置Git的WebHook，只要发生更新或者是其他事件，Git托管平台便会自动访问我们的/actuator/bus-refresh 接口，从而自动刷新配置文件就完成了！","updated":"2020-06-17T13:36:19.391Z","categories":[{"name":"微服务架构","slug":"微服务架构","permalink":"https://zouchanglin.cn/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://zouchanglin.cn/tags/SpringCloud/"},{"name":"配置中心","slug":"配置中心","permalink":"https://zouchanglin.cn/tags/%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/"}]},{"title":"统一配置中心","date":"2020-06-17T01:25:00.000Z","path":"2020/06/17/统一配置中心/","text":"我们之前做单体应用的时候是直接把配置写在application.yml中，但是如果是采用微服务架构的模式进行开发，这样的方式会存在哪些问题呢？首先维护困难、安全因素、更新配置时项目需要重启等等。针对这些问题，本文主要讲述的就是Spring cloud config这个组件，使用该组件可以很好的处理如下问题。 原始做法的缺陷1、维护困难：假如一个服务，由多人开发，其中A在开发的时候，修改了配置，B再来开发的时候，需要测试别的一些功能，这个时候配置文件已经被A修改得面目全非了，这就造成了冲突。 2、安全因素：而且处于安全因素考虑，公司项目线上的配置基本是不对开发公开的，特别是数据库的账号密码这种，基本是只有运维才知道，把配置放在项目里面的话，每个开发人员都能看到，这种情况就需要对配置文件进行隔离。 3、更新配置需要重启：线上更新配置是经常发生的事情，比如更新一点小小的配置，难道都需要重启吗？使用spring cloud config就可以解决这一点。 统一配置中心的架构 配置中心到时候也会作为一个服务，这些配置为了方便管理，我们都把它放到git上，使用git控制起来会比较方便。如下图所示，其中箭头代表数据流动的方向： 最开始是把配置放在远端的Git，如Gitlab，Github或者自己搭建的私服，config-server把配置从远端Git拉下来之后，放到本地Git。config-server与本地Git之间是双向流动的，既会把远端的Git放到本地中，假如远端Git不能访问了，也会从本地Git把配置拉出来，拿到配置之后，就可以给微服务模块来使用。shop和order这两个服务，需要集成config-client这个组件。这就是统一配置中心整体的架构。 Config Server端使用流程1、引入相关依赖因为Config配置中心也是作为一个Client服务注册到Eureka Server的，所以必须引入Eureka Client的依赖，作为一个Eureka Client注册到Eureka Server上面。作为统一配置中心，必不可少的引入SpringCloud Config组件的依赖： 12345678910&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2、启动类加注解在启动类上添加注解支持@EnableConfigServer，其实不难发现很多SpringCloud组件的使用方式是一致的，都是先引入依赖，然后添加启动类的注解等流程，在这里不但需要@EnableConfigServer这个注解，同时不要忘记这也是一个Eureka Client，所以Eureka Client的注解和配置也是必不可少的。 12345678@SpringBootApplication@EnableDiscoveryClient@EnableConfigServerpublic class ConfigApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConfigApplication.class, args); &#125;&#125; application.yml 配置文件： 123456789101112131415161718spring: application: name: config cloud: config: server: git: uri: https://gitee.com/zouchanglin/config-repo username: zouchanglin password: 00101101010 # 指定配置文件存放的目录 basedir: /root/configeureka: instance: appname: config client: service-url: defaultZone: http://localhost:8761/eureka 3、配置文件放入仓库不难发现，我们在上面的配置文件中配置了Git的仓库地址、用户名以及密码，因为从架构图可以看出，其他服务组件配置文件需要放在一个Git仓库中，可以是Github、GitlabGit、Gitee或者是自己搭建的Git私服。我演示的时候直接放在了Gitee（码云）上面： 4、尝试访问配置文件接下来可以尝试访问一下是否生效，开始Eureka Server，然后启动统一配置中心注册到Eureka Server上面。 可以看到，虽然我们只是在Master分支上提交一个order.yml，但是我们访问order.yml却无法访问，访问order-a.yml、order-b.yml、order-a.properties、order-b.properties、order-a.json、order-b.json却没有问题，也就是说SpringCloudConfig帮我们做了转换，那么具体的转换规则是什么呢？在日志里可以看到/{label}/{name}-{profiles}.yml的字样，这些分别代表的意义如下： label：Git的分支，如master、dev、test、pre-release等等 name：服务名称，在这里订单服务则为 order profiles：环境，比如测试环境、预上线环境、线上环境等 Config Client端使用流程1、引入Config Client依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-client&lt;/artifactId&gt;&lt;/dependency&gt; 2、修改配置文件将application.yml修改为bootstrap.yml，意思就是从指定的配置中心来获取配置文件，然后再执行启动SpringBoot的核心流程： 123456789spring: application: name: shop cloud: config: discovery: enabled: true service-id: CONFIG profile: test 修改为bootstrap.yml后，也从原来的图表变成了一朵云，意思就是从服务器（统一注册中心）获取了 3、验证是否成功在仓库中放置了 shop-test.yml 文件： 12345678910111213server: port: 8090eureka: client: service-url: defaultZone: http://localhost:8761/eureka/spring: application: name: shopenv: test 还在仓库中放置了 shop-dev.yml 文件 12345678910111213server: port: 8090eureka: client: service-url: defaultZone: http://localhost:8761/eureka/spring: application: name: shopenv: dev 两个配置文件的env属性不同，写一个Controller测试一下从统一配置中心拿到的配置文件的内容： 123456789101112@RestController@RequestMapping(\"/env\")public class EnvController &#123; @Value(\"$&#123;env&#125;\") private String env; @GetMapping(\"print\") public String printEnv()&#123; return env; &#125;&#125; 当我们把bootstrap.yml中的profile属性设置为dev的时候，拿到的配置文件就是shop-dev.yml，此时访问打印环境属性的接口打印出来就是dev；当我们把bootstrap.yml中的profile属性设置为test的时候，拿到的配置文件就是shop-test.yml，此时访问打印环境属性的接口打印出来就是test： 统一配置中心的高可用在配置Eureka的高可用时，采用了相互注册的方式来实现高可用性。统一配置中心服务的高可用其实很简单，因为统一配置中心也Eureka的客户端，所以只要拥有足够的统一配置中心实例向Eureka Server注册即可实现高可用。由于是在本地开发环境，所以通过指定不同的端口号的方式来启动三个统一配置中心的实例： 123-Dserver.port=8080-Dserver.port=8081-Dserver.port=8082 在Eureka Server的界面可以看到，三个配置中心的实例已经注册到了Eureka Server上面： 使用配置中心的注意点http://localhost:8761/eureka/ 这个地址是默认的地址，假设我们改成其他的端口或者其他的IP地址就会报错找不到Eureka，其实要理解这一点并不难，因为要拿到配置文件的前提是得先找到统一配置中心的实例，统一配置中心的实例的前提是你得去Eureka Server上面找，前提是自身得注册到Eureka Server，但是此时并没有配置Eureka Server的注册地址，所以相当于与世隔绝是一个孤立的模块，自然会发生启动失败，所以关于Eureka Server的配置，无需由统一配置中心来分发，而是直接写在配置文件里即可。 当然还有一种方式，那就是指定统一配置中心的URL，这样就能直接找到统一配置中心的实例，在bootstrap.yml中： 12345678910spring: application: name: shop cloud: config: discovery: enabled: true service-id: CONFIG profile: dev uri: http://localhost:8888 总结一下就是： 方式一：先找到统一配置中心，获取配置，从配置中心给的配置中找到Eureka Server，再注册到Eureka Server 方式二：先找到Eureka Server，从Eureka Server找配置中心，从而获取对应的配置（推荐做法） 另外还有一点需要注意，那就是如果你需要的配置文件是shop-dev.yml，Git仓库同时存在shop.yml与shop-dev.yml，那么统一配置中心在拉取配置文件的时候，会把shop.yml和shop-dev.yml两个文件同时拉取下来，并且进行内容合并，所以我们通常遵循如下原则，那就是公共的配置内容写在shop.yml（对于本例而言）中，不同环境的配置内容写在shop-dev.yml或者shop-test.yml中。","updated":"2020-06-17T13:34:51.867Z","categories":[{"name":"微服务架构","slug":"微服务架构","permalink":"https://zouchanglin.cn/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://zouchanglin.cn/tags/SpringCloud/"},{"name":"配置中心","slug":"配置中心","permalink":"https://zouchanglin.cn/tags/%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/"}]},{"title":"Feign与项目多模块","date":"2020-06-16T04:25:00.000Z","path":"2020/06/16/Feign与项目多模块/","text":"本篇文章主要是记录了Feign的使用方式，并且重点讲述了使用Maven构建多模块项目，从而更好地适应微服务架构的软件开发模式。在服务调用的场景中，我们经常调用基于HTTP协议的服务，Feign封装了Http调用流程，更适合面向接口化的变成习惯。Feign底层使用了Ribbon作为负载均衡的客户端，而有关Ribbon的负载均衡的实现请见《RestTemplate与负载均衡器》。 一、Feign基本使用1、引入依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 2、启动类注解 1@EnableFeignClients 3、声明伪RPC调用接口（因为本质还是HTTP） 12345@FeignClient(name = \"SHOP-CLIENT\")public interface ShopClient &#123; @GetMapping(\"/shop/show\") String getShop();&#125; 4、注入接口对象并使用 123456789101112@RestController@RequestMapping(\"/order\")public class OrderController &#123; @Autowired private ShopClient shopClient; @GetMapping public String getOrder()&#123; return shopClient.getShop(); &#125;&#125; 使用Feign可以让调用者无感知这是一个远程调用，获得与本地方法一致的体验 二、项目多模块改造其实多模块改造主要就是为了复用，毕竟让调用者去声明Client端是不合理的，Client应该由服务端声明，也就是我自己知道自己提供了哪些服务，别人来使用这些服务即可，而不是别人看我的源代码才知道我提供哪些接口。另外像接口输入参数，输出参数也都是需要为调用者提供的。 商品服务有两个功能：查看所有商品、新增商品，商品在数据库中的定义如下： 1234567@Data@AllArgsConstructorpublic class ShopInfo &#123; private String shopId; private String shopName; private Integer shopStock;&#125; 分别是商品ID、商品名称、商品库存等字段，我们需要给外界展示的是ID和名称，库存多少是没有必要展示费消费者的。同样的对于商城的管理系统来说，如果要添加新的商品，那么主键ID是没有必要让用户手动传入的，所以对于这两种情况分别就是此服务模块的输入参数和输出参数，分别定义对应的JavaBean： 123456789101112@Datapublic class ShopInfoInput &#123; private String shopName; private Integer shopStock;&#125; @Data@AllArgsConstructorpublic class ShopInfoOutput &#123; private String shopId; private String shopName;&#125; 并且将这两个类放到shop-common模块中，作为商品服务的通用模块。接下来是逻辑的编写，应该放到shop-serivce模块中，shop-service和我们平时的SpringBoot工程无区别，实现的都是主体业务逻辑： 123456789101112131415161718192021222324252627282930313233@RestController@RequestMapping(\"/shop\")public class ShopController &#123; private static final List&lt;ShopInfo&gt; *list* = Arrays.*asList*( new ShopInfo(UUID.*randomUUID*().toString(), \"ThinkPad X1\", 10), new ShopInfo(UUID.*randomUUID*().toString(), \"MacBook Air\", 5), new ShopInfo(UUID.*randomUUID*().toString(), \"MacBook Pro\", 20) ); private static final CopyOnWriteArrayList&lt;ShopInfo&gt; *collect* = new CopyOnWriteArrayList&lt;&gt;(*list*); @GetMapping(\"show\") public List&lt;ShopInfoOutput&gt; getAllShop()&#123; return *collect*.stream() .map(x -&gt; new ShopInfoOutput(x.getShopId(), x.getShopName())) .collect(Collectors.*toList*()); &#125; @PostMapping(\"create\") public List&lt;ShopInfoOutput&gt; addOneShop(@RequestBody ShopInfoInput shopInfoInput)&#123; *collect*.add(new ShopInfo(UUID.*randomUUID*().toString(), shopInfoInput.getShopName(), shopInfoInput.getShopStock())); return *collect*.stream() .map(x -&gt; new ShopInfoOutput(x.getShopId(), x.getShopName())) .collect(Collectors.*toList*()); &#125; &#125; @Data @AllArgsConstructor class ShopInfo &#123; private String shopId; private String shopName; private Integer shopStock; &#125; 同时配置好Eureka，在application.yml中： 123456789server: port: 8080eureka: client: service-url: defaultZone: http://localhost:8761/eureka/spring: application: name: shop-client 最后需要把服务提供暴露给外界使用，所以直接使用Feign来完成shop-client模块的编写: 123456789@FeignClient(name = \"SHOP-CLIENT\")public interface ShopClient &#123; @GetMapping(\"/shop/show\") List&lt;ShopInfoInput&gt; getAllShop(); @PostMapping(\"/shop/create\") List&lt;ShopInfoOutput&gt; addOneShop(@RequestBody ShopInfoInput shopInfoInput);&#125; 通过上面的代码我们不难发现，项目被分成了三个模块，分别是shop-common、shop-service和shop-client，他们之间的依赖关系如下图所示： 所以，接下来介绍一下1个大工程的pom文件和3个小模块的pom文件，首先是父工程的pom文件： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- Spring Boot版本 --&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.3.0.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;groupId&gt;xpu.edu&lt;/groupId&gt; &lt;artifactId&gt;shop_server&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;shop_server&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;!-- 共用的一些配置 --&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;spring-cloud.version&gt;Hoxton.SR4&lt;/spring-cloud.version&gt; &lt;shop-common-version&gt;0.0.1-SNAPSHOT&lt;/shop-common-version&gt; &lt;/properties&gt; &lt;!-- 包含的子模块 --&gt; &lt;modules&gt; &lt;module&gt;shop_client&lt;/module&gt; &lt;module&gt;shop_common&lt;/module&gt; &lt;module&gt;shop_service&lt;/module&gt; &lt;/modules&gt; &lt;!-- 打包方式必须是pom --&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;!-- 依赖管理 --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 公用模块 --&gt; &lt;dependency&gt; &lt;groupId&gt;xpu.edu&lt;/groupId&gt; &lt;artifactId&gt;shop_common&lt;/artifactId&gt; &lt;version&gt;$&#123;shop-common-version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- SpringCloud --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt;&lt;/project&gt; 父工程的还是基于SpringBoot，另外包含了一些公用配置，包含SpringCloud的版本等信息，还包含了公用模块的依赖。接下来看看shop-service模块： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;xpu.edu&lt;/groupId&gt; &lt;artifactId&gt;shop_server&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;artifactId&gt;shop_service&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;shop_service&lt;/name&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;xpu.edu&lt;/groupId&gt; &lt;artifactId&gt;shop_common&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt; &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 这个模块和我们之前写的工程一致，各种必要的依赖（不要忘记公用模块shop-common），编译插件、SpringBoot插件。最后需要看的是shop-client模块，因为这个模块相当于是整个系统的使用手册： 1234567891011121314151617181920212223242526272829303132&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;xpu.edu&lt;/groupId&gt; &lt;artifactId&gt;shop_server&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;artifactId&gt;shop_client&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;shop_client&lt;/name&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-openfeign-core&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;xpu.edu&lt;/groupId&gt; &lt;artifactId&gt;shop_common&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 因为这个模块用到了@FeignClient(name = “SHOP-CLIENT”)、@GetMapping(“/shop/show”)、@PostMapping(“/shop/create”)等注解，所以需要引入spring-web、spring-cloud-openfeign-core等依赖，同样的公用模块需要引入，所以加上了shop-common这个模块的依赖。最后是shop-common的pom文件 : 12345678910111213141516171819&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;xpu.edu&lt;/groupId&gt; &lt;artifactId&gt;shop_server&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;artifactId&gt;shop_common&lt;/artifactId&gt; &lt;name&gt;shop_common&lt;/name&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 其实就是引入了一个lombok，没啥其他的东西。 那么别的模块如何使用shop-server这个工程提供的服务呢？下面是一个order-server即订单服务。它也是一个多模块的项目，分为order-common、order-client、order-service。只是为了测试所以，order-common与order-client模块都是空的，我们直接使用order-service模块测试一下即可： 1、引入依赖 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;edu.xpu&lt;/groupId&gt; &lt;artifactId&gt;order_server&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;artifactId&gt;order_service&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;order_service&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;xpu.edu&lt;/groupId&gt; &lt;artifactId&gt;shop_client&lt;/artifactId&gt; &lt;version&gt;$&#123;shop_client.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.67&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;edu.xpu&lt;/groupId&gt; &lt;artifactId&gt;order_common&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt; &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 2、添加注解，其实就是为了把FeignClient给添加到IOC容器中 123456789@SpringBootApplication@EnableDiscoveryClient@EnableFeignClients(basePackages = \"xpu.edu.shop_client\")public class OrderServiceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(OrderServiceApplication.class, args); &#125;&#125; 3、使用其他(shop-client)模块的FeignClient 123456789101112131415@RestController@RequestMapping(\"/\")public class TestShopClient &#123; @Autowired private ShopClient shopClient; @GetMapping public String test()&#123; ShopInfoInput infoInput = new ShopInfoInput(); infoInput.setShopName(\"iPad Pro\"); infoInput.setShopStock(100); shopClient.addOneShop(infoInput); return JSON.toJSONString(shopClient.getAllShop()); &#125;&#125; 完整的代码请见： https://github.com/zouchanglin/practic_code/tree/master/maven_test","updated":"2020-06-16T13:58:12.269Z","categories":[{"name":"微服务架构","slug":"微服务架构","permalink":"https://zouchanglin.cn/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/"},{"name":"消息通信","slug":"微服务架构/消息通信","permalink":"https://zouchanglin.cn/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/%E6%B6%88%E6%81%AF%E9%80%9A%E4%BF%A1/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://zouchanglin.cn/tags/SpringCloud/"}]},{"title":"RestTemplate与负载均衡器","date":"2020-05-15T10:25:00.000Z","path":"2020/05/15/RestTemplate与负载均衡器/","text":"本文主要是介绍SpringCloud构建微服务系统的Ribbon负载均衡器和网络请求框架RestTemplate，另外将会分析负载均衡器的源码，通过实例证明如何通过Ribbon和RestTemplate相结合实现负载均衡。现在假设有一个分布式系统，该系统由在不同计算机上运行的许多服务组成。当用户数量很大时，通常会为服务创建多个副本。每个副本都在另一台计算机上运行，此时有助于在服务器之间平均分配传入流量。 客户端发现与服务端发现 在一个系统中，服务通常需要调用其他服务。单体应用中，服务通过语言级别的方法或者过程调用另外的服务。在传统的分布式部署中，服务运行在固定，已知的地址主机和端口，因此可以请求的通过HTTP/REST或其他RPC机制调用。 然而，一个现代的微服务应用通常运行在虚拟或者容器环境，服务实例数和它们的地址都在动态改变。因此需要实现一种机制，允许服务的客户端向动态变更的一组短暂的服务实例发起请求，这就是服务注册与发现，服务注册与发现是微服务架构中最重要的基础组件。 我们接下来需要搞清楚的是什么是客户端发现，什么是服务端发现？ 这里的注册中心其实就相当于青楼的老鸨，A是嫖客，B是小姐。这样一比喻相信各位老司机都知道三者之间的交互逻辑了。客户端发现就是当A需要调用B服务时，请求注册中心（B服务在启动时会将信息注册到注册中心），注册中心将一份完整的可用服务列表返回给 A 服务，A 服务自行决定使用哪个 B 服务。客户端发现的特点： 简单直接，不需要代理的介入 客户端（A）知道所有实际可用的服务地址 客户端（A）需要自己实现负载均衡逻辑 使用客户端发现的例子：Eureka 服务端发现相对于客户端发现，多了一个代理，代理帮A从众多的B中挑选一个B。服务端发现的特点： 由于代理的介入，服务（B）与注册中心，对 A 是不可见的 使用服务端发现的例子：Nginx、ZooKeeper、Kubernetes 客户端与服务端负载均衡通过理解客户端发现与服务端发现的区别，我们明白其实调用哪个服务取决于客户端还是服务端是由什么决定的呢？那就是取决于服务注册与发现使用的是客户端发现还是服务端发现。 服务端负载均衡服务器端负载均衡器，我们比较常见的例如Nginx、F5是放置在服务器端的组件。当请求来自客户端时，它们将转到负载均衡器，负载均衡器将为请求指定服务器。负载均衡器使用的最简单的算法是随机指定。在这种情况下，大多数负载平衡器是用于控制负载平衡的硬件集成软件。服务端负载均衡的特点： 对客户端不透明，客户端不知道服务器端的服务列表，甚至不知道自己发送请求的目标地址存在负载均衡器。 服务器端维护负载均衡服务器，控制负载均衡策略和算法。 客户端负载均衡当负载均衡器位于客户端时，客户端得到可用的服务器列表然后按照特定的负载均衡策略，分发请求到不同的服务器 。 客户端负载均衡的特点： 对客户端透明，客户端需要知道服务器端的服务列表，需要自行决定请求要发送的目标地址。 客户端维护负载均衡服务器，控制负载均衡策略和算法。 目前单独提供的客户端实现比较少（本文只分析Ribbon），大部分都是在框架内部自行实现。 RestTemplate三种使用方式RestTemplate是Spring框架提供的一种用于访问Rest服务的客户端，RestTemplate提供了多种便捷访问远程Http服务的方法,能够大大提高客户端的编写效率。 之前我们使用的较多的是Apache的OKHttp这个包库，或者是根据HttpUrlConnection封装的库，现在有了更好的选择，那就是RestTemplate： 1、直接填写服务地址Order应用想要直接访问Shop应用的接口，填写服务地址直访问即可。 2、使用LoadBalancerClient使用LoadBalancerClient的choose()获得ServiceInstance，也就是这两个应用必须先向Eureka Server注册，然后通过Client的名称来选择对应的服务实例： 3、注入RestTemplate Bean注入RestTemplate bean，使用服务名称访问即可： Ribbon负载均衡源码分析在上面的例子中我们使用了RestTemplate并且开启了客户端负载均衡功能，开启负载均衡很简单，只需要在RestTemplate的bean上再添加一个@LoadBalanced注解即可，我们可以从这个注解开始分析： 12345678910111213/** * Annotation to mark a RestTemplate or WebClient bean to be configured to use a * LoadBalancerClient. * @author Spencer Gibb */@Target(&#123; ElementType.FIELD, ElementType.PARAMETER, ElementType.METHOD &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@Qualifierpublic @interface LoadBalanced &#123;&#125; 这个注解是用来给RestTemplate做标记，配置LoadBalancerClient，那么我们需要关注的类就是LoadBalancerClient了，LoadBalancerClient表示客户端负载均衡器，并且继承了ServiceInstanceChooser： 12345678910public interface LoadBalancerClient extends ServiceInstanceChooser &#123; // 使用从负载均衡器中挑选出来的服务实例来执行请求 &lt;T&gt; T execute(String serviceId, LoadBalancerRequest&lt;T&gt; request) throws IOException; // 使用从负载均衡器中挑选出来的服务实例来执行请求 &lt;T&gt; T execute(String serviceId, ServiceInstance serviceInstance, LoadBalancerRequest&lt;T&gt; request) throws IOException; // 为系统构建一个合适的URI // 如 http://SHOP-CLIENT/shop/show -&gt; http://localhost:8080/shop/show URI reconstructURI(ServiceInstance instance, URI original);&#125; ServiceInstanceChooser从名字上我们就可以看出，这是需要给出服务实例选择的具体实现，也就是实现choose方法： 根据传入的服务名serviceId从客户端负载均衡器中挑选一个对应服务的实例： 1ServiceInstance choose(String serviceId); 至于具体的配置我们还需要看LoadBalancerAutoConfiguration类的源码，该类是客户端负载均衡服务器的自动化配置类，该类的源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * Auto-configuration for Ribbon (client-side load balancing). */@Configuration(proxyBeanMethods = false)@ConditionalOnClass(RestTemplate.class)@ConditionalOnBean(LoadBalancerClient.class)@EnableConfigurationProperties(LoadBalancerRetryProperties.class)public class LoadBalancerAutoConfiguration &#123; @LoadBalanced @Autowired(required = false) private List&lt;RestTemplate&gt; restTemplates = Collections.emptyList(); @Autowired(required = false) private List&lt;LoadBalancerRequestTransformer&gt; transformers = Collections.emptyList(); @Bean public SmartInitializingSingleton loadBalancedRestTemplateInitializerDeprecated( final ObjectProvider&lt;List&lt;RestTemplateCustomizer&gt;&gt; restTemplateCustomizers) &#123; return () -&gt; restTemplateCustomizers.ifAvailable(customizers -&gt; &#123; for (RestTemplate restTemplate : LoadBalancerAutoConfiguration.this.restTemplates) &#123; for (RestTemplateCustomizer customizer : customizers) &#123; customizer.customize(restTemplate); &#125; &#125; &#125;); &#125; @Bean @ConditionalOnMissingBean public LoadBalancerRequestFactory loadBalancerRequestFactory( LoadBalancerClient loadBalancerClient) &#123; return new LoadBalancerRequestFactory(loadBalancerClient, this.transformers); &#125; @Configuration(proxyBeanMethods = false) @ConditionalOnMissingClass(\"org.springframework.retry.support.RetryTemplate\") static class LoadBalancerInterceptorConfig &#123; @Bean public LoadBalancerInterceptor ribbonInterceptor( LoadBalancerClient loadBalancerClient, LoadBalancerRequestFactory requestFactory) &#123; return new LoadBalancerInterceptor(loadBalancerClient, requestFactory); &#125; @Bean @ConditionalOnMissingBean public RestTemplateCustomizer restTemplateCustomizer( final LoadBalancerInterceptor loadBalancerInterceptor) &#123; return restTemplate -&gt; &#123; List&lt;ClientHttpRequestInterceptor&gt; list = new ArrayList&lt;&gt;( restTemplate.getInterceptors()); list.add(loadBalancerInterceptor); restTemplate.setInterceptors(list); &#125;; &#125; &#125; @Configuration(proxyBeanMethods = false) @ConditionalOnClass(RetryTemplate.class) public static class RetryAutoConfiguration &#123; @Bean @ConditionalOnMissingBean public LoadBalancedRetryFactory loadBalancedRetryFactory() &#123; return new LoadBalancedRetryFactory() &#123; &#125;; &#125; &#125; @Configuration(proxyBeanMethods = false) @ConditionalOnClass(RetryTemplate.class) public static class RetryInterceptorAutoConfiguration &#123; @Bean @ConditionalOnMissingBean public RetryLoadBalancerInterceptor ribbonInterceptor( LoadBalancerClient loadBalancerClient, LoadBalancerRetryProperties properties, LoadBalancerRequestFactory requestFactory, LoadBalancedRetryFactory loadBalancedRetryFactory) &#123; return new RetryLoadBalancerInterceptor(loadBalancerClient, properties, requestFactory, loadBalancedRetryFactory); &#125; @Bean @ConditionalOnMissingBean public RestTemplateCustomizer restTemplateCustomizer( final RetryLoadBalancerInterceptor loadBalancerInterceptor) &#123; return restTemplate -&gt; &#123; List&lt;ClientHttpRequestInterceptor&gt; list = new ArrayList&lt;&gt;( restTemplate.getInterceptors()); list.add(loadBalancerInterceptor); restTemplate.setInterceptors(list); &#125;; &#125; &#125;&#125; LoadBalancerAutoConfiguration类上有两个关键注解，分别是@ConditionalOnClass(RestTemplate.class)和@ConditionalOnBean(LoadBalancerClient.class)，说明Ribbon如果想要实现负载均衡的自动化配置需要满足两个条件：第一个，RestTemplate类必须存在于当前工程的环境中；第二个，在Spring容器中必须有LoadBalancerClient的实现Bean。 RetryInterceptorAutoConfiguration类的ribbonInterceptor方法返回了一个拦截器叫做LoadBalancerInterceptor，这个拦截器的作用主要是在客户端发起请求时进行拦截，进而实现客户端负载均衡功能， 其中的restTemplateCustomizer方法返回了一个RestTemplateCustomizer，这个方法主要用来给RestTemplate添加LoadBalancerInterceptor拦截器。LoadBalancerAutoConfiguration中的restTemplates是一个被@LoadBalanced注解修饰的RestTemplate对象列表，通过restTemplateCustomizer方法对每个 RestTemplate对象添加上LoadBalancerInterceptor拦截器。 那其实就是这些拦截器让一个普通的RestTemplate对象拥有了负载均衡的功能，LoadBalancerInterceptor的源码可以来看下： 123456789101112131415161718192021222324252627282930313233343536public class LoadBalancerInterceptor implements ClientHttpRequestInterceptor &#123; private LoadBalancerClient loadBalancer; private LoadBalancerRequestFactory requestFactory; public LoadBalancerInterceptor(LoadBalancerClient loadBalancer, LoadBalancerRequestFactory requestFactory) &#123; this.loadBalancer = loadBalancer; this.requestFactory = requestFactory; &#125; public LoadBalancerInterceptor(LoadBalancerClient loadBalancer) &#123; // for backwards compatibility this(loadBalancer, new LoadBalancerRequestFactory(loadBalancer)); &#125; @Override public ClientHttpResponse intercept(final HttpRequest request, final byte[] body, final ClientHttpRequestExecution execution) throws IOException &#123; final URI originalUri = request.getURI(); String serviceName = originalUri.getHost(); Assert.state(serviceName != null, \"Request URI does not contain a valid hostname: \" + originalUri); return this.loadBalancer.execute(serviceName, this.requestFactory.createRequest(request, body, execution)); &#125;&#125;@FunctionalInterfacepublic interface ClientHttpRequestInterceptor &#123; ClientHttpResponse intercept(HttpRequest var1, byte[] var2, ClientHttpRequestExecution var3) throws IOException;&#125; 当一个被@LoadBalanced注解修饰的RestTemplate对象向外发起HTTP请求时，会被LoadBalancerInterceptor类的intercept方法拦截，在这个方法中直接通过getHost方法就可以获取到服务名（因为我们在使用RestTemplate调用服务的时候，使用的是服务名而不是域名，所以这里可以通过getHost直接拿到服务名然后去调用execute方法发起请求）。 接下来我们去看看LoadBalancerClient的具体实现 —— RibbonLoadBalancerClient：在execute方法的具体视线中，不难发现首先获取到的就是ILoadBalancer： 这是一个接口，添加服务实例，选择服务实例，获取所有服务实例等方法均在其中： 12345678910111213141516public interface ILoadBalancer &#123; // 向负载均衡器中维护的实例列表增加服务实例 void addServers(List&lt;Server&gt; var1); // 表示通过某种策略，从负载均衡服务器中挑选出一个具体的服务实例 Server chooseServer(Object var1); // 表示用来通知和标识负载均衡器中某个具体实例已经停止服务 void markServerDown(Server var1); // 表示获取当前正常工作的服务实例列表 List&lt;Server&gt; getReachableServers(); // 表示获取所有的服务实例列表，包括正常的服务和停止工作的服务 List&lt;Server&gt; getAllServers();&#125; 我们看最基础的BaseLoadBalancer即可： 不难发现，其实默认的负载均衡策略采用的是轮询的方式。至于负载均衡的策略，其实也有很多种实现： 总结一下就是RestTemplate发起一个请求，这个请求被LoadBalancerInterceptor给拦截了，拦截后将请求的地址中的服务逻辑名转为具体的服务地址，然后继续执行请求的一个过程。","updated":"2020-06-15T16:39:28.881Z","categories":[{"name":"微服务架构","slug":"微服务架构","permalink":"https://zouchanglin.cn/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/"},{"name":"负载均衡","slug":"微服务架构/负载均衡","permalink":"https://zouchanglin.cn/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"name":"消息通信","slug":"微服务架构/负载均衡/消息通信","permalink":"https://zouchanglin.cn/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/%E6%B6%88%E6%81%AF%E9%80%9A%E4%BF%A1/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://zouchanglin.cn/tags/SpringCloud/"}]},{"title":"服务拆分方法论","date":"2020-05-14T10:25:00.000Z","path":"2020/05/14/服务拆分方法论/","text":"微服务是当下非常热门的话题，微服务发展到现在，已经不再单单局限于微服务架构本身，还与容器化、DevOps等新的理念相结合，成为当前移动互联网时代最先进的业务架构解决方案，能更好地迎合移动互联网业务快速迭代的要求。 本篇文章中我主要探讨的是什么时候适合微服务改造，如何做服务拆分等问题。 微服务适用场景这些年关于服务拆分的理论层出不穷，在我看来我们首先需要搞明白起点和终点，然后还需要考虑的因素与坚持的原则。什么是起点和终点呢？第一种由于历史原因，公司的产品不得不从传统应用架构转为微服务架构，第二种原因呢就是马上开发一个全新的系统，需要用上微服务架构（当然不排除是BOSS装B，要追求新技术，因为微服务比较潮流嘛）。我们自己无论如何还是需要审视一下起点，也就是现有的架构是个什么样子，要考虑是否真的需要转成为微服务架构。至于终点呢，以一言蔽之：好的架构不是设计出来的，而是进化出来的，而且是一直在演进，生命不息，进化不止。吾生也有涯，而知也无涯。 我们可以看看Dubbo架构路线图：从单一应用架构 -&gt; 垂直应用架构 -&gt; 分布式服务架构 -&gt; 流动计算架构 微服务与SOA之间就只是差了个ESB企业服务总线，如果此时的已经是SOA的架构，那么此时需要关心的也就是ESB了。从单体应用迁移到微服务架构的过程中，需要关注的重点是不一样的。微服务系统很可能是异构的，那么当前Java在整个系统中的占比是多少呢，有没有已经包含了服务注册与发现相关的组件呢，负载均衡的组件是弃用还是保留，如何以最小的代价切换过去。不过我们最需要优先考虑的就是，这个系统是否真的那么适合用微服务架构呢？在我看来，下列业务形态是不适合使用微服务架构的： 1、系统中包含很多很多强事务场景的不适合使用微服务架构。因为微服务是分布式的，如果是强事务场景，是不适合用微服务架构的。2、业务相对稳定，迭代周期长。比如系统本来就是一个非常稳定的系统，几乎也没什么变更迭代，几个月代码都不会更新一次，如果一定要切换到微服务的话代价还是比较大的。3、访问压力不高，可用性要求不高。比如中小型企业的内部OA系统，没啥访问量，偶尔出问题挂了个把小时其实也所谓，这样的项目如果用微服务架构岂不是杀鸡用牛刀。 所以微服务也不是放之四海而皆准的。 服务拆分方法论我们可以先看看什么是扩展立方模型 (Scale Cube)，Scale Cube是用于定义微服务和扩展技术产品的模型。AKF Partners于2007年发明了Scale Cube，最初于2017年在博客上发布《SPLITTING APPLICATIONS OR SERVICES FOR SCALE》 ，在《可扩展的艺术》一书中也出现过扩展立方模型： 立方体有三个轴线，每个轴线描述扩展性的一个维度！X轴：代表无差别的克隆服务和数据，通过负载均衡工作可以很均匀的分散在不同的服务实例上；Y轴：关注应用中职责的划分，比如数据类型，交易执行类型的划分；Z轴：关注服务和数据的优先级划分，如分地域划分。 通过这三个维度上的扩展，可以快速提高产品的扩展能力，适应不同场景下产品的快速增长。不同维度上的扩展，有着不同的优缺点： 1、X轴扩展 优点：成本最低，实施简单； 缺点：受指令集多少和数据集大小的约束。当单个产品或应用过大时，服务响应变慢，无法通过X轴的水平扩展提高速度； 场景：发展初期，业务复杂度低，需要增加系统容量。 2、Y轴扩展 优点：可以解决指令集和数据集的约束，解决代码复杂度问题，可以实现隔离故障，可以提高响应时间，可以使团队聚焦更利于团队成长； 缺点：成本相对较高； 场景：业务复杂，数据量大，代码耦合度高，团队规模大。 3、Z轴扩展 优点：能解决数据集的约束，降低故障风险，实现渐进交付，可以带来最大的扩展性。 缺点：成本最昂贵，且不一定能解决指令集的问题； 场景：用户指数级快速增长。 三个维度拆分后，微服务的架构图就如下图所示： 功能拆分的角度1、单一职责、松耦合、高内聚 2、关注点分离 按职责分离 按通用性分离 按粒度级别 将理论付诸实践1、为扩展分割应用 X轴：从单体系统或服务，水平克隆出许多系统，通过负载均衡平均分配请求； Y轴 ：面向服务分割，基于功能或者服务分割，例如电商网站可以将登陆、搜索、下单等服务进行Y轴的拆分，每一组服务再进行X轴的扩展； Z轴 ：面向查找分割，基于用户、请求或者数据分割，例如可以将不同产品的SKU分到不同的搜索服务，可以将用户哈希到不同的服务等。 2、为扩展分割数据库 X轴：从单库，水平克隆为多个库上读，一个库写，通过数据库的自我复制实现，要允许一定的读写时延； Y轴 ：根据不同的信息类型，分割为不同的数据库，即分库，例如产品库，用户库等； Z轴 ：按照一定算法，进行分片，例如将搜索按照MapReduce的原理进行分片，把SKU的数据按照不同的哈希值进行分片存储，每个分片再进行X轴冗余。 3、为扩展而缓存在理想情况下，处理大流量最好的方法是通过高速缓存来避免处理它。从架构层面看，我们能控制的主要有以下三个层次的缓存：① 对象缓存：对象缓存用来存储应用的对象以供重复使用，一般在系统内部，通过使用应用缓存可以帮助数据库和应用层卸载负载。 ② 应用缓存：应用缓存包括代理缓存和反向代理缓存，一个在用户端，一个在服务端，目标是提高性能或减少资源的使用量。 ③ 内容交付网络缓存：CDN的总原则是将内容推送到尽可能接近用户终端的地方，通过不同地区使用不同ISP的网关缓存，达到更快的响应时间和对源服务的更少请求。 4、为扩展而异步同步改异步：同步调用，由于调用间的同步依赖关系，有可能会导致雪崩效应，出现一系列的连锁故障，进而导致整个系统出现问题，所以在进行系统设计时，要尽可能的考虑异步调用方式，邮件系统就是一个非常好的异步调用例子。 应用无状态：当进行AKF扩展立方体的任何一个轴上的扩展时，都要首先解决应用的状态问题，即会话的管理，可以通过避免、集中和分散的方式进行解决。 AKF扩展立方体是一套通用的扩展性理论，它不仅可以应用到系统的架构扩展上，也可以应用到人员的组织架构扩展上甚至其他相关的工业领域。当然并不是所有公司都需要同时在XYZ三个方向上进行扩展，并且每个方向上的扩展都有它的利弊，我们不可避免的要进行适当的权衡。","updated":"2020-05-14T11:38:09.780Z","categories":[{"name":"微服务架构","slug":"微服务架构","permalink":"https://zouchanglin.cn/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"https://zouchanglin.cn/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"}]},{"title":"软件工程方法之DevOps","date":"2020-05-14T00:25:00.000Z","path":"2020/05/14/软件工程方法之DevOps/","text":"我们经常看到DevOps这个词，那么DevOps究竟是什么呢？DevOps 是一种软件开发方法。它将持续开发、持续测试、持续集成、持续部署和持续监控贯穿于软件开发的整个生命周期。当前几乎所有的顶尖公司均采用了该方法，用以提高软件开发质量，并缩短软件开发生命周期。从而以达到每个公司对软件产品的期望，交付出客户最满意的产品。 什么是瀑布模型在了解DevOps之前，我们先看一下什么是瀑布模型，瀑布模型是将软件生命周期的各项活动规定为按固定顺序而连接的若干阶段工作，形如瀑布流水，最终得到软件产品。 其过程是将上一项活动的输出作为该项活动的输入，利用这一输入实施该项活动应完成的内容，然后对当前活动的工作结果进行验证，如果验证通过，则该结果作为下一项活动的输入，继续进行下一项活动，否则返回修改。 传统的瀑布模型过于理想化，早期的错误只有等到开发后期才能发现，进而带来严重的后果。为尽早发现错误，在瀑布模型中加入迭代过程。当后面阶段发现前面阶段的错误时，需要沿图中左侧的反馈线返回前面的阶段，修正前面阶段的产品之后再回来继续完成后面阶段的任务。 由此可见，传统的瀑布模型的缺点是非常明显的，而且从总体上来看，瀑布模型的项目整体进度是比较慢的，那么现在被大多数公司采用的则是DevOps。 什么是敏捷开发敏捷开发是一种价值观与原则，指导我们更加高效的开发。 敏捷开发以用户需求为核心，采用迭代(时间周期)、增量(循序渐进，功能模块) 的方式开发软件，目的在于快速覆盖、响应市场需求。大项目划分为小项目，分别完成，独立运行，如微服务开发过程，就是将系统独立进行开发。传统的开发模式，注重文档约束，而敏捷开发原则的推行原则要求团队内部交流便利、文化相对开发，除去必要的文档约束，如Api接口文档，最注重的是团队成员的高效交流，以此来提高产品、项目的开发效率、开发质量。 敏捷开发提倡用户参与到产品或项目开发的整个流程当中，通过用户反馈使得产品更加符合用户频繁变动的需求。 持续集成 / 持续交付 / 持续部署在当前 DevOps 的趋势下，持续集成（CI）和持续部署（CD）具有支柱性地位，持续集成就是不断的尝试在一起。能够成功搭建 CI/CD 流水线就至关重要了。为了在开发团队和运营团队之间搭建桥梁，CI/CD 流水线实现了应用程序的自动构建、自动测试和自动部署，那我们接下来看看什么是 CI/CD 流水线，以及它是如何工作的。 CI代表持续集成（Continuous Integration），CD代表持续交付（Continuous Delivery）和持续部署（Continuous Deployment）。也可以将它们看作是类似于软件开发生命周期的过程。 该流水线展示了一个软件在其最终交付给客户或者投入上线之前，它在其生命周期内各个阶段中的移动过程。 其实就是版本控制 -&gt; 构建 -&gt; 测试 -&gt; 部署 -&gt; 自动化测试 -&gt; 部署上线 -&gt; 验证测试的这样的一个流程。假设我们要构建一款Web应用程序，并将它部署在一个现场Web服务器上。假设现在开发团队已经将代码提交到版本控制系统中了（假设版本控制工具为Git）。 构建阶段在此之前，开发者已经将他们的代码加上合适的标签，并提交到版本控制系统中了。假如我们采用的是Java语言，那么还需要先进行代码编译。因此，代码在通过版本控制阶段之后，会先在构建阶段予以编译。该阶段会从代码库的各个分支中获取到所有的功能代码，合并后最终通过一个编译器来编译它们。这整个过程都被称为构建阶段。 测试阶段构建阶段结束后，将会继续进入到代码的测试阶段。在这个阶段中，我们会进行各种各样的测试，单元测试就是其中之一。在该阶段中，会测试代码中多个组件间的关系或者单个组件的功能，同时也会进行软件的可用性测试。 部署阶段测试阶段完成后，就要进入部署阶段了。在该阶段，代码将会被部署到准生产环境服务器或者测试环境服务器中。同时在该阶段中，我们既可以查看程序代码，也可以在模拟器中运行该应用程序。 自动测试阶段只要我们的代码部署成功，我们就可以运行另一组可用性测试了。该阶段结束后，如果所有的测试都通过了，那么就可以将其部署到生产环境中了。 部署上线阶段可能在每一个阶段的执行过程中遇到一些错误。在这种情况下，可以将错误邮件发回到开发团队中，以便他们能够及时修复这些错误。当开发团队修复完成后，就可以将代码重新提交到版本控制系统中，然后再次从头开始执行该流水线。如果在执行测试的过程中遇到了任何错误，那么这些错误也将反馈给开发团队，等他们修复完成后，同样会再次触发该流水线，进行新一轮的持续迭代。 验证阶段整个生命周期将会继续迭代下去，直到我们得到可以直接部署到生产环境中的代码或者产品。除此之外，在生产环境中我们还需要对代码进行度量和验证，以实时监控应用的线上运行状态。到目前为止，我们已经了解了 CI/CD 流水线及其工作原理。 流程总结为什么需要一个统一的代码仓库Git来做代码管理呢？是为了代码集成在一起。 为什么需要进行构建build呢？就是代码逻辑需要集成在一起，编译不出错。 为什么要单元测试呢？一个模块的功能集成在一起能够正确工作。 为什么需要联调预上线（准生产）环境呢？需要将不同模块之间集成在一起，在一个类生产的环境中进行测试。 最终才是部署到生产环境中，将所有人分开做的工作才算真正的合在了一起。 什么是DevOps关于敏捷开发是什么我在上面已经说到了，敏捷开发就是一种开发流程，是一种快速迭代的开发流程，每个开发流程非常短，长到一个月，短到两个星期，就会是一个周期，在这个周期中，每天都要开会同步，每天都要集成。正是因为周期短，才需要持续的做这件事情，如果一个开发周期长达几个月，则不需要持续的集成，最后留几个星期的集成时间一起做也是可以的，但是这样就不能达到互联网公司的快速迭代，也是我们常常看到传统公司的做法。 DevOps不仅仅是CI/CD，除了技术和流程，还包含文化。例如容器化带来的一个巨大的转变是，原来只有运维关心环境的部署，无论是测试环境，还是生产环境，都是运维搞定的，而容器化之后，需要开发自己写Dockerfile，自己关心环境的部署。因为微服务之后，模块太多了，让少数的运维能够很好的管理所有的服务，压力大，易出错，然而开发往往分成很多的团队，每个模块自己关心自己的部署，则不易出错，这就需要运维一部分的工作让研发来做，需要研发和运维的打通，如果公司没有这个文化，研发不写Dockerfile，则DevOps是无法实施的。 参考资料：《How to build CI/CD pipeline from scratch》","updated":"2020-05-14T04:39:40.708Z","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://zouchanglin.cn/categories/DevOps/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://zouchanglin.cn/tags/Jenkins/"},{"name":"软件工程","slug":"软件工程","permalink":"https://zouchanglin.cn/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"}]},{"title":"Eureka的高可用","date":"2020-05-13T00:25:00.000Z","path":"2020/05/13/Eureka的高可用/","text":"Eureka是Netfilx开源的服务发现组件，本身是一个基于REST的服务。它包含EurekaServer和EurekaClient两部分，SpringCloud将它集成在子项目Spring Cloud Netfilx中，实现了微服务的注册与发现。 Eureka作为服务注册中心对整个微服务架构起着最核心的整合作用。 Eureka简单来讲就是Netflix开源的一款提供服务注册和发现的产品，Spring Cloud Netflix提供的胶水代码更换了一些初始化配置，并增加了更人性化的界面，可以这样理解，Spring Cloud Netflix让Eureka更简单易用，下面我们看看如何实现Eureka的高可用。 当其中一台的服务发生故障时不影响整体服务状况，不能因为一台服务器的问题导致服务停止，高可用的方法有三种：主从方式、双机双工方式、集群工作方式。而Zookeeper采用的是主从方式、Eureka则采用的是集群方式，当多台服务器相互注册就形成了高可用，这样当其中的一台停止提供服务时，剩余的则会继续提供服务。 我们需要将原来的单个EurekaClient与单个EurekaServer进行注册的方式改成两个单个EurekaServer相互注册，然后，Client分别在两个EurekaServer上注册。这样就算其中一台单个EurekaServer挂掉了，另一个还能继续工作。实现了最简单的高可用的架构。那么如何实现呢？ 首先需要在xxApplication类中加上@EnableEurekaServer注解，表示这是一个EurekaServer，然后配置如下： 12345678910eureka: client: service-url: defaultZone: http://127.0.0.1:8761/eureka/ register-with-eureka: false server: enable-self-preservation: falsespring: application: name: eureka 接下来我开启两个EurekaServer，端口分别为8761与8762： 在启动EurekaServer1的时候，把它的注册地址改为http://127.0.0.1:8762/eureka/，在启动EurekaServer2的时候，把它的注册地址改为http://127.0.0.1:8761/eureka/，这样两个EurekaServer便完成了相互注册。 这里需要注意，虽然localhost与127.0.0.1在我们平时使用起来是一样的，但是如果这里写成了http://localhost:8761/eureka/ 就无法相互注册，一定要写127.0.0.1，如果不可以的话需要修改Windows的Host文件，通过假域名来替代localhost。 这里还有个问题，那就是注册地址为什么是 http://127.0.0.1:8761/eureka/ ？为什么context-path是eureka，因为在SpringCloud环境下，context-path就是eureka 启动两个EurekaServer，我们可以看到相互注册的结果： 接下来我们把一个CLient注册到其中一个EurekaServer1上面： 1234567eureka: client: service-url: defaultZone: http://localhost:8761/eureka/spring: application: name: clientA 但是我们打开EurekaServer2，却发现这个Client同时注册到了EurekaServer2上面： 那么这样做的缺点就是如果EurekaServer1挂掉了，那么Client也就无法再EurekaServer2中注册，为了避免这种情况，Client需要在多个EurekaServer中注册，配置文件可以这么写（同时不要忘记需要在xxApplication类上注解@EnableDiscoveryClient）： 1234567eureka: client: service-url: defaultZone: http://localhost:8761/eureka/, http://localhost:8762/eureka/spring: application: name: clientA 其实我们还可以弄三台EurekaServer相互注册，像下图这样： 我们需要再开启一台EurekaServer3： 那么这三台EurekaServer在启动的配置分别为： 123456789101112131415161718192021222324252627282930313233eureka: client: service-url: defaultZone: http://127.0.0.1:8762/eureka/, http://127.0.0.1:8763/eureka/ register-with-eureka: false server: enable-self-preservation: falsespring: application: name: eureka ##################### EurekaServer2eureka: client: service-url: defaultZone: http://127.0.0.1:8761/eureka/, http://127.0.0.1:8763/eureka/ register-with-eureka: false server: enable-self-preservation: falsespring: application: name: eureka##################### EurekaServer3eureka: client: service-url: defaultZone: http://127.0.0.1:8761/eureka/, http://127.0.0.1:8762/eureka/ register-with-eureka: false server: enable-self-preservation: falsespring: application: name: eureka 上面介绍了双节点注册中心和三节点注册中心，如果是在生产环境中需要至少三台或者三台以上的注册中心，保证服务的高可用性。不过，我们还需要重点了解的就是Eureka的心跳检测、健康检查以及负载均衡是如何做到的等原理性的东西。另外，理解微服务架构也是很重要的，服务注册中心是最重要的基础部分。","updated":"2020-06-15T15:06:19.173Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaEE","slug":"JavaEE","permalink":"https://zouchanglin.cn/tags/JavaEE/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://zouchanglin.cn/tags/SpringCloud/"}]},{"title":"自定义SpringBootStarter","date":"2020-05-05T00:25:00.000Z","path":"2020/05/05/自定义SpringBootStarter/","text":"在日常使用SpringBoot的开发中，我们想要引入某个组件，几乎都是直接引入一个SpringBootStarter就完事，什么配置文件大发布分省略甚至是完全省略的。在前面的文章中，我们分析了SpringBoot的具体启动流程《 SpringBoot启动流程探究 》、以及SpringBoot自动配置的原理 《 SpringBoot自动配置原理 》 。通过这两篇文章其实很容易弄清楚那些官方的starter是如何运行起来的，并且我们可以制作一个自己的SpringBootStarter，并且我会把自制的SpringBootStarter推送到公服，本篇文章会记录一个完整的开发流程。 创建SpringBootStarter其实就是创建一个普通的SpringBoot项目，无论是用Gradle还是Maven都可以，我这里选择的是Maven的方式，只不过项目的命名方式略微有所不同，因为Spring官方的starter命令为spring-boot-starter-xxx，所以我们开发的项目不要以spring-boot开头。 建议写成：xxx-spring-boot-starter，代表我们这是一个非官方的SpringBootStarter。 pom.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.2.6.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.github.zouchanglin&lt;/groupId&gt; &lt;artifactId&gt;example-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;example-spring-boot-starter&lt;/name&gt; &lt;description&gt;A custom Spring-Boot-Starter sample.&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;spring.boot&gt;2.2.6.RELEASE&lt;/spring.boot&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.boot&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-autoconfigure&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 我们编写一个工具类AESHandleUtil.java，假设我们需要封装一个AES加密的工具类，在这个工具类里面呢，我们想把初始化秘钥随机串的长度作为用户的自定义参数，用户可以根据自己的实际需要定义是长度为128？256还是一些其他的值。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.github.zouchanglin.examplespringbootstarter.util;import javax.crypto.Cipher;import javax.crypto.KeyGenerator;import javax.crypto.SecretKey;import javax.crypto.spec.SecretKeySpec;import java.nio.charset.StandardCharsets;import java.security.SecureRandom;public class AESHandleUtil &#123; /** * AES加密字符串 * * @param content 需要被加密的字符串 * @param password 加密需要的密码 * @param length 初始化秘钥随机串的长度 * @return 密文 */ public static byte[] encrypt(String content, String password, Integer length) &#123; try &#123; // 创建AES的Key生产者 KeyGenerator keyGenerator = KeyGenerator.getInstance(\"AES\"); // 利用用户密码作为随机数初始化出 keyGenerator.init(length, new SecureRandom(password.getBytes())); // 加密没关系，SecureRandom是生成安全随机数序列 // 根据用户密码，生成一个密钥 SecretKey secretKey = keyGenerator.generateKey(); // 返回基本编码格式的密钥，如果此密钥不支持编码，则返回 byte[] enCodeFormat = secretKey.getEncoded(); // 转换为AES专用密钥 SecretKeySpec key = new SecretKeySpec(enCodeFormat, \"AES\"); // 创建密码器 Cipher cipher = Cipher.getInstance(\"AES\"); byte[] byteContent = content.getBytes(StandardCharsets.UTF_8); // 初始化为加密模式的密码器 cipher.init(Cipher.ENCRYPT_MODE, key); // 加密 return cipher.doFinal(byteContent); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 解密AES加密过的字符串 * * @param content AES加密过过的内容 * @param password 加密时的密码 * @param length 初始化秘钥随机串的长度 * @return 明文 */ public static byte[] decrypt(byte[] content, String password, Integer length) &#123; try &#123; // 创建AES的Key生产者 KeyGenerator keyGenerator = KeyGenerator.getInstance(\"AES\"); keyGenerator.init(length, new SecureRandom(password.getBytes())); // 根据用户密码，生成一个密钥 SecretKey secretKey = keyGenerator.generateKey(); // 返回基本编码格式的密钥 byte[] enCodeFormat = secretKey.getEncoded(); // 转换为AES专用密钥 SecretKeySpec key = new SecretKeySpec(enCodeFormat, \"AES\"); // 创建密码器 Cipher cipher = Cipher.getInstance(\"AES\"); // 初始化为解密模式的密码器 cipher.init(Cipher.DECRYPT_MODE, key); // 明文 return cipher.doFinal(content); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 接下来我们写一个AESHandleService.java，其实就是对工具类做了一个封装： 12345678910111213141516171819package com.github.zouchanglin.examplespringbootstarter.service;import com.github.zouchanglin.examplespringbootstarter.util.AESHandleUtil;public class AESHandleService &#123; private final Integer length; public AESHandleService(Integer length) &#123; this.length = length; &#125; public byte[] encrypt(String content, String password) &#123; return AESHandleUtil.encrypt(content, password, length); &#125; public byte[] decrypt(byte[] content, String password) &#123; return AESHandleUtil.decrypt(content, password, length); &#125;&#125; 由于我们需要用户去自定义一些参数，那么我们先用一个类把自定义参数给装起来，AESHandleServiceProperties.java：如果对这些注解有疑问的话可以参考我的一篇文章《 SpringBoot自定义配置文件 》，里面对SpringBoot自定义配置的操作解释的比较详细。 12345678910111213141516package com.github.zouchanglin.examplespringbootstarter.config;import org.springframework.boot.context.properties.ConfigurationProperties;@ConfigurationProperties(prefix = \"aes\")public class AESHandleServiceProperties &#123; private Integer length = 128; public Integer getLength() &#123; return length; &#125; public void setLength(Integer length) &#123; this.length = length; &#125;&#125; 接下来就是最重要的一步：编写自动装配类AESHandleAutoConfiguration.java： 1234567891011121314151617181920package com.github.zouchanglin.examplespringbootstarter.config;import com.github.zouchanglin.examplespringbootstarter.service.AESHandleService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.context.properties.EnableConfigurationProperties;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configuration@EnableConfigurationProperties(AESHandleServiceProperties.class)public class AESHandleAutoConfiguration &#123; @Autowired private AESHandleServiceProperties properties; @Bean AESHandleService aesHandleService() &#123; return new AESHandleService(properties.getLength()); &#125;&#125; 当然关于Bean的实例化条件控制等，也可以加上@ConditionalOnBean与@ConditionalOnClass这些注解，在这里就不详细介绍这些注解了。官方的参数文档在这里：《49.3.2 Bean Conditions》 。接下来需要编写spring.factories，这一步也很重要，Spring Boot自动注入的原理来源于 Spring Boot应用在启动过程中会通过SpringFactoriesLoader 加载所有 META-INF/spring.factories 文件，通过一系列的处理流程最终将spring.factories 文件中的定义的各种 beans 装载入ApplicationContext容器。所以编写spring.factories一定别忘记： 12org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ com.github.zouchanglin.examplespringbootstarter.config.AESHandleAutoConfiguration 最后，由于我们把测试的依赖删除了，所以测试代码也可以选择不要，那么整个工程目录如下图所示： 最后一步，只需要mvn install，就可以把这个starter发布到本地仓库。 测试SpringBootStarter新建SpringBoot项目并且引入这个Starter： 12345&lt;dependency&gt; &lt;groupId&gt;com.github.zouchanglin&lt;/groupId&gt; &lt;artifactId&gt;example-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 并且我们可以在配置文件里面指定这个starter可以配置的参数，即随机串的长度（其实不配置也有默认值） 接下来通过测试代码去测试一下： 12345678910111213141516171819202122232425262728293031323334package com.example.demo;import com.github.zouchanglin.examplespringbootstarter.service.AESHandleService;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;import static org.junit.Assert.*;@SpringBootTest@RunWith(SpringRunner.class)public class AESHandleServiceTest &#123; @Autowired private AESHandleService aesHandleService; @Test public void encryptAndDecrypt() &#123; String src = \"Hello, SpringBootStarter\"; String password = \"123321\"; System.out.println(\"源字符串:\" + src); byte[] encryptResult = aesHandleService.encrypt(src, password); String encryptString = new String(encryptResult); System.out.println(\"加密后:\" + encryptString); byte[] decryptResult = aesHandleService.decrypt(encryptResult, password); String decryptString = new String(decryptResult); System.out.println(\"解密后:\" + decryptString); assertEquals(src, decryptString); &#125;&#125; 推送至远端仓库这个步骤比较容器，我们选择JitPack，JitPack在得到我们的GitHub账户授权之后可以拉取我们的仓库里的代码，我们只需要把项目放在GitHub上，并且打一个Tag（其实就是发布一个Release版本），我以我之前测试的MD5的一个starter来说，md5-spring-boot-starter ，只需要有发布版本即可： 然后在jitpack授权，授权之后即可看到自己的仓库和Release版本： 最后，在需要的项目中引入即可： 123456789101112&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jitpack.io&lt;/id&gt; &lt;url&gt;https://jitpack.io&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.zouchanglin&lt;/groupId&gt; &lt;artifactId&gt;md5-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;v1.1&lt;/version&gt;&lt;/dependency&gt;","updated":"2020-05-05T10:30:06.821Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaEE","slug":"JavaEE","permalink":"https://zouchanglin.cn/tags/JavaEE/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://zouchanglin.cn/tags/SpringBoot/"}]},{"title":"Spring如何解决循环依赖","date":"2020-04-28T00:25:00.000Z","path":"2020/04/28/Spring如何解决循环依赖/","text":"Spring如何解决循环依赖是经常会被面试官问到的一个问题，这个问题算是关于Spring的一个高频面试题，因为如果不刻意研读，相信即使读过源码，面试者也不一定能够一下子思考出其中的原理。本文主要针对这个问题，从源码的角度对其实现原理进行讲解，彻底搞懂Spring究竟是如何解决循环依赖的。 循环依赖问题循环依赖就是循环引用，指两个或多个bean互相持有对方，比如说TestA引用TestB、TestB引用TestA，最终形成一个闭环。 循环依赖问题分为构造器循环依赖（无法解决）、setter循环依赖（可以解决）。 下面的代码就是一个setter循环依赖 (引用) 的示例： 1234567891011121314151617@Componentpublic class A &#123; private B b; public void setB(B b) &#123; this.b = b; &#125;&#125;@Componentpublic class B &#123; private A a; public void setA(A a) &#123; this.a = a; &#125;&#125; 再看一下构造器循环依赖的示例： 123456789101112131415public class C &#123; private D d; public C(D d) &#123; this.d = d; &#125;&#125;public class D &#123; private C c; public D(C c) &#123; this.c = c; &#125;&#125; application.xml 1234567891011&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean id=\"c\" class=\"tim.edu.spring_study.depend.C\"&gt; &lt;constructor-arg index=\"0\" ref=\"d\"&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean id=\"d\" class=\"tim.edu.spring_study.depend.D\"&gt; &lt;constructor-arg index=\"0\" ref=\"c\"&gt;&lt;/constructor-arg&gt; &lt;/bean&gt;&lt;/beans&gt; Main.java 12345public class CycleDependTest &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(\"application.xml\"); &#125;&#125; 由此可见， Spring解决不了构造器的循环依赖问题。 解决过程分析关于Spring bean的创建，其本质上还是一个对象的创建，一定要明白一点就是，一个完整的对象包含两部分：当前对象实例化和对象属性的实例化。在Spring中，对象的实例化是通过反射实现的，而对象的属性则是在对象实例化之后通过一定的方式设置的。这个过程可以按照如下方式进行理解： 我们从上面的setter循环依赖的例子中可以看到，A和B中各自都以对方为自己的全局属性。这里首先需要说明的一点是，Spring实例化bean是通过ApplicationContext.getBean()方法来进行的。如果要获取的对象依赖了另一个对象，那么其首先会创建当前对象，然后通过递归的调用ApplicationContext.getBean()方法来获取所依赖的对象，最后将获取到的对象注入到当前对象中。 我们以上面的首先初始化A对象实例为例进行讲解。首先Spring尝试通过ApplicationContext.getBean()方法获取A对象的实例，由于Spring容器中还没有A对象实例，因而其会创建一个A对象，然后发现其依赖了B对象，因而会尝试递归的通过ApplicationContext.getBean()方法获取B对象的实例，但是Spring容器中此时也没有B对象的实例，因而其还是会先创建一个B对象的实例。读者需要注意这个时间点，此时A对象和B对象都已经创建了，并且保存在Spring容器中了，只不过A对象的属性b和B对象的属性a都还没有设置进去。 在前面Spring创建B对象之后，Spring发现B对象依赖了属性A，因而此时还是会尝试递归的调用ApplicationContext.getBean()方法获取A对象的实例，因为Spring中已经有一个A对象的实例，虽然只是半成品（其属性b还未初始化），但其也还是目标bean，因而会将该A对象的实例返回。此时，B对象的属性a就设置进去了，然后还是ApplicationContext.getBean()方法递归的返回，也就是将B对象的实例返回，此时就会将该实例设置到A对象的属性b中。这个时候，注意A对象的属性b和B对象的属性a都已经设置了目标对象的实例了。 图中getBean()表示调用Spring的ApplicationContext.getBean()方法，而该方法中的参数，则表示我们要尝试获取的目标对象。图中的黑色箭头表示一开始的方法调用走向，走到最后，返回了Spring中缓存的A对象之后，表示递归调用返回了，此时使用绿色的箭头表示。从图中我们可以很清楚的看到，B对象的a属性是在第三步中注入的半成品A对象，而A对象的b属性是在第二步中注入的成品B对象，此时半成品的A对象也就变成了成品的A对象，因为其属性已经设置完成了。 解决原理分析对于Spring处理循环依赖问题的方式，我们这里通过上面的流程图其实很容易就可以理解，需要注意的一个点就是，Spring是如何标记开始生成的A对象是一个半成品，并且是如何保存A对象的。 这里的标记工作Spring是使用ApplicationContext的属性SetsingletonsCurrentlyInCreation来保存的，而半成品的A对象则是通过Map&lt;string, objectfactory&gt; singletonFactories来保存的，这里的ObjectFactory是一个工厂对象，可通过调用其getObject()方法来获取目标对象。在AbstractBeanFactory.doGetBean()方法中获取对象的方法如下： 123456789101112131415161718192021protected &lt;T&gt; T doGetBean(final String name, @Nullable final Class&lt;T&gt; requiredType, @Nullable final Object[] args, boolean typeCheckOnly) throws BeansException &#123; // 尝试通过bean名称获取目标bean对象，比如这里的A对象 Object sharedInstance = getSingleton(beanName); // 我们这里的目标对象都是单例的 if (mbd.isSingleton()) &#123; // 这里就尝试创建目标对象，第二个参数传的就是一个ObjectFactory类型的对象，这里是使用Java8的lamada // 表达式书写的，只要上面的getSingleton()方法返回值为空，则会调用这里的getSingleton()方法来创建 // 目标对象 sharedInstance = getSingleton(beanName, () -&gt; &#123; try &#123; // 尝试创建目标对象 return createBean(beanName, mbd, args); &#125; catch (BeansException ex) &#123; throw ex; &#125; &#125;); &#125; return (T) bean;&#125; 这里的doGetBean()方法是非常关键的一个方法（中间省略了其他代码），上面也主要有两个步骤，第一个步骤的getSingleton()方法的作用是尝试从缓存中获取目标对象，如果没有获取到，则尝试获取半成品的目标对象；如果第一个步骤没有获取到目标对象的实例，那么就进入第二个步骤，第二个步骤的getSingleton()方法的作用是尝试创建目标对象，并且为该对象注入其所依赖的属性。 这里其实就是主干逻辑，我们前面图中已经标明，在整个过程中会调用三次doGetBean()方法，第一次调用的时候会尝试获取A对象实例，此时走的是第一个getSingleton()方法，由于没有已经创建的A对象的成品或半成品，因而这里得到的是null，然后就会调用第二个getSingleton()方法，创建A对象的实例，然后递归的调用doGetBean()方法，尝试获取B对象的实例以注入到A对象中，此时由于Spring容器中也没有B对象的成品或半成品，因而还是会走到第二个getSingleton()方法，在该方法中创建B对象的实例，创建完成之后，尝试获取其所依赖的A的实例作为其属性，因而还是会递归的调用doGetBean()方法，此时需要注意的是，在前面由于已经有了一个半成品的A对象的实例，因而这个时候，再尝试获取A对象的实例的时候，会走第一个getSingleton()方法，在该方法中会得到一个半成品的A对象的实例。 然后将该实例返回，并且将其注入到B对象的属性a中，此时B对象实例化完成。然后将实例化完成的B对象递归的返回，此时就会将该实例注入到A对象中，这样就得到了一个成品的A对象。我们这里可以阅读上面的第一个getSingleton()方法： 123456789101112131415161718192021222324@Nullableprotected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; // 尝试从缓存中获取成品的目标对象，如果存在，则直接返回 Object singletonObject = this.singletonObjects.get(beanName); // 如果缓存中不存在目标对象，则判断当前对象是否已经处于创建过程中，在前面的讲解中，第一次尝试获取A对象 // 的实例之后，就会将A对象标记为正在创建中，因而最后再尝试获取A对象的时候，这里的if判断就会为true if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; synchronized (this.singletonObjects) &#123; singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; // 这里的singletonFactories是一个Map，其key是bean的名称，而值是一个ObjectFactory类型的 // 对象，这里对于A和B而言，调用图其getObject()方法返回的就是A和B对象的实例，无论是否是半成品 ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; // 获取目标对象的实例 singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return singletonObject;&#125; 这里我们会存在一个问题就是A的半成品实例是如何实例化的，然后是如何将其封装为一个ObjectFactory类型的对象，并且将其放到上面的singletonFactories属性中的。这主要是在前面的第二个getSingleton()方法中，其最终会通过其传入的第二个参数，从而调用createBean()方法，该方法的最终调用是委托给了另一个doCreateBean()方法进行的，这里面有如下一段代码： 123456789101112131415161718192021222324252627282930313233protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException &#123; // 实例化当前尝试获取的bean对象，比如A对象和B对象都是在这里实例化的 BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) &#123; instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); &#125; if (instanceWrapper == null) &#123; instanceWrapper = createBeanInstance(beanName, mbd, args); &#125; // 判断Spring是否配置了支持提前暴露目标bean，也就是是否支持提前暴露半成品的bean boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) &#123; // 如果支持，这里就会将当前生成的半成品的bean放到singletonFactories中，这个singletonFactories // 就是前面第一个getSingleton()方法中所使用到的singletonFactories属性，也就是说，这里就是 // 封装半成品的bean的地方。而这里的getEarlyBeanReference()本质上是直接将放入的第三个参数，也就是 // 目标bean直接返回 addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); &#125; try &#123; // 在初始化实例之后，这里就是判断当前bean是否依赖了其他的bean，如果依赖了， // 就会递归的调用getBean()方法尝试获取目标bean populateBean(beanName, mbd, instanceWrapper); &#125; catch (Throwable ex) &#123; // 省略... &#125; return exposedObject;&#125; 到这里，Spring整个解决循环依赖问题的实现思路已经比较清楚了。对于整体过程，读者朋友只要理解两点： Spring是通过递归的方式获取目标bean及其所依赖的bean的 Spring实例化一个bean的时候，是分两步进行的，首先实例化目标bean，然后为其注入属性 结合这两点，也就是说，Spring在实例化一个bean的时候，是首先递归的实例化其所依赖的所有bean，直到某个bean没有依赖其他bean，此时就会将该实例返回，然后反递归的将获取到的bean设置为各个上层bean的属性的。不过使用注解的方式注入，对象都是懒加载创建，就不存在相互依赖的问题。 转载自：《Spring如何解决循环依赖的问题》","updated":"2020-04-28T04:37:59.087Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaEE","slug":"JavaEE","permalink":"https://zouchanglin.cn/tags/JavaEE/"},{"name":"SpringCore","slug":"SpringCore","permalink":"https://zouchanglin.cn/tags/SpringCore/"}]},{"title":"SpringBoot启动流程探究","date":"2020-04-25T10:00:00.000Z","path":"2020/04/25/SpringBoot启动流程探究/","text":"Spring的丰富生态备受开发者青睐，尤其是自从SpringBoot出现之后去掉了原来的复杂配置，因为SpringBoot的理念就是约定大于配置，这让我们省去了很多需要手动配置的过程，就拿SpringMVC来说吧各种XML配置直接劝退初学者，但是SpringBoot的易用性简直是成为了推广Spring生态的利器。本篇文章主要是结合SpringBoot的源码，来探究SpringBoot应用程序的启动流程！ 新建一个SpringBoot项目，首先映入眼帘的恐怕就是下面的这个关键的Main函数与@SpringBootApplication注解吧，我们将从这个注解开始，逐步探究SpringBoot应用的启动流程： 12345678@SpringBootApplicationpublic class SpringBootStartApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootStartApplication.class, args); &#125;&#125; @SpringBootApplication@SpringBootApplication注解实际上是SpringBoot提供的一个复合注解，我们来看一看其源码： 1234567891011@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = &#123; @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;)public @interface SpringBootApplication &#123; ...&#125; 关于这里面某些元注解的功能，可以参考我之前的写的一篇博客《 注解的原理与实现 》 。在这里我们只需要看@SpringBootConfiguration、@EnableAutoConfiguration、@ComponentScan这三个注解。在 SpringBoot 应用的启动类上用这个三个注解代替@SpringBootApplication注解其实也是没问题的： 12345678910@SpringBootConfiguration@EnableAutoConfiguration@ComponentScanpublic class SpringBootStartApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootStartApplication.class, args); &#125;&#125; 那我们接下来就需要分贝探究这三个注解的功能。 @SpringBootConfiguration12345678910@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Configurationpublic @interface SpringBootConfiguration &#123; @AliasFor( annotation = Configuration.class ) boolean proxyBeanMethods() default true;&#125; @SpringBootConfiguration也是来源于@Configuration，二者功能都是将当前类标注为配置类，@Configuration用于定义配置类，可替换xml配置文件，被注解的类内部包含有一个或多个被@Bean注解的方法，这些方法将会被AnnotationConfigApplicationContext或AnnotationConfigWebApplicationContext类进行扫描，并用于构建Bean定义，初始化Spring容器，这个貌似一点都不新奇。 @EnableAutoConfiguration123456789@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage@Import(AutoConfigurationImportSelector.class)public @interface EnableAutoConfiguration &#123; ...&#125; @EnableAutoConfiguration 注解启用自动配置，其中最关键的要属@Import(AutoConfigurationImportSelector.class)，借AutoConfigurationImportSelector，@EnableAutoConfiguration可以帮助SpringBoot应用将所有符合条件的@Configuration配置都加载到当前SpringBoot创建并使用的IoC容器。 借助于Spring框架原有的一个工具类：SpringFactoriesLoader的支持，@EnableAutoConfiguration可以智能的自动配置功效才得以大功告成！ 关于这个注解可以参考我的另一篇文章《SpringBoot自动配置原理》 ，里面有详细介绍并且有例子。 @ComponentScan@ComponentScan 对应于XML配置形式中的 context:component-scan，用于将一些标注了特定注解的bean定义批量采集注册到Spring的IoC容器之中，这些特定的注解大致包括： @Controller @Entity @Component @Service @Repository 对于该注解可以通过 basePackages 属性来更细粒度的控制该注解的自动扫描范围，比如： 1@ComponentScan(basePackages = &#123;\"xpu.tim.controller\",\"xpu.tim.entity\"&#125;) SpringApplication对象构造流程@SpringBootApplication这个注解看完了， 那么接下来就来看看这个SpringApplication以及run()方法究竟干了些啥。原始的SpringCore中并没有这个类，SpringApplication里面封装了一套Spring 应用的启动流程，然而这对用户完全透明，因此我们上手 SpringBoot 时感觉简洁、轻量。 通过阅读run方法的源码我们不难发现，其实是需要构造一个SpringApplication对象： 12345678910/** * Static helper that can be used to run a &#123;@link SpringApplication&#125; from the * specified sources using default settings and user supplied arguments. * @param primarySources the primary sources to load * @param args the application arguments (usually passed from a Java main method) * @return the running &#123;@link ApplicationContext&#125; */public static ConfigurableApplicationContext run(Class&lt;?&gt;[] primarySources, String[] args) &#123; return new SpringApplication(primarySources).run(args);&#125; 默认的 SpringApplication执行流程已经可以满足大部分需求，但是若用户想干预这个过程，则可以通过SpringApplication在流程某些地方开启的扩展点来完成对流程的扩展，典型的扩展方案那就是使用 set 方法。 12345678910@SpringBootApplicationpublic class SpringBootStartApplication &#123; public static void main(String[] args) &#123; //SpringApplication.run(SpringBootStartApplication.class, args); SpringApplication application = new SpringApplication(SpringBootStartApplication.class); application.set...(); // 用户自定义扩展点 application.set...(); // 用户自定义扩展点 application.run(args); &#125;&#125; 这样一拆解后我们发现，我们也需要先构造 SpringApplication 类对象，然后调用该对象的 run() 方法。那么接下来就讲讲 SpringApplication 的构造过程以及其 run() 方法的流程，搞清楚了这个，那么也就搞清楚了SpringBoot应用是如何运行起来的！ 主要需要看以下四个方法： 1、deduceFromClasspath：用来推断应用的类型：创建的是 REACTIVE应用、SERVLET应用、NONE 三种中的一种 NONE表示当前的应用即不是一个web应用也不是一个REACTIVE应用，是一个纯后台的应用。SERVLET表示当前应用是一个标准的web应用。REACTIVE是spring5当中的新特性，表示是一个响应式的web应用。而判断的依据就是根据Classloader中加载的类。如果是servlet，则表示是web，如果是DispatcherHandler，则表示是一个REACTIVE应用，如果两者都不存在，则表示是一个非web环境的应用。 2、setInitializers：使用SpringFactoriesLoader查找并加载classpath下 META-INF/spring.factories文件中所有可用的 ApplicationContextInitializer 使用 SpringFactoriesLoader查找并加载classpath下META-INF/spring.factories文件中的所有可用的 ApplicationListener 3、setListeners：使用 SpringFactoriesLoader查找并加载classpath下 META-INF/spring.factories文件中的所有可用的 ApplicationListener 4、deduceMainApplicationClass：推断并设置main方法的定义类 通过这个几个关键步骤，SpringApplication完成了实例化。 run()方法探究之前我们弄清楚了SpringApplication的实例化过程，现在看看它的run方法究竟干了什么： 1、通过 SpringFactoriesLoader 加载META-INF/spring.factories文件，获取并创建SpringApplicationRunListener对象； 2、然后由SpringApplicationRunListener来发出starting消息； 3、把参数args封装成DefaultApplicationArguments，并配置当前SpringBoot应用将要使用的Environment； 4、完成之后，依然由SpringApplicationRunListener来发出 environmentPrepared（环境已准备）消息； 5、创建上下文，根据项目类型创建上下文； 6、初始化ApplicationContext，并设置 Environment，加载相关配置等； 7、由SpringApplicationRunListener来发出contextPrepared消息，告知SpringBoot应用使用的ApplicationContext已准备OK； 8、将各种 beans 装载入ApplicationContext，继续由SpringApplicationRunListener来发出contextLoaded消息，告知 SpringBoot 应用使用的ApplicationContext已装填OK； 9、refresh ApplicationContext，完成IoC容器可用的最后一步； 10、由SpringApplicationRunListener来发出started消息 ； 11、完成最终的程序的启动； 12、SpringApplicationRunListener来发出running消息，告知程序已运行起来了； 步骤4和5之间还有个PrintBanner，用来打印Banner createApplicationContext()下面这段代码主要是根据项目类型创建上下文，并且会注入几个核心组件类： 12345678910111213141516171819202122protected ConfigurableApplicationContext createApplicationContext() &#123; Class&lt;?&gt; contextClass = this.applicationContextClass; if (contextClass == null) &#123; try &#123; switch (this.webApplicationType) &#123; case SERVLET: contextClass = Class.forName(DEFAULT_SERVLET_WEB_CONTEXT_CLASS); break; case REACTIVE: contextClass = Class.forName(DEFAULT_REACTIVE_WEB_CONTEXT_CLASS); break; default: contextClass = Class.forName(DEFAULT_CONTEXT_CLASS); &#125; &#125; catch (ClassNotFoundException ex) &#123; throw new IllegalStateException( \"Unable create a default ApplicationContext, please specify an ApplicationContextClass\", ex); &#125; &#125; return (ConfigurableApplicationContext) BeanUtils.instantiateClass(contextClass);&#125; Web类型项目创建上下文对象AnnotationConfigServletWebServerApplicationContext 。这里会把ConfigurationClassPostProcessor 、AutowiredAnnotationBeanPostProcessor 等一些核心组件加入到Spring容器。 refreshContext()下面一起来看下refreshContext(context) 这个方法，这个方法启动spring的代码加载了bean，还启动了内置web容器： 1234567891011private void refreshContext(ConfigurableApplicationContext context) &#123; refresh(context); if (this.registerShutdownHook) &#123; try &#123; context.registerShutdownHook(); &#125; catch (AccessControlException ex) &#123; // Not allowed in some environments. &#125; &#125;&#125; 点击跟进后发现方法里面是spring容器启动代码： 我们可以看到一个onRefresh方法，点进去需要看的是子类实现，我们只看其中一个子类实现： 1234567891011121314151617181920212223242526272829@Overrideprotected void onRefresh() &#123; super.onRefresh(); try &#123; createWebServer(); &#125; catch (Throwable ex) &#123; throw new ApplicationContextException(\"Unable to start web server\", ex); &#125;&#125;private void createWebServer() &#123; WebServer webServer = this.webServer; ServletContext servletContext = getServletContext(); if (webServer == null &amp;&amp; servletContext == null) &#123; // 这个获取webServerFactory还是要进去看看 ServletWebServerFactory factory = getWebServerFactory(); this.webServer = factory.getWebServer(getSelfInitializer()); &#125; else if (servletContext != null) &#123; try &#123; getSelfInitializer().onStartup(servletContext); &#125; catch (ServletException ex) &#123; throw new ApplicationContextException(\"Cannot initialize servlet context\", ex); &#125; &#125; initPropertySources();&#125; 我们继续看下getWebServletFactory() 这个方法，这个里面其实就是选择出哪种类型的web容器了: 12345678910111213protected ServletWebServerFactory getWebServerFactory() &#123; // Use bean names so that we don't consider the hierarchy String[] beanNames = getBeanFactory().getBeanNamesForType(ServletWebServerFactory.class); if (beanNames.length == 0) &#123; throw new ApplicationContextException(\"Unable to start ServletWebServerApplicationContext due to missing \" + \"ServletWebServerFactory bean.\"); &#125; if (beanNames.length &gt; 1) &#123; throw new ApplicationContextException(\"Unable to start ServletWebServerApplicationContext due to multiple \" + \"ServletWebServerFactory beans : \" + StringUtils.arrayToCommaDelimitedString(beanNames)); &#125; return getBeanFactory().getBean(beanNames[0], ServletWebServerFactory.class);&#125; 我们再去看factory.getWebServer(getSelfInitializer()) ，转到定义就会看到很熟悉的名字tomcat： 内置的Servlet容器就是在onRefresh()方法里面启动的，至此一个Servlet容器就启动OK了。 SpringBoot启动过程简述1、new了一个SpringApplication对象，使用SPI技术加载加载 ApplicationContextInitializer、ApplicationListener 接口实例； 2、调用SpringApplication.run()方法； 3、调用createApplicationContext()方法创建上下文对象，创建上下文对象同时会注册spring的核心组件类（ConfigurationClassPostProcessor 、AutowiredAnnotationBeanPostProcessor 等）； 4、调用refreshContext() 方法启动Spring容器和内置的Servlet容器；","updated":"2020-04-25T11:04:18.362Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaEE","slug":"JavaEE","permalink":"https://zouchanglin.cn/tags/JavaEE/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://zouchanglin.cn/tags/SpringBoot/"},{"name":"SpringCore","slug":"SpringCore","permalink":"https://zouchanglin.cn/tags/SpringCore/"}]},{"title":"索引堆的实现与优化","date":"2020-04-24T10:00:00.000Z","path":"2020/04/24/索引堆的实现与优化/","text":"在之前文章中记述了堆的实现（插入方式建堆、heapify方式建堆以及堆排序）《 堆的实现及其应用 》。今天来看看索引堆是个什么东西，对于我们所关心的这个数组而言，数组中的元素位置发生了改变。正是因为这些元素的位置发生了改变，我们才能将其构建为最大堆。 如果元素十分复杂的话，比如像每个位置上存的是一篇上万字的文章。那么交换它们之间的位置将产生大量的时间消耗。并且由于数组元素的位置在构建成堆之后发生了改变，那么我们就很难索引到它，很难去改变它。可以在每一个元素上再加上一个属性来表示原来的位置可以解决，但是这样的话，必须将这个数组遍历一下才能解决。针对以上问题，我们就需要引入索引堆（Index Heap）的概念。 索引堆基本实现对于索引堆来说，我们将数据和索引这两部分分开存储。真正表征堆的这个数组是由索引这个数组构建成的。 而在构建堆（以最大索引堆为例）的时候，比较的是data中的值（即原来数组中对应索引所存的值），构建成堆的却是index域。而构建完之后，data域并没有发生改变，位置改变的是index域。 那么现在这个最大堆该怎么解读呢？例如，堆顶元素为Index=10代表的就是索引为10的data域的值，即62。这时我们来看，构建堆的过程就是简单地索引之间的交换，索引就是简单的int型。效率很高。 现在如果我们想对这个数组进行一些改变，比如我们想将索引为7的元素值改为100，那我们需要做的就是将索引7所对应data域的28改为100。时间复杂度为O(1)。当然改完之后，我们还需要进行一些操作来维持最大堆的性质。不过调整的过程改变的依旧是index域的内容。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133package imooc.heap;public class MaxIndexHeap &#123; protected int[] data; protected int[] indexes; //堆里有多少元素 protected int count; //堆的容量 protected int capacity; //因为0号位置不使用，所以capacity + 1 public MaxIndexHeap(int capacity) &#123; data = new int[capacity + 1]; indexes = new int[capacity + 1]; count = 0; this.capacity = capacity; &#125; public MaxIndexHeap(int[] arr)&#123; data = new int[arr.length + 1]; capacity = arr.length + 1; for (int i = 0; i &lt; arr.length; i++) &#123; data[i + 1] = arr[i]; &#125; count = arr.length; //从第一个不是叶子节点的位置开始 for (int i = count / 2; i &gt;= 1; i--) &#123; shiftDown(i); &#125; &#125; //获取现存元素个数 public int size()&#123; return count; &#125; //判断是否为空 public boolean isEmpty()&#123; return count == 0; &#125; //插入数据(传入的i对于用户而言，是从0开始索引的) public void insert(int i, int item)&#123; assert i + 1 &gt;= 1; i++; //i += 1 //判断容量知否超出 if(count + 1 &gt;= capacity)&#123; //开始扩容 resize(); &#125; //先存储到末尾 data[i] = item; indexes[count + 1] = i; count++; //开始向上调堆 shiftUp(count); &#125; //取出数据的索引 public int extractIndexMax()&#123; if(count == 0) throw new RuntimeException(\"Heap is null\"); int ret = indexes[1] - 1; swapIndexes(1, count); count--; //开始向下调堆 shiftDown(1); return ret; &#125; //根据索引获得元素 public int getItemByIndex(int index)&#123; return data[index]; &#125; //根据索引修改某个元素 public void changeItem(int index, int newValue)&#123; index++; data[index] = newValue; //找到indexes[j] = index; j表示data[index]在堆中的位置 //找到shiftUp(j)，再shiftDown(j) for (int j = 1; j &lt;= count; j++) &#123; if(indexes[j] == index)&#123; shiftUp(j); shiftDown(j); return; &#125; &#125; &#125; //向下调堆 private void shiftDown(int k) &#123; while (2 * k &lt;= count)&#123; int j = 2 * k; if(j + 1 &lt;= count &amp;&amp; data[indexes[j+1]] &gt; data[indexes[j]])&#123; j++; &#125; if(data[indexes[k]] &gt;= data[indexes[j]])&#123; break; &#125; swapIndexes(k, j); k = j; &#125; &#125; //向上调堆 private void shiftUp(int k) &#123; while(k &gt; 1 &amp;&amp; data[indexes[k / 2]] &lt; data[indexes[k]])&#123; swapIndexes(k/2, k); k /= 2; &#125; &#125; //交换对应两个位置的值(这是其实是交换索引的位置) private void swapIndexes(int i, int j)&#123; int tmp = indexes[i]; indexes[i] = indexes[j]; indexes[j] = tmp; &#125; //扩充容量 private void resize() &#123; int[] newData = new int[capacity * 2]; System.arraycopy(data, 0, newData, 0, count); data = newData; capacity *= 2; int[] newIndexes = new int[capacity * 2]; System.arraycopy(indexes, 0, newIndexes, 0, count); indexes = newIndexes; &#125;&#125; 索引堆的优化：反向查找如何优化呢？ 反向查找：再建立一个数组，这个数组的下标和原始数据数组的下标的意思是一样的，就是索引的意思。而数组中存储的元素则是索引在索引堆数组中的位置。 对反向查找表的维护就是，将索引堆中的值取出来（值就是索引值），这个值就是方向查找表的下标，那这个下标应该对应的元素就是索引堆中的位置。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180public class MaxIndexHeapOptimize &#123; protected int[] data; protected int[] indexes; protected int[] reverse; //堆里有多少元素 protected int count; //堆的容量 protected int capacity; //因为0号位置不使用，所以capacity + 1 public MaxIndexHeapOptimize(int capacity) &#123; data = new int[capacity + 1]; indexes = new int[capacity + 1]; reverse = new int[capacity + 1]; count = 0; for (int i = 0; i &lt;= capacity ; i++) &#123; reverse[i] = 0; &#125; this.capacity = capacity; &#125; public MaxIndexHeapOptimize(int[] arr)&#123; data = new int[arr.length + 1]; capacity = arr.length + 1; for (int i = 0; i &lt; arr.length; i++) &#123; data[i + 1] = arr[i]; &#125; count = arr.length; //从第一个不是叶子节点的位置开始 for (int i = count / 2; i &gt;= 1; i--) &#123; shiftDown(i); &#125; &#125; //获取现存元素个数 public int size()&#123; return count; &#125; //判断是否为空 public boolean isEmpty()&#123; return count == 0; &#125; //插入数据(传入的i对于用户而言，是从0开始索引的) public void insert(int i, int item)&#123; assert i + 1 &gt;= 1; i++; //i += 1 //判断容量知否超出 if(count + 1 &gt;= capacity)&#123; //开始扩容 resize(); &#125; //先存储到末尾 data[i] = item; indexes[count + 1] = i; reverse[i] = count + 1; count++; //开始向上调堆 shiftUp(count); &#125; //取出数据的索引 public int extractIndexMax()&#123; if(count == 0) throw new RuntimeException(\"Heap is null\"); int ret = indexes[1] - 1; swapIndexes(1, count); reverse[indexes[1]] = 1; reverse[indexes[count]] = 0; count--; //开始向下调堆 shiftDown(1); return ret; &#125; //取出数据的索引 public int extractMax()&#123; if(count == 0) throw new RuntimeException(\"Heap is null\"); int ret = data[indexes[1]]; swapIndexes(1, count); reverse[indexes[1]] = 1; reverse[indexes[count]] = 0; count--; //开始向下调堆 shiftDown(1); return ret; &#125; //根据索引获得元素 public int getItemByIndex(int index)&#123; if(contain(index))&#123; throw new RuntimeException(\"This index is not in heap!\"); &#125; return data[index]; &#125; //根据索引修改某个元素 public void changeItemOld(int index, int newValue)&#123; index++; data[index] = newValue; //找到indexes[j] = index; j表示data[index]在堆中的位置 //找到shiftUp(j)，再shiftDown(j) for (int j = 1; j &lt;= count; j++) &#123; if(indexes[j] == index)&#123; shiftUp(j); shiftDown(j); return; &#125; &#125; &#125; //根据索引修改某个元素 public void changeItem(int index, int newValue)&#123; if(contain(index))&#123; throw new RuntimeException(\"This index is not in heap!\"); &#125; index++; data[index] = newValue; //找到indexes[j] = index; j表示data[index]在堆中的位置 int j = reverse[index]; //O(1)的时间复杂度 shiftUp(j); shiftDown(j); &#125; private boolean contain(int index) &#123; if(!(index + 1 &gt;= 0 &amp;&amp; index + 1 &lt;= capacity))&#123; throw new RuntimeException(\"This index is illegal\"); &#125; return reverse[index + 1] == 0; &#125; //向下调堆 private void shiftDown(int k) &#123; while (2 * k &lt;= count)&#123; int j = 2 * k; if(j + 1 &lt;= count &amp;&amp; data[indexes[j+1]] &gt; data[indexes[j]])&#123; j++; &#125; if(data[indexes[k]] &gt;= data[indexes[j]])&#123; break; &#125; swapIndexes(k, j); reverse[indexes[k]] = k; reverse[indexes[j]] = j; k = j; &#125; &#125; //向上调堆 private void shiftUp(int k) &#123; while(k &gt; 1 &amp;&amp; data[indexes[k / 2]] &lt; data[indexes[k]])&#123; swapIndexes(k/2, k); reverse[indexes[k/2]] = k/2; reverse[indexes[k]] = k; k /= 2; &#125; &#125; //交换对应两个位置的值(这是其实是交换索引的位置) private void swapIndexes(int i, int j)&#123; int tmp = indexes[i]; indexes[i] = indexes[j]; indexes[j] = tmp; &#125; //扩充容量 private void resize() &#123; int[] newData = new int[capacity * 2]; System.arraycopy(data, 0, newData, 0, count); data = newData; capacity *= 2; int[] newIndexes = new int[capacity * 2]; System.arraycopy(indexes, 0, newIndexes, 0, count); indexes = newIndexes; &#125;&#125; 其他和堆相关的问题1、使用堆来实现优先队列 动态选择优先级最高的任务执行 2、实现多路归并排序 将整个数组分成n个子数组，子数组排完序之后，将每个子数组中最小的元素取出，放到一个最小堆里面，每次从最小堆里取出最小值放到归并结束的数组中，被取走的元素属于哪个子数组，就从哪个子数组中再取出一个补充到最小堆里面，如此循环，直到所有子数组归并到一个数组中。","updated":"2020-04-25T10:52:36.021Z","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"堆","slug":"堆","permalink":"https://zouchanglin.cn/tags/%E5%A0%86/"},{"name":"二叉树","slug":"二叉树","permalink":"https://zouchanglin.cn/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"}]},{"title":"什么是可中断锁","date":"2020-04-23T10:00:00.000Z","path":"2020/04/23/什么是可中断锁/","text":"Lock是可中断锁，而synchronized不是可中断锁。现假设线程A和B都要获取对象O的锁定，假设A获取了对象O锁，B将等待A释放对O的锁定，如果使用synchronized，如果A不释放，B将一直等下去，不能被中断；如果使用ReentrantLock，如果A不释放，可以使B在等待了足够长的时间以后，中断等待，而干别的事情。获取锁超时机制还是属于不可中断，属于超时被动放弃去竞争锁，而lockInterruptibly是可主动放弃竞争锁行为的一种方式。 Lock接口的线程获取锁的三种方式1、lock()，如果获取了锁立即返回，如果别的线程持有锁，当前线程则一直处于休眠状态，直到获取锁； 2、tryLock()，如果获取了锁立即返回true，如果别的线程正持有锁，立即返回false； 3、tryLock(long timeout，TimeUnit unit)，如果获取了锁定立即返回true，如果别的线程正持有锁，会等待参数给定的时间，在等待的过程中，如果获取了锁定，就返回true，如果等待超时，返回false； lockInterruptibly()方法先说说线程的打扰机制，每个线程都有一个打扰标志。这里分两种情况: 线程在sleep或wait、join，此时如果别的进程调用此进程的interrupt()方法，此线程会被唤醒并被要求处理InterruptedException；(Thread在做IO操作时也可能有类似行为) 此线程在运行中，则不会收到提醒。但是此线程的 打扰标志会被设置，可以通过isInterrupted()查看并作出处理。 lockInterruptibly()和上面的第一种情况是一样的， 线程在请求lock并被阻塞时，如果被interrupt，则此线程会被唤醒并被要求处理InterruptedException。lock()的代码演示： 12345678910111213141516171819import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class ReentrantLockDemo &#123; public static void main(String[] args) throws InterruptedException &#123; final Lock lock = new ReentrantLock(); lock.lock(); Thread.sleep(1000); Thread t1 = new Thread(() -&gt; &#123; lock.lock(); System.out.println(Thread.currentThread().getName() + \" interrupted.\"); &#125;); t1.start(); Thread.sleep(1000); //试图将t1中断执行，但并不能中断t1 t1.interrupt(); Thread.sleep(2000); &#125;&#125; lockInterruptibly()代码演示： 12345678910111213141516171819202122import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class ReentrantLockDemo &#123; public static void main(String[] args) throws InterruptedException &#123; final Lock lock = new ReentrantLock(); lock.lock(); Thread.sleep(1000); Thread t1 = new Thread(() -&gt; &#123; try &#123; lock.lockInterruptibly(); &#125; catch (InterruptedException e) &#123; System.out.println(Thread.currentThread().getName() + \" interrupted.\"); &#125; &#125;); t1.start(); Thread.sleep(1000); //试图将t1中断执行，是可以的，产生了一个InterruptedException异常 t1.interrupt(); Thread.sleep(1000); &#125;&#125; lockInterruptibly()源码说明：线程被唤醒后检测到中断请求，则立即抛出中断异常（由上层调用者来处理这个异常），该操作导致方法结束。下面是源码：lockInterruptibly() -&gt; sync.acquireInterruptibly(1) -&gt; doAcquireInterruptibly()： 1234567891011121314151617181920212223242526/** * Acquires in exclusive interruptible mode. * @param arg the acquire argument */private void doAcquireInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 下面是lock()的源码：lock() -&gt; sync.lock() -&gt; acquire(1) -&gt; acquireQueued()： 1234567891011121314151617181920212223242526272829/** * Acquires in exclusive uninterruptible mode for thread already in * queue. Used by condition wait methods as well as acquire. * * @param node the node * @param arg the acquire argument * @return &#123;@code true&#125; if interrupted while waiting */final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 可中断锁与非中断锁结论ReentrantLock的中断和非中断加锁模式的区别在于：线程尝试获取锁操作失败后，在等待过程中，如果该线程被其他线程中断了，它是如何响应中断请求的。lock方法会忽略中断请求，继续获取锁直到成功；而lockInterruptibly则直接抛出中断异常来立即响应中断，由上层调用者处理中断。","updated":"2020-04-23T11:48:46.816Z","categories":[{"name":"并发编程","slug":"并发编程","permalink":"https://zouchanglin.cn/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"}]},{"title":"JDK11新特性","date":"2020-04-17T10:00:00.000Z","path":"2020/04/17/JDK11新特性/","text":"Oracle官方于2018年9月26日宣布Java11正式发布。目前Oracle 官方也已经宣布Java11正式可以商用，这是Java 大版本周期变化后的第一个长期支持版本，非常值得关注。最新发布的Java11一共包含17个JEP（JDK Enhancement Proposals，JDK 增强提案）。对于企业来说，选择Java11将意味着长期的、可靠的、可预测的技术路线图。其中免费的OpenJDK11确定将得到OpenJDK社区的长期支持，LTS版本将是可以放心选择的版本。由于JDK9和JDK10都是一个过渡版本，JDK11的特性是在JDK9中就有的，我在博客里也做了总结《 JDK9新特性(一)》， 《JDK9新特性(二)》 ，其中已经介绍过的新特性将不会在这篇文章出现。 从JVM GC的角度，JDK11引入了两种新的GC，其中包括也许是划时代意义的ZGC，虽然其目前还是实验特性，但是从能力上来看，这是JDK的一个巨大突破，为特定生产环境的苛刻需求提供了一个可能的选择。例如，对部分企业核心存储等产品，如果能够保证不超过10ms的GC暂停，可靠性会上一个大的台阶，这是过去我们进行GC调优几乎做不到的，是能与不能的问题。对于G1 GC，相比于JDK8，升级到JDK11即可免费享受到：并行的Full GC，快速的CardTable扫描，自适应的堆占用比例调整（IHOP），在并发标记阶段的类型卸载等等。这些都是针对G1的不断增强，其中串行Full GC等甚至是曾经被广泛诟病的短板，你会发现GC配置和调优在JDK11中越来越方便。云计算时代的监控、诊断和Profiling能力，这个是相比ZGC更具生产实践意义的特性。Java的应用场景跨度很大，从单机长时间运行的Java应用，发展成为分布式、大的单体应用或小的Function、瞬时或长时间运行等，应用场景非常复杂。 JDK11为我们提供了更加强大的基础能力，主要是两部分 第一部分：JEP 328: Flight Recorder（JFR）是Oracle刚刚开源的强大特性。JFR是一套集成进入JDK、JVM内部的事件机制框架，通过良好架构和设计的框架，硬件层面的极致优化，生产环境的广泛验证，它可以做到极致的可靠和低开销。在SPECjbb2015等基准测试中，JFR的性能开销最大不超过1%，所以，工程师可以基本没有心理负担地在大规模分布式的生产系统使用，这意味着，我们既可以随时主动开启JFR进行特定诊断，也可以让系统长期运行JFR，用以在复杂环境中进行After-the-fact分析。在保证低开销的基础上，JFR提供的能力可以应用在对锁竞争、阻塞、延迟，JVM GC、SafePoint等领域，进行非常细粒度分析。甚至深入JIT Compiler内部，全面把握热点方法、内联、逆优化等等。JFR提供了标准的Java、C++等扩展API，可以与各种层面的应用进行定制、集成，为复杂的企业应用栈或者复杂的分布式应用，提供All-in-One解决方案。Flight Recorder相当于飞机的黑匣子，不会影响JVM的运行（最大性能开销不超过1%），而且是不断记录JVM的运行监控参数，而这一切都是内建在JDK和JVM内部的，并不需要额外的依赖，开箱即用。第二部分：JEP 331: Low-Overhead Heap Profiling。它来源于Google等业界前沿厂商的一线实践，通过获取对象分配细节，为JDK补足了对象分配诊断方面的一些短板，工程师可以通过JVMTI使用这个能力增强自身的工具。 从 Java 类库发展的角度来看，JDK 11 最大的进步也是两个方面 第一部分：HTTP/2 Client API，新的HTTP API提供了对HTTP/2等业界前沿标准的支持，精简而又友好的API接口，与主流开源API（如Apache HttpClient， Jetty， OkHttp 等）对等甚至更高的性能。与此同时它是JDK在Reactive-Stream方面的第一个生产实践，广泛使用了Java Flow API等，终于让Java标准HTTP类库在扩展能力等方面，满足了现代互联网的需求。第二部分：安全类库、标准等方面的大范围升级，其中特别是 JEP 332: Transport Layer Security (TLS) 1.3，除了在安全领域的重要价值，它还是中国安全专家范学雷所领导的JDK项目，完全不同于以往的修修补补，是个非常大规模的工程。除此之外，JDK 还在逐渐进行瘦身工作，或者偿还 JVM、Java 规范等历史欠账，例如：Deprecate the Nashorn JavaScript Engine，它进一步明确了 Graal 很有可能将成为 JVM 向前演进的核心选择，Java-on-Java 正在一步步的成为现实。 Dynamic Class-File ConstantsJava的类型文件格式将被拓展，支持一种新的常量池格式：CONSTANT_Dynamic，加载CONSTANT_Dynamic会将创建委托给bootstrap方法。其目标是降低开发新形式的可实现类文件约束带来的成本和干扰。上面的说法可能难以理解，我将换一个方式来说明这个新特性。在JDK1.5之前，Java程序中的常量值只能是字符串或原始类型。这些常量作为文字内置在语言中，甚至由javac编译器假定以减小类文件的大小。 123456class ConstantSample &#123; final String field = \"foo\"; void hello() &#123; System.out.print(field); &#125;&#125; 为了表示这样的常量值，任何Java类文件都包含了一个常量池，这意味着在方法中使用或用作字段值的常量，但也包含描述类的其他不可变信息，例如类的名称或被调用方法的名称及其声明的类型名称等信息。一旦在类的常量池中记录了一个值，就可以通过指向常量池中特定条目的偏移量来引用该值。这样做，在整个类中重复的值仅需要存储一次，因为偏移量可以多次引用。 JVM甚至可以通过遍历在常量池中找到的字符串来跨类对常量字符串进行重复数据删除。接下来我要说的是常量池存储的局限性，类文件的常量池中值的这种表示形式非常适合简单的值，例如字符串和数字等基本常量。但是同时，当javac没有发现常量时，它可能会带来非常直观的后果： 123456class NoConstantSample &#123; final String field = \"foo\".toString(); void hello() &#123; System.out.print(field); &#125;&#125; 尽管toString方法对于字符串来说是微不足道的，但是这种情况对于不评估Java方法的javac仍然未知。因此，编译器不能再发出常量池的值作为print语句的输入。相反，它不得不发出该字段的字段读取指令，该指令需要额外的字节，如前所述。这次，如果使用反射更改了字段的值，则调用hello方法也将打印更新的值。虽然这是一个人为的例子，但是不难想象，在实践中如何用经典方法限制Java中的常量呢？如Math.max(CONST_A, CONST_B) 在编译的时候最大值本身就是常量，但是由于编译器无法对方法进行预估，因为也不能被编译器发现，哦！！这里原来是个常量哦！ 局部变量类型推断 var如下图，但是需要注意的是var并不是一个关键字，var仅仅是一个语法上的改进，在编译时期便已经将var转换为了对应的变量类型。然而在使用var定义变量时，必须立刻赋值，编译器能根据右边的表达式自动推断类型，所以var只是用来减少代码量的。 12345678910111213public class VarTest &#123; public static void main(String[] args) &#123; Consumer&lt;String&gt; consumer = (@Deprecated var t) -&gt; System.out.println(t.toUpperCase()); consumer.accept(\"tim\"); //这种就是错误的，因为t没有类型 Consumer&lt;String&gt; consumer = (@Deprecated t) -&gt; System.out.println(t.toUpperCase()); consumer.accept(\"tim\"); Consumer&lt;String&gt; consumer = (t) -&gt; System.out.println(t.toUpperCase()); consumer.accept(\"tim\"); &#125;&#125; 在声明隐式类型的lambda表达式的形参时允许使用var，使用var的好处是在使用lambda表达式时给参数加上注解。 新增字符串处理 API123456789101112131415161718192021public class StringAPITest &#123; public static void main(String[] args) &#123; // 判断字符串是否为空白 \" \".isBlank(); // true // 去除首尾空白 \" Javastack \".strip(); // \"Javastack\" // 去除尾部空格 \" Javastack \".stripTrailing(); // \" Javastack\" // 去除首部空格 \" Javastack \".stripLeading(); // \"Javastack \" // 复制字符串 \"Java\".repeat(3);// \"JavaJavaJava\" // 行数统计 \"A\\nB\\nC\".lines().count(); // 3 &#125;&#125; 新增的文件 APIInputStream加强：InputStream终于有了一个非常有用的方法：transferTo，可以用来将数据直接传输到OutputStream，这是在处理原始数据流时非常常见的一种用法，如下示例： 12345678910111213public class FileSystemTest &#123; public static void main(String[] args) &#123; ClassLoader loader = FileSystemTest.class.getClassLoader(); try (var resourceAsStream = loader.getResourceAsStream(\"myFile.txt\"))&#123; FileOutputStream outputStream = new FileOutputStream(\"myFile2.txt\"); assert resourceAsStream != null; resourceAsStream.transferTo(outputStream); outputStream.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; JDK11 废弃的项目 移除的项目 移除了com.sun.awt.AWTUtilities 移除了sun.misc.Unsafe.defineClass，使用java.lang.invoke.MethodHandles.Lookup.defineClass来替代 移除了Thread.destroy()以及 Thread.stop(Throwable)方法 移除了sun.nio.ch.disableSystemWideOverlappingFileLockCheck、sun.locale.formatasdefault属性 移除了jdk.snmp模块 移除了javafx，openjdk是从java10版本就移除了，oracle jdk10还尚未移除javafx，而java11版本则oracle的jdk版本也移除了javafx 移除了Java Mission Control，从JDK中移除之后，需要自己单独下载 移除了这些Root Certificates ：Baltimore Cybertrust Code Signing CA，SECOM ，AOL and Swisscom 废弃选项 -XX:+AggressiveOpts选项 -XX:+UnlockCommercialFeatures（解锁商业性的一些组件，目前已经无需手动加上了） -XX:+LogCommercialFeatures选项也不再需要（Log统一了） Unicode 10Unicode 10增加了8518个字符，总计达到了136690个字符。并且增加了4个脚本，同时还有56个新的emoji表情符号。 Remove the JavaEE and CORBA Moudles在Java11中移除了不太使用的JavaEE模块和CORBA技术。CORBA来自于二十世纪九十年代，Oracle说现在用CORBA开发现代Java应用程序已经没有意义了，维护CORBA的成本已经超过了保留它带来的好处。但是删除CORBA将使得那些依赖于JDK提供部分CORBA API的CORBA实现无法运行，目前还没有第三方CORBA版本，也不确定是否会有第三方愿意接手CORBA API的维护工作。在Java11中将Java9标记废弃的Java EE及CORBA模块移除掉，具体如下： java.xml.ws java.xml.bind java.xml.ws java.xml.ws.annotation jdk.xml.bind jdk.xml.ws被移除 只剩下java.xml、java.xml.crypto、jdk.xml.dom这几个模块； java.corba java.se.ee java.activation java.transaction等被移除 但是Java11新增一个java.transaction.xa模块 废除Nashorn javascript引擎废除Nashorn javascript引擎，在后续版本准备移除掉，有需要的可以考虑使用GraalVM。Graal是一个用Java编写的新的JVM即时编译器，集成到HotSpot虚拟机，侧重性能和语言互操作性。Graal为Java代码提供性能优势，这得益于方法内联、流转对象分配和推理执行等新技术，从而可以实现高性能的脚本语言引擎。与传统的HotSpot编译器不同，脚本语言解释器可以使用Graal包含的Truffle API发出原生代码，这样无需提前编译，即可让编程语言获得Java的性能。语言解释器可以使用HotSpot所用技术提高 Java 代码的速度，包括发出优化的原生代码（含去优化的条件），该技术令即时编译的代码在性能方面优于提前编译的代码。这里是它的官网，可以看到它的官网说明文档： https://www.graalvm.org/ 弃用Pack200 ToolsJDK1.5中带了一个压缩工具：Pack200，这个工具能对普通的jar文件进行高效压缩。其实现原理是根据Java类特有的结构，合并常数池，去掉无用信息等来实现对Java类的高效压缩。由于是专门对Java类进行压缩的，所以对普通文件的压缩和普通压缩软件没有什么两样，但是对于Jar文件却能轻易达到10-40%的压缩率。这在Java应用部署中很有用，尤其对于移动Java计算，能够大大减小代码下载量。JDK1.5中还提供了这一技术的API接口，你可以将其嵌入到你的程序中使用。使用的方法很简单，下面的短短几行代码即可以实现jar的压缩和解压： 12345678910111213141516public class VarTest &#123; public static void main(String[] args) &#123; //压缩 Packer packer = Pack200.newPacker(); OutputStream output=new BufferedOutputStream(new FileOutputStream(outfile)); packer.pack(new JarFile(jarFile), output); output.close(); //解压 Unpacker unpacker = Pack200.newUnpacker(); output=new JarOutputStream(new FileOutputStream(jarFile)); unpacker.unpack(pack200File, output); output.close(); &#125;&#125; Pack200的压缩和解压缩速度是比较快的，而且压缩率也是很惊人的，在我是使用的包4.46MB压缩后成了1.44MB（0.322%），而且随着包的越大压缩率会根据明显，据说如果jar包都是class类可以压缩到1/9的大小。其实JavaWebStart还有很多功能，例如可以按不同的jar包进行lazy下载和单独更新，设置可以根据jar中的类变动进行class粒度的下载。但是在Java11中废除了Pack200以及unpack200工具以及java.util.jar中的Pack200 API。因为Pack200主要是用来压缩jar包的工具，由于网络下载速度的提升以及Java9引入模块化系统之后不再依赖Pack200，因此这个版本将其移除掉。 Epsilon垃圾收集器对这个特性的描述是：开发一个处理内存分配但不实现任何实际内存回收机制的GC，一旦可用堆内存用完，JVM就会退出。如果有System.gc()调用，实际上什么也不会发生(这种场景下和-XX:+DisableExplicitGC效果一样)，因为没有内存回收，这个实现可能会警告用户尝试强制GC是徒劳。用法 :-XX:+UnlockExperimentalVMOptions -XX:+UseEpsilonGC 12345678910111213141516171819202122class Garbage &#123; int n = (int)(Math.random() * 100); @Override public void finalize() &#123; System.out.println(this + \" : \" + n + \" is dying\"); &#125;&#125;public class EpsilonTest &#123; public static void main(String[] args) &#123; boolean flag = true; List&lt;Garbage&gt; list = new ArrayList&lt;&gt;(); long count = 0; while (flag) &#123; list.add(new Garbage()); if (list.size() == 1000000 &amp;&amp; count == 0) &#123; list.clear(); count++; &#125; &#125; System.out.println(\"程序结束\"); &#125;&#125; 使用这个选项的原因：提供完全被动的GC实现，具有有限的分配限制和尽可能低的延迟开销，但代价是内存占用和内存吞吐量。众所周知，Java实现可广泛选择高度可配置的GC实现，各种可用的收集器最终满足不同的需求，即使它们的可配置性使它们的功能相交。有时更容易维护单独的实现，而不是在现有GC实现上堆积另一个配置选项。它的主要用途如下 : 性能测试（它可以帮助过滤掉GC引起的性能假象） 内存压力测试（例如，知道测试用例应该分配不超过1GB的内存，我们可以使用-Xmx1g –XX:+UseEpsilonGC, 如果程序有问题，则程序会崩溃） 非常短的JOB任务(对象这种任务, 接受GC清理堆那都是浪费空间) VM接口测试 Last-drop 延迟&amp;吞吐改进 ZGC垃圾收集器ZGC，这应该是JDK11最为瞩目的特性，没有之一。但是后面带了Experimental，说明这还不建议用到生产环境。 ZGC，A Scalable Low-Latency Garbage Collector(Experimental)，一个可伸缩低延迟的GC。GC暂停时间不会超过10ms，既能处理几百兆的小堆，也能处理几个T的大堆(OMG)。和G1相比，应用吞吐能力不会下降超过15%，为未来的GC功能和利用colord指针以及Load barriers优化奠定基础，初始只支持64位系统。 ZGC的设计目标是：支持TB级内存容量，暂停时间低（&lt;10ms），对整个程序吞吐量的影响小于15%。 将来还可以扩展实现机制，以支持不少令人兴奋的功能，例如多层堆（即热对象置于DRAM和冷对象置于NVMe闪存），或压缩堆。GC是java主要优势之一，然而当GC停顿太长，就会开始影响应用的响应时间。消除或者减少GC停顿时长，Java将对更广泛的应用场景是一个更有吸引力的平台。此外，现代系统中可用内存不断增长，用户和程序员希望JVM能够以高效的方式充分利用这些内存，并且无需长时间的GC暂停时间。 ZGC是一个并发，基于region，压缩型的垃圾收集器，只有root扫描阶段会STW，因此GC停顿时间不会随着堆的增长和存活对象的增长而变长。 ZGC : avg 1.091ms / max:1.681ms G1 : avg 156.806ms / max:543.846ms 用法： -XX:+UnlockExperimentalVMOptions –XX:+UseZGC ，因为ZGC还处于实验阶段，所以需要通过JVM参数来解锁这个特性。 完全支持Linux容器（包括Docker）许多运行在Java虚拟机中的应用程序（包括Apache Spark和Kafka等数据服务以及传统的企业应用程序）都可以在Docker容器中运行。但是在Docker容器中运行Java应用程序一直存在一个问题，那就是在容器中运行JVM程序在设置内存大小和CPU使用率后，会导致应用程序的性能下降。这是因为Java应用程序没有意识到它正在容器中运行。随着Java 10的发布，这个问题总算得以解决，JVM现在可以识别由容器控制组（cgroups）设置的约束。可以在容器中使用内存和CPU约束来直接管理Java应用程序，其中包括： 遵守容器中设置的内存限制 在容器中设置可用的CPU 在容器中设置CPU约束 Java 10的这个改进在Docker for Mac、Docker for Windows以及Docker Enterprise Edition等环境均有效。 容器的内存限制：在Java 9之前，JVM无法识别容器使用标志设置的内存限制和CPU限制。而在Java10中，内存限制会自动被识别并强制执行。 Java将服务器类机定义为具有2个CPU和2GB内存，以及默认堆大小为物理内存的1/4。例如，Docker企业版安装设置为2GB内存和4个CPU的环境，我们可以比较在这个Docker容器上运行Java8和Java10的区别。 支持G1上的并行完全垃圾收集对于G1 GC，相比于JDK 8，升级到 JDK11即可免费享受到：并行的Full GC，快速的CardTable扫描(这也是G1这个GC的原理)，自适应的堆占用比例调整（IHOP），在并发标记阶段的类型卸载等等。这些都是针对G1的不断增强，其中串行Full GC等甚至是曾经被广泛诟病的短板，你会发现GC配置和调优在JDK11中越来越方便。 免费的低耗能飞行记录仪和堆分析仪这个在文章开篇已经说到了，Low-Overhead Heap Profiling相当于是一个非常低耗能的黑匣子，通过JVMTI（TI即ToInterface）的SampledObjectAlloc回调提供了一个开销低的Heap分析方式，提供一个低开销的，为了排错Java应用问题，以及JVM问题的数据收集框架，希望达到的目标如下： 提供用于生产和消费数据作为事件的API 提供缓存机制和二进制数据格式 允许事件配置和事件过滤 提供OS、JVM和JDK库的事件 实现ChaCha20和Poly1305加密算法JDK11实现了RFC7539中指定的ChaCha20和Poly1305两种加密算法，用来代替RC4。RFC7748定义的秘钥协商方案更高效，更安全。JDK增加两个新的接口： 123456789101112131415161718public class EpsilonTest &#123; public static void main(String[] args) throws Exception&#123; KeyPairGenerator kpg = KeyPairGenerator.getInstance(\"XDH\"); NamedParameterSpec paramSpec = new NamedParameterSpec(\"X25519\"); kpg.initialize(paramSpec); KeyPair kp = kpg.generateKeyPair(); KeyFactory kf = KeyFactory.getInstance(\"XDH\"); BigInteger u = new BigInteger(\"123456\"); XECPublicKeySpec pubSpec = new XECPublicKeySpec(paramSpec, u); PublicKey pubKey = kf.generatePublic(pubSpec); KeyAgreement ka = KeyAgreement.getInstance(\"XDH\"); ka.init(kp.getPrivate()); ka.doPhase(pubKey, true); byte[] secret = ka.generateSecret(); &#125;&#125; 默认根权限证书与HTTPS安全协议TLS1.3实现TLS协议1.3版本，TLS允许客户端和服务器端通过互联网以一种防止窃听，篡改以及消息伪造的方式进行通信。 Java Flight RecorderFlight Recorder源自飞机的黑盒子，Flight Recorder以前是商业版的特性，在java11当中开源出来，它可以导出事件到文件中，之后可以用Java Mission Control来分析。可以在应用启动时配置java -XX:StartFlightRecording，或者在应用启动之后，使用jcmd来录制，比如： 123$ jcmd &lt;pid&gt; JFR.start$ jcmd &lt;pid&gt; JFR.dump filename=recording.jfr$ jcmd &lt;pid&gt; JFR.stop Flight Recorder是Oracle 刚刚开源的强大特性。我们知道在生产系统进行不同角度的 Profiling，有各种工具、框架，但是能力范围、可靠性、开销等，大都差强人意，要么能力不全面，要么开销太大，甚至不可靠可能导致Java 应用进程宕机。 而 JFR 是一套集成进入 JDK、JVM 内部的事件机制框架，通过良好架构和设计的框架，硬件层面的极致优化，生产环境的广泛验证，它可以做到极致的可靠和低开销。在 SPECjbb2015 等基准测试中，JFR 的性能开销最大不超过1%，所以，工程师可以基本没有心理负担地在大规模分布式的生产系统使用，这意味着，我们既可以随时主动开启 JFR 进行特定诊断，也可以让系统长期运行 JFR，用以在复杂环境中进行After-the-fact分析。还需要苦恼重现随机问题吗？JFR 让问题简化了很多。 在保证低开销的基础上，JFR 提供的能力也令人眼前一亮，例如：我们无需 BCI 就可以进行 Object Allocation Profiling，终于不用担心BTrace之类把进程搞挂了。对锁竞争、阻塞、延迟，JVM GC、SafePoint 等领域，进行非常细粒度分析。甚至深入JIT Compiler内部，全面把握热点方法、内联、逆优化等等。JFR 提供了标准的 Java、C++等扩展API，可以与各种层面的应用进行定制、集成，为复杂的企业应用栈或者复杂的分布式应用，提供All-in-One解决方案，而这一切都是内建在JDK和 JVM内部的，并不需要额外的依赖，开箱即用。","updated":"2020-04-20T06:14:53.785Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"GC","slug":"GC","permalink":"https://zouchanglin.cn/tags/GC/"}]},{"title":"JDK9新特性(二)","date":"2020-04-16T10:00:00.000Z","path":"2020/04/16/JDK9新特性(二)/","text":"在上一篇的文章我记述了JDK9的两个最重要的特性：一个是模块化，一个是jShell。另外就是String底层存储结构和StreamAPI的一些改动与优化。本次要记述主要的特性是全新的多分辨率图像API、全新的HTTP客户端API（其实是借鉴OkHTTP的框架，或者说是整合吧）、Deprecated相关API、智能Java编译工具与动态编译器、统一的JVM日志系统、javadoc对H5的支持、JavaScript引擎升级：Nashorn。然后再谈谈JDK9还需要什么吧，也就是对未来Java的展望。 多分辨率图像API在Mac上，JDK已经支持视网膜显示，但在Linux和Windows上，并没有支持。Java程序在当前的高分辨率屏幕上可能看起来很小，不能使用它们。这是因为像素用于这些系统的大小计算（无论像素实际有多大）。毕竟，高分辨率显示器的有效部分是像素非常小。 JEP 263以这样的方式扩展了JDK，即Windows和Linux也考虑到像素的大小。为此，使用比现在更多的现代API：Direct2D for Windows和GTK+，而不是Xlib for Linux。图形，窗口和文本由此自动缩放。JEP 251还提供处理多分辨率图像的能力，即包含不同分辨率的相同图像的文件。根据屏幕的DPI度量，然后以适当的分辨率使用图像。 新的API定义在java.awt.image包下 将不同分辨率的图像封装到一张（多分辨率的）图像中，作为它的变体 获取这个图像的所有变体 获取特定分辨率的图像变体-表示一张已知分辨率单位为DPI的特定尺寸大小的逻辑图像，并且这张图像是最佳的变体。 基于当前屏幕分辨率大小和运用的图像转换算法，java.awt.Graphics类可以从接口MultiResolutionImage获取所需的变体。 MultiResolutionImage的基础实现是java.awt.image.BaseMultiResolutionImage。 全新的HTTP客户端http://openjdk.java.net/jeps/110 ，这个是改动说明文档。2015年，HTTP2成为标准。HTTP/1.1和HTTP/2的主要区别是如何在客户端和服务器之间构建和传输数据。HTTP/1.1依赖于请求/响应周期。 HTTP/2允许服务器push数据：它可以发送比客户端请求更多的数据。 这使得它可以优先处理并发送对于首先加载网页至关重要的数据。 Java 9中有新的方式来处理HTTP调用。它提供了一个新的HTTP客户端（HttpClient），它将替代仅适用于阻塞模式的HttpURLConnection （HttpURLConnection是在HTTP 1.0的时代创建的，并使用了协议无关的方法），并提供对WebSocket 和 HTTP/2的支持。 其实在Android中就已经有了异步调用API，这些API已经被封装在了新的AndroidSDK中，现在Java终于支持了。此外，HTTP客户端还提供API来处理HTTP/2的特性，比如流和服务器推送等功能。全新的HTTP客户端API可以从jdk.incubator.httpclient模块中获取。因为在默认情况下，这个模块是不能根据classpath获取的，需要使用add modules命令选项配置这个模块，将这个模块添加到classpath中。 123456789101112public class HTTPAPITest &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; HttpClient client = HttpClient.newHttpClient(); HttpRequest req = HttpRequest.newBuilder(URI.create(\"http://zouchanglin.cn\")) .GET() .build(); HttpResponse&lt;String&gt; response = client.send(req, HttpResponse.BodyHandler.asString()); System.out.println(response.statusCode()); System.out.println(response.version().name()); System.out.println(response.body()); &#125;&#125; Deprecated的相关APIJava 9 废弃或者移除了几个不常用的功能。其中最主要的是 Applet API，现在是标记为废弃的。随着对安全要求的提高，主流浏览器已经取消对 Java 浏览器插件的支持。HTML5 的出现也进一步加速了它的消亡。开发者现在可以使用像 Java Web Start 这样的技术来代替 Applet，它可以实现从浏览器启动应用程序或者安装应用程序。同时，appletviewer 工具也被标记为废弃。 http://openjdk.java.net/jeps/211 http://openjdk.java.net/jeps/214 http://openjdk.java.net/jeps/277 http://openjdk.java.net/jeps/289 http://openjdk.java.net/jeps/291 智能Java编译工具官方Feature：http://openjdk.java.net/jeps/199 智能Java编译工具(sjavac) 的第一个阶段始于JEP139这个项目，用于在多核处理器情况下提升JDK的编译速度。如今，这个项目已经进入第二阶段，即JEP199，其目的是改进Java编译工具，并取代目前JDK编译工具javac，继而成为Java环境默认的通用的智能编译工具。JDK 9 还更新了javac 编译器以便能够将Java9的代码编译运行在低版本 Java 中。 统一的JVM日志系统官方Feature： http://openjdk.java.net/jeps/158 http://openjdk.java.net/jeps/271 日志是解决问题的唯一有效途径：曾经很难知道导致JVM性能问题和导致JVM崩溃的根本原因。不同的JVM日志的碎片化和日志选项（例如：JVM组件对于日志使用的是不同的机制和规则），这使得JVM难以进行调试。 解决该问题最佳方法：对所有的JVM组件引入一个单一的系统，这些JVM组件支持细粒度的和易配置的JVM日志 javadoc的H5支持官方Feature： http://openjdk.java.net/jeps/224 http://openjdk.java.net/jeps/225 jdk 8 ：生成的java帮助文档是在HTML4 中，而HTML4 已经是很久的标准了。jdk 9 ：javadoc的输出，现在符合兼容HTML5 标准。 Javascript引擎升级：Nashorn官方Feature： http://openjdk.java.net/jeps/236 http://openjdk.java.net/jeps/292 Nashorn 项目在 JDK 9 中得到改进（因为Nashorn是在JDK8中被引入的），它为 Java 提供轻量级的 Javascript 运行时。Nashorn 项目跟随 Netscape 的 Rhino 项目，目的是为了在 Java 中实现一个高性能但轻量级的Javascript 运行时。Nashorn 项目使得 Java 应用能够嵌入 Javascript。它在 JDK 8 中为 Java 提供一个Javascript 引擎。 JDK 9 包含一个用来解析 Nashorn 的 ECMAScript 语法树的 API。这个 API 使得 IDE 和服务端框架不需要依赖 Nashorn 项目的内部实现类，就能够分析 ECMAScript 代码。 java的动态编译器官方Feature： http://openjdk.java.net/jeps/243 http://openjdk.java.net/jeps/295 Oracle 一直在努力提高 Java 启动和运行时性能，希望其能够在更广泛的场景达到或接近本地语言的性能。但是，直到今天，谈到 Java，很多 C/C++ 开发者还是会不屑地评价为启动慢，吃内存。简单说，这主要是因为 Java 编译产生的类文件是 Java 虚拟机可以理解的二进制代码，而不是真正的可执行的本地代码，需要 Java 虚拟机进行解释和编译，这带来了额外的开销。 JIT（Just-in-time）编译器可以在运行时将热点编译成本地代码，速度很快。但是 Java 项目现在变得很大很复杂，因此 JIT 编译器需要花费较长时间才能热身完，而且有些 Java 方法还没法编译，性能方面也会下降。AOT 编译就是为了解决这些问题而生的。在 JDK 9 中， AOT（JEP 295: Ahead-of-Time Compilation）作为实验特性被引入进来，开发者可以利用新的 jaotc工具将重点代码转换成类似类库一样的文件。虽然仍处于试验阶段，但这个功能使得 Java 应用在被虚拟机启动之前能够先将 Java 类编译为原生代码。此功能旨在改进小型和大型应用程序的启动时间，同时对峰值性能的影响很小。这个AOT特性我在之前的文章中也说过：[《 HotSpot JVM类型以及编译模式 》](https://zouchanglin.cn/2019/10/25/HotSpot JVM类型以及编译模式 ) 。 另外 JVMCI （JEP 243: Java-Level JVM Compiler Interface）等特性，对于整个编程语言的发展，可能都具有非常重要的意义，虽然未必引起了广泛关注。目前 Graal Core API 已经被集成进入 Java 9，虽然还只是初始一小步，但是完全用 Java 语言来实现的可靠的、高性能的动态编译器，似乎不再是遥不可及，这是 Java 虚拟机开发工程师的福音。与此同时，随着 Truffle 框架和 Substrate VM 的发展，已经让个别信心满满的工程师高呼One VM to Rule Them All!， 也许就在不远的将来 Ploygot 以一种另类的方式成为现实。 对Java未来的期望标准化的JSON API一个标准化和轻量级的JSON API被许多java开发人员所青睐。但是由于资金问题无法在Java 9中见到，但并不会削减掉。Java平台首席架构师Mark Reinhold在JDK 9邮件列中说：“这个JEP将是平台上的一个有用的补充，但是在计划中，它并不像Oracle资助的其他功能那么重要，可能会重新考虑JDK 10或更高版本中实现。 ” 但是我们目前使用的仍然是阿里的FastJSON或者谷歌的Gson等API。 新的货币 API对许多应用而言货币价值都是一个关键的特性，但JDK对此却几乎没有任何支持。严格来讲，现有的java.util.Currency类只是代表了当前ISO 4217货币的一个数据结构，但并没有关联的值或者自定义货币。JDK对货币的运算及转换也没有内建的支持，更别说有一个能够代表货币值的标准类型了。 此前，Oracle 公布的JSR 354定义了一套新的Java货币API：JavaMoney，计划会在Java 9中正式引入。但是目前没有出现在JDK 9 中。 不过，如果你用的是Maven的话，可以做如下的添加，即可使用相关的API处理货币。代码参考，可以访问https://github.com/JavaMoney，里面已经给出了使用说明和示例。 12345&lt;dependency&gt; &lt;groupId&gt;org.javamoney&lt;/groupId&gt; &lt;artifactId&gt;moneta&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt;&lt;/dependency&gt; 面临的其他问题随着云计算和 AI 等技术浪潮，当前的计算模式和场景正在发生翻天覆地的变化，不仅对 Java 的发展速度提出了更高要求，也深刻影响着 Java 技术的发展方向。传统的大型企业或互联网应用，正在被云端、容器化应用、模块化的微服务甚至是函数（FaaS， Function-as-a-Service）所替代。 Java虽然标榜面向对象编程，却毫不顾忌的加入面向接口编程思想，又扯出匿名对象之概念，每增加一个新的东西，对Java的根本所在的面向对象思想的一次冲击。反观Python，抓住面向对象的本质，又能在函数编程思想方面游刃有余。Java对标C/C++，以抛掉内存管理为卖点，却又陷入了JVM优化的噩梦。选择比努力更重要，选择Java的人更需要对它有更清晰的认识。 Java 需要在新的计算场景下，改进开发效率。这话说的有点笼统，我谈一些自己的体会，Java 代码虽然进行了一些类型推断等改进，更易用的集合 API 等，但仍然给开发者留下了过于刻板、形式主义的印象，这是一个长期的改进方向，但是不得不说Java确实还是越变越优秀了。","updated":"2020-04-19T12:09:02.919Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"模块化","slug":"模块化","permalink":"https://zouchanglin.cn/tags/%E6%A8%A1%E5%9D%97%E5%8C%96/"},{"name":"交互式","slug":"交互式","permalink":"https://zouchanglin.cn/tags/%E4%BA%A4%E4%BA%92%E5%BC%8F/"}]},{"title":"JDK9新特性(一)","date":"2020-04-15T10:00:00.000Z","path":"2020/04/15/JDK9新特性(一)/","text":"Java8在2014年3月份推出的，而历经曲折的Java9终于终于在2017年9月21日发布，中间历经3年多时间，Java9提供了超过150项新功能特性，包括备受期待的模块化系统、可交互的 REPL 工具：jshell，JDK 编译工具，Java 公共 API 和私有代码，以及安全增强、扩展提升、性能管理改善等。可以说Java 9是一个庞大的系统工程，完全做了一个整体改变。Java8中最核心的新特性就是Lambda表达式和Stream API，那么对于Java9来说其中最核心莫过于模块化系统和JShell命令。 Java 更快的发布周期意味着开发者将不需要像以前一样为主要发布版本望眼欲穿。这也意味着开发者将可能跳过 Java 9和它的不成熟的模块化功能，只需要再等待 6 个月就可以迎来新版本，这将可能解决开发者的纠结。反正Java11已经支持正式商用了，Java 11将会获得Oracle提供的长期支持服务，直至2026年的9月。所以想用上稳定的、最新的JDK还是选择Java11吧，我将在后面的文章记述Java11的新特性以及Java14的部分新特性。 在这个网站上可以看到JavaSE9的新特性 《Overview of What’s New in JDK 9》 JEP与JSRJEP(JDK Enhancement Proposals)：jdk 改进提案，每当需要有新的设想时候, JEP可以在JCP(java community Process)之前或者同时提出非正式的规范(specification)，被正式认可的JEP正式写进JDK的发展路线图并分配版本号。JSR(Java Specification Requests): java 规范提案，新特性的规范出现在这一阶段，是指向JCP(Java Community Process)提出新增一个标准化技术规范的正式请求。请求可以来自于小组/项目、JEP、JCP成员或者Java社区(community)成员的提案，每个Java版本都由相应的JSR支持。 小组：对特定技术内容，比如安全、网络、HotSpot 等有共同兴趣的组织和个人 项目： 编写代码、文档以及其他工作，至少由一个小组赞助支持，比如最近的Lambda计划，JigSaw 计划等 目录结构变化 模块化系统谈到 Java 9 大家往往第一个想到的就是Jigsaw项目。众所周知，Java 已经发展超过 20 年（95 年最初发布），Java 和相关生态在不断丰富的同时也越来越暴露出一些问题： 问题一：Java 运行环境的膨胀和臃肿。每次JVM启动的至少会加载30-60MB内存，原因是JVM需要加载rt.jar，不管其中的类是否被classloader加载，整个jar都会被JVM加载到内存当中去（而模块化可以根据模块的需要加载程序运行需要的class） 问题二：当代码库越来越大，创建复杂。不同版本的类库交叉依赖导致让人头疼的问题，这些都阻碍了 Java 开发和运行效率的提升。 问题三：很难真正地对代码进行封装，而系统并没有对不同部分(也就是JAR文件)之间的依赖关系有个明确的概念。每一个公共类都可以被类路径之下任何其它的公共类所访问到，这样就会导致无意中使用了并不想被公开访问的 API。 问题四：类路径本身也存在问题，你怎么知晓所有需要的 JAR 都已经有了, 或者是不是会有重复的项呢? 模块化的概念，其实就是package外再裹一层，也就是说，用模块来管理各个package，通过声明某个package暴露，不声明默认就是隐藏。因此，模块化使得代码组织上更安全，因为可以指定哪些部分暴露，哪些部分隐藏。 模块化实现目标： 主要目的在于减少内存的开销 只须必要模块，而非全部jdk模块，可简化各种类库和大型应用的开发和维护 改进JavaSE平台，使其可以适应不同大小的计算设备 改进其安全性，可维护性，提高性能 下面是模块化演示，我新建一个项目叫做java9news，然后通过IDEA的new module功能，生成了两个模块，一个是java9demo、一个是java9test： java9demo模块中有很简单的两个类Person、User： 1234567891011public class Person &#123; private String name; private int age; //Getter / Setter / toString&#125;public class User &#123; private String name; private int age; //Getter / Setter / toString&#125; java9test模块是无法使用java9demo中的类的，必须引入一个module-info.java的文件，在java9demo模块中的module-info.java: 12345module java9demo &#123; //指出我们想导出的包 exports xpu.tim.bean;&#125; 在java9test模块中的module-info.java: 1234567module java9test &#123; // 指明我们想导入的模块 requires java9demo; // 导入日志模块 requires java.logging;&#125; 接下来测试一下，之所以无法用User，就是因为在java9demo模块中的module-info.java中并未指定把xpu.tim.entity这个包给导出，所以使用User类报Error： 123456789101112131415/** * 测试Java9模块化特性 */public class ModuleTest &#123; private static final Logger LOGGER = Logger.getLogger(\"Tim\"); public static void main(String[] args) &#123; Person person = new Person(\"Tim\", 20); System.out.println(person); //User user = new User(); // Error LOGGER.info(\"This one log\"); &#125;&#125; Java的REPl工具 jShell像Python和Scala 之类的语言早就有交互式编程环境REPL (read - evaluate - print- loop)了，以交互式的方式对语句和表达式进行求值。开发者只需要输入一些代码，就可以在编译前获得对程序的反馈。而之前的Java 版本要想执行代码，必须创建文件、声明类、提供测试方法方可实现。 jShell的实现目标 &gt; 1、Java9中终于拥有了REPL工具：jShell。利用jShell 在没有创建类的情况下直接声明变量，计算表达式，执行语句。即开发时可以在命令行里直接运行Java的代码，而无需创建Java文件，无需跟人解释public static void main(String[] args)这句废话。 2、jShell也可以从文件中加载语句或者将语句保存到文件中。 3、jShell也可以是tab键进行自动补全和自动添加分号。 jShell使用示例，jShell可以使用Tab键补全： 1234567891011121314151617181920212223242526272829D:&gt;jshell| 欢迎使用 JShell -- 版本 9.0.1| 要大致了解该版本, 请键入: /help introjshell&gt; System.out.println(\"HelloWorld\")HelloWorldjshell&gt; int i = 10;i ==&gt; 10jshell&gt; int j = 20;j ==&gt; 20jshell&gt; int k = i + j;k ==&gt; 30jshell&gt; System.out.println(k)30jshell&gt; public int add(int i, int j)&#123; ...&gt; return i + j; ...&gt; &#125;| 已创建 方法 add(int,int)jshell&gt; System.out.println(add(50, 100))150jshell&gt; add(10, 20)$8 ==&gt; 30 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657jshell&gt; /help| 键入 Java 语言表达式, 语句或声明。| 或者键入以下命令之一:| /list [&lt;名称或 id&gt;|-all|-start]| 列出您键入的源| /edit &lt;名称或 id&gt;| 编辑按名称或 id 引用的源条目| /drop &lt;名称或 id&gt;| 删除按名称或 id 引用的源条目| /save [-all|-history|-start] &lt;文件&gt;| 将片段源保存到文件。| /open &lt;file&gt;| 打开文件作为源输入| /vars [&lt;名称或 id&gt;|-all|-start]| 列出已声明变量及其值| /methods [&lt;名称或 id&gt;|-all|-start]| 列出已声明方法及其签名| /types [&lt;名称或 id&gt;|-all|-start]| 列出已声明的类型| /imports| 列出导入的项| /exit| 退出 jshell| /env [-class-path &lt;路径&gt;] [-module-path &lt;路径&gt;] [-add-modules &lt;模块&gt;] ...| 查看或更改评估上下文| /reset [-class-path &lt;路径&gt;] [-module-path &lt;路径&gt;] [-add-modules &lt;模块&gt;]...| 重启 jshell| /reload [-restore] [-quiet] [-class-path &lt;路径&gt;] [-module-path &lt;路径&gt;]...| 重置和重放相关历史记录 -- 当前历史记录或上一个历史记录 (-restore)| /history| 您键入的内容的历史记录| /help [&lt;command&gt;|&lt;subject&gt;]| 获取 jshell 的相关信息| /set editor|start|feedback|mode|prompt|truncation|format ...| 设置 jshell 配置信息| /? [&lt;command&gt;|&lt;subject&gt;]| 获取 jshell 的相关信息| /!| 重新运行上一个片段| /&lt;id&gt;| 按 id 重新运行片段| /-&lt;n&gt;| 重新运行前面的第 n 个片段|| 有关详细信息, 请键入 '/help', 后跟| 命令或主题的名称。| 例如 '/help /list' 或 '/help intro'。主题:|| intro| jshell 工具的简介| shortcuts| 片段和命令输入提示, 信息访问以及| 自动代码生成的按键说明| context| /env /reload 和 /reset 的评估上下文选项jshell&gt; jshell还可以从外部文件加载源代码，如下面是我在桌面上的一个HelloWorld.java文件： 123456// 测试从外部文件加载源代码void printHello() &#123; System.out.println(\"测试从外部文件加载源代码\");&#125;printHello(); jShell没有受检异常（编译时异常），本来应该强迫我们捕获一个IOException，但却没有出现。因为jShell在后台为我们隐藏了。 多版本兼容jar包新版本的Java出现时，用户要花费数年时间才会切换到这个新的版本。这就意味着库得去向后兼容你想要支持的最老的Java版本（许多情况下就是Java 6 或者 Java7）。这实际上意味着未来的很长一段时间，你都不能在库中运用Java 9所提供的新特性。幸运的是，多版本兼容jar功能能让你创建仅在特定版本的Java环境中运行库程序选择使用的class版本。 如上图所示：root.jar 可以在Java9 中使用，不过A或B类使用的不是顶层的root.A或root.B这两个class, 而是处在META-INF/versions/9下面的这两个。这是特别为 Java 9 准备的 class 版本，可以运用 Java 9 所提供的特性和库。同时，在早期的 Java 诸版本中使用这个JAR也是能运行的，因为较老版本的Java只会看到顶层的A类或B 类。 现有目录结构如下： 12345678910111213141516171819202122232425262728293031323334353637// java中Application.javaimport java.io.IOException;import java.util.List;import java.util.ArrayList;import java.util.Set;public class Application &#123; public static void testMultiJar()&#123; Generator gen = new Generator(); System.out.println(\"Generated strings: \" + gen.createStrings()); &#125;&#125;// java中Generator.javaimport java.util.Set;import java.util.HashSet;public class Generator &#123; public Set&lt;String&gt; createStrings() &#123; Set&lt;String&gt; strings = new HashSet&lt;String&gt;(); strings.add(\"Java\"); strings.add(\"8\"); return strings; &#125;&#125;// java9中Generator.javaimport java.util.Set;public class Generator &#123; public Set&lt;String&gt; createStrings() &#123; return Set.of(\"Java\", \"9\"); &#125;&#125; 现在将其编译为Jar包： 123javac -d build --release 8 src/main/java/*.javajavac -d build9 --release 9 src/main/java-9/*.javajar --create --main-class=Application --file multijar.jar -C build . --release 9 -C build9 . 接下来分别在JDK8和JDK9的环境中调用Jar包中的方法，结果如下图： 接口的私有方法Java 8中规定接口中的方法除了抽象方法之外，还可以定义静态方法和默认的方法。一定程度上，扩展了接口的功能，此时的接口更像是一个抽象类。 在Java 9中，接口更加的灵活和强大，连方法的访问权限修饰符都可以声明为private的了，此时方法将不会成为你对外暴露的API的一部分。 123456789101112131415161718192021222324252627282930313233343536interface MyInterface &#123; //JDK7 void method1(); //JDK8: 静态方法 static void method2()&#123; System.out.println(\"method2\"); &#125; //JDK8：默认方法 default void method3()&#123; System.out.println(\"method3\"); method4(); &#125; //JDK9：私有方法 private void method4()&#123; System.out.println(\"method\"); &#125;&#125;class MyInterfaceImpl implements MyInterface&#123; @Override public void method1() &#123; &#125;&#125;public class MyInterfaceTest&#123; public static void main(String[] args) &#123; MyInterface myInterface = new MyInterfaceImpl(); myInterface.method3(); //myInterface.method4(); Error &#125;&#125; 钻石操作符使用升级我们将能够与匿名实现类共同使用钻石操作符（diamond operator）在java 8中如下的操作是会报错的： 12345678public class MyOperatorTest &#123; private List&lt;String&gt; flattenStrings(List&lt;String&gt;... lists) &#123; Set&lt;String&gt; set = new HashSet&lt;&gt;()&#123;&#125;; for(List&lt;String&gt; list : lists) &#123; set.addAll(list); &#125; return new ArrayList&lt;&gt;(set); &#125; 那么在JDK9中呢？其实就是我们的匿名子类和泛型可以一起使用了： 123456789101112public class MyOperatorTest &#123; public static void main(String[] args) &#123; Set&lt;String&gt; set = new HashSet&lt;&gt;()&#123; @Override public int size() &#123; return super.size() * 100; &#125; &#125;; set.addAll(Arrays.asList(\"AAA\", \"BBB\", \"CCC\")); System.out.println(set.size()); // 300 &#125;&#125; try语句升级JDK7之前的版本如何进行资源关闭呢？无非就是try-catch-finally这种结构，在finally中保证资源关闭： 123456789101112131415161718public class MyTryCatchTest &#123; public static void main(String[] args) &#123; InputStreamReader reader = null; reader = new InputStreamReader(System.in); try &#123; //数据读取过程.. reader.read(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; JDK7出现了try-with-resource，不用显式处理资源的关闭，但是要求执行后必须关闭的所有资源必须在try子句中初始化，否则编译不通过： 12345678910public class MyTryCatchTest &#123; public static void main(String[] args) &#123; try(InputStreamReader reader = new InputStreamReader(System.in)) &#123; //数据读取过程.. reader.read(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; JDK9 中，用资源语句编写try将更容易，我们可以在try子句中使用已经初始化过的资源，此时的资源是final的： 12345678910111213public class MyTryCatchTest &#123; public static void main(String[] args) &#123; InputStreamReader reader = new InputStreamReader(System.in); OutputStreamWriter writer = new OutputStreamWriter(System.out); try(reader; writer) &#123; //数据读取过程.. reader.read(); //reader = null; //Error 此时reader和writer是final的，不可再次赋值 &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; UnderScore使用限制UnderScore其实就是下划线，在java 8 中，标识符可以独立使用_来命名： 1String _ = \"HelloWorld\"; 但是在java 9 中规定_不再可以单独命名标识符了，如果使用则报错： String存储结构变更String 再也不用 char[] 来存储啦，改成了 byte[] 加上编码标记，节约了一些空间。在这里可以看到官方文档的说明： http://openjdk.java.net/jeps/254 1234567891011Motivation (修改动机)The current implementation of the String class stores characters in a char array, using two bytes (sixteen bits) for each character. Data gathered from many different applications indicates that strings are a major component of heap usage and, moreover, that most String objects contain only Latin-1 characters. Such characters require only one byte of storage, hence half of the space in the internal char arrays of such String objects is going unused.DescriptionWe propose to change the internal representation of the String class from a UTF-16 char array to a byte array plus an encoding-flag field. The new String class will store characters encoded either as ISO-8859-1&#x2F;Latin-1 (one byte per character), or as UTF-16 (two bytes per character), based upon the contents of the string. The encoding flag will indicate which encoding is used.String-related classes such as AbstractStringBuilder, StringBuilder, and StringBuffer will be updated to use the same representation, as will the HotSpot VM&#39;s intrinsic string operations.This is purely an implementation change, with no changes to existing public interfaces. There are no plans to add any new public APIs or other interfaces.The prototyping work done to date confirms the expected reduction in memory footprint, substantial reductions of GC activity, and minor performance regressions in some corner cases. String类的当前实现将字符存储在char数组中，每个字符使用两个字节（十六个比特位）。从许多不同应用程序收集的数据表明，字符串是堆使用的主要组成部分，而且大多数String对象仅包含拉丁字符。 这样的字符只需要存储一个字节，因此此类String对象的内部char数组中的一半空间都没有使用。 我们建议将String类的内部表示形式从UTF-16字符数组更改为字节数组，再加上一个编码标志字段。 新的String类将基于字符串的内容存储编码为ISO-8859-1 / Latin-1（每个字符一个字节）或UTF-16（每个字符两个字节）的字符。 编码标志将指示使用哪种编码。 与字符串相关的类（例如AbstractStringBuilder，StringBuilder和StringBuffer）将更新为使用相同的表示形式，HotSpot VM的固有字符串操作也将使用相同的表示形式。这纯粹是实现更改，不更改现有的公共接口。 没有计划添加任何新的公共API或其他接口。 迄今为止完成的原型工作证实了在某些特殊情况下预期的内存占用减少，GC活动大大减少以及性能下降的预期。 那StringBuffer 和 StringBuilder 是否仍无动于衷呢？其实由于String类的底层存储结构的更改会影响到StringBuffer和StringBuier，我们看到StringBuffer和StringBuilder的源码，发现多了@HotSpotIntrinsicCandidate这个注解： 1234567891011121314151617/** * Constructs a string buffer with no characters in it and an * initial capacity of 16 characters. */@HotSpotIntrinsicCandidatepublic StringBuffer() &#123; super(16);&#125;/** * Constructs a string builder with no characters in it and an * initial capacity of 16 characters. */@HotSpotIntrinsicCandidatepublic StringBuilder() &#123; super(16);&#125; JDK的源码中，被@HotSpotIntrinsicCandidate标注的方法，在HotSpot中都有一套高效的实现，该高效实现基于CPU指令，运行时，HotSpot维护的高效实现会替代JDK的源码实现，从而获得更高的效率。 所以可见StringBuffer和StringBuilder都是通过HotSpot的高效实现，其还也就是底层通过byte[]来实现的。 集合工厂方法：快速创建只读集合要创建一个只读、不可改变的集合，必须构造和分配它，然后添加元素，最后包装成一个不可修改的集合。可以参考这些， http://openjdk.java.net/jeps/269 。 在JDK1.8中创建只读集合的方式： 12345678910111213public class CollectionTest &#123; public static void main(String[] args) &#123; List&lt;String&gt; namesList = new ArrayList&lt;&gt;(); namesList.add(\"Joe\"); namesList.add(\"Bob\"); namesList.add(\"Bill\"); namesList = Collections.unmodifiableList(namesList); List&lt;String&gt; namesList = new ArrayList&lt;&gt;(); namesList.addAll(Arrays.asList(\"Joe\", \"Bob\", \"Bill\")); namesList = Collections.unmodifiableList(namesList); &#125;&#125; 但是在JDK9中可以直接这样操作（其实很多地方参考了这种设计，比如JPA中的分页参数就是典例）： 1234567891011Map&lt;String, Integer&gt; map = Collections.unmodifiableMap(new HashMap&lt;&gt;()&#123; &#123; put(\"AAA\", 1); put(\"BBB\", 1); put(\"CCC\", 1); &#125;&#125;);//甚至如下的写法更简单List&lt;String&gt; namesList = List.of(\"Joe\", \"Bob\", \"Bill\");Map&lt;String, Integer&gt; map = Map.of(\"AAA\", 1, \"BBB\", 1, \"CCC\", 1); 在创建后，继续添加元素到这些集合会导致 UnsupportedOperationException。由于Java 8中接口方法的实现，可以直接在List，Set和Map的接口内定义这些方法，便于调用。 StreamAPI增强Java 的 Steam API 是java标准库最好的改进之一，让开发者能够快速运算，从而能够有效的利用数据并行计算。Java 8 提供的 Steam 能够利用多核架构实现声明式的数据处理。在 Java 9 中，Stream API 变得更好，Stream 接口中添加了 4 个新的方法：dropWhile, takeWhile, ofNullable，还有个 iterator方法的新重载方法，可以让你提供一个 Predicate (判断条件)来指定什么时候结束迭代。 除了对 Stream 本身的扩展，Optional 和 Stream 之间的结合也得到了改进。现在可以通过 Optional 的新方法stream() 将一个 Optional 对象转换为一个(可能是空的) Stream对象。 takeWhile()的使用：用于从 Stream 中获取一部分数据，接收一个 Predicate 来进行选择。在有序的 Stream 中，takeWhile 返回从开头开始的尽量多的元素。 1234567891011public class StreamAPITest &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = Arrays.asList(45, 43, 76, 87, 42, 77); list.stream().takeWhile(x -&gt; x &lt; 50) .forEach(System.out::println); System.out.println(); list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8); list.stream().takeWhile(x -&gt; x &lt; 5) .forEach(System.out::println); &#125;&#125; dropWhile()的使用：dropWhile 的行为与 takeWhile 相反，返回剩余的元素。 1234567891011public class StreamAPITest &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = Arrays.asList(45, 43, 76, 87, 42, 77, 90, 73, 67, 88); list.stream().dropWhile(x -&gt; x &lt; 50) .forEach(System.out::println); System.out.println(); list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8); list.stream().dropWhile(x -&gt; x &lt; 5) .forEach(System.out::println); &#125;&#125; ofNullable()的使用：Java 8 中 Stream 不能完全为null，否则会报空指针异常。而 Java 9 中的ofNullable 方法允许我们创建一个单元素 Stream，可以包含一个非空元素，也可以创建一个空 Stream。 1234567891011121314151617public class StreamAPITest &#123; public static void main(String[] args) &#123; Stream&lt;String&gt; stringStream = Stream.of(\"AA\", \"BB\", null); System.out.println(stringStream.count()); //3 List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(\"AA\"); list.add(null); System.out.println(list.stream().count()); //2 Stream&lt;Object&gt; stream1 = Stream.ofNullable(null); System.out.println(stream1.count()); //0 Stream&lt;String&gt; stream = Stream.ofNullable(\"hello world\"); System.out.println(stream.count()); //1 &#125;&#125; iterator()重载的使用： 123456789public class StreamAPITest &#123; public static void main(String[] args) &#123; //原来的控制终止方式： Stream.iterate(1,i -&gt; i + 1).limit(10) .forEach(System.out::println); //现在的终止方式： Stream.iterate(1,i -&gt; i &lt; 100,i -&gt; i + 1) .forEach(System.out::println); &#125;&#125; Optional类中stream()的使用： 123456789101112public class StreamAPITest &#123; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(\"Tom\"); list.add(\"Jerry\"); list.add(\"Tim\"); Optional&lt;List&lt;String&gt;&gt; optional = Optional.ofNullable(list); Stream&lt;List&lt;String&gt;&gt; stream = optional.stream(); stream.flatMap(x -&gt; x.stream()).forEach(System.out::println); &#125;&#125;","updated":"2020-04-20T03:26:49.452Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"模块化","slug":"模块化","permalink":"https://zouchanglin.cn/tags/%E6%A8%A1%E5%9D%97%E5%8C%96/"},{"name":"交互式","slug":"交互式","permalink":"https://zouchanglin.cn/tags/%E4%BA%A4%E4%BA%92%E5%BC%8F/"}]},{"title":"JDK8新特性","date":"2020-04-14T10:00:00.000Z","path":"2020/04/14/JDK8新特性/","text":"虽然已经用过了一些Java8的新特性，但是总来没有仔细总结一下。Java8自从2014年就发布了，到目前为止只有一小部分公司在用JDK7及其以下的版本，大部分已经迁移至Java8，甚至Java11（关于Java9和Java11的特性我会在之后两篇文章中记述），目前只看Java8那些最主要的、也是最常用的新特性，我到目前为止用到的最多的也就是Stream API和Lambda表达式，新时间日期的API也比较常用。 Java8新特性简介JDK8的新特性主要的从以下几个方面谈起： 1、速度更快：优化垃圾回收机制（永久代被移除，使用元空间，元空间受物理内存大小限制）；数据结构整改（如HashMap，这也就意味着HashSet也跟着变化了）；ConcurrentHashMap也变了，从之前的锁分段机制改成了大量的CAS操作，HashMap和ConcurrentHashMap都是由原来的链表改成了链表+红黑树的结构；所以速度明显提高。 2、代码更少：通过Lambda表达式来减少不必要的代码编写量，代码更少更简洁。 3、强大的Stream API：有了Stream API就意味着在Java中操作数据就像SQL语句一样简单，其实比SQL语句还简单 4、便于并行：对Fork/Join框架进行了提升，之前得开发者自己给任务做分隔，代码复杂度很高。但是自从JDK8以来，对Fork/Join框架进行了大幅度的提升，很方便的从串行切换到并行。 5、最大化减少空指针异常 Optional：通过Optional容器类来提供一些解决方法，最大化避免空指针异常 Lambda表达式Lambda是一个匿名函数，我们可以把Lambda表达式理解为是一段可以传递的代码(将代码像数据一样进行传递)。可以写出更简洁、更灵活的代码。作为一种更紧凑的代码风格，使Java的语言表达能力得到了提升。 12345678910111213141516171819public class LambdaDemo &#123; public static void main(String[] args) &#123; Comparator&lt;Integer&gt; integerComparator = new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; return o1.compareTo(o2); &#125; &#125;; TreeSet&lt;Integer&gt; integerTreeSet = new TreeSet&lt;&gt;(integerComparator); //lambda表达式 Comparator&lt;Integer&gt; integerComparator1 = (o1, o2) -&gt; Integer.compare(o1, o2); TreeSet&lt;Integer&gt; integerTreeSet1 = new TreeSet&lt;&gt;(integerComparator1); //lambda表达式 Comparator&lt;Integer&gt; integerComparator2 = Integer::compare; TreeSet&lt;Integer&gt; integerTreeSet2 = new TreeSet&lt;&gt;(integerComparator1); &#125;&#125; 上面的例子可能不是很形象的说明Lambda表达式的作用，下面可以看看实际一点的使用例子，有一个员工集合Employee，现在需要根据年龄或者薪水过滤出对应的数据： 123456789101112131415public class Employee &#123; private int age; // 年龄 private int salary; // 薪水 private String name; // 姓名 public Employee(int age, int salary, String name) &#123; this.age = age; this.salary = salary; this.name = name; &#125; // Getter / Setter / toString&#125; 各种过滤条件的演示： 12345678910111213141516171819202122232425262728293031323334353637public class LambdaDemo &#123; public static void main(String[] args) &#123; List&lt;Employee&gt; employeeList = Arrays.asList( new Employee(18, 5500, \"Tom\"), new Employee(28, 4500, \"Jone\"), new Employee(20, 3500, \"Jack\"), new Employee(25, 8500, \"Tim\") ); List&lt;Employee&gt; retListByAge = filterByAge(employeeList); retListByAge.forEach(System.out::println); System.out.println(\"----------------------------------------\"); List&lt;Employee&gt; retListBySalary = filterBySalary(employeeList); retListBySalary.forEach(System.out::println); &#125; // 根据年龄过滤 private static List&lt;Employee&gt; filterByAge(List&lt;Employee&gt; employeeList) &#123; ArrayList&lt;Employee&gt; retList = new ArrayList&lt;&gt;(); for(Employee employee: employeeList)&#123; if(employee.getAge() &gt; 20) retList.add(employee); &#125; return retList; &#125; // 根据工资过滤 private static List&lt;Employee&gt; filterBySalary(List&lt;Employee&gt; employeeList) &#123; ArrayList&lt;Employee&gt; retList = new ArrayList&lt;&gt;(); for(Employee employee: employeeList)&#123; if(employee.getSalary() &gt; 5000) retList.add(employee); &#125; return retList; &#125;&#125; 现在我们用策略模式进行改进： 1234// 过滤策略public interface MyFilterPredict &#123; boolean filter(Employee employee);&#125; 策略对应的实现类： 123456789101112131415// 根据年龄定义的过滤器public class EmployeeAgeFilterPredict implements MyFilterPredict &#123; @Override public boolean filter(Employee employee) &#123; return employee.getAge() &gt; 20; &#125;&#125;// 根据薪水定义的过滤器public class EmployeeSalaryFilterPredict implements MyFilterPredict &#123; @Override public boolean filter(Employee employee) &#123; return employee.getSalary() &gt; 5000; &#125;&#125; 使用的时候： 1234567891011121314151617181920212223242526public class LambdaDemo &#123; public static void main(String[] args) &#123; List&lt;Employee&gt; employeeList = Arrays.asList( new Employee(18, 5500, \"Tom\"), new Employee(28, 4500, \"Jone\"), new Employee(20, 3500, \"Jack\"), new Employee(25, 8500, \"Tim\") ); List&lt;Employee&gt; retListByAge = commonFilter(employeeList, new EmployeeAgeFilterPredict()); retListByAge.forEach(System.out::println); System.out.println(\"-----------------------------\"); List&lt;Employee&gt; retListBySalary = commonFilter(employeeList, new EmployeeSalaryFilterPredict()); retListBySalary.forEach(System.out::println); &#125; // 按照自定义策略过滤 private static List&lt;Employee&gt; commonFilter(List&lt;Employee&gt; employeeList, MyFilterPredict myFilterPredict) &#123; ArrayList&lt;Employee&gt; retList = new ArrayList&lt;&gt;(); for (Employee employee: employeeList) if(myFilterPredict.filter(employee)) retList.add(employee); return retList; &#125;&#125; 但是我们实际上并不需要写策略接口对应的实现类，直接使用匿名内部类即可： 1234567891011121314151617181920212223242526public static void main(String[] args) &#123; List&lt;Employee&gt; employeeList = Arrays.asList( new Employee(18, 5500, \"Tom\"), new Employee(28, 4500, \"Jone\"), new Employee(20, 3500, \"Jack\"), new Employee(25, 8500, \"Tim\") ); List&lt;Employee&gt; retListByAge = commonFilter(employeeList, new MyFilterPredict() &#123; @Override public boolean filter(Employee employee) &#123; return employee.getAge() &gt; 20; &#125; &#125;); retListByAge.forEach(System.out::println); System.out.println(\"-----------------------------\"); List&lt;Employee&gt; retListBySalary = commonFilter(employeeList, new MyFilterPredict() &#123; @Override public boolean filter(Employee employee) &#123; return employee.getSalary() &gt; 5000; &#125; &#125;); retListBySalary.forEach(System.out::println);&#125; 直接使用匿名内部类那么就意味着可以直接用Lambda表达式来代替： 12345678910111213141516public static void main(String[] args) &#123; List&lt;Employee&gt; employeeList = Arrays.asList( new Employee(18, 5500, \"Tom\"), new Employee(28, 4500, \"Jone\"), new Employee(20, 3500, \"Jack\"), new Employee(25, 8500, \"Tim\") ); List&lt;Employee&gt; retListByAge = commonFilter(employeeList, employee -&gt; employee.getAge() &gt; 20); retListByAge.forEach(System.out::println); System.out.println(\"-----------------------------\"); List&lt;Employee&gt; retListBySalary = commonFilter(employeeList, employee -&gt; employee.getSalary() &gt; 5000); retListBySalary.forEach(System.out::println);&#125; 其实，retListByAge.forEach(System.out::println); 也是Lambda表达式的一个用法。但是还有更骚的用法，那就是用Stream来解决这个问题： 12345678910111213141516171819202122232425262728293031public class LambdaDemo &#123; public static void main(String[] args) &#123; List&lt;Employee&gt; employeeList = Arrays.asList( new Employee(18, 5500, \"Tom\"), new Employee(28, 4500, \"Jone\"), new Employee(20, 3500, \"Jack\"), new Employee(25, 8500, \"Tim\") ); employeeList.stream() .filter((e) -&gt; e.getAge() &gt; 20) .forEach(System.out::println); System.out.println(\"-----------------------------\"); employeeList.stream() .filter((e) -&gt; e.getSalary() &gt; 5000) .forEach(System.out::println); &#125; // 薪水大于1000的有4个，但是我只需要前面两个 employeeList.stream() .filter((e) -&gt; e.getSalary() &gt; 1000) .limit(2) .forEach(System.out::println); // 只把名字提取出来 List&lt;String&gt; nameList = employeeList.stream() .map(Employee::getName) .collect(Collectors.toList()); nameList.forEach(System.out::println);&#125; 不知道上面的例子是否能体会到Lambda表达式的简介易用呢？现在具体来看看Lambda表达式的语法： Lambda表达式在Java语言中引入了一个新的语法元素和操作符。这个操作符为-&gt;，该操作符被称为Lambda操作符或箭头操作符。它将Lambda 分为两个部分: 左侧:指定了Lambda表达式需要的所有参数； 右侧:指定了Lambda体，即Lambda表达式要执行的功能。 语法格式一：无参数、无返回值 123456public class LambdaDemo &#123; public static void main(String[] args) &#123; Runnable runnable = ()-&gt; System.out.println(\"Hello\"); runnable.run(); &#125;&#125; 语法格式二：有一个参数、无返回值（只有一个参数时，参数的小括号可不写） 1234567891011import java.util.function.Consumer;public class LambdaDemo &#123; public static void main(String[] args) &#123; Consumer&lt;String&gt; consumer = (e) -&gt; System.out.println(e); consumer.accept(\"Hello\"); Consumer&lt;String&gt; consumer = e -&gt; System.out.println(e); consumer.accept(\"Hello\"); &#125;&#125; 语法格式三：有两个以上的参数、并且Lambda体中有多条语句 123456789public class LambdaDemo &#123; public static void main(String[] args) &#123; Comparator&lt;Integer&gt; comparator = (x, y) -&gt; &#123; System.out.println(\"Hello\"); return Integer.compare(x, y); &#125;; &#125;&#125; 语法格式四：若Lambda体中一条语句，return 和大括号都可以省略不写 12345public class LambdaDemo &#123; public static void main(String[] args) &#123; Comparator&lt;Integer&gt; comparator = (x, y) -&gt; Integer.compare(x, y); &#125;&#125; 语法格式六：Lambda表达式中的参数列表的数据类型可以不写，JVM会根据上下文推导 123456public class LambdaDemo &#123; public static void main(String[] args) &#123; Comparator&lt;Integer&gt; comparator = (Integer x, Integer y) -&gt; Integer.compare(x, y); Comparator&lt;Integer&gt; comparator = (x, y) -&gt; Integer.compare(x, y); &#125;&#125; Lambda表达式需要函数式接口的支持，接口中只有一个抽象方法的接口，称为函数式接口。可以使用@FunctionInterface注解修饰，可以检查是否是函数式接口，如下图MyFilterPredict接口由于有两个接口，所以不能被称作是函数式接口，@FunctionInterface注解自然就会报错，因为如果接口中含有两个或两个以上的接口，那么Lambda表达式就无法表示到执行的是哪个方法，所以就不能被称为函数式接口： 下面我们看看Jaba提供的四大内置核心函数式接口： 消费型接口：对传入的参数进行操作，并且无返回值 123456789public class LambdaDemo &#123; public static void main(String[] args) &#123; consume(100.0, (m)-&gt; System.out.println(\"旅游消费金额：\" + m + \"元\")); &#125; private static void consume(double money, Consumer&lt;Double&gt; consumer) &#123; consumer.accept(money); &#125;&#125; 消费型接口：对传入的参数进行操作，并且无返回值 1234567891011121314151617import java.util.function.Supplier;public class LambdaDemo &#123; public static void main(String[] args) &#123; //Lambda表达式内定义数字的产生方式 List&lt;Integer&gt; integerList = supply(10, () -&gt; (int) (Math.random() * 100)); integerList.forEach(System.out::println); &#125; //获得N个数字存入的List private static List&lt;Integer&gt; supply(int length, Supplier&lt;Integer&gt; supplier) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; length; i++) list.add(supplier.get()); return list; &#125;&#125; 函数型接口：参数类型为T、返回类型是R 12345678910public class LambdaDemo &#123; public static void main(String[] args) &#123; Integer length = calcLength(\"Hello\", (x) -&gt; x.length()); System.out.println(length); &#125; private static Integer calcLength(String string, Function&lt;String, Integer&gt; function) &#123; return function.apply(string); &#125;&#125; 断言型接口：做一些判断操作 12345678910111213141516171819import java.util.ArrayList;import java.util.Arrays;import java.util.List;import java.util.function.Predicate;import java.util.stream.Collectors;public class LambdaDemo &#123; public static void main(String[] args) &#123; List&lt;String&gt; stringList = Arrays.asList(\"And\", \"Animal\", \"Basic\", \"ABC\"); List&lt;String&gt; retList = predication(stringList, (x) -&gt; x.startsWith(\"A\")); for(String str: retList) System.out.print(str + \" \"); &#125; private static List&lt;String&gt; predication(List&lt;String&gt; stringList, Predicate&lt;String&gt; predicate) &#123; return stringList.stream() .filter(predicate) .collect(Collectors.toList()); &#125;&#125; 其实除了这四大核心函数式接口还有其他的接口： 方法引用与构造器引用方法引用当要传递给Lambda体的操作，已经有实现的方法了，可以使用方法引用！(实现抽象方法的参数列表，必须与方法引用方法的参数列表保持一致! ) 方法引用：使用操作符:: 将方法名和对象或类的名字分隔开来。如下三种主要使用情况： 对象 :: 实例方法 类 :: 静态方法 类 :: 实例方法 下面是 对象::实例方法 这种格式： 12345678import java.util.function.Consumer;public class MedthodRef &#123; public static void main(String[] args) &#123; Consumer&lt;String&gt; consumer = (x) -&gt; System.out.println(x); Consumer&lt;String&gt; consumer = System.out::println; &#125;&#125; 下面是 类::静态方法 这种格式： 12345678import java.util.function.Supplier;public class MedthodRef &#123; public static void main(String[] args) &#123; Supplier&lt;Double&gt; supplier = Math::random; Comparator&lt;Integer&gt; comparator = Integer::compareTo; &#125;&#125; Lambda体中调用方法的参数列表与返回值类型，要与函数式接口中抽象方法的函数列表和返回值类型保持一致！ 下面是 类::实例方法 这种格式： 12345678import java.util.function.BiPredicate;public class MedthodRef &#123; public static void main(String[] args) &#123; BiPredicate&lt;String, String&gt; biPredicate = (x, y) -&gt; x.equals(y); BiPredicate&lt;String, String&gt; biPredicate = String::equals; &#125;&#125; 注意：当需要引用方法的第一个参数是调用对象，并且第二个参数是需要引用方法的第二个参数（或无参数）时：ClassName: :methodName 构造器引用格式：ClassName::new 与函数式接口相结合，自动与函数式接口中方法兼容。可以把构造器引用赋值给定义的方法，与构造器参数列表要与接口中抽象方法的参数列表一致！ 123456789101112131415161718192021222324252627public class Employee &#123; // 年龄 private int age; // 薪水 private int salary; // 姓名 private String name; public Employee() &#123; &#125; public Employee(int age, int salary, String name) &#123; this.age = age; this.salary = salary; this.name = name; &#125; public Employee(int age) &#123; this.age = age; &#125; public Employee(Integer age, Integer salary) &#123; this.age = age; this.salary = salary; &#125; // Getter / Setter / toString ...&#125; 由于构造器参数列表要与接口中抽象方法的参数列表一致，所以我给Employee类加了上述几个构造方法 1234567891011121314151617181920212223import java.util.Comparator;import java.util.function.BiFunction;import java.util.function.BiPredicate;import java.util.function.Function;import java.util.function.Supplier;public class MedthodRef &#123; public static void main(String[] args) &#123; // 自动匹配无参构造器 Supplier&lt;Employee&gt; supplier = Employee::new; Function&lt;Integer, Employee&gt; function0 = (x) -&gt; new Employee(x); // 自动匹配Age带参构造器 Function&lt;Integer, Employee&gt; function1 = Employee::new; Employee employee0 = function1.apply(18); System.out.println(employee0); BiFunction&lt;Integer, Integer, Employee&gt; biFunction = Employee::new; Employee employee1 = biFunction.apply(18, 5500); System.out.println(employee1); &#125;&#125; 数组引用数组引用其实也是和上面一样的： 1234567891011import java.util.function.Function;public class MedthodRef &#123; public static void main(String[] args) &#123; Function&lt;Integer, String[]&gt; function = (x) -&gt; new String[x]; Function&lt;Integer, String[]&gt; function = String[]::new; String[] strings = function.apply(10); System.out.println(strings.length); &#125;&#125; Stream APIJava8中有两大最为重要的改变。第一个是Lambda 表达式；另外一个则是Stream API(java.util.stream.*)。Stream是Java8中处理集合的关键抽象概念，它可以指定你希望对集合进行的操作，可以执行非常复杂的查找、过滤和映射数据等操作。使用Stream API对集合数据进行操作，就类似于使用SQL 执行的数据库查询。也可以使用Stream API来并行执行操作。简而言之，StreamAPI提供了一种高效且易于使用的处理数据的方式。 Stream的概念那么流(Stream)到底是什么呢？其实流可以理解为数据渠道，用于操作数据源(集合、数组等)所生成的元素序列。集合讲的是数据，流讲的是计算！需要注意以下几点： Stream自己不会存储元素。 Stream不会改变源对象。相反，他们会返回一个持有结果的新Stream。 Stream操作是延迟执行的。这意味着他们会等到需要结果的时候才执行。 Stream三个操作步骤1、创建Stream：一个数据源(如:集合、数组) ，获取一个流 2、中间操作：一个中间操作链，对数据源的数据进行处理 3、终止操作(终端操作)：一个终止操作，执行中间操作链，并产生结果 下面是常用的创建的操作： 123456789101112131415161718192021222324252627282930313233import java.util.ArrayList;import java.util.Arrays;import java.util.List;import java.util.stream.Stream;/* * Stream&lt;E&gt; stream() 返回一个顺序流 * Stream&lt;E&gt; parallelStream() 返回一个并行流 */public class StreamDemo &#123; public static void main(String[] args) &#123; // 1、获取流的第一种方式: stream()获取数组流 List&lt;String&gt; list = new ArrayList&lt;&gt;(); Stream&lt;String&gt; stringStream0 = list.stream(); // 2、获取流的第二种方式：Arrays的静态方法stream()获取数组流 Employee[] employeeArray = new Employee[10]; Stream&lt;Employee&gt; employeeStream = Arrays.stream(employeeArray); // 3、获取流的第三种方式：通过Stream类中的静态方法of() Stream&lt;String&gt; stringStream1 = Stream.of(\"AAA\", \"BBB\", \"CCC\"); // 4、获取流的第四种方式：创建无限流 // ①迭代的方式 Stream&lt;Integer&gt; integerStream = Stream.iterate(0, (x) -&gt; x + 2); integerStream.limit(10).forEach(System.out::println); // ②生成的方式 Stream&lt;Double&gt; doubleStream = Stream.generate(() -&gt; Math.random()); doubleStream.limit(5).forEach(System.out::println); &#125;&#125; 多个中间操作可以连接起来形成一个流水线，除非流水线上触发终止操作，否则中间操作不会执行任何的处理！而在终止操作时一次性全部处理，称为惰性求值 下面是一些中间操作： 下面是筛选重复对象、根据条件过滤对象的示例： 12345678910111213141516171819public class StreamDemo &#123; private static List&lt;Employee&gt; employeeList = Arrays.asList( new Employee(18, 5500, \"Tom\"), new Employee(28, 4500, \"Jone\"), new Employee(20, 3500, \"Jack\"), new Employee(25, 8500, \"Tim\"), new Employee(25, 8500, \"Tim\") ); public static void main(String[] args) &#123; employeeList.stream() .filter((x)-&gt; x.getAge() &gt; 20) .forEach(System.out::println); System.out.println(\"----------------\"); employeeList.stream() .distinct() .forEach(System.out::println); &#125;&#125; 那么映射又是什么意思呢？map——接收Lambda，将元素转换成其他形式或提取信息。接收一个函数作为参数， 该函数会被应用到每个元素上，并将其映射成一个新的元素。 flatMap接收一个函数作为参数，将流中的每个值都换成另一个流，然后把所有流连接成一个流。 1234567891011121314151617181920public class StreamDemo &#123; private static List&lt;Employee&gt; employeeList = Arrays.asList( new Employee(18, 5500, \"Tom\"), new Employee(28, 4500, \"Jone\"), new Employee(20, 3500, \"Jack\"), new Employee(25, 8500, \"Tim\"), new Employee(25, 8500, \"Tim\") ); public static void main(String[] args) &#123; List&lt;String&gt; stringList = Arrays.asList(\"aaa\", \"bbb\", \"ccc\", \"ddd\", \"eee\"); stringList.stream() .map(String::toUpperCase) .forEach(System.out::println); System.out.println(\"-------------------\"); employeeList.stream() .map(Employee::getName) .forEach(System.out::println); &#125; &#125; 如何用Stream排序呢？其实也很简单，在之前的讲解Lambda表达式的例子中我们已经用过了： 12345678910111213141516171819202122public class StreamDemo &#123; private static List&lt;Employee&gt; employeeList = Arrays.asList( new Employee(18, 5500, \"Tom\"), new Employee(28, 4500, \"Jone\"), new Employee(20, 3500, \"Jack\"), new Employee(25, 8500, \"Tim\"), new Employee(25, 8500, \"Tim\") ); public static void main(String[] args) &#123; employeeList.stream() .sorted((x, y) -&gt; &#123; //年龄一样按照姓名排序 if(x.getAge() == y.getAge())&#123; return x.getName().compareTo(y.getName()); &#125;else&#123; return x.getAge() - y.getAge(); &#125; &#125;) .forEach(System.out::println); &#125;&#125; 接下来看看Stream的终止操作，终止操作会从流的流水线生成结果。其结果可以是任何不是流的值，例如: List、Integer， 甚至是void。 接下来看看Stream查找与匹配： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class StreamDemo &#123; private static List&lt;Employee&gt; employeeList = Arrays.asList( new Employee(18, 5500, \"Tom\"), new Employee(28, 4500, \"Jone\"), new Employee(20, 3500, \"Jack\"), new Employee(25, 3500, \"Tim\"), new Employee(25, 3500, \"Tim\") ); public static void main(String[] args) &#123; // 判断是不是所有员工工资都是3500 boolean match = employeeList.stream() .allMatch((e) -&gt; e.getSalary() == 3500); System.out.println(match); // 判断是不是至少有一个员工姓名是Tim boolean timExist = employeeList.stream() .anyMatch((e) -&gt; e.getName().equals(\"Tim\")); System.out.println(timExist); // 判断是否存在员工年龄小于20 boolean ageMatch = employeeList.stream() .noneMatch((e) -&gt; e.getAge() &lt; 20); System.out.println(ageMatch); // 根据员工工资排序，并得到第一个结果 Optional&lt;Employee&gt; employee = employeeList.stream() .sorted(Comparator.comparingInt(Employee::getSalary)) .findFirst(); System.out.println(employee.get()); // 获取员工工资最高的员工信息 Optional&lt;Employee&gt; maxEmployee = employeeList.stream() .max(Comparator.comparingInt(Employee::getSalary)); System.out.println(maxEmployee.get()); // 获取员工最低工资 Optional&lt;Integer&gt; minSalary = employeeList.stream() .map(Employee::getSalary) .min(Integer::compareTo); System.out.println(minSalary); &#125;&#125; 接下来看看Stream的归约，归约可以将流中元素反复结合起来，得到一个值。 12345678910111213141516171819202122public class StreamDemo &#123; private static List&lt;Employee&gt; employeeList = Arrays.asList( new Employee(18, 5500, \"Tom\"), new Employee(28, 4500, \"Jone\"), new Employee(20, 3500, \"Jack\"), new Employee(25, 3500, \"Tim\"), new Employee(25, 3500, \"Tim\") ); public static void main(String[] args) &#123; List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9); Integer sum = list.stream() .reduce(0, (x, y) -&gt; x + y); System.out.println(sum); System.out.println(\"------------------------\"); Optional&lt;Integer&gt; salarySum = employeeList.stream() .map(Employee::getSalary) .reduce(Integer::sum); System.out.println(salarySum.get()); &#125;&#125; 备注: map和reduce的连接通常称为map-reduce 模式，因Google用它来进行网络搜索而出名。 接下来看看收集，Collector接口中方法的实现决定了如何对流执行收集操作(如收集到List、Set、Map)。但是Collectors 实用类提供了很多静态方法，可以方便地创建常见收集器实例，具体方法与实例如下表: 1234567891011121314151617181920212223242526272829303132333435363738394041public class StreamDemo &#123; private static List&lt;Employee&gt; employeeList = Arrays.asList( new Employee(18, 5500, \"Tom\"), new Employee(28, 4500, \"Jone\"), new Employee(20, 3500, \"Jack\"), new Employee(25, 3500, \"Tim\"), new Employee(25, 3500, \"Tim\") ); public static void main(String[] args) &#123; // 收集员工的姓名到List中 List&lt;String&gt; nameList = employeeList.stream() .map(Employee::getName) .collect(Collectors.toList()); nameList.forEach(System.out::println); System.out.println(\"---------------------\"); // 收集员工的姓名到Set中 Set&lt;String&gt; nameSet = employeeList.stream() .map(Employee::getName) .collect(Collectors.toSet()); nameSet.forEach(System.out::println); // 收集员工的姓名到其他结构中 LinkedHashSet&lt;String&gt; linkedHashSet = employeeList.stream() .map(Employee::getName) .collect(Collectors.toCollection(LinkedHashSet::new)); linkedHashSet.forEach(System.out::println); // 收集员工的工资平均值 Double averageSalary = employeeList.stream() .collect(Collectors.averagingInt(Employee::getSalary)); System.out.println(averageSalary); // 收集员工工资总和 Long summarySalary = employeeList.stream() .collect(Collectors.summingLong(Employee::getSalary)); System.out.println(summarySalary); &#125;&#125; 并行流与顺序流并行流就是把一个内容分成多个数据块，并用不同的线程分别处理每个数据块的流。Java8中将并行进行了优化，我们可以很容易的对数据进行并行操作。Stream API可以声明性地通过parallel() 与sequential()在并行流与顺序流之间进行切换。 Fork/Join框架：就是在必要的情况下，将一个大任务，进行拆分(fork)成若千个小任务(拆到不可再拆时)，再将一个个的小任务运算的结果进行join汇总。关于Fork/Join框架可以看我之前的一篇博客《 ForkJoin框架与读写锁 》 早在JDK1.7的时候Fork/Join框架就有了，但是使用起来稍微复杂。Fork/Join框架采用“工作窃取” 模式(work-stealing)当执行新的任务时它可以将其拆分分成更小的任务执行，并将小任务加到线程队列中，然后再从一个随机线程的队列中偷一个并把它放在自己的队列中。相对于一般的线程池实现，fork/join框架的优势体现在对其中包含的任务的处理方式上，在一般的线程池中，如果一个线程正在执行的任务由于某些原因无法继续运行，那么该线程会处于等待状态。而在fork/join框架实现中，如果某个子问题由于等待另外一个子问题的完成而无法继续运行。那么处理该子问题的线程会主动寻找其他尚未运行的子问题来执行。这种方式减少了线程的等待时间，提高了性能。 123456789101112131415161718192021222324252627282930313233343536373839import java.util.concurrent.RecursiveTask;// 一个并行计算的示例public class ForkJoinCalculate extends RecursiveTask&lt;Long&gt; &#123; private static final long serialVersionUID = -2761358406351641206L; public ForkJoinCalculate(long start, long end) &#123; this.start = start; this.end = end; &#125; // 范围 private long start; private long end; // 临界值 private static final long THRESHOLD = 10000; @Override protected Long compute() &#123; long length = end - start; if(length &lt;= THRESHOLD)&#123; long sum = 0; for (long i = start; i &lt;= end; i++) &#123; sum += i; &#125; return sum; &#125;else&#123; // 拆分为子任务 long mid = (end - start) / 2 + start; ForkJoinCalculate calculateLeft = new ForkJoinCalculate(start, mid); calculateLeft.fork(); ForkJoinCalculate calculateRight = new ForkJoinCalculate(mid + 1, end); calculateRight.fork(); return calculateLeft.join() + calculateRight.join(); &#125; &#125;&#125; 测试性能： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package newjdk8.forkjoin;import java.time.Duration;import java.time.Instant;import java.util.concurrent.ForkJoinPool;import java.util.concurrent.ForkJoinTask;import java.util.stream.LongStream;public class TestForkJoinCalculate &#123; public static void main(String[] args) &#123; // 计算500亿的累加 long n = 50000000000L; forkJoinTest(n); //8723毫秒 oneThreadCalc(n); //14337毫秒 streamCalc(n); //4375毫秒 &#125; private static void streamCalc(long n) &#123; Instant start = Instant.now(); long reduce = LongStream.range(0, n) .parallel() .reduce(0, Long::sum); System.out.println(reduce); Instant end = Instant.now(); System.out.println(\"Stream \" + Duration.between(start, end).toMillis()); &#125; private static void oneThreadCalc(long n) &#123; Instant start = Instant.now(); long sum = 0L; for (long i = 0; i &lt;= n; i++) &#123; sum += i; &#125; System.out.println(sum); Instant end = Instant.now(); System.out.println(\"单线程 \" + Duration.between(start, end).toMillis()); &#125; private static void forkJoinTest(long n) &#123; Instant start = Instant.now(); ForkJoinPool pool = new ForkJoinPool(); ForkJoinTask&lt;Long&gt; forkJoinTask = new ForkJoinCalculate(0, n); Long sum = pool.invoke(forkJoinTask); System.out.println(sum); Instant end = Instant.now(); System.out.println(\"Fork/Join \" + Duration.between(start, end).toMillis()); &#125;&#125; 注意：parallel并行流底层就是使用了Fork/Join框架 Option尽量避免空指针Optional&lt;T&gt;类(java. util. Optional)是一个容器类，代表一个值存在或不存在，原来用null表示一个值不存在，现在Optional可以更好的表达这个概念。并且可以避免空指针异常。 Option这种容器在SpringDataJpa中经常用到，所以在此不再记述。 接口中的默认方法与静态方法接口中的默认方法Java 8中允许接口中包含具有具体实现的方法，该方法称为默认方法，默认方法使用default 关键字修饰。我觉得JDK8出现了函数式接口，为了兼容JDK7所以出现了default修饰的接口级别的默认方法。 12345678910111213141516public interface MyFunc&#123; default String getName()&#123; return \"HelloWorld\"; &#125;&#125;public class MyCLass implements MyFunc &#123; &#125;public class Test &#123; public static void main(String[] args) &#123; MyFunc myFunc = new MyCLass(); System.out.println(myFunc.getName()); // HelloWorld &#125;&#125; 接口默认方法的类优先原则：若一个接口中定义了一个默认方法，而另外一个父类或接口中又定义了一个同名的方法时： 选择父类中的方法。如果一个父类提供了具体的实现，那么接口中具有相同名称和参数的默认方法会被忽略。 接口冲突。如果一个父接口提供一个默认方法，而另一个接口也提供了一个具有相同名称和参数列表的方法(不管方法是否是默认方法)，那么必须覆盖该方法来解决冲突。 123456789101112131415161718public interface MyFunc&#123; default String getName()&#123; return \"HelloWorld\"; &#125;&#125;public class MyCLass implements MyFunc &#123; public String getName()&#123; return \"MyClass\"; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; MyFunc myFunc = new MyCLass(); System.out.println(myFunc.getName()); // MyClass &#125;&#125; 那么如果有两个接口应该怎么办呢？ MyFunc.java 12345public interface MyFunc&#123; default String getName()&#123; return \"HelloWorld\"; &#125;&#125; MyFunc2.java 12345public interface MyFunc2 &#123; default String getName()&#123; return \"HelloWorld2\"; &#125;&#125; MyClass.java，因为不知道该用谁的默认方法，所以报错 123public class MyCLass implements MyFunc, MyFunc2 &#123; //Error 因为不知道该用谁的默认方法&#125; MyCLass.Java ，以下两种解决方案： 12345678910111213public class MyCLass implements MyFunc, MyFunc2 &#123; @Override public String getName() &#123; // 1、要么就指定用谁的 return MyFunc.super.getName(); &#125; @Override public String getName() &#123; // 2、要么就实现自己的 return \"MyClass\"; &#125;&#125; 接口中的静态方法这个其实没啥好说的，就是接口中允许存在静态方法： MyFunc.java 12345public interface MyFunc&#123; static void show()&#123; System.out.println(\"Show Static Method\"); &#125;&#125; Test.java 12345public class Test &#123; public static void main(String[] args) &#123; MyFunc.show(); &#125;&#125; 新时间日期APILocalDate、LocalTime、 LocalDateTime 类的实例是不可变的对象，分别表示使用ISO-8601日历系统的日期、时间、日期和时间。它们提供了简单的日期或时间，并不包含当前的时间信息，且也不包含与时区相关的信息。 比如我们比较常用的SimpleDateFormat，这个我们经常使用的类存在线程安全问题： 1234567891011121314151617181920212223import java.text.SimpleDateFormat;import java.util.ArrayList;import java.util.Date;import java.util.List;import java.util.concurrent.*;public class TestSimpleDateFormat &#123; public static void main(String[] args) throws Exception &#123; SimpleDateFormat format = new SimpleDateFormat(\"yyyy-MM-dd\"); ExecutorService executorService = Executors.newFixedThreadPool(10); Callable&lt;Date&gt; callable = () -&gt; format.parse(\"2020-04-17\"); List&lt;Future&lt;Date&gt;&gt; futureList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 20; i++) &#123; futureList.add(executorService.submit(callable)); &#125; for(Future&lt;Date&gt; dateFuture: futureList)&#123; System.out.println(dateFuture.get()); &#125; executorService.shutdown(); &#125;&#125; 我们可以用ThreadLocal，DateFormatThreadLocal.java 1234567891011121314151617import java.text.DateFormat;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;public class DateFormatThreadLocal &#123; private static final ThreadLocal&lt;DateFormat&gt; df = new ThreadLocal&lt;DateFormat&gt;()&#123; @Override protected DateFormat initialValue() &#123; return new SimpleDateFormat(\"yyy-MM-dd\"); &#125; &#125;; public static Date convert(String source) throws ParseException &#123; return df.get().parse(source); &#125;&#125; 这样的话我们只需要按照如下方式使用即可： 1234567891011121314151617181920212223import java.text.DateFormat;import java.text.SimpleDateFormat;import java.util.ArrayList;import java.util.Date;import java.util.List;import java.util.concurrent.*;public class TestSimpleDateFormat &#123; public static void main(String[] args) throws Exception &#123; ExecutorService executorService = Executors.newFixedThreadPool(10); Callable&lt;Date&gt; callable = () -&gt; DateFormatThreadLocal.convert(\"2020-04-17\"); List&lt;Future&lt;Date&gt;&gt; futureList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 20; i++) &#123; futureList.add(executorService.submit(callable)); &#125; for(Future&lt;Date&gt; dateFuture: futureList)&#123; System.out.println(dateFuture.get()); &#125; executorService.shutdown(); &#125;&#125; 现在，我们不需要使用ThreadLocal来辅助了，直接用LocalDate这个线程安全的工具来搞定，就和String一样，线程安全，无论做出怎么样的改变都会产生一个新的实例对象： 12345678910111213141516171819202122232425262728import java.text.DateFormat;import java.text.SimpleDateFormat;import java.time.LocalDate;import java.time.format.DateTimeFormatter;import java.util.ArrayList;import java.util.Date;import java.util.List;import java.util.concurrent.*;public class TestSimpleDateFormat &#123; public static void main(String[] args) throws Exception &#123; ExecutorService executorService = Executors.newFixedThreadPool(10); //DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ISO_LOCAL_DATE; DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\"); Callable&lt;LocalDate&gt; callable = () -&gt; LocalDate.parse(\"2020-04-17\", dateTimeFormatter); List&lt;Future&lt;LocalDate&gt;&gt; futureList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 20; i++) &#123; futureList.add(executorService.submit(callable)); &#125; for(Future&lt;LocalDate&gt; dateFuture: futureList)&#123; System.out.println(dateFuture.get()); &#125; executorService.shutdown(); &#125;&#125; 下面是这些API的使用示例： Duration：用于计算两个时间间隔。Period：用于计算两个日期间隔。 Instant时间戳用于时间戳的运算。它是以Unix元年(传统的设定为UTC时区1970年1月1日午夜时分)开始所经历的描述进行运算。 TemporalAdjuster：时间校正器。有时我们可能需要获取例如：将日期调整到下个周日等操作。TemporalAdjusters：该类通过静态方法提供了大量的常用TemporalAdjuster的实现。 java.time.format.DateTimeFormatter类：该类提供了三种格式化方法： 预定义的标准格式 语言环境相关的格式 自定义的格式 Java8中加入了对时区的支持，带时区的时间为分别为：ZonedDate、ZonedTime、 ZonedDateTime 其中每个时区都对应着ID，地区ID都为{区 域}/{城市}的格式，例如: Asia/Shanghai等 Zoneld：该类中包含了所有的时区信息 getAvailableZonelds()：可以获取所有时区时区信息 of(id)：用指定的时区信息获取Zoneld对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146import java.time.DayOfWeek;import java.time.Duration;import java.time.Instant;import java.time.LocalDate;import java.time.LocalDateTime;import java.time.OffsetDateTime;import java.time.Period;import java.time.ZoneId;import java.time.ZoneOffset;import java.time.ZonedDateTime;import java.time.format.DateTimeFormatter;import java.time.temporal.TemporalAdjusters;import java.util.Set;import org.junit.Test;public class TestLocalDateTime &#123; // ZonedDate、ZonedTime、ZonedDateTime：带时区的时间或日期 @Test public void test7()&#123; LocalDateTime ldt = LocalDateTime.now(ZoneId.of(\"Asia/Shanghai\")); System.out.println(ldt); ZonedDateTime zdt = ZonedDateTime.now(ZoneId.of(\"US/Pacific\")); System.out.println(zdt); &#125; @Test public void test6()&#123; Set&lt;String&gt; set = ZoneId.getAvailableZoneIds(); set.forEach(System.out::println); &#125; // DateTimeFormatter : 解析和格式化日期或时间 @Test public void test5()&#123; //DateTimeFormatter dtf = DateTimeFormatter.ISO_LOCAL_DATE; DateTimeFormatter dtf = DateTimeFormatter.ofPattern(\"yyyy年MM月dd日 HH:mm:ss E\"); LocalDateTime ldt = LocalDateTime.now(); String strDate = ldt.format(dtf); System.out.println(strDate); LocalDateTime newLdt = ldt.parse(strDate, dtf); System.out.println(newLdt); &#125; // TemporalAdjuster : 时间校正器 @Test public void test4()&#123; LocalDateTime ldt = LocalDateTime.now(); System.out.println(ldt); LocalDateTime ldt2 = ldt.withDayOfMonth(10); System.out.println(ldt2); LocalDateTime ldt3 = ldt.with(TemporalAdjusters.next(DayOfWeek.SUNDAY)); System.out.println(ldt3); //自定义：下一个工作日 LocalDateTime ldt5 = ldt.with((l) -&gt; &#123; LocalDateTime ldt4 = (LocalDateTime) l; DayOfWeek dow = ldt4.getDayOfWeek(); if(dow.equals(DayOfWeek.FRIDAY))&#123; return ldt4.plusDays(3); &#125;else if(dow.equals(DayOfWeek.SATURDAY))&#123; return ldt4.plusDays(2); &#125;else&#123; return ldt4.plusDays(1); &#125; &#125;); System.out.println(ldt5); &#125; // Duration : 用于计算两个“时间”间隔 // Period : 用于计算两个“日期”间隔 @Test public void test3()&#123; Instant ins1 = Instant.now(); System.out.println(\"--------------------\"); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; &#125; Instant ins2 = Instant.now(); System.out.println(\"所耗费时间为：\" + Duration.between(ins1, ins2)); System.out.println(\"----------------------------------\"); LocalDate ld1 = LocalDate.now(); LocalDate ld2 = LocalDate.of(2011, 1, 1); Period pe = Period.between(ld2, ld1); System.out.println(pe.getYears()); System.out.println(pe.getMonths()); System.out.println(pe.getDays()); &#125; // Instant : 时间戳（使用 Unix元年1970年1月1日 00:00:00 所经历的毫秒值） @Test public void test2()&#123; Instant ins = Instant.now(); //默认使用 UTC 时区 System.out.println(ins); OffsetDateTime odt = ins.atOffset(ZoneOffset.ofHours(8)); System.out.println(odt); System.out.println(ins.getNano()); Instant ins2 = Instant.ofEpochSecond(5); System.out.println(ins2); &#125; // LocalDate、LocalTime、LocalDateTime @Test public void test1()&#123; LocalDateTime ldt = LocalDateTime.now(); System.out.println(ldt); LocalDateTime ld2 = LocalDateTime.of(2016, 11, 21, 10, 10, 10); System.out.println(ld2); LocalDateTime ldt3 = ld2.plusYears(20); System.out.println(ldt3); LocalDateTime ldt4 = ld2.minusMonths(2); System.out.println(ldt4); System.out.println(ldt.getYear()); System.out.println(ldt.getMonthValue()); System.out.println(ldt.getDayOfMonth()); System.out.println(ldt.getHour()); System.out.println(ldt.getMinute()); System.out.println(ldt.getSecond()); &#125;&#125; 重复注解与类型注解Java 8对注解处理提供了两点改进：可重复的注解及可用于类型的注解。 假设现在我有如下注解： 1234567891011import static java.lang.annotation.ElementType.*;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Target(&#123;TYPE, FIELD, METHOD, PARAMETER, CONSTRUCTOR, LOCAL_VARIABLE&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface MyAnnotation &#123; String value() default \"Tim\";&#125; 测试如下，像下面这种重复注解是不被允许的： 123456789package newjdk8.dateapi.annotation;public class TestAnnotation &#123; @MyAnnotation(\"AAA\") @MyAnnotation(\"BBB\") // Error! public void show()&#123; &#125;&#125; 那么如何解决这个问题呢？我们还需要定义一个注解容器： MyAnnotations.java 123456789101112import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;import static java.lang.annotation.ElementType.*;import static java.lang.annotation.ElementType.LOCAL_VARIABLE;@Target(&#123;TYPE, FIELD, METHOD, PARAMETER, CONSTRUCTOR, LOCAL_VARIABLE&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface MyAnnotations &#123; MyAnnotation[] value();&#125; MyAnnotation.java 1234567891011121314import static java.lang.annotation.ElementType.*;import java.lang.annotation.Repeatable;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Repeatable(MyAnnotations.class) // 指定容器@Target(&#123;TYPE, FIELD, METHOD, PARAMETER, CONSTRUCTOR, LOCAL_VARIABLE&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface MyAnnotation &#123; String value() default \"Tim\";&#125; TestAnnotation.java 123456789101112131415161718import java.lang.reflect.Method;public class TestAnnotation &#123; public static void main(String[] args) throws NoSuchMethodException &#123; Class&lt;TestAnnotation&gt; annotationClass = TestAnnotation.class; Method method = annotationClass.getMethod(\"show\"); MyAnnotation[] myAnnotations = method.getAnnotationsByType(MyAnnotation.class); for (MyAnnotation myAnnotation: myAnnotations)&#123; System.out.println(myAnnotation.value()); &#125; &#125; @MyAnnotation(\"Hello\") @MyAnnotation(\"World\") public void show()&#123; &#125;&#125; 什么是类型注解呢？Target支持TYPE_PARAMETER，我们通过源码也是可以看到起始于JDK1.8 12345678910111213@Repeatable(MyAnnotations.class) // 指定容器@Target(&#123;TYPE, FIELD, METHOD, PARAMETER, CONSTRUCTOR, LOCAL_VARIABLE, TYPE_PARAMETER&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface MyAnnotation &#123; String value() default \"Tim\";&#125;/** * Type parameter declaration * * @since 1.8 */TYPE_PARAMETER 那么就可以对类型进行注解： 12345678public class TestAnnotation &#123; @MyAnnotation(\"Hello\") @MyAnnotation(\"World\") // 可以注解类型 public void show(@MyAnnotation(\"abc\") String str)&#123; &#125;&#125;","updated":"2020-04-17T04:07:16.105Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"Stream","slug":"Stream","permalink":"https://zouchanglin.cn/tags/Stream/"},{"name":"Lambda","slug":"Lambda","permalink":"https://zouchanglin.cn/tags/Lambda/"}]},{"title":"堆的实现及其应用","date":"2020-04-13T10:00:00.000Z","path":"2020/04/13/堆的实现及其应用/","text":"本篇文章记述的是堆排序，这个名字看起来好像又要介绍一个排序算法，但是排序算法是次要的，主要的是一个数据结构——堆。堆排序问题就是堆这种数据结构所衍生出来的一个应用，我们先了解一下优先队列的概念。普通的队列就是满足先进先出、后进后出的一个结构。那么优先级队列呢？出队顺序和入队顺序无关，和优先级相关，这就比如在医院看病，肯定是急诊病人优先看病。 优先级队列的应用在操作系统中就会用到优先级队列，操作系统要同时执行多个任务，实际上操作系统是将CPU的执行周期划分为时间片，在每个时间片里只能执行一个任务，那么执行哪个任务呢？那就需要根据任务的优先级动态地选择优先级最高的任务来执行。 如何实现一个优先级队列呢？方法有很多，但是用堆来实现是最直接、最高效的。这是因为，堆和优先级队列非常相似。一个堆就可以看作一个优先级队列。很多时候，它们只是概念上的区分而已。往优先级队列中插入一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。 优先级队列的应用场景非常多。赫夫曼编码、图的最短路径、最小生成树算法很多数据结构和算法都要依赖于优先级队列。 堆的定义及其特点堆是一种特殊的树。我们现在就来看看，什么样的树才是堆。我罗列了两点要求，只要满足这两点，它就是一个堆。 堆是一个完全二叉树； 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。 第一点，堆必须是一个完全二叉树。完全二叉树要求，除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列。 第二点，堆中的每个节点的值必须大于等于（或者小于等于）其子树中每个节点的值。实际上，我们还可以换一种说法，堆中每个节点的值都大于等于（或者小于等于）其左右子节点的值。这两种表述是等价的。 对于每个节点的值都大于等于子树中每个节点值的堆，我们叫作“大堆”。对于每个节点的值都小于等于子树中每个节点值的堆，我们叫作“小堆”。 对于上图，1和2都是大堆；3是个小堆，而4不是堆。 完全二叉树比较适合用数组来存储。用数组来存储完全二叉树是非常节省存储空间的。因为我们不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。 如下图所示： 我们不难发现，这样的结构蕴含的规律是：左孩子在数组中的坐标是父节点的二倍，而右孩子在数组中的坐标是父节点的二倍加一。但是数组的索引却是从0开始的，堆的一个经典的实现就是数组0号位置空着，则parent (i) = i / 2(这里的除法是计算机除法，即取整)，left child (i) = 2 * i，right child (i) = 2 * i +1。 堆的具体代码实现根据上述的堆这种数据结构的，我们可以先实现下面的基础框架代码： 1234567891011121314151617181920public class MaxHeap &#123; private int[] data; //堆里有多少元素 private int count; //因为0号位置不使用，所以capacity + 1 public MaxHeap(int capacity) &#123; data = new int[capacity + 1]; count = 0; &#125; public int size()&#123; return count; &#125; public boolean isEmpty()&#123; return count == 0; &#125;&#125; 接下来需要关注的焦点是如何向上调堆，我们在向堆中添加新的元素的时候，其实是向数组的末尾添加了一个新的元素，但是往堆中插入一个元素后，我们需要继续满足堆的两个特性。如果我们把新插入的元素直接放到堆的最后，是不是不符合堆的特性了？ 于是我们就需要进行调整，让其重新满足堆的特性，这个过程叫做堆化： 堆的调整方式有两种：向上调堆和向下调堆！在这里我们可以先看看向上调堆。 向上调堆（插入元素）其实向上调堆的过程比较简单，那就是逐步和自己的父节点进行比较，如果不满足规则就交换即可，如下图： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class MaxHeap &#123; protected int[] data; //堆里有多少元素 protected int count; //堆的容量 protected int capacity; //因为0号位置不使用，所以capacity + 1 public MaxHeap(int capacity) &#123; data = new int[capacity + 1]; count = 0; this.capacity = capacity; &#125; //获取现存元素个数 public int size()&#123; return count; &#125; //判断是否为空 public boolean isEmpty()&#123; return count == 0; &#125; //插入数据 public void insert(int item)&#123; //判断容量知否超出 if(count + 1 &gt;= capacity)&#123; //开始扩容 resize(); &#125; //先存储到末尾 data[count + 1] = item; count++; //开始调堆 shiftUp(count); &#125; //向上调堆 private void shiftUp(int k) &#123; while(k &gt; 1 &amp;&amp; data[k / 2] &lt; data[k])&#123; swap(k/2, k); k /= 2; &#125; &#125; //交换对应两个位置的值 private void swap(int i, int j)&#123; int tmp = data[i]; data[i] = data[j]; data[j] = tmp; &#125; //扩充容量 private void resize() &#123; int[] newData = new int[capacity * 2]; System.arraycopy(data, 0, newData, 0, count); data = newData; capacity *= 2; &#125;&#125; 上面就是这个堆的实现，而且这个堆拥有扩容的功能。为了在控制台打印方便观察，实现一个打印堆的功能(数字太多控制台容易乱掉，所以限定在100个元素之内)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class PrintableMaxHeap extends MaxHeap &#123; public PrintableMaxHeap(int capacity)&#123; super(capacity); &#125; // 以树状打印整个堆结构 public void treePrint()&#123; if( size() &gt;= 100 )&#123; System.out.println(\"This print function can only work for less than 100 integer\"); return; &#125; System.out.println(\"The max heap size is: \" + size()); System.out.println(\"Data in the max heap: \"); for( int i = 1 ; i &lt;= size() ; i ++ )&#123; // 我们的print函数要求堆中的所有整数在[0, 100)的范围内 assert data[i] &gt;= 0 &amp;&amp; data[i] &lt; 100; System.out.print(data[i] + \" \"); &#125; System.out.println(); System.out.println(); int n = size(); int maxLevel = 0; int numberPerLevel = 1; while( n &gt; 0 )&#123; maxLevel += 1; n -= numberPerLevel; numberPerLevel *= 2; &#125; int maxLevelNumber = (int)Math.pow(2, maxLevel-1); int curTreeMaxLevelNumber = maxLevelNumber; int index = 1; for( int level = 0 ; level &lt; maxLevel ; level ++ )&#123; String line1 = new String(new char[maxLevelNumber*3-1]).replace('\\0', ' '); int curLevelNumber = Math.min(count-(int)Math.pow(2,level)+1,(int)Math.pow(2,level)); boolean isLeft = true; for( int indexCurLevel = 0 ; indexCurLevel &lt; curLevelNumber ; index ++ , indexCurLevel ++ )&#123; line1 = putNumberInLine(data[index] , line1 , indexCurLevel , curTreeMaxLevelNumber*3-1 , isLeft ); isLeft = !isLeft; &#125; System.out.println(line1); if( level == maxLevel - 1 ) break; String line2 = new String(new char[maxLevelNumber*3-1]).replace('\\0', ' '); for( int indexCurLevel = 0 ; indexCurLevel &lt; curLevelNumber ; indexCurLevel ++ ) line2 = putBranchInLine( line2 , indexCurLevel , curTreeMaxLevelNumber*3-1 ); System.out.println(line2); curTreeMaxLevelNumber /= 2; &#125; &#125; private String putNumberInLine( Integer num, String line, int indexCurLevel, int curTreeWidth, boolean isLeft)&#123; int subTreeWidth = (curTreeWidth - 1) / 2; int offset = indexCurLevel * (curTreeWidth+1) + subTreeWidth; assert offset + 1 &lt; line.length(); if( num &gt;= 10 ) line = line.substring(0, offset+0) + num.toString() + line.substring(offset+2); else&#123; if( isLeft) line = line.substring(0, offset+0) + num.toString() + line.substring(offset+1); else line = line.substring(0, offset+1) + num.toString() + line.substring(offset+2); &#125; return line; &#125; private String putBranchInLine( String line, int indexCurLevel, int curTreeWidth)&#123; int subTreeWidth = (curTreeWidth - 1) / 2; int subSubTreeWidth = (subTreeWidth - 1) / 2; int offsetLeft = indexCurLevel * (curTreeWidth+1) + subSubTreeWidth; assert offsetLeft + 1 &lt; line.length(); int offsetRight = indexCurLevel * (curTreeWidth+1) + subTreeWidth + 1 + subSubTreeWidth; assert offsetRight &lt; line.length(); line = line.substring(0, offsetLeft+1) + \"/\" + line.substring(offsetLeft+2); line = line.substring(0, offsetRight) + \"\\\\\" + line.substring(offsetRight+1); return line; &#125;&#125; 接下来测试一下是否成功： 123456789public class MaxHeapTest &#123; public static void main(String[] args) &#123; PrintableMaxHeap maxHeap = new PrintableMaxHeap(10); for (int i = 0; i &lt; 15; i++) &#123; maxHeap.insert((int)(Math.random() * 100)); &#125; maxHeap.treePrint(); &#125;&#125; 可以看出，我们不断插入数据的时候其实就是不断调堆的过程。 向下调堆（取出元素）取出堆顶元素，任然需要维持堆的特性，所以我们只把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是从上往下的堆化方法，也叫做向下调堆。 所以整个堆的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public class MaxHeap &#123; protected int[] data; //堆里有多少元素 protected int count; //堆的容量 protected int capacity; //因为0号位置不使用，所以capacity + 1 public MaxHeap(int capacity) &#123; data = new int[capacity + 1]; count = 0; this.capacity = capacity; &#125; //获取现存元素个数 public int size()&#123; return count; &#125; //判断是否为空 public boolean isEmpty()&#123; return count == 0; &#125; //插入数据 public void insert(int item)&#123; //判断容量知否超出 if(count + 1 &gt;= capacity)&#123; //开始扩容 resize(); &#125; //先存储到末尾 data[count + 1] = item; count++; //开始向上调堆 shiftUp(count); &#125; //向上调堆 private void shiftUp(int k) &#123; while(k &gt; 1 &amp;&amp; data[k / 2] &lt; data[k])&#123; swap(k/2, k); k /= 2; &#125; &#125; //取出数据 public int extractMax()&#123; if(count == 0) throw new RuntimeException(\"Heap is null\"); int ret = data[1]; swap(1, count); count--; //开始向下调堆 shiftDown(1); return ret; &#125; //向下调堆 private void shiftDown(int k) &#123; while (2 * k &lt;= count)&#123; int j = 2 * k; //在此轮循环中，data[k]和data[j]交换位置 if(j + 1 &lt;= count &amp;&amp; data[j+1] &gt; data[j])&#123; j++; &#125; if(data[k] &gt;= data[j])&#123; break; &#125; swap(k, j); k = j; &#125; &#125; //交换对应两个位置的值 private void swap(int i, int j)&#123; int tmp = data[i]; data[i] = data[j]; data[j] = tmp; &#125; //扩充容量 private void resize() &#123; int[] newData = new int[capacity * 2]; System.arraycopy(data, 0, newData, 0, count); data = newData; capacity *= 2; &#125;&#125; 一个包含n个节点的完全二叉树，树的高度不会超过logn。堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比，也就是O(logn)。插入数据和删除堆顶元素的主要逻辑就是堆化，所以往堆中插入一个元素和删除堆顶元素的时间复杂度都是O(logn)。 堆排序与heapify建堆我们通过堆的插入操作把数组中的元素逐个插入到堆中，然后逐个取出堆顶元素防区数组中（如果是大堆从后往前放置即可）。堆排序不是稳定的排序算法，因为在排序的过程，存在将堆的最后一个节点跟堆顶节点互换的操作，所以就有可能改变值相同数据的原始相对顺序。 1234567891011public class HeapSort &#123; public static void heapSort(int[] arr)&#123; MaxHeap maxHeap = new MaxHeap(arr.length); for (int i = 0; i &lt; arr.length; i++) &#123; maxHeap.insert(arr[i]); &#125; for (int i = arr.length - 1; i &gt;= 0; i--) &#123; arr[i] = maxHeap.extractMax(); &#125; &#125;&#125; 我们进行排序的时候，首先得把数组中的元素逐个插入到堆中，这种建堆思路的处理过程是从前往后处理数组数据，并且每个数据插入堆中时，都是从下往上堆化。但是有没有一种无需插入操作，直接把数组变成堆的方法呢？其实是有的： 因为叶子节点往下堆化只能自己跟自己比较，所以我们直接从第一个非叶子节点开始，依次堆化就行了。 非叶子节点其实很容易找出来，元素个数除以二即是第一个非叶子节点，如下图9个元素，4号即是第一个非叶子节点： 所以我们需要加入这样一个构造方法： 12345678910111213141516171819public class MaxHeap &#123; ... public MaxHeap(int[] arr)&#123; data = new int[arr.length + 1]; capacity = arr.length + 1; for (int i = 0; i &lt; arr.length; i++) &#123; data[i + 1] = arr[i]; &#125; count = arr.length; //从第一个不是叶子节点的位置开始 for (int i = count / 2; i &gt;= 1; i--) &#123; shiftDown(i); &#125; &#125; ...&#125; 将n个元素逐个插入到堆中，这个操作的时间复杂度是O(nlogn)，而heapify建堆的时间复杂度为O(n)。 原地堆排序其实堆排序完全可以变成一个原地排序算法，直接在数组上进行。因为堆的经典实现就是从1号位置开始，但是我们现在要实现的是原地排序的算法，规律完全相同，只是规律的表达式稍微有所不同。因为在上面的堆排序算法中，都需要先将数组中的元素放到堆中，然后再把堆中的元素取出来。整个程序中又额外的开辟了n个空间，事实上我们通过上面的理论方法，完全可以使一个数组在原地完成堆排序，而不需要任何的额外空间： 我们可以应用之前讲到的堆化（heapify）是我们的数组构建成一个最大堆。在这个最大堆中第一个元素就是这个数组的最大值： 最后一个非叶子节点的索引：(count - 2)/ 2、 parent(i) = (i - 1) / 2、left child (i) = 2 * i +1、right child (i) = 2 * i +2 1234567891011121314151617181920212223242526272829303132public class HeapSort &#123; public static void heapSort_03(int[] arr)&#123; //heapify for (int i = (arr.length - 1)/2; i &gt;= 0; i--) &#123; shiftDown(arr, arr.length, i); &#125; for (int i = arr.length - 1; i &gt; 0; i--) &#123; swap(arr, 0, i); shiftDown(arr, i, 0); &#125; &#125; private static void shiftDown(int[] arr, int length, int k) &#123; while (2 * k + 1 &lt; length)&#123; int j = 2 * k + 1; if(j + 1 &lt; length &amp;&amp; arr[j+1] &gt; arr[j])&#123; j++; &#125; if(arr[k] &gt;= arr[j])&#123; break; &#125; swap(arr, k, j); k = j; &#125; &#125; private static void swap(int[] arr, int i, int j) &#123; int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp; &#125;&#125;","updated":"2020-04-24T05:05:12.894Z","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序算法","slug":"排序算法","permalink":"https://zouchanglin.cn/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"name":"堆","slug":"堆","permalink":"https://zouchanglin.cn/tags/%E5%A0%86/"},{"name":"二叉树","slug":"二叉树","permalink":"https://zouchanglin.cn/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"}]},{"title":"注解的原理与实现","date":"2020-04-12T10:00:00.000Z","path":"2020/04/12/注解的原理与实现/","text":"注解这个东西自从SpringBoot以来一直是Java开发者们必备的生存技巧呀，我们平时几乎大部分时间都是面向注解编程，通过注解我们可以节约大量的时间。用过了这么多的注解，那么我们否有关注过注解的实现原理呢？所以本篇文章主要是讲述注解的有关操作，自己实现一个注解来体会注解的实现原理，注解也不是特别高深的东西，掌握了自然就明白了。 注解的基本原理注解本来的意思就是用来做标注用：可以在类、字段变量、方法、接口等位置进行一个特殊的标记，为后续做一些诸如: 代码生成、数据校验、资源整合等工作做铺垫。所以注解就是做标记用的，注解一旦对代码标注完成，后续就可以结合Java强大的反射机制，在运行时动态地获取到注解的标注信息，从而可以执行很多其他逻辑，完成我们想要的自动化工作。所以，反射机制很重要。 注解的使用示例假设我们现在有个Person类，这个Person类要当做参数传入，我们要对参数进行校验： 1234567public class Person &#123; private Integer id; private String name; private Integer age; //Getter and Setter&#125; 如果没有注解，那么我们就需要写这样一长串的if else校验： 12345678910111213141516171819202122232425public String addPerson(Person person)&#123; if(person == null)&#123; return \"参数为空\"; &#125; if(person.getId() == null || \"\".equals(person.getId()))&#123; return \"Person's id is null\"; &#125; if(person.getName() == null || \"\".equals(person.getName()))&#123; return \"Person's name is null.\"; &#125; if(person.getName().length() &lt; 3)&#123; return \"Person's name length must lager 3.\"; &#125; if(person.getAge() == 0)&#123; return \"Person's age is null.\"; &#125; if(person.getAge() &lt;= 0 || person.getAge() &gt;= 150)&#123; return \"Person's age error.\"; &#125;&#125; 所以，可以参考一下如何使用注解来校验这些参数： 1234567891011121314151617public class Person &#123; @NotNull(message = \"传入的Id为空值\") @NotEmpty(message = \"传入的Id为空字符串\") private String id; @NotNull(message = \"传入的Name为空值\") @NotEmpty(message = \"传入的Name为空字符串\") @Length(min = 3, max = 30, message = \"姓名长度必须3-30之间\") private String name; @NotNull(message = \"传入的Age为空值\") @Min(value = 0, message = \"年龄应该在0-150之间\") @Max(value = 150, message = \"年龄应该在0-150之间\") private Integer age; //Getter and Setter.&#125; @Length注释的实现本篇文章中，我们就来实现一下@Length这个注解，这个注解学会了，其他注解也都是一样的： step1.定义注解 @Length1234567891011121314151617import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Target(&#123;ElementType.FIELD&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface Length&#123; // 允许的字符串长度最小值 int min(); // 允许的字符串长度最大值 int max(); // 自定义错误提示 String errorMsg();&#125; 1、注解的定义有点像定义接口interface，但唯一不同的是前面需要加一个@符号 2、注解的成员变量只能使用基本类型、String或者enum枚举，比如int可以，但Integer这种包装类型就不行 3、像上面@Target、@Retention这种加在注解定义上面的注解，我们称为“元注解”， 元注解就是专门用于给注解添加注解的注解，简单理解就是：元注解就是天生就有的注解，可直接用于注解的定义上 4、@Target(xxx)用来说明该自定义注解可以用在什么位置，比如： ElementType. FIELD:说明自定义的注解可以用于类的变量 ElementType. METHOD:说明自定义的注解可以于类的方法 ElementType. TYPE:说明自定义的注解可以用于类本身、接口或enum类型 其实还有很多，如果记不住的话还是建议现用现查 5、@Retention (xxx)用说明你自定义注解的生命周期，比如: @Retention (RetentionPolicy.RUNTIME)：表示注解可以一直保留到运行时，因此可以通过反射获取注解信息 @Retention (RetentionPolicy.CLASS)：表示注解被编译器编译进class文件，但运行时会忽略 @Retention (RetentionPolicy.SOURCE)：表示注解仅在源文件中有效, 编译时就会被忽略 所以声明周期从长到短分别为：RUNTIME &gt; CLASS &gt; SOURCE，一般来说，如果需要在运行时去动态获取注解的信息，还是得用RUNTIME，就像本文所用。 step2.获取注解并对其验证在运行时想获取注解所代包含的信息，该怎么办？我们得用Java的反射相关的知识！下面写了一个验证函数validate()，代码中会逐行用注释去解释想要达到的目的，认真看一下每一行的注释: 1234567891011121314151617181920212223242526public class LengthValidator &#123; public static String validateField(Object object) throws IllegalAccessException &#123; // 获取字段值 // 对本文来说就是Person的id、name、age三个字段 Field[] fields = object.getClass().getDeclaredFields(); // 逐个字段校验，看看哪个字段标了注解 for (Field field: fields)&#123; // if判断：检查字段上面有没有标注@Length注解 if(field.isAnnotationPresent(Length.class))&#123; // 通过反射获取到该字段上标注的@Length的注解的详细信息 Length length = field.getAnnotation(Length.class); // 让我们在反射时看到私有变量 field.setAccessible(true); // 获取实际字段的值 int value = ((String)field.get(object)).length(); // 将实际字段的值和注解的标记值进行对比 if(value &lt; length.min() || value &gt; length.max())&#123; return length.errorMsg(); &#125; &#125; &#125; return null; &#125;&#125; step3.使用自定义注解此时，Person类只需要加上此注解 12345678910public class Person &#123; private String id; @Length(min = 3, max = 30, errorMsg = \"姓名长度必须3-30之间\") private String name; private Integer age; //Getter and Setter&#125; 然后使用即可： 1234567891011121314public class AnnotationTest &#123; public static void main(String[] args) throws IllegalAccessException &#123; Person person = new Person(); person.setName(\"13\"); person.setAge(10); person.setId(\"001\"); String validateField = LengthValidator.validateField(person); if(validateField == null) System.out.println(person); else System.out.println(validateField); &#125;&#125;","updated":"2020-04-15T06:04:25.034Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://zouchanglin.cn/tags/JavaWeb/"},{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JavaEE","slug":"JavaEE","permalink":"https://zouchanglin.cn/tags/JavaEE/"}]},{"title":"关于快排和归并的思考","date":"2020-04-11T10:00:00.000Z","path":"2020/04/11/关于快排和归并的思考/","text":"归并排序和快速排序是两种稍微复杂的排序算法，它们用的都是分治的思想，代码都通过递归来实现，过程非常相似。归并排序算法是一种在任何情况下时间复杂度都比较稳定的排序算法，这也使它存在致命的缺点，即归并排序不是原地排序算法，空间复杂度比较高，是 O(n)。正因为此它也没有快排应用广泛。快速排序算法虽然最坏情况下的时间复杂度是 O(n²)，但是平均情况下时间复杂度都是 O(nlogn)。且快速排序算法时间复杂度退化到 O(n²) 的概率非常小，我们可以通过合理地选择基准值来避免这种情况。 什么是数组的逆序度如果用概率论方法定量分析平均时间复杂度，涉及的数学推理和计算就会很复杂。我们其实还有一种思路，通过有序度和逆序度这两个概念来进行分析。有序度是指数组中具有有序关系的元素对的个数，如下图所示： 所以对于一个倒序排列的数组，比如 6，5，4，3，2，1，有序度是 0；对于一个完全有序的数组，比如 1，2，3，4，5，6，有序度就是n\\*(n-1)/2，也就是 15。我们把这种完全有序的数组的有序度叫作满有序度。逆序度的定义正好跟有序度相反（默认从小到大为有序），所以满有序度 - 有序度 = 逆序度。 求数组的逆序度数组的逆序度也就是数组的逆序对的个数，如果要求出来也是比较简单，直接暴力解法就可以，检查每一个数对，但是这样的时间复杂度为O(n²)。我们能否使用它更快捷的方法呢？其实是有的，那就是我们用归并排序的思路思路来解决这个问题，时间复杂度降到O(nlogn)。 如上图所示，对于归并排序，红线两边都是已经排好序的数组，此时做归并需要把1给挪到2的位置，意思就是1这个元素比2-8这一部分元素都大，所以无序度直接+4就达到了省时间的目的。接下来的步骤如图所示： 2会放在2的位置上，这也就意味着2比4以及4以后的元素都要小，那么2和4以及4以后的元素都组成了顺序对。当归并完成以后就求得了数组的逆序度： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// merge函数求出在arr[l...mid]和arr[mid+1...r]有序的基础上, arr[l...r]的逆序数对个数private static long merge(int[] arr, int l, int mid, int r) &#123; int[] aux = Arrays.copyOfRange(arr, l, r+1); // 初始化逆序数对个数 res = 0 long res = 0L; // 初始化，i指向左半部分的起始索引位置l；j指向右半部分起始索引位置mid+1 int i = l, j = mid+1; for(int k = l ; k &lt;= r; k++ )&#123; // 如果左半部分元素已经全部处理完毕 if(i &gt; mid)&#123; arr[k] = aux[j-l]; j++; &#125; // 如果右半部分元素已经全部处理完毕 else if(j &gt; r)&#123; arr[k] = aux[i-l]; i++; &#125; // 左半部分所指元素 &lt;= 右半部分所指元素 else if( aux[i-l] &lt; aux[j-l])&#123; arr[k] = aux[i-l]; i++; &#125; else&#123; // 右半部分所指元素 &lt; 左半部分所指元素 arr[k] = aux[j-l]; j++; // 此时, 因为右半部分k所指的元素小 // 这个元素和左半部分的所有未处理的元素都构成了逆序数对 // 左半部分此时未处理的元素个数为 mid - j + 1 res += (long)(mid - i + 1); &#125; &#125; return res;&#125;// 求arr[l..r]范围的逆序数对个数private static long solve(int[] arr, int l, int r) &#123; if (l &gt;= r) return 0L; int mid = l + (r-l)/2; // 求出 arr[l...mid] 范围的逆序数 long res1 = solve(arr, l, mid); // 求出 arr[mid+1...r] 范围的逆序数 long res2 = solve(arr, mid + 1, r); return res1 + res2 + merge(arr, l, mid, r);&#125;public static long solve(int[] arr)&#123; int n = arr.length; return solve(arr, 0, n-1);&#125; 求数组中第N小的元素其实这个问题是一个明显的Top K问题，但是我们除了用堆还能用其他方法吗？其实快速排序也可以解决这个问题，我们回顾一下快速排序的过程（《 快速排序及其优化 》)），其实就是通过基准值每次把数组进行划分，我们只需要保留存在第N大的元素的那一部分即可，这么处理的话时间复杂度是O(n)，复杂度 = n + n/2 + n/4 + n/8 + … ，所有看成是O(n)或者O(2n)的时间复杂度，但是我们需要注意的是需要使用随机化法，来确定基准值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class QuickSortTopK &#123; // 对arr[l...r]部分进行partition操作 // 返回p, 使得arr[l...p-1] &lt; arr[p] ; arr[p+1...r] &gt; arr[p] // partition 过程, 和快排的partition一样 private static int partition(int[] arr, int l, int r)&#123; // 随机在arr[l...r]的范围中, 选择一个数值作为标定点pivot swap(arr, l , (int)(Math.random()*(r-l+1))+l ); int v = arr[l]; int j = l; // arr[l+1...j] &lt; v ; arr[j+1...i) &gt; v for( int i = l + 1 ; i &lt;= r ; i ++ ) if( arr[i] &lt; v)&#123; j ++; swap(arr, j, i); &#125; swap(arr, l, j); return j; &#125; // 求出nums[l...r]范围里第k小的数 private static int solve(int[] nums, int l, int r, int k)&#123; if( l == r ) return nums[l]; // partition之后, nums[p]的正确位置就在索引p上 int p = partition(nums, l, r); // 如果 k == p, 直接返回nums[p] if( k == p ) return nums[p]; // 如果 k &lt; p, 只需要在nums[l...p-1]中找第k小元素即可 else if( k &lt; p ) return solve( nums, l, p-1, k); else// 如果 k &gt; p, 则需要在nums[p+1...r]中找第k-p-1小元素 // 注意: 由于我们传入__selection的依然是nums, 而不是nums[p+1...r], // 所以传入的最后一个参数依然是k, 而不是k-p-1 return solve( nums, p+1, r, k ); &#125; // 寻找nums数组中第k小的元素 // 注意: 在我们的算法中, k是从0开始索引的, 即最小的元素是第0小元素, 以此类推 // 如果希望我们的算法中k的语意是从1开始的, 只需要在整个逻辑开始进行k--即可, 可以参考solve2 public static int solve(int[] nums, int k) &#123; assert nums != null &amp;&amp; k &gt;= 0 &amp;&amp; k &lt; nums.length; return solve(nums, 0, nums.length - 1, k); &#125; private static void swap(int[] arr, int i, int j) &#123; int t = arr[i]; arr[i] = arr[j]; arr[j] = t; &#125; // 测试 Selection public static void main(String[] args) &#123; // 生成一个大小为n, 包含0...n-1这n个元素的随机数组arr int N = 10; int[] arr = SortTestHelper.generate(N, 0, N); for(int i: arr) System.out.print(i + \" \"); System.out.println(); int solve = solve(arr, 3); System.out.println(solve); &#125;&#125;","updated":"2020-04-14T12:01:02.853Z","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序算法","slug":"排序算法","permalink":"https://zouchanglin.cn/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"}]},{"title":"快速排序及其优化","date":"2020-04-10T10:00:00.000Z","path":"2020/04/10/快速排序及其优化/","text":"快速排序(Quick Sort)被称为20世纪对世界影响最大的算法之一，现在我们来看快速排序算法，习惯性把它简称为快排，快排利用的也是分治思想。乍看起来，它有点像归并排序，但是思路其实完全不一样。现在，我们先来看下快排的核心思想，最后将讲述快速排序的两个优化方案，其实还有一种三路快排的优化方案也是可以的，但是本片文章重点在于快速排序的原理和实现，所以三路快排的优化方案不会出现在这篇文章里，以后再详细记录一下。 快速排序(QuickSort)原理快排的思想是这样的：如果要排序数组中下标从 p到 r之间的一组数据，我们选择p到r之间的任意一个数据作为pivot(分区点)，我们遍历p到r之间的数据，将小于pivot的放到左边，将大于pivot的放到右边，将pivot放到中间。经过这一步骤之后，数组p到r之间的数据就被分成了三个部分，前面p到q-1之间都是小于pivot的，中间是pivot，后面的q+1到r之间是大于pivot的。 根据分治、递归的处理思想，我们可以用递归排序下标从p到q-1之间的数据和下标从q+1到r之间的数据，直到区间缩小为 1，就说明所有的数据都有序了。 12345678910111213141516171819202122232425262728293031323334public static void quickSort(int[] arr)&#123; quickSortChild(arr, 0, arr.length - 1);&#125;//对arr[start... end]部分进行快速排序private static void quickSortChild(int[] arr, int start, int end)&#123; if(start &gt;= end) return; int p = partition(arr, start, end); quickSortChild(arr, start, p - 1); quickSortChild(arr, p + 1, end);&#125;/** * 将arr[start...end]部分进行partition操作 * 返回p,使得arr[start...p-1] &lt; arr[p]; arr[p+1...end] &gt; arr[p] */private static int partition(int[] arr, int start, int end) &#123; int v = arr[start]; //取第一个元素作为基准值 //arr[start+1...j] &lt; v; arr[j+1...i) &gt; v int j = start; int tmp; for (int i = start + 1; i &lt;= end ; i++) &#123; if(arr[i] &lt; v)&#123; tmp = arr[j + 1]; arr[j + 1] = arr[i]; arr[i] = tmp; j++; &#125; &#125; tmp = arr[start]; arr[start] = arr[j]; arr[j] = tmp; return j;&#125; 现在测试一下快速排序和归并排序的效率，排500万个完全随机数快排比归并排序快10倍左右： 快速排序核心 partition我们重点需要关注的是partition函数，我们知道快排无非就是找个基准点，然后把小于基准点的放在左边，大于基准点的放在右边，partition过程就是需要进行如下操作的过程： 关键是如何把小的元素放前面，大的放后面呢？ 我们可以假设上面的这一种情况，以第一次元素作为基准值，大于V的放后面，小于v的放前面，j作为分隔位置的坐标，现在到了该判断e是大于v还是小于v的时候了，如果e大于v，那么很容易，就如上图所示，只需要把e给并入到蓝色部分中即可，然后i++，去判断下一个元素是否大于或者小于v。 那么如果e小于v呢？应该如何调整呢？ 其实也很简单，只需要让蓝色部分(也就是大于基准值的那一部分)的第一个元素与e的位置交换，这样剩下的步骤就是把j++(即移动分界位置坐标)，然后i++，去判断下一个元素是否大于或者小于v。 当上述步骤完成后，只需要把v和橙色部分的最后一个元素交换位置即可，这样便使得v前面的元素比它小，后面的元素比它大，于是我们就成功的把一个数组给分成了两组，并且只需要把基准值的坐标给返回了。 快速排序的优化（随机化）其实第一中优化方式和之前一致，那就是在递归到数据规模比较小的时候用直接插入排序，这样可以稍微提高一些效率，但是这个优化不是我们现在优化的重点： 12345678910111213141516... //对arr[start... end]部分进行快速排序private static void quickSortChild(int[] arr, int start, int end)&#123; //if(start &gt;= end) return; //优化点：在递归到数据规模比较小的时候就用插入排序 if(end - start &lt;= 15)&#123; insertionSort(arr, start, end); return; &#125; int p = partition(arr, start, end); quickSortChild(arr, start, p - 1); quickSortChild(arr, p + 1, end);&#125;... 之前通过对完全随机数进行排序测试，我们发现快排比归并排序快十倍左右，现在我们需要测试一下对近乎有序的数组进行排序结果又是怎样的，我们对500000个数字进行随机位置10次交换，对比一下和归并排序的效率： 结果栈溢出了，这是因为我们几乎每次都找的是第一个元素作为基准值，所以导致了栈溢出。可以先回想一下，为什么归并排序的时间复杂度能稳定在nlogn呢？其实很简单，因为对于归并排序来讲，总是能将数据均等的一分为二，这是归并排序的原理图： 所以对于快速排序来说，明显并没有归并排序那么均等分配，于是乎在极端情况下，直接一个元素一组，剩下的所有元素一组，那么这样肯定是不符合我们预期要求的，上面的栈溢出的情况就是如下图，这就是快速排序最差的情况，在这种情况下，时间复杂度退化为O(n²) 所以这就说明了我们不能直接把第一个元素当成基准值，那么基准值的位置如何确定呢？我们只需要随机基准值位置就好了，之前的代码不用修改，只需要把我们的随机位置的基准值和数组第一个元素做交换即可： 1234567891011121314151617181920212223242526272829303132... /** * 将arr[start...end]部分进行partition操作 * 返回p,使得arr[start...p-1] &lt; arr[p]; arr[p+1...end] &gt; arr[p] */private static int partition(int[] arr, int start, int end) &#123; //int v = arr[start]; //取第一个元素作为基准值 //优化点： 找个随机位置的元素和头元素交换 int randomIndex = (int)(Math.random() * 100) % (end - start + 1) + start; int tmp = arr[randomIndex]; arr[randomIndex] = arr[start]; arr[start] = tmp; int v = arr[start]; //arr[start+1...j] &lt; v; arr[j+1...i) &gt; v int j = start; for (int i = start + 1; i &lt;= end ; i++) &#123; if(arr[i] &lt; v)&#123; tmp = arr[j + 1]; arr[j + 1] = arr[i]; arr[i] = tmp; j++; &#125; &#125; tmp = arr[start]; arr[start] = arr[j]; arr[j] = tmp; return j;&#125;... 快速排序的优化（双路快排）上面只是快速排序遇到的第一种极端情况，接下来看看另一种情况，现在10万个数随机数，但是范围都是0-10之间的数字，意味着有很多重复的数字，这种情况下快速排序的表现怎么样呢？ 我们不难发现，快速排序又比规归并排序慢了，原因其实很简单：我们在partition函数中，把比基准值小的划到左边，比基准值大的划到右边，那么相等的呢？其实根据我们上面写的代码遇到相等的是划到右边的，所以当遇到很多重复元素的时候，本来nlogn的时间复杂度又变成近乎O(n²)，那么划到左边行不行呢？其实也不行，因为即使把相等的划到左边在大规模相同的数据情况下还是同样面临极端情况，还是会退化到O(n²)。 以前我们在进行快速排序的时候总是把i++，直到累加到数组的末尾，现在我们只需要放在两头即可，小于基准值的放左边，大于基准值的放右边，两边同时开始向中间排： 其实上图稍微有一点错误，那就是黄色部分是小于等于基准值的，蓝色部分是大于等于基准值的，我们看一下这个过程演示： 这样就解决了大量的重复元素集中在一端的情况，即使遇到了很多重复的元素也能将他们几乎平分开来。 12345678910111213141516171819202122232425262728293031323334353637... private static int partition2(int[] arr, int start, int end) &#123; //随机位置基准值 int randomIndex = (int)(Math.random() * 100) % (end - start + 1) + start; //和首元素交换 int tmp = arr[randomIndex]; arr[randomIndex] = arr[start]; arr[start] = tmp; int v = arr[start]; // arr[start+1...i) &lt;= v, arr(j...end] &gt;= v int i = start + 1, j = end; while(true)&#123; while(i &lt;= end &amp;&amp; arr[i] &lt; v) i++; while(j &gt;= start +1 &amp;&amp; arr[j] &gt; v) j--; if(i &gt; j) break; //swap(arr[i], arr[j]) tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp; i++; j--; &#125; //swap(arr[start], arr[j]) tmp = arr[start]; arr[start] = arr[j]; arr[j] = tmp; return j;&#125;...","updated":"2020-04-14T10:06:23.620Z","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序算法","slug":"排序算法","permalink":"https://zouchanglin.cn/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"}]},{"title":"O(nLogn)的归并排序","date":"2020-04-09T10:00:00.000Z","path":"2020/04/09/O(nLogn)的归并排序/","text":"之前几篇文章我介绍了三种O(n²)的排序算法《O(n²)的三个排序算法》（选择排序、插入排序和冒泡排序）以及它们的优化，然后顺便还写了一篇希尔排序的文章《插入排序的优化之希尔排序》，但是其实用的比较多的还是直接插入排序，它们比较适合于小规模数据的排序 。下面我将记录时间复杂度为nlog(n)的几种排序算法之一 —— 归并排序算法，这种排序算法适合大规模的数据排序，比之前的O(n²)的三种排序算法更为常用，在学习之前我们可以先对比一下nlog(n)和n²是什么概念。 nlog(n)比n²快多少 我们可以看出，在n越来越大的时候，nlogn比n²快上千倍甚至上万倍，所以nlogn的排序算法相对于n²的排序算法在大规模的数据的情况下更常用，也更实用。 归并排序 MergeSort归并排序的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。 归并排序使用的就是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。 分治思想跟递归思想很像，分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。 通过上面的示意图，很容易可以写出对应的伪代码： 123456789101112131415161718&#x2F;&#x2F; 归并排序算法, A 是数组，n 表示数组大小merge_sort(A, n) &#123; merge_sort_c(A, 0, n-1)&#125; &#x2F;&#x2F; 递归调用函数merge_sort_c(A, p, r) &#123; &#x2F;&#x2F; 递归终止条件 if p &gt;&#x3D; r then return &#x2F;&#x2F; 取 p 到 r 之间的中间位置 q q &#x3D; (p+r) &#x2F; 2 &#x2F;&#x2F; 分治递归 merge_sort_c(A, p, q) merge_sort_c(A, q+1, r) &#x2F;&#x2F; 将 A[p...q] 和 A[q+1...r] 合并为 A[p...r] merge(A[p...r], A[p...q], A[q+1...r])&#125; 可以发现，其实最重要的两个步骤就是分支递归的过程和合并的过程，那么是如何合并的呢？ 下面是Java版本的伪码 12345678910111213141516171819202122232425262728293031323334353637383940public static void mergeSort(int[] arr)&#123; mergeChild(arr, 0, arr.length-1);&#125;//使用递归进行归并排序，对arr[start, end]的范围进行排序private static void mergeChild(int[] arr, int start, int end)&#123; //代表处理的数据集为空 if(start &gt;= end) return; int mid = (end - start) / 2 + start; mergeChild(arr, start, mid); mergeChild(arr, mid + 1, end); merge(arr, start, mid, end);&#125;//将arr[start ... mid]和arr[mid+1 ... end] 两部分进行归并private static void merge(int[] arr, int start, int mid, int end) &#123; //创建临时数组 int[] aux = new int[end - start + 1]; System.arraycopy(arr, start, aux, 0, end - start + 1); int i = start; int j = mid + 1; for (int k = start; k &lt;= end ; k++) &#123; //左部分的已经归并完毕 if(i &gt; mid)&#123; arr[k] = aux[j - start]; j++; //右部分的已经归并完毕 &#125;else if(j &gt; end)&#123; arr[k] = aux[i - start]; i++; &#125;else if(aux[i - start] &lt; aux[j - start])&#123; arr[k] = aux[i - start]; i++; &#125;else&#123; arr[k] = aux[j - start]; j++; &#125; &#125;&#125; 算法稳定性归并排序是稳定的排序算法吗？归并排序稳不稳定关键要看 merge() 函数，也就是两个有序子数组合并成一个有序数组的那部分代码。 由于是直接拷贝进了临时数组，这样就保证了值相同的元素，在合并前后的先后顺序不变。所以，归并排序是一个稳定的排序算法。 最好和最坏情况从我们的原理分析和伪代码可以看出，归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是 O(nlogn)。 归并排序的优化通过测试我们发现如果是对大批的随机数进行排序的话，相对于插入排序性能有百倍或者千倍的优势，如如我现在对50万个随机数用归并和插入进行排序： 可以看出，归并排序的速度快280倍。可是如果是排接近有序的数组呢？我们知道，在插入排序最优的情况下就是已经有序，此时时间复杂度为O(n)，那么归并排序并不能达到O(n)的时间复杂度。现在对1000万个接近有序的数进行排序，只交换十次： 所以我们需要对归并排序进行优化： 我们可以回顾一下归并排序的过程，假设在merge之前，如果arr[mid] &lt;= arr[mid+1]，那么是不是说明已经有序了呢？因为我们保证了arr[start, mid]是有序的，arr[mid+1, end]也是有序的。例如：[1,2 , 3 , 4, 5]、[6, 7, 8, 9, 10]，如果arr[mid] &lt;= arr[mid+1]，那么此时数组已经有序，无需执行merge操作。 优化后的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142public static void mergeSort(int[] arr)&#123; mergeChild(arr, 0, arr.length-1);&#125;//使用递归进行归并排序，对arr[start, end]的范围进行排序private static void mergeChild(int[] arr, int start, int end)&#123; //代表我们处理的数据集为空 if(start &gt;= end) return; int mid = (end - start) / 2 + start; mergeChild(arr, start, mid); mergeChild(arr, mid + 1, end); //优化掉不必要的merge操作 if(arr[mid] &gt; arr[mid] + 1) merge(arr, start, mid, end);&#125;//将arr[start ... mid]和arr[mid+1 ... end] 两部分进行归并private static void merge(int[] arr, int start, int mid, int end) &#123; //创建临时数组 int[] aux = new int[end - start + 1]; System.arraycopy(arr, start, aux, 0, end - start + 1); int i = start; int j = mid + 1; for (int k = start; k &lt;= end ; k++) &#123; //左部分的已经归并完毕 if(i &gt; mid)&#123; arr[k] = aux[j - start]; j++; //右部分的已经归并完毕 &#125;else if(j &gt; end)&#123; arr[k] = aux[i - start]; i++; &#125;else if(aux[i - start] &lt; aux[j - start])&#123; arr[k] = aux[i - start]; i++; &#125;else&#123; arr[k] = aux[j - start]; j++; &#125; &#125;&#125; 上面是第一个优化点，其实还能优化，就是我们在递归到最后数据量非常小的时候我们直接使用插入排序来解决排序问题。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public static void mergeSort(int[] arr)&#123; mergeChild(arr, 0, arr.length-1);&#125;//使用递归进行归并排序，对arr[start, end]的范围进行排序private static void mergeChild(int[] arr, int start, int end)&#123; //代表我们处理的数据集为空 //if(start &gt;= end) return; if(end - start &lt;= 15)&#123; insertionSort(arr, start, end); return; &#125; int mid = (end - start) / 2 + start; mergeChild(arr, start, mid); mergeChild(arr, mid + 1, end); //优化点：优化掉不必要的merge操作 if(arr[mid] &gt; arr[mid] + 1) merge(arr, start, mid, end);&#125;//将arr[start ... mid]和arr[mid+1 ... end] 两部分进行归并private static void merge(int[] arr, int start, int mid, int end) &#123; //创建临时数组 int[] aux = new int[end - start + 1]; System.arraycopy(arr, start, aux, 0, end - start + 1); int i = start; int j = mid + 1; for (int k = start; k &lt;= end ; k++) &#123; //左部分的已经归并完毕 if(i &gt; mid)&#123; arr[k] = aux[j - start]; j++; //右部分的已经归并完毕 &#125;else if(j &gt; end)&#123; arr[k] = aux[i - start]; i++; &#125;else if(aux[i - start] &lt; aux[j - start])&#123; arr[k] = aux[i - start]; i++; &#125;else&#123; arr[k] = aux[j - start]; j++; &#125; &#125;&#125;//优化点：在数据规模小的时候直接使用插入排序（对arr[start, end]范围的插入数据排序）private static void insertionSort(int[] arr, int start, int end) &#123; for (int i = start + 1; i &lt;= end; i++) &#123; int e = arr[i]; int j; for (j = i; j &gt; start &amp;&amp; arr[j - 1] &lt; e; j--) &#123; arr[i] = arr[j - 1]; &#125; arr[j] = e; &#125;&#125; 经过上面的两个优化步骤之后，直接插入排序和归并排序在排序大量几乎有序的数组的时候，效率并差不了多少 归并排序空间复杂度归并排序的时间复杂度任何情况下都是 O(nlogn)，看起来非常优秀。 即便是快速排序，最坏情况下，时间复杂度也是 O(n²)，但是归并排序并没有像快排那样，应用广泛，这是为什么呢？因为它有一个致命的弱点，那就是归并排序不是原地排序算法。 实际上，递归代码的空间复杂度并不能像时间复杂度那样累加。尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。在任意时刻，CPU 只会有一个函数在执行，也就只会有一个临时的内存空间在使用。临时内存空间最大也不会超过 n 个数据的大小，所以空间复杂度是 O(n)。 但是在如今更多的场景是时间换空间，所以归并排序时间复杂度稳定为nlog(n)，空间复杂度稳定为O(n)，而且是稳定排序算法，用处还是挺多的。 自底向上的归并排序如下图，我们先把数组划分为4部分，排序后划分为4部分进行归并为两部分，再把两部分归并为一部分： 这样就免去了递归的过程，而是一个迭代的过程。 123456789101112131415161718192021222324252627282930313233343536public static void bottomUpMergeSort(int[] arr)&#123; for (int size = 1; size &lt;= arr.length; size += size) &#123; //i + size &lt; arr.length 防止越界 for (int i = 0; i + size &lt; arr.length; i += size + size) &#123; int index = i + size + size - 1; //取i + size + size - 1 和 arr.length的最小值 merge(arr, i, i + size - 1, index &gt; arr.length ? arr.length - 1: index); &#125; &#125;&#125;//将arr[start ... mid]和arr[mid+1 ... end] 两部分进行归并private static void merge(int[] arr, int start, int mid, int end) &#123; //创建临时数组 int[] aux = new int[end - start + 1]; System.arraycopy(arr, start, aux, 0, end - start + 1); int i = start; int j = mid + 1; for (int k = start; k &lt;= end ; k++) &#123; //左部分的已经归并完毕 if(i &gt; mid)&#123; arr[k] = aux[j - start]; j++; //右部分的已经归并完毕 &#125;else if(j &gt; end)&#123; arr[k] = aux[i - start]; i++; &#125;else if(aux[i - start] &lt; aux[j - start])&#123; arr[k] = aux[i - start]; i++; &#125;else&#123; arr[k] = aux[j - start]; j++; &#125; &#125;&#125; 上面的代码其实还没有加入之前的两个优化点，加上之后效果更好。这种归并排序有一个很明显的特点就是没有使用数组的特性，即没有使用数组下标，正因为如此，这样的自底向上的归并排序可以对链表进行排序。","updated":"2020-04-12T01:13:31.777Z","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序算法","slug":"排序算法","permalink":"https://zouchanglin.cn/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"}]},{"title":"插入排序的优化之希尔排序","date":"2020-04-09T03:00:00.000Z","path":"2020/04/09/插入排序的优化之希尔排序/","text":"希尔排序是插入排序的一种，又称缩小增量排序，是直接插入排序算法的一种更高效的改进版本。希尔排序是非稳定排序算法，可以说它是插入排序的高级版。我们可以先回顾一下直接插入排序的过程： 排序前将第一个元素看成有序的数列 第1趟排序后：得到一个长度为2的有序数列 第2趟排序后：得到一个长度为3的有序数列 第3趟排序后：得到一个长度为4的有序数列 ……..每趟插入排序，都可以将一个无序值插入一个有序数列，直到全部元素有序 最坏情况下，直接插入排序的时间复杂度是O(n²)。从上图可以看出，6这个元素被移动了三次才到末尾。我在直接插入排序的博客《O(n²)的三个排序算法》中写到，直接插入排序最好的情况就是本来就有序，所以直接插入排序用来排序那些近乎有序的数组是非常适合的。所以我们对于一个无序数组，先把它变成大致有序，然后超级大致有序，然后变成超级超级大致有序……最后变成完全有序，这样可以省略掉很多不必要的元素交换。 那么如何让大元素一开始就分批向后移动呢？那就是通过分组排序的方式， 应该怎么分呢？比如有10个元素的序列，分成几个才合适？每次缩减又是多少呢？ 将一个序列分成好几个序列，用一个数来表示：那个数称为增量，我们可以发现增量越来越小，其实就是数组越来越有序的过程。 下面来看看希尔排序的过程： 下面通过代码来实现一下希尔排序： 123456789101112131415public static void shellSort(int[] arr)&#123; //每次都缩小增量 for (int step = arr.length / 2; step &gt; 0; step /= 2) &#123; for (int i = step; i &lt; arr.length; i++) &#123; int j = i; int tmp = arr[j]; //注意这里不是结构上的前一个元素，而是对于组而言的，同组之间的元素交换 while(j - step &gt;= 0 &amp;&amp; arr[j - step] &gt; tmp)&#123; arr[j] = arr[j - step]; j = j - step; &#125; arr[j] = tmp; &#125; &#125;&#125; 希尔排序和直接插入排序的对比，对30万个随机数进行排序 最后再思考一个问题，希尔排序是稳定排序吗？ 当然不是，因为通过上面的结论我们知道，希尔排序是依靠分组来进行排序的，值相同的元素完全有可能被调换位置，所以希尔排序是一个不稳定排序。","updated":"2020-04-09T04:42:28.387Z","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序算法","slug":"排序算法","permalink":"https://zouchanglin.cn/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"}]},{"title":"O(n²)的三个排序算法","date":"2020-04-07T10:00:00.000Z","path":"2020/04/07/O(n²)的三个排序算法/","text":"今天复习下最简单的三个排序算法，一个是选择排序，一个是插入排序，一个是冒泡排序，三者时间复杂度都是O(n²)，通过分析来发现三者的优劣，以及对最好的情况和最坏的情况进行分析。 另外，这三中排序算法都是基于比较的排序算法。基于比较的排序算法的执行过程，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。 一、选择排序 SelectionSort选择排序是一种简单直观的排序算法，工作原理为：在未排序的序列中找出最小(大)元素与第一个位置的元素交换位置，原理如下图所示： 选择排序的静态图： 123456789101112131415public static void selectSort(int[] arr) &#123; for (int i = 0; i &lt; arr.length; i++) &#123; int tmp; //寻找[i,n]之间的最小值 int minIndex = i; for (int j = i+1; j &lt; arr.length; j++) &#123; if(arr[j] &lt; arr[minIndex])&#123; minIndex = j; &#125; &#125; tmp = arr[i]; arr[i] = arr[minIndex]; arr[minIndex] = tmp; &#125;&#125; 稳定性选择排序是一种不稳定的排序算法。 从图中可以看出来，选择排序每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样破坏了稳定性。 最好和最坏情况选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为O(n²) 二、插入排序 InsertionSort首先，我们将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。 静态图展示 12345678910111213141516public static void execSort(int[] arr) &#123; int tmp; for (int i = 1; i &lt; arr.length; i++) &#123; //寻找元素arr[i]合适的插入位置 for (int j = i; j &gt; 0; j--) &#123; if(arr[j] &lt; arr[j-1])&#123; tmp = arr[j]; arr[j] = arr[j-1]; arr[j-1] = tmp; &#125;else &#123; //到这里说明前面的所有元素已经比当前值小了，没有继续比较的必要了 break; &#125; &#125; &#125;&#125; 换个比较简洁的写法： 123456789101112//更简洁的写法：替换上面的breakpublic void execSort(int[] arr) &#123; int tmp; for (int i = 1; i &lt; arr.length; i++) &#123; //寻找元素arr[i]合适的插入位置 for (int j = i; j &gt; 0 &amp;&amp; arr[j] &lt; arr[j-1]; j--) &#123; tmp = arr[j]; arr[j] = arr[j-1]; arr[j-1] = tmp; &#125; &#125;&#125; 插入排序的改进 12345678910public static void insertionSortOptimize(int[] arr)&#123; for (int i = 0; i &lt; arr.length; i++) &#123; int e = arr[i]; int j; //用来保存元素e应该插入的位置 for (j = i; j &gt; 0 &amp;&amp; arr[j-1] &gt; e; j--) &#123; arr[j] = arr[j-1]; &#125; arr[j] = e; &#125;&#125; 插入排序和选择排序的对比 其实我们很容易发现选择排序的最致命的缺点就是两层for循环需要完全走完才能完成排序，而插入排序则不需要。所以选择排序在任何情况下都是比较慢的。插入排序则不同，最坏的情况下（其实就是逆序了）才是O(n²)，如果是已经有序，或者大部分有序，还是非常快的，比如在我自己的电脑上排一亿个有序的数，才0.035s，这种速度比n*Log(n)的排序算法性能还高，至于选择排序的话我等了1分钟还没排出来，直接stop了。 所以O(n²)级别的排序算法并非一无是处！下面我们看看冒泡排序吧！ 稳定性在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。 最好和最坏情况上面已经说过了，如果要排序的数据已经是有序的，我们并不需要搬移任何数据。如果我们从尾到头在有序数据组里面查找插入位置，每次只需要比较一个数据就能确定插入的位置。所以这种情况下，最好是时间复杂度为 O(n) 如果数组是倒序的，每次插入都相当于在数组的第一个位置插入新的数据，所以需要移动大量的数据，所以最坏情况时间复杂度为 O(n²)。 三、冒泡排序 BubbleSort冒泡排序也属于一种典型的交换排序，交换排序顾名思义就是通过元素的两两比较，判断是否符合要求，如过不符合就交换位置来达到排序的目的。冒泡排序名字的由来就是因为在交换过程中，类似水冒泡，小（大）的元素经过不断的交换由水底慢慢的浮到水的顶端。 如果用静态图来展示的话就是这个样子： 1234567891011public static void bubbleSort(int[] arr) &#123; for (int i = 0; i &lt; arr.length; i++) &#123; for (int j = 0; j &lt; arr.length - i - 1; j++) &#123; if(arr[j] &gt; arr[j+1])&#123; int tmp = arr[j]; arr[j] = arr[j+1]; arr[j+1] = tmp; &#125; &#125; &#125;&#125; 优化版本定义一个标志位用来表示当前第i趟是否有交换，如果有，则要进行i+1趟，如果没有则说明当前数组已经完成排序，不需要剩下的比较： 12345678910111213141516public static void bubbleSortOptimize(int[] arr) &#123; for (int i = 0; i &lt; arr.length; i++) &#123; //用来表示当前第i趟是否有交换， //如果有则要进行i+1趟，如果没有则说明当前数组已经完成排序 boolean flag = true; for (int j = 0; j &lt; arr.length - i - 1; j++) &#123; if(arr[j] &gt; arr[j+1])&#123; int tmp = arr[j]; arr[j] = arr[j+1]; arr[j+1] = tmp; flag = false; &#125; &#125; if(flag) return; &#125;&#125; 其实冒泡排序还有更优化的做法，那就是记录最后一次交换的位置： 123456789101112131415161718192021public static void bubbleSortOptimizePlus(int[] arr) &#123; int pos; //用来记录最后一次交换的位置 int k = arr.length - 1; for (int i = 0; i &lt; arr.length; i++) &#123; pos = 0; //用来表示当前第i趟是否有交换， //如果有则要进行i+1趟，如果没有则说明当前数组已经完成排序 boolean flag = true; for (int j = 0; j &lt; k; j++) &#123; if(arr[j] &gt; arr[j+1])&#123; int tmp = arr[j]; arr[j] = arr[j+1]; arr[j+1] = tmp; flag = false; pos = j; &#125; &#125; if(flag) return; k = pos; &#125;&#125; 排序随机数组的时候，优化效果一般，但是排序几乎有序的数组和本来就有序的数组还是有优化效果的： 第二种优化之后才0.049秒，不优化是22秒。 稳定性冒泡排序是稳定排序，在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。 最好和最坏情况最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以最好情况时间复杂度是 O(n)。而最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进行 n 次冒泡操作，所以最坏情况时间复杂度为 O(n²)。 四、三种排序总结 一看图就明白了什么排序算法最实用了吧，那就是插入排序。 这三种时间复杂度为 O(n²) 的排序算法中，冒泡排序、选择排序，可能就纯粹停留在理论的层面了，学习的目的也只是为了开拓思维，实际开发中应用并不多，但是插入排序还是挺有用的。","updated":"2020-04-10T01:41:15.470Z","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序算法","slug":"排序算法","permalink":"https://zouchanglin.cn/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"}]},{"title":"SpringBoot自动配置原理","date":"2020-04-06T10:00:00.000Z","path":"2020/04/06/SpringBoot自动配置原理/","text":"我们知道SpringBoot的理念就是约定大于配置，这也使得我们在开发应用程序的过程更加便捷，以前的大量XML配置直接是噩梦呀，现在出现了SpringBoot明显降低了开发成本，而且大量的注解的使用帮我们省略掉了很多代码。本篇文章主要探究的是SpringBoot是如何实现自动配置并且如何加载配置Bean的，其实主要就是探究@EnableAutoConfiguration注解究竟发挥了怎样的作用。 @EnableAutoConfiguration@SpringBootApplication其实是三个注解的组合体，这三个注解中@Configuration和@ComponentScan对我们来说并不陌生，今天我们主要探究的是@EnableAutoConfiguration 。 123456789@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage@Import(AutoConfigurationImportSelector.class)public @interface EnableAutoConfiguration &#123; ...&#125; 其中最关键的要属@Import(AutoConfigurationImportSelector.class)，借助AutoConfigurationImportSelector，@EnableAutoConfiguration可以帮助SpringBoot应用将所有符合条件的@Configuration配置都加载到当前SpringBoot创建并使用的IoC容器中。 借助于Spring框架原有的一个工具类：SpringFactoriesLoader的支持，@EnableAutoConfiguration可以智能的自动配置功效才得以大功告成！ 在AutoConfigurationImportSelector类中可以看到通过SpringFactoriesLoader.loadFactoryNames()把spring-boot-autoconfigure.jar/META-INF/spring.factories中每一个xxxAutoConfiguration文件都加载到容器中，spring.factories文件里每一个xxxAutoConfiguration文件一般都会有下面的条件注解。 @ConditionalOnClass ： classpath中存在该类时起效 @ConditionalOnMissingClass ： classpath中不存在该类时起效 @ConditionalOnBean ： DI容器中存在该类型Bean时起效 @ConditionalOnMissingBean ： DI容器中不存在该类型Bean时起效 @ConditionalOnSingleCandidate ： DI容器中该类型Bean只有一个或@Primary的只有一个时起效 @ConditionalOnExpression ： SpEL表达式结果为true时 @ConditionalOnProperty ： 参数设置或者值一致时起效 @ConditionalOnResource ： 指定的文件存在时起效 @ConditionalOnJndi ： 指定的JNDI存在时起效 @ConditionalOnJava ： 指定的Java版本存在时起效 @ConditionalOnWebApplication ： Web应用环境下起效 @ConditionalOnNotWebApplication ： 非Web应用环境下起效 SpringFactoriesLoaderSpringFactoriesLoader属于Spring框架私有的一种扩展方案（类似于Java的SPI方案java.util.ServiceLoader)，其主要功能就是从指定的配置文件META-INF/spring-factories加载配置，spring-factories是一个典型的java properties文件，只不过Key和Value都是Java类型的完整类名，比如： 1xpu.MyService = xpu.MyServiceImpl 对于@EnableAutoConfiguration来说，SpringFactoriesLoader的用途稍微不同一些，其本意是为了提供SPI扩展的场景，而在@EnableAutoConfiguration场景中，它更多提供了一种配置查找的功能支持，即根据@EnableAutoConfiguration的完整类名org.springframework.boot.autoconfig.EnableAutoConfiguration作为查找的Key，获得对应的一组@Configuration类。 SpringFactoriesLoader是一个抽象类，类中定义的静态属性定义了其加载资源的路径public static final String FACTORIES_RESOURCE_LOCATION = “META-INF/spring.factories”，此外还有三个静态方法 loadFactories：加载指定的factoryClass并进行实例化。 loadFactoryNames：加载指定的factoryClass的名称集合。 instantiateFactory：对指定的factoryClass进行实例化。 loadFactories还是调用了loadFactoryNames与instantiateFactory方法。loadFactories方法首先获取类加载器，然后调用loadFactoryNames方法获取所有的指定资源的名称集合、接着调用instantiateFactory方法实例化这些资源类并将其添加到result集合中。最后调用AnnotationAwareOrderComparator.sort方法进行集合的排序。 我们可以看一下SpringBoot的自动配置的Bean有哪些，见下图： 自动配置示例下面结合一个例子来加深理解，例子展示的是当项目启动时如果某个类存在就自动配置这个Bean，并且这个属性可以在application.properties中配置。新建一个Maven项目，pom.xml文件如下： pom.xml 12345678910111213141516171819202122232425262728293031&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;xpu.tim&lt;/groupId&gt; &lt;artifactId&gt;autoconfig-demo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-autoconfigure&lt;/artifactId&gt; &lt;version&gt;2.0.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;version&gt;2.0.4.RELEASE&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; Person.java 1234567891011121314151617package xpu.edu.tim;public class Person &#123; private String name; public String sayName()&#123; return \"My Name is \" + name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; PersonProperties.java 123456789101112131415161718package xpu.edu.tim;import org.springframework.boot.context.properties.ConfigurationProperties;@ConfigurationProperties(prefix = \"person\") //获取属性值public class PersonProperties &#123; private static final String NAME = \"Jock.Tim\"; private String name = NAME ; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; PersonAutoConfiguration.java 12345678910111213141516171819202122232425262728package xpu.edu.tim;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.autoconfigure.condition.ConditionalOnClass;import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;import org.springframework.boot.context.properties.EnableConfigurationProperties;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configuration//为带有@ConfigurationProperties注解的Bean提供有效的支持。//这个注解可以提供一种方便的方式来将带有@ConfigurationProperties注解的类注入为Spring容器的Bean。@EnableConfigurationProperties(PersonProperties.class)//开启属性注入,通过@autowired注入@ConditionalOnClass(Person.class)//判断这个类是否在classpath中存在，如果存在，才会实例化一个Bean@ConditionalOnProperty(prefix=\"person\", value=\"enabled\", matchIfMissing = true)public class PersonAutoConfiguration &#123; @Autowired private PersonProperties personProperties; @Bean @ConditionalOnMissingBean(Person.class)//容器中如果没有Person这个类,那么自动配置这个Person public Person person() &#123; Person person = new Person(); person.setName(personProperties.getName()); return person; &#125;&#125; spring.factories 1org.springframework.boot.autoconfigure.EnableAutoConfiguration=xpu.edu.tim.PersonAutoConfiguration 最后使用mvn package将上面项目打包，使用mvn install:install-file命令将打包文件上传到本地Maven仓库进行测试，下面再新建一个Maven项目用于测试。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.2.6.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;edu.tim&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-start&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;spring-boot-start&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入上面的模块 --&gt; &lt;dependency&gt; &lt;groupId&gt;xpu.tim&lt;/groupId&gt; &lt;artifactId&gt;autoconfig-demo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt; &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; IndexController.java 1234567891011@RestController@RequestMapping(\"/\")public class IndexController &#123; @Autowired private Person person; @GetMapping public String index()&#123; return person.sayName(); &#125;&#125;","updated":"2020-04-25T04:46:07.632Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaEE","slug":"JavaEE","permalink":"https://zouchanglin.cn/tags/JavaEE/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://zouchanglin.cn/tags/SpringBoot/"},{"name":"SpringCore","slug":"SpringCore","permalink":"https://zouchanglin.cn/tags/SpringCore/"}]},{"title":"SpringIOC原理与应用","date":"2020-04-05T10:00:00.000Z","path":"2020/04/05/SpringIOC原理与应用/","text":"ICO容器的结构如上图所示，首先要让IOC容器去读取Bean的配置信息，并在容器中生成一份相应的Bean定义注册表，根据这张注册表去实例化Bean，装配好Bean之间的依赖关系，为上层提供准备就绪的环境，Spring提供一个配置文件描述Bean还有Bean之间的依赖关系，利用Java语言的反射功能实例化Bean并建立Bean之间的依赖关系。 SpringIOC容器支持的功能1、依赖注入 2、依赖检查 3、自动装配 4、支持集合 5、指定初始化和销毁的方法 6、支持回调方法，但是需要实现Spring的接口，略带有侵入性 其中最核心的功能就是依赖注入和自动装配了。 SpringIOC源码分析Spring 作者 Rod Johnson 设计了两个接口用以表示容器。BeanFactory和ApplicationContext， BeanFactory 粗暴简单，可以理解为就是个 HashMap，Key 是BeanName，Value 是 Bean 实例。通常只提供注册（put），获取（get）这两个功能。我们可以称之为 “低级容器”。 ApplicationContext 可以称之为 “高级容器”。因为他比 BeanFactory 多了更多的功能。他继承了多个接口。因此具备了更多的功能。 BeanDefinitionBeanDefinition与Bean的关系, 就好比类与对象的关系，类在spring的数据结构就是BeanDefinition。根据BeanDefinition得到的对象就是我们需要的Bean。 理解Bean与BeanDefinition是理解spring的整个架构的基础与关键， BeanDefinition接口是顶级基础接口，用来描述Bean，里面存放Bean元数据，比如Bean类名、scope、属性、构造函数参数列表、依赖的bean、是否是单例类、是否是懒加载等一些列信息。 BeanDefinition继承的两个接口： BeanMetadataElement接口：BeanDefinition元数据，返回该Bean的来源 AttributeAccessor接口：提供对BeanDefinition属性操作能力， 继承或实现BeanDefinition的接口： AbstractBeanDefinition类：抽象类统一实现了BeanDefinition定义的一部分操作，可以说是定义了BeanDefinition很多默认的属性。 正是在AbstractBeanDefinition基础上， Spring衍生出了一些列BeaDefinition。 这里我们可以关注下重写的equals()、hashcode()、toString()方法 此外initMethodName属性，destroyMethodName 属性，这两个属性bean的生命周期有关，此处只提一句，后续讲解。 接下来，我们看看从AbstractBeanDefinition上衍生出来的几个类 RootBeanDefinition: 代表一个xml，java Config来的BeanDefinition ChildBeanDefinition: 可以让子BeanDefinition定义拥有从父母哪里继承配置的能力 GenericBeanDefinition: spring2.5后注册bean首选的是GenericBeanDefinition。GenericBeanDefinition允许动态的设置父bean.GenericBeanDefinition可以作为RootBeanDefinition与ChildBeanDefinition的替代品。 AnnotatedBeanDefinition接口： 表示注解类型BeanDefinition。有两个重要的属性，AnnotationMetadata，MethodMetadata分别表示BeanDefinition的注解元信息和方法元信息 实现了此接口的BeanDefinition可以获取到注解元数据和方法元数据。 AnnotatedGenericBeanDefinition类: 表示@Configuration注解注释的BeanDefinition类 ScannedGenericBeanDefinition类: 表示@Component、@Service、@Controller等注解注释的Bean类 BeanDefinitionRegistry提供向IOC容器注册BeanDefinition对象的方法，将定义Bean 的资源文件解析成 BeanDefinition 后需要将其注入容器中，这个过程就是由 BeanDefinitionRegistry 来完成。 BeanDefinitionRegistry 继承了 AliasRegistry 接口，其核心子类有三个：SimpleBeanDefinitionRegistry、DefaultListableBeanFactory、GenericApplicationContext。 AliasRegistry： 用于别名管理的通用型接口，作为 BeanDefinitionRegistry 的顶层接口。AliasRegistry 定义了一些别名管理的方法。 BeanDefinitionRegistry：BeanDefinition 的注册接口，如 RootBeanDefinition 和 ChildBeanDefinition。它通常由 BeanFactories 实现，在 Spring 中已知的实现者为：DefaultListableBeanFactory 和 GenericApplicationContext。BeanDefinitionRegistry 是 Spring 的 Bean 工厂包中唯一封装 BeanDefinition 注册的接口。BeanDefinitionRegistry 接口定义了关于 BeanDefinition 注册、注销、查询等一系列的操作。 SimpleBeanDefinitionRegistry：SimpleBeanDefinitionRegistry 是 BeanDefinitionRegistry 一个简单的实现，它还继承 SimpleAliasRegistry（ AliasRegistry 的简单实现），它仅仅只提供注册表功能，无工厂功能。SimpleBeanDefinitionRegistry 使用 ConcurrentHashMap 来存储注册的 BeanDefinition。 DefaultListableBeanFactory： DefaultListableBeanFactory，ConfigurableListableBeanFactory（其实就是 BeanFactory ） 和BeanDefinitionRegistry 接口的默认实现：一个基于BeanDefinition元数据的完整 Bean工厂。所以相对于SimpleBeanDefinitionRegistry而言，DefaultListableBeanFactory则是一个具有注册功能的完整Bean工厂。它同样是用ConcurrentHashMap数据结构来存储注册的BeanDefinition。 BeanFactory提供IOC的配置机制、包含Bean的各种定义，便于实例化Bean、建立Bean之间的依赖关系、Bean生命周期的控制等功能。 BeanFactory是个Factory，也就是IOC容器或对象工厂，FactoryBean是个Bean。在Spring中，所有的Bean都是由BeanFactory(也就是IOC容器)来进行管理的。但对FactoryBean而言，这个Bean不是简单的Bean，而是一个能生产或者修饰对象生成的工厂Bean,它的实现与设计模式中的工厂模式和修饰器模式类似，下图是BeanFactory的体系结构： ListableBeanRegistry，该接口定义了访问容器中并基本信息的若干方法。查看Bean的个数，获取某一类型Bean的配置名，查看容器中是否包括某一Bean等方法。 HierarchicalBeanFactory，即父子级联ICO接口，此容器可以通过接口方法访问父容器，通过HierarchicalBeanFactory接口，Spring得IOC容器可以建立父子关联得容器体系，子容器可以访问父容器中的Bean，但是父容器是不能访问子容器中的Bean的。Spring使用父子容器实现了很多功能，比如在SpringMVC中，展现层的Bean位于一个子容器中，而业务层和持久层的Bean位于父容器中，这样展现层的Bean就可以引用业务层和持久层的Bean，而持久层和业务层的却看不到展现层的Bean。 ConfigurableBeanFactory，它在SpringCore中是一个很重要的接口，增强了IOC容器的可定制性，它定义了设置类加载器、属性编辑器、以及属性初始化后置处理器等方法。 AutowireCapableFactory，它定义了将容器中的Bean按照某种规则，比如按照名称匹配，按照类型匹配等，按照这些规则，对Bean进行自动装配。 SingletonBeanRegistry，它允许在运行期间向容器注册Singleton实例Bean的方法，通过这些接口也证明了BeanFactory体系确实是提供了IOC的基础，即依赖注入和Bean的装载等功能。 BeanFactory与ApplicationContextBeanFactory是Spring框架的基础设施，面向SpringApplicationContext面向使用Spring框架的开发者。 ApplicationContext的功能(继承多个接口) BeanFactory：能够管理、装配Bean ResourcePatternResolver：能够加载资源文件 MessageSource：能够实现国际化等功能 ApplicationEventPublisher：能够注册监听器,实现监听机制 ApplicationContext 包含 BeanFactory 的所有特性，通常推荐使用前者。但是也有一些限制情形，比如移动应用内存消耗比较严苛，在那些情景中，使用更轻量级的 BeanFactory 是更合理的。然而，在大多数企业级的应用中，ApplicationContext 是你的首选。 SpringIOC应用示例下面演示一个我们手动装配的例子： Person.java 1234567891011121314151617181920public class Person &#123; private Integer id; private String name; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; ApplicationConfig.java 12345678910@Configurationpublic class ApplicationConfig &#123; @Bean(name = \"person\") public Person initPerson()&#123; Person person = new Person(); person.setId(1); person.setName(\"Tim\"); return person; &#125;&#125; SpringStudyApplication.java 12345678@SpringBootApplicationpublic class SpringStudyApplication &#123; public static void main(String[] args) &#123; ApplicationContext ctx = SpringApplication.run(SpringStudyApplication.class, args); Person person = ctx.getBean(Person.class); System.out.println(\"Name is \" + person.getName()); &#125;&#125; 接下来使用另一种方式，这样就不需要ApplicationConfig了， @SpringBootApplication已经含有了@ComponentScan注解，即已经拥有了扫描器的功能： Person.java 123456789101112131415161718192021222324252627import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Component;@Component(\"person\")public class Person &#123; @Value(\"1\") private Integer id; @Value(\"Tim\") private String name; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 现在看看如果是多个实现类，如何自动注入呢？ 123456789101112131415161718192021222324252627282930public interface Pet &#123; void move();&#125;@Componentpublic class Dog implements Pet &#123; @Override public void move() &#123; System.out.println(\"Running...\"); &#125;&#125;@Componentpublic class Bird implements Pet &#123; @Override public void move() &#123; System.out.println(\"flying...\"); &#125;&#125;@Component(\"person\")public class Person &#123; private Pet pet; public Person(Pet pet) &#123; this.pet = pet; &#125; ...&#125; 此时应该注入哪个呢？其实IOC容器也不知道该注入谁，只能报错。 1Consider marking one of the beans as @Primary, updating the consumer to accept multiple beans, or using @Qualifier to identify the bean that should be consumed @Reource（默认按名称装配，当找不到与名称匹配的bean才会按类型装配）, @Autowired（默认按类型装配，如果我们想使用按名称装配，可以结合@Qualifier注解和@Primary注解一起使用） @Primary自动装配时当出现多个Bean候选者时，被注解为@Primary的Bean将作为首选者，否则将抛出异常 @Qualifier的意思是合格者，通过这个标示，表明了哪个实现类才是我们所需要的","updated":"2020-04-14T03:02:16.327Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaEE","slug":"JavaEE","permalink":"https://zouchanglin.cn/tags/JavaEE/"},{"name":"SpringCore","slug":"SpringCore","permalink":"https://zouchanglin.cn/tags/SpringCore/"}]},{"title":"如何理解SpringIOC","date":"2020-04-05T06:00:00.000Z","path":"2020/04/05/如何理解SpringIOC/","text":"SpringIOC是Spring Core最核心的部分，要了解控制反转(Inversion of Control)，我觉得有必要先了解软件设计的一个重要思想：依赖倒置原则。 1、高层模块不应该依赖底层模块，二者都应该依赖抽象2、抽象不应该依赖细节，细节应该依赖抽象。3、依赖倒置的中心思想是面向接口编程。4、依赖倒置原则是基于这样的设计理念：相对于细节的多变性，抽象的东西要稳定的多。以抽象为基础搭建的架构比以细节为基础搭建的架构要稳定的多。5、使用接口或抽象类的目的是指定好规范，而不涉及任何具体操作，展现细节的任务交给他们的实现类来完成。 依赖注入DI现在假设我们需要设计一个行李箱，行李箱依赖于箱体，箱体依赖于底盘，底盘依赖于轮子。 现在假设轮子需要按尺寸需求更改，那么底盘也得改，箱体也得改，行李箱也要改。代码如下： 现在如果需要把轮子得大小改为动态可调整的，那么上层代码也得跟着变，由此我们可以看到，仅仅是为了修改轮子的构造函数，这种设计却需要修改整个上层所有类的构造函数！在软件工程中，这样的设计几乎是不可维护的——在实际工程项目中，有的类可能会是几千个类的底层，如果每次修改这个类，我们都要修改所有以它作为依赖的类，那软件的维护成本就太高了，这就是典型的上层建筑依赖下层建筑。 所以我们需要进行控制反转（IoC），及上层控制下层，而不是下层控制着上层。我们用依赖注入（Dependency Injection）这种方式来实现控制反转。所谓依赖注入，就是把底层类作为参数传入上层类，实现上层类对下层类的控制。这里我们用构造方法传递的依赖注入方式 重写各个类的构造函数： 依赖注入是如何解决这个问题的呢？我们可以先设计行李箱箱，根据行李箱设计箱体，根据箱体设计底盘，根据底盘设计轮子，如下图所示： 所以，依赖注入即把底层类作为参数传递给上层类，实现上层对下层的控制。 这样即实现了底层类的更改不会影响到上层类的修改，增加了代码的可维护性。 这里我们是采用的构造函数传入的方式进行的依赖注入。其实还有另外三种方法：Setter传递和接口传递和注解的方式。这里就不多讲了，核心思路都是一样的，都是为了实现控制反转。 IOC与DI、DL的关系 EJB即使用DL来实现的IOC，DI是当今IOC的主流实现。 DI提供了Setter注入、接口注入、注解注入、构造器注入等多种注入方式。依赖倒置原则和IOC的关系是怎样的呢？ 依赖倒置原则、IOC、DI、以及spring IOC容器，这四者的关系就是：依赖倒置原则上是一种思想，它主要含义是高层模块，不应该依赖于低层模块，两者都应该依赖其抽象。正是依赖倒置原则的指导，才有了IOC控制反转的思路，需要怎么实现这个思路呢？就离不开依赖注入之类的支撑，Spring等框架，基于IOC才提出了容器的概念。对于IOC来说。最重要的就是容器把容器管理的Bean的生命周期进行控制依赖注入，那什么是控制反转容器IOC container呢？其实就是承载对象的容器，便于组装对象，避免在各处使用new来创建类，并且可以做到统一维护。 如果我们一步一步去new的话，就会是图中的上面的情况，需要根据构造函数一步一步来注入： 而ICO容器注入的过程如图中下面的部分所示，先查找对象的依赖关系，然后自底向上进行注入。","updated":"2020-04-14T03:02:25.225Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaEE","slug":"JavaEE","permalink":"https://zouchanglin.cn/tags/JavaEE/"},{"name":"SpringCore","slug":"SpringCore","permalink":"https://zouchanglin.cn/tags/SpringCore/"}]},{"title":"JUC的四个并发工具类","date":"2020-04-05T04:00:00.000Z","path":"2020/04/05/JUC的四个并发工具类/","text":"本篇文章主要记录了JUC的四个并发工具类，闭锁CountDownlatch、栅栏CyclicBarrier、信号量Semaphore、交换器Exchanger。CountDownlatch通常用于主线程等待其他任务线程执行完毕的场景；CyclicBarrier主要阻塞当前线程，等待其他线程（大家无论谁先跑到A点，必须要等其他线程也到达了A点，大家才能继续）。信号量Semaphore可以用来控制同时访问特定资源的线程数量（比如100个线程只能有10个线程可以获得MySQL连接）。交换器Exchanger很少用，只适用于两个线程在同步点交换数据的场景（如下图）。 闭锁CountDownlatchCountDownLatch也叫闭锁，使得一(多)个主线程必须等待其他线程完成操作后再执行。 CountDownLatch内部维护一个计数器(父类的int state)，主线程先执行await方法，如果此时计数器大于0，则阻塞等待。当一个线程完成任务后，计数器值减1。直到计数器为0时，表示所有的线程已经完成任务，等待的主线程被唤醒继续执行。 CountDownLatch实现主要基于Java同步器AQS，关于AQS的分析可以看我这篇文章：《ReentrantLock与AQS》 ，countDown方法核心实现如下： 12345678910111213141516171819202122232425262728293031public void countDown() &#123; sync.releaseShared(1);&#125;public final boolean releaseShared(int arg) &#123; // countDownLatch -&gt; tryReleaseShared if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125;private void doReleaseShared() &#123; for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125;&#125; CountDownlatch的使用案例，下面使用三个线程来打印三个List，三个线程任务都完成得时候才输出Print Task Finish！ 12345678910111213141516171819202122232425262728293031323334353637import java.util.Arrays;import java.util.List;import java.util.concurrent.CountDownLatch;public class CountDownLatchDemo &#123; public static void main(String[] args) &#123; List&lt;String&gt; list1 = Arrays.asList(\"AAA\", \"BBB\", \"CCC\"); List&lt;String&gt; list2 = Arrays.asList(\"DDD\", \"EEE\", \"FFF\"); List&lt;String&gt; list3 = Arrays.asList(\"GGG\", \"HHH\", \"III\"); CountDownLatch countDownLatch = new CountDownLatch(3); new Thread(()-&gt;&#123; for (String string: list1) &#123; System.out.println(Thread.currentThread().getName() + \":\" + string); &#125; countDownLatch.countDown(); &#125;).start(); new Thread(()-&gt;&#123; for (String string: list2) &#123; System.out.println(Thread.currentThread().getName() + \":\" + string); &#125; countDownLatch.countDown(); &#125;).start(); new Thread(()-&gt;&#123; for (String string: list3) &#123; System.out.println(Thread.currentThread().getName() + \":\" + string); &#125; countDownLatch.countDown(); &#125;).start(); try &#123; countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"Print Task Finish！\"); &#125;&#125; 栅栏CyclicBarrierCyclicBarrier：阻塞当前线程，等待其他线程。等待其它线程，且会阻塞自己当前线程，所有线程必须同时到达栅栏位置后才能继续执行；所有线程到达栅栏处，可以触发执行另外一个预先设置的线程。 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.util.concurrent.BrokenBarrierException;import java.util.concurrent.CyclicBarrier;public class CyclicBarrierDemo &#123; public static void main(String[] args) throws InterruptedException &#123; new CyclicBarrierDemo().go(); &#125; private void go() throws InterruptedException &#123; //初始化栅栏得参与者数为3 CyclicBarrier cyclicBarrier = new CyclicBarrier(3); new Thread(new Task(cyclicBarrier), \"Thread1\").start(); Thread.sleep(1000); new Thread(new Task(cyclicBarrier), \"Thread2\").start(); Thread.sleep(1000); new Thread(new Task(cyclicBarrier), \"Thread3\").start(); &#125; class Task implements Runnable&#123; private CyclicBarrier cyclicBarrier; public Task(CyclicBarrier cyclicBarrier) &#123; this.cyclicBarrier = cyclicBarrier; &#125; @Override public void run() &#123; System.out.println(\"线程\" + Thread.currentThread().getName() + \"已经送达\" + System.currentTimeMillis()); try &#123; cyclicBarrier.await(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"线程\" + Thread.currentThread().getName() + \"开始处理\" + System.currentTimeMillis()); &#125; &#125;&#125; 信号量SemaphoreSemaphore也叫信号量，在JDK1.5被引入，可以用来控制同时访问特定资源的线程数量，通过协调各个线程，以保证合理的使用资源。Semaphore内部维护了一组虚拟的许可，许可的数量可以通过构造函数的参数指定。 访问特定资源前，必须使用acquire方法获得许可，如果许可数量为0，该线程则一直阻塞，直到有可用许可。访问资源后，使用release释放许可。Semaphore和ReentrantLock类似，获取许可有公平策略和非公平许可策略，默认情况下使用非公平策略。 信号量Semaphore得应用场景：Semaphore可以用来做流量分流，特别是对公共资源有限的场景，比如数据库连接。假设有这个的需求，读取几万个文件的数据到数据库中，由于文件读取是IO密集型任务，可以启动几十个线程并发读取，但是数据库连接数只有10个，这时就必须控制最多只有10个线程能够拿到数据库连接进行操作。这个时候，就可以使用Semaphore做流量控制。 12345678910111213141516171819202122232425262728293031package thread_study;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Semaphore;public class SemaphoreDemo &#123; public static void main(String[] args) &#123; ExecutorService pool = Executors.newCachedThreadPool(); //只能5个线程同时访问 Semaphore semaphore = new Semaphore(5); //模拟20个客户端访问 for (int i = 0; i &lt; 20; i++) &#123; final int NO = i; pool.execute(()-&gt;&#123; try &#123; //获取许可 semaphore.acquire(); System.out.println(\"Accessing: \" + NO); Thread.sleep((long)(Math.random() * 10000)); //访问完毕后释放 semaphore.release(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; //退出线程池 pool.shutdown(); &#125;&#125; 交换器ExchangerExchanger（交换者）是一个用于线程间数据交换协作的工具类。它提供一个同步点，在这个同步点多个线程间两两之间线程可以交换彼此的数据。这两个线程通过exchange方法交换数据， 如果第一个线程先执行exchange方法，它会一直等待第二个线程也执行exchange方法，当两个线程都到达同步点时，这两个线程就可以交换数据，将本线程生产出来的数据传递给对方。 12345678910111213141516171819202122232425262728293031323334import java.util.concurrent.Exchanger;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;public class ExchangerDemo &#123; public static void main(String[] args) &#123; Exchanger&lt;String&gt; exchanger = new Exchanger&lt;&gt;(); ExecutorService threadPool = Executors.newFixedThreadPool(2); threadPool.execute(()-&gt;&#123; try &#123; //男生对女生说的话 String girl = exchanger.exchange(\"我其实暗恋你很久了....\"); System.out.println(\"女生说: \" + girl); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); threadPool.execute(()-&gt;&#123; try &#123; System.out.println(\"女生慢慢的从教室走出来.... \"); TimeUnit.SECONDS.sleep(3); //男生对女生说的话 String boy = exchanger.exchange(\"我很喜欢你....\"); System.out.println(\"男生说：\" + boy); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); threadPool.shutdown(); &#125;&#125;","updated":"2020-04-14T03:05:32.002Z","categories":[{"name":"并发编程","slug":"并发编程","permalink":"https://zouchanglin.cn/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"异步","slug":"异步","permalink":"https://zouchanglin.cn/tags/%E5%BC%82%E6%AD%A5/"},{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JUC","slug":"JUC","permalink":"https://zouchanglin.cn/tags/JUC/"}]},{"title":"ForkJoin框架与读写锁","date":"2020-04-04T04:00:00.000Z","path":"2020/04/04/ForkJoin框架与读写锁/","text":"Fork/Join框架就是在必要的情况下，将一个大任务，进行拆分（fork）成若千个小任务（拆到不可再拆时），再将一个个的小任务运算的结果进行join汇总。 ForkJoin框架采用工作窃取模式(work-stealing) :当执行新的任务时它可以将其拆分分成更小的任务执行，并将小任务加到线程队列中，然后再从一个随机线程的队列中偷一个并把它放在自己的队列中。 相对于一般的线程池实现，fork/join框架的优势体现在对其中包含的任务的处理方式上，在一般的线程池中，如果一个线程正在执行的任务由于某些原因无法继续运行，那么该线程会处于等待状态。而在fork/join框架实现中，如果某个子问题由于等待另外一个子问题的完成而无法继续运行。那么处理该子问题的线程会主动寻找其他尚未运行的子问题来执行。这种方式减少了线程的等待时间，提高了性能。 ForkJoin框架下面是一个很简单的示例，即用Fork/Join框架来计算0-500亿的和，普通For用时13745毫秒，Fork/Join框架用时8846毫秒，而且还有拆装箱的时间，足以看出Fork/Join框架的优势。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import java.time.Duration;import java.time.Instant;import java.util.concurrent.ForkJoinPool;import java.util.concurrent.ForkJoinTask;import java.util.concurrent.RecursiveTask;public class TestForkJoinPool &#123; public static void main(String[] args) &#123; Instant start = Instant.now(); ForkJoinPool forkJoinPool = new ForkJoinPool(); ForkJoinTask&lt;Long&gt; task = new ForkJoinSunCalculate(0L, 50000000000L); Long invoke = forkJoinPool.invoke(task); System.out.println(invoke); Instant end = Instant.now(); System.out.println(Duration.between(start, end).toMillis()); //8846 &#125; public static void main(String[] args) &#123; Instant start = Instant.now(); long sum = 0L; for (long i = 0; i &lt; 50000000000L; i++) &#123; sum += i; &#125; System.out.println(sum); Instant end = Instant.now(); System.out.println(Duration.between(start, end).toMillis()); //13745 &#125;&#125;class ForkJoinSunCalculate extends RecursiveTask&lt;Long&gt; &#123; private long start; private long end; //临界值 private static final long VALUE = 10000L; public ForkJoinSunCalculate(long start, long end)&#123; this.start = start; this.end = end; &#125; @Override protected Long compute() &#123; long length = end - start; if(length &lt;= VALUE)&#123; long sum = 0L; for (long i = start; i &lt;= end; i++) &#123; sum += i; &#125; return sum; &#125;else&#123; long middle = (end - start) / 2 + start; ForkJoinSunCalculate leftCalculate = new ForkJoinSunCalculate(start, middle); leftCalculate.fork(); //进行拆分，同时压入线程队列 ForkJoinSunCalculate rightCalculate = new ForkJoinSunCalculate(middle + 1, end); rightCalculate.fork();//进行拆分，同时压入线程队列 return leftCalculate.join() + rightCalculate.join(); &#125; &#125;&#125; 用JDK8的特性stream谁快呢？ 123456789101112131415161718import java.time.Duration;import java.time.Instant;import java.util.concurrent.ForkJoinPool;import java.util.concurrent.ForkJoinTask;import java.util.concurrent.RecursiveTask;import java.util.stream.LongStream;public class TestForkJoinPool &#123; public static void main(String[] args) &#123; Instant start = Instant.now(); long sum = LongStream.rangeClosed(0L, 50000000000L) .parallel() .reduce(0L, Long::sum); System.out.println(sum); Instant end = Instant.now(); System.out.println(Duration.between(start, end).toMillis()); //5156 &#125;&#125; 其实这简单的计算还是stream更快，底层优化太多了。 ReadWriteLockReadWriteLock维护了一对相关的锁，一个用于只读操作，另一个用于写入操作。只要没有writer，读取锁可以由多个reader线程同时保持，写入锁是独占的。 ReadWriteLock读取操作通常不会改变共享资源，但执行写入操作时，必须独占方式来获取锁。对于读取操作占多数的数据结构。ReadWriteLock 能提供比独占锁更高的并发性。而对于只读的数据结构，其中包含的不变性可以完全不需要考虑加锁操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243package thread_study;import java.util.concurrent.locks.ReadWriteLock;import java.util.concurrent.locks.ReentrantReadWriteLock;//读写锁public class TestReadWriteLock &#123; public static void main(String[] args) &#123; ReadWriteLockDemo writeLockDemo = new ReadWriteLockDemo(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 30; i++) &#123; writeLockDemo.set(i); &#125; &#125;, \"Write Thread\").start(); for (int i = 0; i &lt; 20; i++) &#123; new Thread(writeLockDemo::get, \"Read Thread\").start(); &#125; &#125;&#125;class ReadWriteLockDemo&#123; private int number = 0; private ReadWriteLock lock = new ReentrantReadWriteLock(); public void get()&#123; lock.readLock().lock(); try&#123; System.out.println(Thread.currentThread().getName() + \" : \" + number); &#125;finally &#123; lock.readLock().unlock(); &#125; &#125; public void set(int number)&#123; lock.writeLock().lock(); try&#123; System.out.println(Thread.currentThread().getName()); this.number = number; &#125;finally &#123; lock.writeLock().unlock(); &#125; &#125;&#125;","updated":"2020-04-14T03:05:45.788Z","categories":[{"name":"并发编程","slug":"并发编程","permalink":"https://zouchanglin.cn/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JUC","slug":"JUC","permalink":"https://zouchanglin.cn/tags/JUC/"}]},{"title":"向新冠肺炎疫情牺牲烈士和逝世同胞深切哀悼","date":"2020-04-04T02:00:00.000Z","path":"2020/04/04/向新冠肺炎疫情牺牲烈士和逝世同胞深切哀悼/","text":"向新冠肺炎疫情牺牲烈士和逝世同胞深切哀悼。","updated":"2020-04-04T02:30:11.663Z","categories":[{"name":"生活","slug":"生活","permalink":"https://zouchanglin.cn/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[]},{"title":"虚假唤醒与Condition控制线程通信","date":"2020-04-03T03:01:35.000Z","path":"2020/04/03/虚假唤醒与Condition控制线程通信/","text":"本文通过生产者消费者模型主要讲述了什么是虚假唤醒，以及处理处理虚假唤醒。另外还使用了Condition 来控制线程间的通信，Condition接口描述了可能会与锁有关联的条件变量，这些变量在用法上与使用Object.wait 访问的隐式监视器类似，但提供了更强大的功能。需要特别指出的是，单个Lock 可能与多个Condition 对象关联。为了避免兼容性问题，Condition 方法的名称与对应的Object 版本中的不同。 在Condition 对象中，与wait、notify 和notifyAll 方法对应的分别是await、signal 和signalAll。 Condition允许发生虚假唤醒，这通常作为对基础平台语义的让步。不过Condition还是应该总是在一个循环中被等待，避免虚假唤醒的发生。 生产者消费者示例首先引入下面这段生产者和消费者的程序，店员类作为生产产品和消费产品的中介，其中的数据product为共享数据，产品最多只能囤积5个，当产品达到5个还在生产时，就会提示库存已满，类似地，如果产品只有0个了还在消费，会提示库存缺货。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970//店员类class Clerk &#123; private int product = 0; // 进货 public synchronized void get() &#123; if (product &gt;= 5) &#123; System.out.println(\"库存已满\"); &#125; else &#123; System.out.println(Thread.currentThread().getName() + \":\" + ++product); &#125; &#125; // 售货 public synchronized void sale() &#123; if (product &lt;= 0) &#123; System.out.println(\"库存缺货\"); &#125; else &#123; System.out.println(Thread.currentThread().getName() + \":\" + --product); &#125; &#125;&#125;// 生产者类class Productor implements Runnable &#123; private Clerk clerk; public Productor(Clerk clerk) &#123; this.clerk = clerk; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; clerk.get(); &#125; &#125;&#125;//消费者类class Consumer implements Runnable &#123; private Clerk clerk; public Consumer(Clerk clerk) &#123; this.clerk = clerk; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; clerk.sale(); &#125; &#125;&#125;public class ProductorAndConsumer &#123; public static void main(String[] args) &#123; Clerk clerk = new Clerk(); Productor productor = new Productor(clerk); Consumer consumer = new Consumer(clerk); new Thread(productor,\"Productor A\").start(); new Thread(consumer,\"Consumer B\").start(); &#125;&#125; 这是一种不好的情况，因为当产品已满时，还在不停地生产，当缺货时，还在不停地消费。为此，我们引入等待唤醒机制： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283//店员类class Clerk &#123; private int product = 0; // 进货 public synchronized void get() &#123; if (product &gt;= 5) &#123; System.out.println(\"库存已满\"); try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; System.out.println(Thread.currentThread().getName() + \":\" + ++product); this.notifyAll(); &#125; &#125; // 售货 public synchronized void sale() &#123; if (product &lt;= 0) &#123; System.out.println(\"库存缺货\"); try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; System.out.println(Thread.currentThread().getName() + \":\" + --product); this.notifyAll(); &#125; &#125;&#125;// 生产者类class Productor implements Runnable &#123; private Clerk clerk; public Productor(Clerk clerk) &#123; this.clerk = clerk; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; clerk.get(); &#125; &#125;&#125;//消费者类class Consumer implements Runnable &#123; private Clerk clerk; public Consumer(Clerk clerk) &#123; this.clerk = clerk; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; clerk.sale(); &#125; &#125;&#125;public class ProductorAndConsumer &#123; public static void main(String[] args) &#123; Clerk clerk = new Clerk(); Productor productor = new Productor(clerk); Consumer consumer = new Consumer(clerk); new Thread(productor,\"Productor A\").start(); new Thread(consumer,\"Consumer B\").start(); &#125;&#125; 再运行程序，就不会再出现上述的情况： 现在，我们将产品的囤积上限设定为1（这种情况在现实中也是有可能出现的）： 1234567891011121314151617181920//店员类class Clerk &#123; private int product = 0; // 将产品的囤积上限设定为1 public synchronized void get() &#123; if (product &gt;= 1) &#123; System.out.println(\"库存已满\"); try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; System.out.println(Thread.currentThread().getName() + \":\" + ++product); this.notifyAll(); &#125; &#125;&#125; 程序的输出貌似没有问题，但请注意图中箭头所指的地方，这表示程序没有结束，还一直在执行。这是因为，当循坏到最后一轮时，由于产品已满引发了wait()操作，然后生产者线程等待，随后消费者消费了一份产品，并唤醒等待的生产者线程，此时，被唤醒的生产者线程由于循环结束，直接结束了线程的执行，但是另一边，消费者线程没有结束，而且由于将产品消费完后再次进入了等待，但是生产者线程此时已经结束了，不能再唤醒消费者线程，所以便进入了死循环。 解决这种问题的方法时去掉Clerk类中get方法和sale方法的else，并将原来else中的代码直接提出，这样，就算线程结束，也会先再次唤醒等待的线程： 目前是没问题的，但是如果现在有两个（多个）消费者线程和生产者线程，并且我们在生产者类的run方法中添加一个sleep()方法的执行，情况会如何呢？ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586//店员类class Clerk &#123; private int product = 0; public synchronized void get() &#123; if (product &gt;= 1) &#123; System.out.println(\"库存已满\"); try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(Thread.currentThread().getName() + \":\" + ++product); this.notifyAll(); &#125; public synchronized void sale() &#123; if (product &lt;= 0) &#123; System.out.println(\"库存缺货\"); try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(Thread.currentThread().getName() + \":\" + --product); this.notifyAll(); &#125;&#125;// 生产者类class Productor implements Runnable &#123; private Clerk clerk; public Productor(Clerk clerk) &#123; this.clerk = clerk; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; clerk.get(); &#125; &#125;&#125;// 消费者类class Consumer implements Runnable &#123; private Clerk clerk; public Consumer(Clerk clerk) &#123; this.clerk = clerk; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; clerk.sale(); &#125; &#125;&#125;public class ProductorAndConsumer &#123; public static void main(String[] args) &#123; Clerk clerk = new Clerk(); Productor productor = new Productor(clerk); Consumer consumer = new Consumer(clerk); new Thread(productor, \"Productor A\").start(); new Thread(consumer, \"Consumer B\").start(); new Thread(productor, \"Productor C\").start(); new Thread(consumer, \"Consumer D\").start(); &#125;&#125; 但是结果明显是不对的， 错误的原因在于，当一个消费者线程遇到产品为0时，等待，并释放锁标志，然后另外一个消费者线程获取到该锁标志，由于产品仍然为0，也等待，并释放锁标志。这时候，生产者线程获取到锁，在生产一个产品后，执行notifyAll()唤醒所有线程，这时候，一个消费者线程消费一个产品使得产品为0，另外一个消费者线程再消费一个产品使得产品变为了负数，这种现象称为虚假唤醒。 在Object.wait()方法的javadoc中叙述了该如何解决这种问题： 意思即将get和sale方法中的if都改为while，这样，每次被唤醒后，都会再次判断产品数是否&gt;=0： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485//店员类class Clerk &#123; private int product = 0; public synchronized void get() &#123; while (product &gt;= 1) &#123; System.out.println(\"库存已满\"); try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(Thread.currentThread().getName() + \":\" + ++product); this.notifyAll(); &#125; public synchronized void sale() &#123; while (product &lt;= 0) &#123; System.out.println(\"库存缺货\"); try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(Thread.currentThread().getName() + \":\" + --product); this.notifyAll(); &#125;&#125;// 生产者类class Productor implements Runnable &#123; private Clerk clerk; public Productor(Clerk clerk) &#123; this.clerk = clerk; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; clerk.get(); &#125; &#125;&#125;// 消费者类class Consumer implements Runnable &#123; private Clerk clerk; public Consumer(Clerk clerk) &#123; this.clerk = clerk; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; clerk.sale(); &#125; &#125;&#125;public class ProductorAndConsumer &#123; public static void main(String[] args) &#123; Clerk clerk = new Clerk(); Productor productor = new Productor(clerk); Consumer consumer = new Consumer(clerk); new Thread(productor, \"Productor A\").start(); new Thread(consumer, \"Consumer B\").start(); new Thread(productor, \"Productor C\").start(); new Thread(consumer, \"Consumer D\").start(); &#125;&#125; Condition控制线程间通信由于我们之前用的是synchronized做的同步，接下来使用Condition控制线程通信试试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;//店员类class Clerk &#123; private int product = 0; private Lock reentrantLock = new ReentrantLock(); Condition condition = reentrantLock.newCondition(); public void get() &#123; reentrantLock.lock(); try &#123; while (product &gt;= 1) &#123; System.out.println(\"库存已满\"); try &#123; //this.wait(); condition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(Thread.currentThread().getName() + \":\" + ++product); //this.notifyAll(); condition.signalAll(); &#125;finally &#123; reentrantLock.unlock(); &#125; &#125; public void sale() &#123; reentrantLock.lock(); try &#123; while (product &lt;= 0) &#123; System.out.println(\"库存缺货\"); try &#123; //this.wait(); condition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(Thread.currentThread().getName() + \":\" + --product); //this.notifyAll(); condition.signalAll(); &#125;finally &#123; reentrantLock.unlock(); &#125; &#125;&#125;// 生产者类class Productor implements Runnable &#123; private Clerk clerk; public Productor(Clerk clerk) &#123; this.clerk = clerk; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; clerk.get(); &#125; &#125;&#125;// 消费者类class Consumer implements Runnable &#123; private Clerk clerk; public Consumer(Clerk clerk) &#123; this.clerk = clerk; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; clerk.sale(); &#125; &#125;&#125;public class ProductorAndConsumerForLock &#123; public static void main(String[] args) &#123; Clerk clerk = new Clerk(); Productor productor = new Productor(clerk); Consumer consumer = new Consumer(clerk); new Thread(productor, \"Productor A\").start(); new Thread(consumer, \"Consumer B\").start(); new Thread(productor, \"Productor C\").start(); new Thread(consumer, \"Consumer D\").start(); &#125;&#125; 效果依旧，即在使用Condition的时候，之前的wait使用了Condition对象的await()方法，而notify和notifyAll使用Condition对象的signal()和signalAll()来搞定。 其中，Lock 替代了 synchronized 方法和语句的使用，Condition替代了 Object 监视器方法的使用。 Condition实现线程交替执行123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;//线程交替执行public class ThreadAlternate &#123; public static void main(String[] args) &#123; ThreadAlternateRun alternateRun = new ThreadAlternateRun(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 20; i++) &#123; alternateRun.loopA(i); &#125; &#125;, \"A\").start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 20; i++) &#123; alternateRun.loopB(i); &#125; &#125;, \"B\").start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 20; i++) &#123; alternateRun.loopC(i); System.out.println(\"---------------------------------\"); &#125; &#125;, \"C\").start(); &#125;&#125;class ThreadAlternateRun&#123; private int position = 1; Lock lock = new ReentrantLock(); Condition conditionA = lock.newCondition(); Condition conditionB = lock.newCondition(); Condition conditionC = lock.newCondition(); public void loopA(int round)&#123; lock.lock(); try&#123; //判断 if(position != 1)&#123; try &#123; conditionA.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //打印 for (int i = 1; i &lt;=3 ; i++) &#123; System.out.println(Thread.currentThread().getName() + \"\\t\" + i + \"\\t\" + round); &#125; //唤醒 position = 2; conditionB.signal(); &#125;finally &#123; lock.unlock(); &#125; &#125; public void loopB(int round)&#123; lock.lock(); try&#123; //判断 if(position != 2)&#123; try &#123; conditionB.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //打印 for (int i = 1; i &lt;=3 ; i++) &#123; System.out.println(Thread.currentThread().getName() + \"\\t\" + i + \"\\t\" + round); &#125; //唤醒 position = 3; conditionC.signal(); &#125;finally &#123; lock.unlock(); &#125; &#125; public void loopC(int round)&#123; lock.lock(); try&#123; //判断 if(position != 3)&#123; try &#123; conditionC.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //打印 for (int i = 1; i &lt;=3 ; i++) &#123; System.out.println(Thread.currentThread().getName() + \"\\t\" + i + \"\\t\" + round); &#125; //唤醒 position = 1; conditionA.signal(); &#125;finally &#123; lock.unlock(); &#125; &#125;&#125;","updated":"2020-04-14T03:06:46.071Z","categories":[{"name":"并发编程","slug":"并发编程","permalink":"https://zouchanglin.cn/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JUC","slug":"JUC","permalink":"https://zouchanglin.cn/tags/JUC/"}]},{"title":"谈谈Java的线程池","date":"2020-04-02T09:01:35.000Z","path":"2020/04/02/谈谈Java的线程池/","text":"本文主要讲述了使用线程池的好处，Executors创建的五种线程池特点，简单介绍了Fork/Join框架。围绕Executor框架展开，阐述了线程池的工作流程，探讨了ThreadPoolExecutor的全部构造参数和意义，以及阿里巴巴不推荐使用Executors创建线程池的原因，另外，介绍了我们应该怎么样合理的创建线程池，对于CPU密集型和IO密集型以及混合型的创建方式。探讨了新任务提交后的执行流程，另外简单画了一下线程池生命周期图。 Java中的线程池是运用场景最多的并发框架，几乎所有需要异步或者并发执行任务的程序都可以使用线程池。开发中使用线程池的三个优点如下：1、降低资源消耗：通过重复利用已创建的线程降低线程创建和销毁带来的消耗。2、提高响应速度：当任务到达时，任务可以不需要等待线程创建就能立即执行。3、提高线程的可管理性：使用线程池可以统一进行线程分配、调度和监控。 Executors创建的五种线程池利用Executors创建不同的线程池满足不同场景的需求 1、newFixedThreadPool(int nThreads)： 指定工作线程数量的线程池， 如果线程池中正在执行的任务达到设置的线程最大数（无可用线程），则新得任务会放到阻塞队列里等待，有可用线程时按顺序执行。 2、newCachedThreadPool()：处理大量短时间工作任务的线程池 试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程； 如果线程闲置的时间超过阈值，则会被终止并移出缓存； 系统长时间闲置的时候，不会消耗什么资源 3、newSingleThreadExecutor()：创建唯一的工作线程来执行任务，如果线程异常结束，会有另一个线程取代它 4、newSingleThreadScheduledExecutor()与newScheduledThreadPool(int corePoolSize)：定时或者周期性的工作调度，两者的区别在于单一工作线程还是多个线程 5、newWorkStealingPool()：内部会构建ForkJoinPool，利用working-stealing算法，并行地处理任务，不保证处理顺序 Fork/Join框架把大任务分割成若干个小任务并行执行，最终汇总每个小任务结果后得到大任务结果的框架。newWorkStealingPool()就是实现了Fork/Join框架的线程池，什么时working-stealing算法呢？简单来说就是某个线程从其他队列里窃取任务来执行。 Executor框架在 Java 5 之后，并发编程引入了一堆新的启动、调度和管理线程的API。Executor 框架便是 Java 5 中引入的，其内部使用了线程池机制，它在 java.util.cocurrent 包下，通过该框架来控制线程的启动、执行和关闭，可以简化并发编程的操作。 J.U.C的三个Executor接口 Executor：运行新任务的简单接口，将任务提交和任务执行细节解耦 ExecutorService：具备管理执行器和任务生命周期的方法，提交任务机制更完善 ScheduledExecutorService：支持Future和定期执行任务 ThreadPoolExecutorThreadPoolExecutor顾名思义，是一个线程池管理工具类，该类主要提供了任务管理，线程的调度和相关的hook方法来控制线程池的状态。 如上图，有新的任务提交的时候，如果线程池已经处于shutdown状态，此时新任务会被拒绝，用户可以通过实现RejectExecutionHandler来自定义处理逻辑。如果没有被拒绝，那么新任务将被放置在WorkQueue中，WorkQueue就是工作队列，里面存储了将要执行的任务，任务将被添加到线程池的工作线程中去执行。WorkQueue的数据结构各不相同，但是作用都是暂时存储用户提交的任务： 然后由线程池对象去执行调度这些任务，我们可以再看看Worker： 这里的线程由线程工厂所创建。 ThreadPoolExecutor构造函数下面我们可以看看ThreadPoolExecutor的构造函数 123456789101112131415161718192021222324public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; corePoolSize：核心线程数，默认情况下核心线程会一直存活，即使处于闲置状态也不会受存keepAliveTime限制，除非将allowCoreThreadTimeOut设置为true。 maximumPoolSize：线程池所能容纳的最大线程数。超过这个数的线程将被阻塞。当任务队列为没有设置大小的LinkedBlockingDeque时，这个值无效。 keepAliveTime：非核心线程的闲置超时时间，超过这个时间就会被回收。 unit：指定keepAliveTime的单位，如TimeUnit.SECONDS。当将allowCoreThreadTimeOut设置为true时对corePoolSize生效。 workQueue：线程池中的任务队列，常用的有三种队列 SynchronousQueue：是一种无缓冲的等待队列，在某次添加元素后必须等待其他线程取走后才能继续添加； LinkedBlockingDeque：是一个无界缓存的等待队列，不指定容量则为Integer最大值，锁是分离的； ArrayBlockingQueue：是一个有界缓存的等待队列，必须指定大小，锁是没有分离的； threadFactory：线程工厂，提供创建新线程的功能，通过线程工厂可以对线程的一些属性进行定制。 RejectedExecutionHandler：当线程池中的资源已经全部使用，添加新线程被拒绝时，会调用RejectedExecutionHandler的rejectedExecution方法，线程池有以下四种拒绝策略。 AbortPolicy：当任务添加到线程池中被拒绝时，它将抛出RejectedExecutionException 异常。 CallerRunsPolicy：当任务添加到线程池中被拒绝时，会在线程池当前正在运行的Thread线程池中处理被拒绝的任务。 DiscardOldestPolicy：当任务添加到线程池中被拒绝时，线程池会放弃等待队列中最旧的未处理任务，然后将被拒绝的任务添加到等待队列中。 DiscardPolicy：当任务添加到线程池中被拒绝时，线程池将丢弃被拒绝的任务。 为什么不推荐Executors创建线程池1、对于SingleThreadExecutor，SingleThreadExecutor是单线程线程池，只有一个核心线程： 123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; 所以当一个任务提交时，首先会创建一个核心线程来执行任务，如果超过核心线程的数量，将会放入队列中，因为LinkedBlockingQueue是长度为Integer.MAX_VALUE的队列，可以认为是无界队列，因此往队列中可以插入无限多的任务，在资源有限的时候容易引起OOM异常，同时因为无界队列，maximumPoolSize和keepAliveTime参数将无效，压根就不会创建非核心线程。 2、对于FixedThreadPool，FixedThreadPool是固定核心线程的线程池，固定核心线程数由用户传入。 12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 它和SingleThreadExecutor类似，唯一的区别就是核心线程数不同，并且由于使用的是LinkedBlockingQueue，在资源有限的时候容易引起OOM异常。 3、对于CachedThreadPool，CachedThreadPool是一个根据需要创建新线程的线程池。 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 当一个任务提交时，corePoolSize为0不创建核心线程，SynchronousQueue是一个不存储元素的队列，可以理解为队里永远是满的，其中每个插入操作必须等待另一个线程进行相应的删除操作，反之亦然。因此最终会创建非核心线程来执行任务。 对于非核心线程空闲60s时将被回收，第二个参数是线程池所能容纳的最大线程数，因为Integer.MAX_VALUE非常大，可以认为是可以无限创建线程的，在资源有限的情况下容易引起OOM异常。 综上所述，不推荐使用Executors创建线程池，自己把控线程池参数比较好！ 线程池大小如何选定既然不推荐Executors创建线程池，那么我们应该如何选择创建线程池的参数呢？ CPU密集型：线程数 = 按照核数或者核数 + 1 , CPU数量可以根据Runtime.availableProcessors方法获取。 I/O密集型：线程数 = CPU核数 * (1 + 平均等待时间/平均工作时间)。 混合型： 将任务分为CPU密集型和IO密集型，然后分别使用不同的线程池去处理，从而使每个线程池可以根据各自的工作负载来调整。 创建线程池的阻塞队列推荐使用有界队列，有界队列有助于避免资源耗尽的情况发生。 拒绝策略可以默认，也可以根据需要自定义策略。 新任务提交execute执行后的判断1、如果运行的线程少于corePoolSize，则创建新线程来处理任务，即使线程池中的其他线程是空闲的； 2、如果线程池中的线程数量大于等于corePoolSize且小于maximumPoolSize，则只有当workQueue满时才创建新的线程去处理任务； 3、如果设置的corePoolSize和maximumPoolSize相同，则创建的线程池的大小是固定的，这时如果有新任务提交，若workQueue未满，则将请求放入workQueue中，等待有空闲的线程去从workQueue中取任务并处理； 4、如果运行的线程数量大于等于maximumPoolSize，这时如果workQueue已经满了，则通过handler所指定的策略来处理任务； 线程池的5种状态 RUNNING：能接受新提交的任务, 并且也能处理阻塞队列中的任务 SHUTDOWN：不再接受新提交的任务，但可以处理存量任务 STOP：不再接受新提交的任务，也不处理存量任务 TIDYING：所有的任务都已终止 TERMINATED：terminated()方法执行完后进入该状态","updated":"2020-04-14T03:07:11.575Z","categories":[{"name":"并发编程","slug":"并发编程","permalink":"https://zouchanglin.cn/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"线程池","slug":"线程池","permalink":"https://zouchanglin.cn/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"}]},{"title":"CAS操作与ABA问题","date":"2020-04-02T06:01:35.000Z","path":"2020/04/02/CAS操作与ABA问题/","text":"我们在使用锁时，线程获取锁是一种悲观锁策略，即假设每一次执行临界区代码都会产生冲突， 所以当前线程获取到锁的时候同时也会阻塞其他线程获取该锁。而CAS操作(又称为无锁操作)是一种乐观锁策略，它假设所有线程访问共享资源的时候不会出现冲突，既然不会出现冲突自然而然就不会阻塞其他线程的操作。因此，线程就不会出现阻塞停顿的状态。那么，如果出现冲突了怎么办？无锁操作是使用CAS(compare and swap)又叫做比较交换来鉴别线程是否出现冲突，出现冲突就重试当前操作直到没有冲突为止。 CAS的操作过程CAS即Compare and Swap，CAS比较交换的过程可以通俗的理解为CAS(V、A、B)，包含三个值分别为: V内存中实际值、 A预期原值(旧值) 、B更新的新值。当V和A相同时，也就是说旧值和内存中实际的值相同表明该值没有被其他线程更改过，即该旧值A就是目前来说最新的值了，自然而然可以将新值B赋值给V。反之，V和A不相同，表明该值已经被其他线程改过了则该旧值A不是最新版本的值了，所以不能将新值B赋给V，返回V即可。当多个线程使用CAS操作一个变量时，只有一个线程会成功，并成功更新，其余会失败。失败的线程会重新尝试，当然也可以选择挂起线程。 CAS的实现需要硬件指令集的支撑，在JDK1.5后虚拟机才可以使用处理器提供的CMPXCHG指令实现。CAS支持原子更新操作，适用于计数器，序列发生器等场景（序列发生器就是用来给变量自增的工具），CAS属于乐观锁机制，号称lock-free（其实只是上层感知无锁，底层还是有加锁操作的）。CAS操作失败时由开发者决定是继续尝试还是执行别的操作。 基于CAS实现的工具java.util.concurrent包都中的实现类都是基于volatile和CAS来实现的。尤其java.util.concurrent.atomic包下的原子类。 就拿AtomicInteger来说： 可以看到 AtomicInteger 底层用的是volatile的变量和CAS来进行更改数据的。volatile保证可见性，多线程并发时，一个线程修改数据，可以保证其它线程立马看到修改后的值，CAS则保证数据更新的原子性。 CAS多数情况下对开发者来说是透明的。J.U.C的atomic包提供了常用的原子性数据类型以及引用、数组等相关原子类型和更新操作工具，是很多线程安全程序的首选。Unsafe类虽提供CAS服务，但因能够操纵任意内存地址读写而有隐患。JDK9以后，可以使用Variable Handle API来替代Unsafe 模拟实现CASCAS需要硬件层面的支持，所以模拟还是用synchronized来实现一下： 1234567891011121314151617181920212223242526272829303132333435363738public class TestImplementCAS &#123; public static void main(String[] args) &#123; final CompareAndSwap cas = new CompareAndSwap(); for (int i = 0; i &lt; 10; i++) &#123; new Thread(()-&gt;&#123; int expectedValue = cas.get(); boolean b = cas.compareAndSet(expectedValue, (int)(Math.random() * 101)); System.out.println(b); &#125;).start(); &#125; &#125;&#125;class CompareAndSwap&#123; private int value; //获取内存值 public synchronized int get()&#123; return value; &#125; //比较 public synchronized int compareAndSwap(int expectedValue, int newValue)&#123; int oldValue = value; if(oldValue == expectedValue)&#123; this.value = newValue; &#125; return oldValue; &#125; //设置 public synchronized boolean compareAndSet(int expectedValue, int newValue)&#123; return expectedValue == compareAndSwap(expectedValue, newValue); &#125;&#125; CAS的缺点1、ABA问题 因为CAS会检查旧值有没有变化，这里存在这样一个有意思的问题。比如一个旧值A变为了成B，然后再变成A，刚好在做CAS时检查发现旧值并没有变化依然为A，但是实际上的确发生了变化。解决方案可以沿袭数据库中常用的乐观锁方式，添加一个版本号可以解决。在JDK1.5后的atomic包中提供 了AtomicStampedReference来解决ABA问题，解决思路就是这样的。如果需要解决ABA问题，互斥与同步可能比CAS更高效。 2、自旋会浪费大量的处理器资源与线程阻塞相比，自旋会浪费大量的处理器资源。这是因为当前线程仍处于运行状况，只不过跑的是无用指令。它期望在运行无用指令的过程中，锁能够被释放出来。JVM给出的方案是自适应自旋，根据以往自旋等待时能否获取锁，来动态调整自旋的时间。 3、公平性问题自旋状态还带来另外一个副作用，不公平的锁机制。处于阻塞状态的线程，无法立刻竞争被释放的锁。然而，处于自旋状态的线程，则很有可能优先获得这把锁。内建锁无法实现公平机制，而lock体系可以实现公平锁。 解决ABA问题在JDK1.5后的atomic包中提供 了AtomicStampedReference来解决ABA问题， 它通过包装[E，Integer]的元组来对对象标记版本stamp，从而避免ABA问题。在了解AtomicStampedReference之前我们可以先分析一下AtomicReference。 12345678910111213141516import java.util.concurrent.atomic.AtomicReference;public class AtomicReferenceDemo &#123; public static void main(String[] args) &#123; AtomicReference&lt;String&gt; atomicReference = new AtomicReference&lt;&gt;(); atomicReference.set(\"AAA\"); //CAS操作更新 boolean result = atomicReference.compareAndSet(\"AAA\", \"BBB\"); System.out.println(result + \" \" + atomicReference.get()); //CAS操作更新 result = atomicReference.compareAndSet(\"AAA\", \"CCC\"); System.out.println(result + \" \" + atomicReference.get()); &#125;&#125; AtomicReference的成员变量 12345678//Unsafe类提供CAS操作private static final Unsafe unsafe = Unsafe.getUnsafe();//value变量的偏移地址，这个偏移地址在static块里初始化private static final long valueOffset;//实际传入需要原子操作的那个类实例private volatile V value; compareAndSet方法是基于Unsafe提供的compareAndSwapObject方法， 这里的compareAndSet方法即CAS操作本身是原子的，但是在某些场景下会出现异常场景，也就是ABA问题。我们使用AtomicStampedReference来解决这个问题，下面是AtomicStampedReference的关键结构： 123456789101112131415161718public class AtomicStampedReference&lt;V&gt; &#123; private static class Pair&lt;T&gt; &#123; final T reference; //维护对象引用 final int stamp; //用于标志版本 private Pair(T reference, int stamp) &#123; this.reference = reference; this.stamp = stamp; &#125; static &lt;T&gt; Pair&lt;T&gt; of(T reference, int stamp) &#123; return new Pair&lt;T&gt;(reference, stamp); &#125; &#125; private volatile Pair&lt;V&gt; pair; ...&#125;","updated":"2020-04-14T03:07:22.701Z","categories":[{"name":"并发编程","slug":"并发编程","permalink":"https://zouchanglin.cn/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JUC","slug":"JUC","permalink":"https://zouchanglin.cn/tags/JUC/"}]},{"title":"ReentrantLock与AQS","date":"2020-04-01T06:01:35.000Z","path":"2020/04/01/ReentrantLock与AQS/","text":"synchronized和ReentrantLock的区别 先说说synchronized和ReentrantLock的区别吧，AQS的分析得等两天了： 1、ReentrantLock (再入锁)，位于java.util.concurrent.locks包2、和CountDownLatch、Future Task、Semaphore一样基于AQS实现3、能够实现比synchronized更细粒度的控制，如控制公平与非公平4、调用lock()之后，必须调用unlock()释放锁5、性能未必比synchronized高，并且也是可重入的6、synchronized是关键字，ReentrantLock是类7、ReentrantLock可以对获取锁的等待时间进行设置，避免死锁的发生8、ReentrantLock可以获取各种锁的信息9、ReentrantLock可以灵活地实现多路通知10、机制：sync操作Mark Word，lock调用Unsafe类的park()方法 关于ReentrantLock公平性的设置ReentrantLock fairLock = new ReentrantLock(true);1、参数为true时，倾向于将锁赋予等待时间最久的线程2、公平锁：获取锁的顺序按先后调用lock方法的顺序(慎用)3、非公平锁：抢占的顺序不一定，看运气4、synchronized是非公平锁，如果比较强调吞吐量，没必要设置为公平锁 12345678910111213141516171819202122232425262728import java.util.concurrent.locks.ReentrantLock;public class ReentrantLockDemo implements Runnable&#123; private static ReentrantLock reentrantLock = new ReentrantLock(true); @Override public void run() &#123; while (true)&#123; try &#123; reentrantLock.lock(); System.out.println(Thread.currentThread().getName() + \" get lock\"); Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; reentrantLock.unlock(); &#125; &#125; &#125; public static void main(String[] args) &#123; ReentrantLockDemo lockDemo = new ReentrantLockDemo(); Thread thread1 = new Thread(lockDemo, \"Thread1\"); Thread thread2 = new Thread(lockDemo, \"Thread2\"); thread1.start(); thread2.start(); &#125;&#125;","updated":"2020-04-14T03:07:34.507Z","categories":[{"name":"并发编程","slug":"并发编程","permalink":"https://zouchanglin.cn/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JUC","slug":"JUC","permalink":"https://zouchanglin.cn/tags/JUC/"}]},{"title":"synchronized实现与优化","date":"2020-03-31T06:01:35.000Z","path":"2020/03/31/synchronized实现与优化/","text":"本文主要讲述了synchronized的实现原理，还有synchronized的优化。其中主要包括了自适应自旋，锁消除、锁粗化、以及偏向锁和轻量级锁。另外阐述了锁的内存语义和对三种锁的总结。其实工程学科就是不断解决实际问题才能得以发展，synchronized从早期的一上来就直接使用Mutex逐步优化到现在的程度，mutex互斥量是最重要的同步原语，但是我们去使用mutex的时候却会出现诸多问题（比如销毁了已加锁的互斥量、死锁问题）Monitor机制是编程语言在语法上提供的语法糖，假设我们用的是C语言，那么很明显无法使用Monitor机制。 很多文章笼统的说synchronized是基于Monitor机制实现的，我认为这是错误的说法，Monitor 机制必须要有临界区、Monitor对象和互斥量、条件变量和wait()、signal()等要素。在synchronized的实现中根本找不到所有对应的元素。我认为Java的Monitor机制包括了synchronized、Object和程序员自己定的条件变量等（因为我们通常使用wait()、 notify()、notifyAll()配合synchronized才能取解决生产者消费者问题），这才一套完整的Monitor机制。JVM的ObjectMonitor用来辅助Java实现Monitor机制，直接把JVM的ObjectMonitor当成Monitor机制是不可取的。 synchronized实现原理实现synchronized的关键是两个东西，Java对象头和Monitor，在HotShot虚拟机中，对象在内存中的布局分为三块：对象头、实例数据、对齐填充。我们需要重点关注的东西是对象头！ 下图即是32位的JVM Mark Word的结构： Monitor：每个Java对象天生自带了一把看不见的锁，也成为管程和监视器锁，如何看到它的结构呢？ 通过这个地址可以看到Monitor的源码： 当一个线扯获取到对象锁时，_owner就会指向当前线程，并把计数器（_count）+1，如果调用wait方法，就会释放当前线程持有的Monitor，_owner 会变成NULL，计数器（_count）-1， 同时该线程实例会进入等待池。 Monitor锁的竞争、获取与释放 这也就是为什么Java的任意对象都可以作为锁的原因。 接下来我们可以看看synchronized在字节码层面的实现： 123456789101112public class SyncBlockAndMethod &#123; public void syncsTask() &#123; synchronized (this)&#123; System.out.println(\"Hello\"); &#125; &#125; public synchronized void syncTask()&#123; System.out.println(\"Hello Again\"); &#125;&#125; 编译后再通过javac -verbose 指令查看字节码： 执行同步代码块后首先要先执行monitorenter指令，退出的时候monitorexit指令。通过分析之后可以看出，使用Synchronized进行同步，其关键就是必须要对对象的监视器monitor进行获取，当线程获取monitor后才能继续往下执行，否则就只能等待。而这个获取的过程是互斥的，即同一时刻只有一个线程能够获取到monitor。 是否注意到了上述字节码中包含一个monitorenter指令以及多个monitorexit指令。这是因为Java虚拟机需要确保所获得的锁在正常执行路径，以及异常执行路径上都能够被解锁。 那么对于同步方法呢？又是如何实现的？ 当用synchronized标记方法时，字节码中方法的访问标记包括ACC_ SYNCHRONIZED。该标记表示在进入该方法时，Java 虚拟机需要进行monitorenter操作。而在退出该方法时，不管是正常返回，还是向调用者抛异常，Java虚拟机均需要进行monitorexit操作。 这里monitorenter和monitorexit操作所对应的锁对象是隐式的。对于实例方法来说，这两个操作对应的锁对象是this；对于静态方法来说，这两个操作对应的锁对象则是所在类的Class 实例。关于monitorenter和monitorexit的作用，我们可以抽象地理解为每个锁对象拥有一个锁计数器和一 个指向持有该锁的线程的指针。其实看ObjectMonitor的源码，也就是ObjectMonitor.hpp也就能明白这一点。 从互斥锁的设计上来说，当一个线程试图操作一个由其他线程持有的对象锁的临界资源时，将会处于阻塞状态，但当一个线程再次请求自己持有对象锁的临界资源时，这种情况属于重入。 12345678&gt;public void syncsTask() &#123; synchronized (this)&#123; System.out.println(\"Hello\"); synchronized (this)&#123; System.out.println(\"World\"); &#125; &#125; &#125; 比如一个线程获取到了锁，执行System.out.println(&quot;Hello&quot;); 这条语句时，可以再次获得锁去执行System.out.println(&quot;World&quot;); ，这就是可重入的情况！ synchronized的优化在JDK的早期版本中，synchronized属于重量级锁，依赖于Mutex Lock实现，因为监视器锁(也就是Monitor)是基于互斥锁Mutex来实现的，线程之间的切换需要从用户态转换到内核态，开销较大。但是从JDK6以来，HotSpot团队对synchronized做了很多优化，现在已经优化得相当不错了。主要的优化方式分为有轻量级锁（Lightweight Locking）和偏向锁（Biased Locking）、自适应自旋、锁消除、锁粗化等等。 自适应自旋 Adaptive Spinning许多情况下，共享数据的锁定状态持续时间较短，切换线程不值得。通过让线程执行忙循环等待锁的释放，不让出CPU。自旋在JDK1.4就被引入了，默认是关闭状态，JDK1.6默认开启。缺点就是若锁被其他线程长时间占用，会带来许多性能上的开销，在JDK中用户可以根据一个叫做PerBlockSpin的参数来控制自旋时间。 什么是自适应自旋呢？即自旋的次数不再固定，由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。打个比方：红绿灯代表是否获取到了锁，如果之前不熄火等到了绿灯，那么这次不熄火的时间就长一点；如果之前不熄火没等到绿灯，那么这次不熄火的时间就短一点。 锁消除 Lock Eliminate锁消除是一种更彻底的优化，JIT编译时，对运行上下文进行扫描，去除不可能存在竞争的锁。锁消除即删除不必要的加锁操作。根据代码逃逸技术，如果判断到一段代码中，堆上的数据不会逃逸出当前线程，那么可以认为这段代码是线程安全的，不必要加锁。看下面这段程序: 12345678public class StringBufferWithoutSynchronized &#123; public void add(String str1, String str2)&#123; //StringBuffer是线程安全,由于sb只会在append方法中使用,不可能被其他线程引用 //因此sb属于不可能共享的资源, JVM会自动消除内部的锁 StringBuffer stringBuffer = new StringBuffer(); stringBuffer.append(str1).append(str2); &#125;&#125; 锁粗化 Lock Coarsening通过扩大加锁的范围，避免反复加锁和解锁 123456789public class StringBufferWithoutSynchronized &#123; public static String copyString(String target)&#123; StringBuffer stringBuffer = new StringBuffer(); for (int i = 0; i &lt; 100; i++) &#123; stringBuffer.append(target); &#125; return stringBuffer.toString(); &#125;&#125; 这里每次调用stringBuffer.append方法都需要加锁和解锁，如果虚拟机检测到有一系列连串的对同一个对象加锁和解锁操作，就会将其合并成一次范围更大的加锁和解锁操作，即在第一次append方法时进行加锁，最后一次append方法结束后进行解锁。 轻量级锁（Lightweight Locking）和偏向锁（Biased Locking）后面会讲。 轻量级锁和偏向锁synchronized的四种状态：无锁、偏向锁、轻量级锁、重量级锁锁膨胀方向：无锁→偏向锁→轻量级锁→重量级锁，很多观点认为锁不会发生降级，这是不对的，当出现闲置的Monitor的时候可能会发生锁降级。 偏向锁减少同一线程获取锁的代价，大多数情况下，锁不存在多线程竞争，总是由同一线程多次获得 偏向锁的核心思想：如果一个线程获得了锁，那么锁就进入偏向模式，此时Mark Word的结构也变为偏向锁结构，当该线程再次请求锁时，无需再做任何同步操作，即获取锁的过程只需要检查Mark Word的锁标记位为偏向锁以及当前线程Id等于Mark Word的ThreadID即可，这样就省去了大量有关锁申请的操作。偏向锁不适用于锁竞争比较激烈的多线程场合，这种场合下偏向锁就失效了。 偏向锁的获取比较简单：线程在获取锁时，检测对象头的Mark Word里是否存在指向当前线程的指针，如果存在则获取所成功；如果测试失败，检查Mark Word偏向锁标识是否设置为1（也就是判断当前是否还是偏向锁），如果还是偏向锁，就通过CAS操作把Mark Word中的指针指向为当前线程。如果Mark Word偏向锁标识没有设置为1（则说明已经不是偏向锁），此时CAS尝试去竞争锁。 偏向锁的撤销：偏向锁使用了一种等到竞争出现才释放锁的机制， 所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。 轻量级锁轻量级锁是由偏向锁升级来的，偏向锁运行在一个线程进入同步块的情况下，当第二个线程加入锁争用的时候，偏向锁就会升级为轻量级锁。轻量级锁的适应的场景是线程交替执行同步块。若存在同一时间访问同一锁的情况，就会导致轻量级锁膨胀为重量级锁。 轻量级锁的加锁过程：线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 轻量级锁解锁过程：轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。下图是两个线程同时争夺锁， 导致锁膨胀的流程图。 因为自旋会消耗CPU，为了避免无用的自旋(比如获得锁的线程被阻塞住了)，一旦锁升级成重量级锁，就不会再恢复到轻量级锁状态。当锁处于这个状态下，其他线程试图获取锁时，都会被阻塞住，当持有锁的线程释放锁之后会唤醒这些线程，被唤醒的线程就会进行新一轮的夺锁之争。 锁的内存语义当线程释放锁时，Java内存模型会把该线程对应的本地内存中的共享变量刷新到主内存中；而当线程获取锁时，Java内存模型会把该线程对应的本地内存置为无效，从而使得被监视器保护的临界区代码必须从主内存中读取共享变量。 关于Java内存模型，可以参考我之前写的一篇博客《重新认识volatile》 和我转载的一篇博客《理解Java内存模型》中来了解Java内存模型和缓存一致性协议。 总结","updated":"2020-04-14T03:07:55.710Z","categories":[{"name":"并发编程","slug":"并发编程","permalink":"https://zouchanglin.cn/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"多线程","slug":"多线程","permalink":"https://zouchanglin.cn/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"synchronized锁的是什么","date":"2020-03-28T06:01:35.000Z","path":"2020/03/28/synchronized锁的是什么/","text":"最近发现synchronized锁的是什么？甚至有人认为synchronized锁的是代码？？！这个我觉得还是很有必要通过实际的示例来说明synchronized锁的到底是什么。 根据获取的锁的分类：获取对象锁和获取类锁！ 获取对象锁的两种用法 1、同步代码块(synchronized(this), synchronized(类实例对象))，锁是小括号()中的实例对象。2、同步非静态方法(synchronized method)，锁是当前对象的实例对象。 获取类锁的两种用法 1、同步代码块(synchronized(类.class))，锁是小括号()中的类对象，即Class对象。2、同步静态方法(synchronized static method)，锁是当前对象的类对象(Class对象)。 对象锁示例看看这样一段代码： SynchronizedDemo.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package thread_study.synchronize;import java.text.SimpleDateFormat;import java.util.Date;import java.util.concurrent.TimeUnit;public class SynchronizedDemo implements Runnable&#123; @Override public void run() &#123; String threadName = Thread.currentThread().getName(); if(threadName.startsWith(\"A\"))&#123; async(); &#125;else if(threadName.startsWith(\"B\"))&#123; syncObjectBlock(); &#125;else if(threadName.startsWith(\"C\"))&#123; syncObjectMethod(); &#125; &#125; private void async() &#123; try &#123; System.out.println(Thread.currentThread().getName() + \"_Async_Start:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); TimeUnit.SECONDS.sleep(1); System.out.println(Thread.currentThread().getName() + \"_Async_End:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private synchronized void syncObjectMethod() &#123; System.out.println(Thread.currentThread().getName() + \"_SyncObjectMethod:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); try &#123; System.out.println(Thread.currentThread().getName() + \"_SyncObjectMethod_Start:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); TimeUnit.SECONDS.sleep(1); System.out.println(Thread.currentThread().getName() + \"_SyncObjectMethod_End:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private void syncObjectBlock() &#123; System.out.println(Thread.currentThread().getName() + \"_SyncObjectBlock:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); synchronized (this)&#123; try &#123; System.out.println(Thread.currentThread().getName() + \"_SyncObjectBlock_Start:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); TimeUnit.SECONDS.sleep(1); System.out.println(Thread.currentThread().getName() + \"_SyncObjectBlock_End:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; SynchronizedMain.java 123456789101112131415161718192021package thread_study.synchronize;public class SynchronizedMain &#123; public static void main(String[] args) &#123; SynchronizedDemo demo = new SynchronizedDemo(); Thread A_thread1 = new Thread(demo, \"A_thread1\"); Thread A_thread2 = new Thread(demo, \"A_thread2\"); Thread B_thread1 = new Thread(demo, \"B_thread1\"); Thread B_thread2 = new Thread(demo, \"B_thread2\"); Thread C_thread1 = new Thread(demo, \"C_thread1\"); Thread C_thread2 = new Thread(demo, \"C_thread2\"); A_thread1.start(); A_thread2.start(); B_thread1.start(); B_thread2.start(); C_thread1.start(); C_thread2.start(); &#125;&#125; 但是如果我们传入不同的对象呢？ 123456789101112131415161718192021package thread_study.synchronize;public class SynchronizedMain &#123; public static void main(String[] args) &#123; //SynchronizedDemo demo = new SynchronizedDemo(); Thread A_thread1 = new Thread(new SynchronizedDemo(), \"A_thread1\"); Thread A_thread2 = new Thread(new SynchronizedDemo(), \"A_thread2\"); Thread B_thread1 = new Thread(new SynchronizedDemo(), \"B_thread1\"); Thread B_thread2 = new Thread(new SynchronizedDemo(), \"B_thread2\"); Thread C_thread1 = new Thread(new SynchronizedDemo(), \"C_thread1\"); Thread C_thread2 = new Thread(new SynchronizedDemo(), \"C_thread2\"); A_thread1.start(); A_thread2.start(); B_thread1.start(); B_thread2.start(); C_thread1.start(); C_thread2.start(); &#125;&#125; 从上面的代码和结果中我们可以得出一个结论，那就是 获取对象锁的两种用法：1、同步代码块(synchronized(this), synchronized(类实例对象))，锁是小括号()中的实例对象。2、同步非静态方法(synchronized method)，锁是当前对象的实例对象。 同步非静态方法锁住整个方法，而同步代码块只是锁住的只是方法中的一部分代码 获取类锁的两种用法1、同步代码块(synchronized(类.class))，锁是小括号()中的类对象，即Class对象。2、同步静态方法(synchronized static method)，锁是当前对象的类对象(Class对象)。 同步静态方法锁住整个方法，而同步代码块只是锁住的只是方法中的一部分代码，与上面一样的 类锁示例下面来看看同步静态方法和同步代码块(synchronized(类.class))的效果吧： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package thread_study.synchronize;import java.text.SimpleDateFormat;import java.util.Date;import java.util.concurrent.TimeUnit;public class SynchronizedDemo implements Runnable&#123; @Override public void run() &#123; String threadName = Thread.currentThread().getName(); if(threadName.startsWith(\"A\"))&#123; async(); &#125;else if(threadName.startsWith(\"B\"))&#123; syncObjectBlock(); &#125;else if(threadName.startsWith(\"C\"))&#123; syncObjectMethod(); &#125;else if(threadName.startsWith(\"D\"))&#123; syncClassBlock(); &#125;else if(threadName.startsWith(\"E\"))&#123; syncClassMethod(); &#125; &#125; private synchronized static void syncClassMethod() &#123; System.out.println(Thread.currentThread().getName() + \"_SyncClassMethod:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); try &#123; System.out.println(Thread.currentThread().getName() + \"_SyncClassMethod_Start:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); TimeUnit.SECONDS.sleep(1); System.out.println(Thread.currentThread().getName() + \"_SyncClassMethod_End:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private void syncClassBlock() &#123; System.out.println(Thread.currentThread().getName() + \"_SyncClassBlock:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); synchronized (SynchronizedDemo.class)&#123; try &#123; System.out.println(Thread.currentThread().getName() + \"_SyncClassBlock_Start:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); TimeUnit.SECONDS.sleep(1); System.out.println(Thread.currentThread().getName() + \"_SyncClassBlock_End:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private void async() &#123; try &#123; System.out.println(Thread.currentThread().getName() + \"_Async_Start:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); TimeUnit.SECONDS.sleep(1); System.out.println(Thread.currentThread().getName() + \"_Async_End:\" + new SimpleDateFormat(\"mm:ss\").format(new Date())); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //...和上面一样的&#125; SynchronizedMain.java 1234567891011121314151617181920package thread_study.synchronize;public class SynchronizedMain &#123; public static void main(String[] args) &#123; Thread A_thread1 = new Thread(new SynchronizedDemo(), \"A_thread1\"); Thread A_thread2 = new Thread(new SynchronizedDemo(), \"A_thread2\"); Thread D_thread1 = new Thread(new SynchronizedDemo(), \"D_thread1\"); Thread D_thread2 = new Thread(new SynchronizedDemo(), \"D_thread2\"); Thread E_thread1 = new Thread(new SynchronizedDemo(), \"E_thread1\"); Thread E_thread2 = new Thread(new SynchronizedDemo(), \"E_thread2\"); A_thread1.start(); A_thread2.start(); D_thread1.start(); D_thread2.start(); E_thread1.start(); E_thread2.start(); &#125;&#125; 对象锁和类锁不会干扰最后看看类锁和对象锁是否会相互影响呢？ 修改一下代码： 123456789101112131415161718192021222324252627282930package thread_study.synchronize;public class SynchronizedMain &#123; public static void main(String[] args) &#123; //SynchronizedDemo demo = new SynchronizedDemo(); Thread A_thread1 = new Thread(new SynchronizedDemo(), \"A_thread1\"); Thread A_thread2 = new Thread(new SynchronizedDemo(), \"A_thread2\"); Thread B_thread1 = new Thread(new SynchronizedDemo(), \"B_thread1\"); Thread B_thread2 = new Thread(new SynchronizedDemo(), \"B_thread2\"); Thread C_thread1 = new Thread(new SynchronizedDemo(), \"C_thread1\"); Thread C_thread2 = new Thread(new SynchronizedDemo(), \"C_thread2\"); Thread D_thread1 = new Thread(new SynchronizedDemo(), \"D_thread1\"); Thread D_thread2 = new Thread(new SynchronizedDemo(), \"D_thread2\"); Thread E_thread1 = new Thread(new SynchronizedDemo(), \"E_thread1\"); Thread E_thread2 = new Thread(new SynchronizedDemo(), \"E_thread2\"); A_thread1.start(); A_thread2.start(); B_thread1.start(); B_thread2.start(); C_thread1.start(); C_thread2.start(); D_thread1.start(); D_thread2.start(); E_thread1.start(); E_thread2.start(); &#125;&#125; 可以看出来，类锁和对象锁是不会相互干扰的！","updated":"2020-04-14T03:08:08.324Z","categories":[{"name":"并发编程","slug":"并发编程","permalink":"https://zouchanglin.cn/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"多线程","slug":"多线程","permalink":"https://zouchanglin.cn/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"线程相关的基础问题","date":"2020-03-23T06:01:35.000Z","path":"2020/03/23/线程相关的基础问题/","text":"本文主要讲述了进程和线程发展简史，对于JVM来讲的进程和线程又是什么，Thread的start()方法的原生调用发生了什么，从而理解start()方法和run()方法有什么不同，另外，还介绍了三种处理线程执行完成后的返回值的方法，其实FutureTask和线程池获取线程执行结束的返回值更加常用。另外，介绍了线程的六种状态，还有sleep和wait的区别，notify和notifyAll的区别，yield函数的作用，以及如何优雅的中断线程等问题。 关于进程和线程 串行：初期的计算机只能串行执行任务，并且需要长时间等待用户输入 批处理：预先将用户的指令集中成清单，批量串行处理用户指令，仍然无法并发执行 进程：进程独占内存空间，保存各自运行状态，相互间不干扰且可以互相切换，为并发处理任务提供了可能 线程：共享进程的内存资源，相互间切换更快速，支持更细粒度的任务控制，使进程内的子任务得以并发执行 进程是资源分配的最小单位，线程是CPU调度的最小单位所有与进程相关的资源，都被记录在PCB中，进程是抢占处理机的调度单位；线程属于某个进程，共享其资源。线程只由堆栈寄存器、程序计数器和TCB组成。 线程不能看做独立应用，而进程可看做独立应用；进程有独立的地址空间，相互不影响，线程只是进程的不同执行路径；线程没有独立的地址空间，多进程的程序比多线程程序健壮。进程的切换比线程的切换开销大。 Java进程和线程的关系Java对操作系统提供的功能进行封装，包括进程和线程。运行一个程序会产生一个进程，进程包含至少一个线程。每个进程对应一个JVM实例，多个线程共享JVM里的堆，Java采用单线程编程模型，程序会自动创建主线程，主线程可以创建子线程，原则上要后于子线程完成执行。 Thread中的start和run方法的区别调用start()方法会创建一个新的子线程并启动，run()方法只是Thread的一个普通方法的调用。 1234567891011public class ThreadTest &#123; public static void main(String[] args) &#123; new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()); //main &#125;).run(); new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()); //Thread-1 &#125;).start(); &#125;&#125; start()点进去其实调用了start0()，而start0()是一个native方法，通过查看源码我们得知： 其实调用start0()，在底层就是创建了一个线程并且让新的线程执行这个run方法 处理线程返回值1、主线程等待法 这个方法比较简单，但是需要自己实现循环等待的逻辑： 1234567891011121314151617181920212223242526package thread_study;import java.util.concurrent.TimeUnit;public class CycleWait implements Runnable &#123; private String value; @Override public void run() &#123; try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; this.value = \"Hello Thread\"; &#125; public static void main(String[] args) throws InterruptedException &#123; CycleWait cycleWait = new CycleWait(); Thread thread = new Thread(cycleWait); thread.start(); while(cycleWait.value == null)&#123; TimeUnit.SECONDS.sleep(1); &#125; System.out.println(cycleWait.value); &#125;&#125; 2、使用Thread类的join()阻塞当前线程以等待子线程处理完毕，能够实现比我们自己循环等到更为精细的控制 123456789101112131415161718192021222324package thread_study;import java.util.concurrent.TimeUnit;public class JoinWait implements Runnable &#123; private String value; @Override public void run() &#123; try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; this.value = \"Hello Thread\"; &#125; public static void main(String[] args) throws InterruptedException &#123; JoinWait cycleWait = new JoinWait(); Thread thread = new Thread(cycleWait); thread.start(); thread.join(); System.out.println(cycleWait.value); &#125;&#125; 3、通过Callable接口实现：通过FutureTask Or 线程池获取在JDK5之前，线程是没有返回值的，通常为了能够获取线程的返回值而颇费周折 1、通过FutureTask来完成 MyCallable.java 123456789101112import java.util.concurrent.Callable;public class MyCallable implements Callable&lt;String&gt; &#123; @Override public String call() throws Exception &#123; String value = \"test\"; System.out.println(\"Ready to work\"); Thread.sleep(5000); System.out.println(\"Task done\"); return value; &#125;&#125; FutureTaskDemo.java 123456789101112131415package thread_study;import java.util.concurrent.FutureTask;public class FutureTaskDemo &#123; public static void main(String[] args) throws Exception &#123; FutureTask&lt;String&gt; futureTask = new FutureTask&lt;&gt;(new MyCallable()); Thread thread = new Thread(futureTask); thread.start(); if(!futureTask.isDone())&#123; System.out.println(\"Task is not finished, please wait!\"); &#125; System.out.println(\"task return: \" + futureTask.get()); &#125;&#125; 2、通过线程池来完成 还是依旧沿用MyCallable.java的代码，下面是ThreePoolDemo.java 12345678910111213141516171819202122package thread_study;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Future;public class ThreePoolDemo &#123; public static void main(String[] args) &#123; ExecutorService cachedThreadPool = Executors.newCachedThreadPool(); Future&lt;String&gt; futureTask = cachedThreadPool.submit(new MyCallable()); if(!futureTask.isDone())&#123; System.out.println(\"Task is not finished, please wait!\"); &#125; try &#123; System.out.println(futureTask.get()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; cachedThreadPool.shutdown(); &#125; &#125;&#125; 线程的六种状态 新建(New)：创建后尚未启动的线程的状态。 运行(Runnable)：包含Running和Ready。 无限期等待(Waiting)：不会被分配CPU执行时间，需要显式被唤醒 没有设置Timeout参数的0bject.wait()方法。 没有设置Timeout参数的Thread.join()方法。 LockSupport.park()方法。 限期等待(Timed Waiting)：在一定时间后会由系统自动唤醒 Thread.sleep()方法。 设置了Timeout参数的0bject.wait()方法。 设置了Timeout参数的Thread.join()方法。 LockSupport.parkNanos()方法。 LockSupport.parkUntil()方法。 阻塞(Blocked)：等待获取排它锁 结束(Terminated)：已终止线程的状态，线程已经结束执行 那么等待和阻塞式什么关系呢？不都是停下来吗？其实两者都表示线程当前暂停执行的状态，而两者的区别，基本可以理解为：进入 waiting 状态是线程主动的，而进入 blocked 状态是被动的。更进一步的说，进入 blocked 状态是在同步（synchronized）代码之外，而进入 waiting 状态是在同步代码之内（然后马上退出同步）。 sleep与waitsleep和wait最主要的本质区别：Thread.sleep只会让出CPU，不会导致锁行为的改变；Object.wait不仅让出CPU，还会释放已经占有的同步资源锁。这也就是wait必须写在synchronized里面的原因，因为我只有获取到锁了，我才可能释放锁嘛。 notify与notifyAll锁池EntryList： 假设线程A已经拥有了某个对象(不是类)的锁，而其它线程B、C想要调用这个对象的某个synchronized方法(或者块)，由于B、C线程在进入对象的synchronized方法(或者块)之前必须先获得该对象锁的拥有权，而恰巧该对象的锁目前正被线程A所占用，此时B、C线程就会被阻塞，进入一个地方去等待锁的释放，这个地方便是该对象的锁池 等待池WaitSet： 假设线程A调用了某个对象的wait()方法，线程A就会释放该对象的锁，同时线程A就进入到了该对象的等待池中，进入到等待池中的线程，不会去竞争该对象的锁。 notify和notifyAll的区别： notifyAll 会让所有处于等待池的线程全部进入锁池去竞争获取锁的机会 notify只会随机选取一个处于等待池中的线程进入锁池去竞争获取锁的机会。 yield的使用1234567891011121314public class YieldDemo &#123; public static void main(String[] args) &#123; Runnable runnable = () -&gt; &#123; for (int i = 0; i &lt;= 10; i++) &#123; System.out.println(Thread.currentThread().getName() + i); if(i == 5)&#123; Thread.yield(); &#125; &#125; &#125;; new Thread(runnable, \"A\").start(); new Thread(runnable, \"B\").start(); &#125;&#125; 从运行结果来看，yield会让当前线程放弃CPU执行权，让给其他的线程，但是这也是不可控的，调用此方法可能没效果，比如下面这样： 需要注意的是yield不会使当前线程放弃持有的锁 优雅地中断线程 interrupt已经被弃用的方法：stop()，这个方法不仅暴力，而且不安全， 可能使一些清理性的工作得不到完成。还可能对锁定的内容进行解锁，容易造成数据不同步的问题。 使用interrupt方法中断线程：调用interrupt()，即是通知线程应该中断了①如果线程处于被阻塞状态，那么线程将立即退出被阻塞状态，并抛出一个InterruptedException异常。②如果线程处于正常活动状态，那么会将该线程的中断标志设置为true。被设置中断标志的线程将继续正常运行,不受影响。 需要被调用的线程配合中断①在正常运行任务时，经常检查本线程的中断标志位，如果被设置了中断标志就自行停止线程。②如果线程处于正常活动状态，那么会将该线程的中断标志设置为true。被设置中断标志的线程将继续正常运行,不受影响。 123456789101112131415161718192021222324252627282930313233package thread_study;public class InterruptDemo &#123; public static void main(String[] args) throws InterruptedException &#123; Runnable task = ()-&gt;&#123; int i = 0; try&#123; while (!Thread.currentThread().isInterrupted())&#123; Thread.sleep(100); i++; System.out.println(Thread.currentThread().getName() + \" (\" + Thread.currentThread().getState()+\") loop \" + i); &#125; &#125;catch (InterruptedException e)&#123; System.out.println(Thread.currentThread().getName() + \" (\" + Thread.currentThread().getState()+\") catch InterruptedException\"); &#125; &#125;; Thread t1 = new Thread(task, \"T1\"); System.out.println(t1.getName() + \" (\" + t1.getState() + \") is new.\"); t1.start(); System.out.println(t1.getName() + \" (\" + t1.getState() + \") is started.\"); Thread.sleep(300); t1.interrupt(); System.out.println(t1.getName() + \" (\" + t1.getState() + \") is interrupted.\"); Thread.sleep(300); System.out.println(t1.getName() + \" (\" + t1.getState() + \") is interrupted now.\"); &#125;&#125;","updated":"2020-04-14T03:08:18.954Z","categories":[{"name":"并发编程","slug":"并发编程","permalink":"https://zouchanglin.cn/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"多线程","slug":"多线程","permalink":"https://zouchanglin.cn/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"OkHttp的使用与跳坑示例","date":"2020-03-22T06:01:35.000Z","path":"2020/03/22/OkHttp的使用与跳坑示例/","text":"OkHttp是一个优秀的网络请求框架，我开始接触Android开发的时候就用过这个网络请求框架，官方的参考文档在这里 https://square.github.io/okhttp/ ，本文演示了使用OkHttp框架进行简单的Get、Post（表单形式和JSON形式）、Delete、附加请求头、请求异步回调，文件的上传和下载等常用操作。最后记录了一个今天调试了很久的坑，那就是response.body().string()只能有效调用一次，注意Debug的时候对结果造成的改变！JDK9的HttpURLConnection有很多变化，下次博客中会有演示和说明。 首选引入这个依赖就可以使用OkHTTP了 123456789101112&lt;dependency&gt; &lt;groupId&gt;com.squareup.okhttp3&lt;/groupId&gt; &lt;artifactId&gt;okhttp&lt;/artifactId&gt; &lt;version&gt;3.6.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 这个是我演示Post请求JSON格式的时候用到的 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.47&lt;/version&gt;&lt;/dependency&gt; 如果是 gradle 管理的项目，则只需要引入： 1compile 'com.squareup.okhttp3:okhttp:3.6.0' 1、Get请求示例123456789String url = \"http://zouchanglin.cn\";OkHttpClient okHttpClient = new OkHttpClient();Request request = new Request.Builder().get().url(url).build();try &#123; Response execute = okHttpClient.newCall(request).execute(); System.out.println(execute.body().string());&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 2、Post请求示例1、JSON请求 1234567891011121314151617public static void main(String[] args) &#123; String url = \"http://zouchanglin.cn/info/create\"; OkHttpClient okHttpClient = new OkHttpClient(); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(\"name\", \"Mike\"); map.put(\"age\", \"20\"); RequestBody requestBody = RequestBody.create( MediaType.parse(\"application/json; charset=utf-8\"), JSONObject.toJSONString(map)); Request request = new Request.Builder().post(requestBody).url(url).build(); try &#123; Response response = okHttpClient.newCall(request).execute(); System.out.println(response.body().string()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 2、表单数据 1234567891011121314public static void main(String[] args) &#123; String url = \"http://zouchanglin.cn/info/create\"; OkHttpClient okHttpClient = new OkHttpClient(); RequestBody requestBody = new FormBody.Builder() .add(\"name\", \"Mike\") .add(\"age\", \"20\").build(); Request request = new Request.Builder().post(requestBody).url(url).build(); try &#123; Response response = okHttpClient.newCall(request).execute(); System.out.println(response.body().string()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 3、Delete请求示例1234567891011121314public static void main(String[] args) &#123; String url = \"http://zouchanglin.cn/info/remove\"; OkHttpClient okHttpClient = new OkHttpClient(); RequestBody requestBody = new FormBody.Builder() .add(\"id\", \"001002003004\").build(); Request request = new Request.Builder() .delete(requestBody).url(url).build(); try &#123; Response response = okHttpClient.newCall(request).execute(); System.out.println(response.body().string()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 4、附加请求头示例12345678910111213141516public static void main(String[] args) &#123; String url = \"http://zouchanglin.cn/info/remove\"; OkHttpClient okHttpClient = new OkHttpClient(); RequestBody requestBody = new FormBody.Builder() .add(\"id\", \"001002003004\").build(); Request request = new Request.Builder() .delete(requestBody) .addHeader(\"Accept\", \"application/vnd..\") .url(url).build(); try &#123; Response response = okHttpClient.newCall(request).execute(); System.out.println(response.body().string()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 5、请求异步回调示例12345678910111213141516171819public static void main(String[] args) &#123; String url = \"http://zouchanglin.cn/info/remove\"; OkHttpClient okHttpClient = new OkHttpClient(); RequestBody requestBody = new FormBody.Builder() .add(\"id\", \"0010003004\").build(); Request request = new Request.Builder().delete(requestBody).url(url).build(); Call call = okHttpClient.newCall(request); call.enqueue(new Callback() &#123; @Override public void onFailure(Call call, IOException e) &#123; //TODO... &#125; @Override public void onResponse(Call call, Response response) throws IOException &#123; //TODO... &#125; &#125;);&#125; 6、上传文件加参数示例1234567891011121314151617181920212223242526272829303132333435363738@PostMapping(\"upload\")public String uploadToRemoteHost(String ip, String path, String fileId) &#123; String url = String.format(\"http://%s:8080//api/host/file/create/\", ip); //找到文件对象 Optional&lt;ImageFile&gt; bigFileById = fileService.getBigFileById(fileId); if(bigFileById.isPresent())&#123; ImageFile imageFile = bigFileById.get(); OkHttpClient okHttpClient = new OkHttpClient(); MultipartBody.Builder requestBody = new MultipartBody.Builder(); requestBody.setType(MultipartBody.FORM); RequestBody body = RequestBody.create( MediaType.parse(\"application/octet-stream\"), imageFile.getContent().getData()); // 参数分别为 请求key 文件名称 RequestBody requestBody.addFormDataPart(\"file\", imageFile.getName(), body); //要上传的文字参数 Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put(\"name\", imageFile.getName()); map.put(\"path\", path); for (String key : map.keySet()) &#123; requestBody.addFormDataPart(key, map.get(key)); &#125; MultipartBody build = requestBody.build(); try &#123; Request request = new Request.Builder().post(build).url(url).build(); Response execute = okHttpClient.newCall(request).execute(); if(execute.isSuccessful())&#123; return execute.body().string(); &#125; return JSONObject.toJSONString(ResultVOUtil.error(1, \"网络错误\")); &#125; catch (IOException e) &#123; e.printStackTrace(); return JSONObject.toJSONString(ResultVOUtil.error(2, \"网络错误2\")); &#125; &#125; return JSONObject.toJSONString(ResultVOUtil.error(3, \"文件不存在\"));&#125; 7、下载文件示例123456789101112131415161718192021222324252627282930313233public void downloadImg(View view)&#123; OkHttpClient client = new OkHttpClient(); final Request request = new Request.Builder().get() .url(\"http://wwwx.yyy/a.png\") .build(); Call call = client.newCall(request); call.enqueue(new Callback() &#123; @Override public void onFailure(Call call, IOException e) &#123; Log.e(\"moer\", \"onFailure: \");; &#125; @Override public void onResponse(Call call, Response response) throws IOException &#123; //拿到字节流 InputStream is = response.body().byteStream(); int len = 0; File file = new File(Environment.getExternalStorageDirectory(), \"n.png\"); FileOutputStream fos = new FileOutputStream(file); byte[] buf = new byte[128]; while ((len = is.read(buf)) != -1)&#123; fos.write(buf, 0, len); &#125; fos.flush(); //关闭流 fos.close(); is.close(); &#125; &#125;);&#125; 8、OkHttp的坑OkHttp请求回调中response.body().string()只能有效调用一次，调用response.body().string()的时候数据流已经关闭了，再次调用就是提示已经closed，抛出java.lang.IllegalStateException: closed异常，所以这个坑还是有点大，我在debug的时候由于已经监视了一次response.body().string()的返回值，在代码中跑完就是IllegalStateException。 调试代码调试时，表达式的监视有时候会影响代码的运行，比如就像OkHttp这种情况。","updated":"2020-04-14T03:08:39.020Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://zouchanglin.cn/tags/JavaWeb/"},{"name":"异步","slug":"异步","permalink":"https://zouchanglin.cn/tags/%E5%BC%82%E6%AD%A5/"},{"name":"上传下载","slug":"上传下载","permalink":"https://zouchanglin.cn/tags/%E4%B8%8A%E4%BC%A0%E4%B8%8B%E8%BD%BD/"}]},{"title":"Docker私服配置HTTPS","date":"2020-03-21T06:01:35.000Z","path":"2020/03/21/Docker私服配置HTTPS/","text":"上图即使我搭建完毕的效果。对于Docker的环境安装，基础命令之类的内容，通读官网文档内容基本都能顺利掌握。 然而，当笔者尝试着搭建一套基于SSL的Docker Registry（官网推荐的做法）却遇到了不少的麻烦，对于这部分内容，大多数博客文档内容都是直接跳过了SSL的环节，采用了HTTP的访问形式。 然而本文就是记录一下对于搭建HTTPS 访问形式的Docker Registry的过程，读者很容易复现的过程！ 我的环境是CentOS7_x64，Docker版本是 Client: Docker Engine - Community 19.03.7、Server: Docker Engine - Community 19.03.7、Nginx源码包的版本是nginx-1.9.9.tar.gz。 1、获取HTTPS证书就以我的域名为例吧，这是我准备的二级域名 docker.zouchanglin.cn： 如何获得这个二级域名的HTTPS的证书呢？进入这个网站可以申请免费的HTTPS证书： https://freessl.cn/ ，点击创建，于是就会提示安装一个KeyManager的软件，下载之后安装即可，设置密码即可运行。 按照提示的需求配置好解析即可，配置完成之后测试一下是否正常解析，网页上有工具的点一下就行 获取成功之后在Key Manager里就会出现秘钥： 接下来导出一个Nginx用的证书就好了： 2、安装并配置Nginx关于Nginx的安装可以参考我之前的博客《CentOS7编译安装nginx》 可以一开始就把HTTPS模块包含进去，不然得二次编译，如何配置Nginx呢？ 我已经把下载好的证书上传至Linux服务器并解压，解压后得到crt和key两个文件，我把它们放在了/usr/local/certs下面。 还是先给原来的Nginx备份一下，再做修改： 1234567891011121314151617181920212223242526272829303132333435363738394041user root;worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; gzip on; # HTTPS server # server &#123; listen 443 ssl; server_name localhost; client_max_body_size 0; ssl_certificate /usr/local/certs/docker.zouchanglin.cn_chain.crt; ssl_certificate_key /usr/local/certs/docker.zouchanglin.cn_key.key; ssl on; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location ~ &#123; proxy_pass_header Server; proxy_pass https://registry; &#125; &#125; upstream registry &#123; server 127.0.0.1:5000; &#125;&#125; client_max_body_size这个配置是为了防止nginx 之 413 Request Entity Too Large（请求实体太大）这个错误， 最后，nginx启动一下！ 1nginx 3、Docker私服运行直接一条命令就很OK了： 12345678910docker run -d \\ -p 5000:5000 \\ -v /usr/local/registry:/var/lib/registry \\ -v /usr/local/certs:/certs \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/docker.zouchanglin.cn_chain.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/docker.zouchanglin.cn_key.key \\ -e REGISTRY_STORAGE_DELETE_ENABLED=true \\ --restart=always \\ --name registry \\ registry:2 -e REGISTRY_STORAGE_DELETE_ENABLED=true 这条指令主要是打开删除镜像的限制！ 4、检验是否配置成功","updated":"2020-04-14T03:09:11.002Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://zouchanglin.cn/tags/Nginx/"},{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"},{"name":"上传下载","slug":"上传下载","permalink":"https://zouchanglin.cn/tags/%E4%B8%8A%E4%BC%A0%E4%B8%8B%E8%BD%BD/"}]},{"title":"优雅的实现单例","date":"2020-03-18T06:01:35.000Z","path":"2020/03/18/优雅的实现单例/","text":"单例模式属于创建型设计模式，一个类在虚拟机中只有一份实例。实现单例模式的核心思想在于构造函数私有化，主要实现方式分为两种：懒汉式和饿汉式。 饿汉式-线程安全12345678910//饿汉式单例public class HungryTypeSingleByFinal &#123; private static final HungryTypeSingleByFinal hungryTypeSingle = new HungryTypeSingleByFinal(); public static HungryTypeSingleByFinal getInstance()&#123; return hungryTypeSingle; &#125; private HungryTypeSingleByFinal()&#123;&#125;&#125; 饿汉式-线程安全与上面一样，都是在类加载的时候就完成了初始化 123456789101112//饿汉式单例public class HungryTypeSingleByStaticBlock &#123; private static HungryTypeSingleByStaticBlock hungryTypeSingleByStaticBlock; static &#123; hungryTypeSingleByStaticBlock = new HungryTypeSingleByStaticBlock(); &#125; private HungryTypeSingleByStaticBlock()&#123;&#125; public static HungryTypeSingleByStaticBlock getInstance()&#123; return hungryTypeSingleByStaticBlock; &#125;&#125; 懒汉式-线程不安全12345678910111213//懒汉式单例(多线程下不安全)public class LazyTypeSingleNoSafe &#123; private static LazyTypeSingleNoSafe lazyTypeSingleNoSafe; public static LazyTypeSingleNoSafe getInstance()&#123; if(lazyTypeSingleNoSafe == null)&#123; lazyTypeSingleNoSafe = new LazyTypeSingleNoSafe(); &#125; return lazyTypeSingleNoSafe; &#125; private LazyTypeSingleNoSafe()&#123;&#125;&#125; 懒汉式-方法加锁-线程安全12345678910111213//懒汉式单例 + 获取对象加锁public class LazyTypeSingleSafe &#123; private static LazyTypeSingleSafe lazyTypeSingleSafe; public static synchronized LazyTypeSingleSafe getInstance()&#123; if(lazyTypeSingleSafe == null)&#123; lazyTypeSingleSafe = new LazyTypeSingleSafe(); &#125; return lazyTypeSingleSafe; &#125; private LazyTypeSingleSafe()&#123;&#125;&#125; 懒汉式-双重检查-线程安全123456789101112131415//懒汉式单例 + 双重锁检查public class LazyTypeSingleSafeDoubleCheck &#123; private static LazyTypeSingleSafeDoubleCheck lazyTypeSingleSafeDoubleCheck; public static LazyTypeSingleSafeDoubleCheck getInstance()&#123; if(lazyTypeSingleSafeDoubleCheck == null)&#123; synchronized (LazyTypeSingleSafeDoubleCheck.class)&#123; if(lazyTypeSingleSafeDoubleCheck == null)&#123; lazyTypeSingleSafeDoubleCheck = new LazyTypeSingleSafeDoubleCheck(); &#125; &#125; &#125; return lazyTypeSingleSafeDoubleCheck; &#125; private LazyTypeSingleSafeDoubleCheck()&#123;&#125;&#125; 这种做法相对于上面的做法的好处就是，如果已经实例化了则直接返回对象，而不是像上面那样每次都进入同步方法，双重检查只是在对象未初始化的时候加锁，一旦对象已经初始化则后面的线程无需加锁直接获取到了单例对象，无疑减小了开销。 表面看起来线程安全，逻辑也没问题，实则有漏洞，后面会讲 懒汉式-静态内部类-线程安全(推荐)12345678910//懒汉式单例 静态内部类实现（推荐）public class LazyTypeSingleSafeInnerClass &#123; private static class LazyTypeSingleSafeInnerClassHolder&#123; private static LazyTypeSingleSafeInnerClass singleSafe = new LazyTypeSingleSafeInnerClass(); &#125; public static LazyTypeSingleSafeInnerClass getInstance()&#123; return LazyTypeSingleSafeInnerClassHolder.singleSafe; &#125; private LazyTypeSingleSafeInnerClass()&#123;&#125;&#125; 这种方式是比较推荐的方式，从外部无法访问静态内部类LazyTypeSingleSafeInnerClassHolder，只有当调用LazyTypeSingleSafeInnerClass.getInstance方法的时候，才能得到单例对象singleSafe。 这里要注意的是singleSafe对象初始化的时机并不是在单例类LazyTypeSingleSafeInnerClass被加载的时候，而是在调用getInstance方法，使得静态内部类LazyTypeSingleSafeInnerClassHolder被加载的时候。因此这种实现方式是利用classloader的加载机制来实现懒加载，并保证构建单例的线程安全。 无法破解的单例模式上述单例模式都可以通过反射的方式构造出新的对象，毕竟反射大法香呀： 123456789101112131415161718192021222324252627282930public static void testLazyTypeSingleSafeInnerClass() throws Exception &#123; System.out.println(LazyTypeSingleSafeInnerClass.getInstance() == LazyTypeSingleSafeInnerClass.getInstance()); //true Class&lt;LazyTypeSingleSafeInnerClass&gt; innerClassClass = LazyTypeSingleSafeInnerClass.class; Constructor&lt;LazyTypeSingleSafeInnerClass&gt; constructor = innerClassClass.getDeclaredConstructor(null); constructor.setAccessible(true); System.out.println(constructor.newInstance(null) == constructor.newInstance(null)); //false&#125;public static void threadEnvTest() throws InterruptedException &#123; Set&lt;LazyTypeSingleNoSafe&gt; typeSingleNoSafeList = new HashSet&lt;&gt;(); AtomicInteger atomicInteger = new AtomicInteger(0); while(atomicInteger.getAndSet(atomicInteger.intValue() + 1) &lt; 50)&#123; new Thread(()-&gt;&#123; LazyTypeSingleNoSafe instance = LazyTypeSingleNoSafe.getInstance(); typeSingleNoSafeList.add(instance); &#125;).start(); &#125; for(LazyTypeSingleNoSafe lazyTypeSingleNoSafe: typeSingleNoSafeList)&#123; System.out.println(lazyTypeSingleNoSafe); &#125; //single.LazyTypeSingleNoSafe@6e5802d8 //single.LazyTypeSingleNoSafe@6706a70b //single.LazyTypeSingleNoSafe@51a22f1d&#125; 通过上述例子我们也看到了，在反射眼里，一切都是弟弟，所以我们根本不可能造出真正的单例，但是我们却可以通过枚举这个特性来实现绝对的单例模式和多例模式！ 1234//绝对的单例模式（之前的通过反射都可以破解）public enum AbsoluteSingleSafe &#123; ONLY_ONE_SINGLE&#125; 我们破解一下枚举试试： 12345678910public static void testAbsoluteSingleSafe() throws Exception &#123; System.out.println(AbsoluteSingleSafe.ONLY_ONE_SINGLE == AbsoluteSingleSafe.ONLY_ONE_SINGLE); //true Class&lt;AbsoluteSingleSafe&gt; absoluteSingleSafeClass = AbsoluteSingleSafe.class; Constructor&lt;AbsoluteSingleSafe&gt; constructor = absoluteSingleSafeClass.getDeclaredConstructor(null); constructor.setAccessible(true); System.out.println(constructor.newInstance(null) == constructor.newInstance(null)); //Exception in thread \"main\" java.lang.NoSuchMethodException: single.AbsoluteSingleSafe.&lt;init&gt;()&#125; DoubleCheck的隐患我们回顾一下DoubleCheck的代码： 1234567891011121314public class LazyTypeSingleSafeDoubleCheck &#123; private static LazyTypeSingleSafeDoubleCheck lazyTypeSingleSafeDoubleCheck; public static LazyTypeSingleSafeDoubleCheck getInstance()&#123; if(lazyTypeSingleSafeDoubleCheck == null)&#123; synchronized (LazyTypeSingleSafeDoubleCheck.class)&#123; if(lazyTypeSingleSafeDoubleCheck == null)&#123; lazyTypeSingleSafeDoubleCheck = new LazyTypeSingleSafeDoubleCheck(); &#125; &#125; &#125; return lazyTypeSingleSafeDoubleCheck; &#125; private LazyTypeSingleSafeDoubleCheck()&#123;&#125;&#125; 问题出在哪里呢？ 我们可以假设这样的情况，当两个线程一先一后访问getInstance方法的时候，当A线程正在构建对象，B线程刚刚进入方法： 这种情况表面看似没什么问题，要么Instance还没被线程A构建，线程B执行if(lazyTypeSingleSafeDoubleCheck== null)的时候得到true；要么Instance已经被线程A构建完成，线程B执行 if(lazyTypeSingleSafeDoubleCheck== null)的时候得到false。真是如此吗？答案是否定的。这里涉及到了JVM编译器的指令重排。 一句简单的lazyTypeSingleSafeDoubleCheck = new LazyTypeSingleSafeDoubleCheck(); 会被编译器编译成如下JVM指令 : memory = allocate(); //1：分配对象的内存空间 ctorInstance(memory); //2：初始化对象 instance =memory; //3：设置instance指向刚分配的内存地址 但是这些指令顺序并非一成不变，有可能会经过JVM和CPU的优化，指令重排成下面的顺序： memory =allocate(); //1：分配对象的内存空间 instance =memory; //3：设置instance指向刚分配的内存地址 ctorInstance(memory); //2：初始化对象 当线程A执行完1和3时，instance对象还未完成初始化，但已经不再指向null。此时如果线程B抢占到CPU资源，执行 if(instance == null)的结果会是false，从而返回一个没有初始化完成的instance对象。如下图所示： 由于线程A还未完成初始化工作，但是线程B检测到对象已经不为空，于是最终返回的是空对象！！那么应该如何避免呢？其实只需要在instance对象前面增加一个修饰符volatile就好了，关于可以看《重新认识volatile》 这篇文章，里面讲述的比较详细，在此不再赘述。所以完整的双重检查的代码是： 123456789101112131415//懒汉式单例 + 双重锁检查public class LazyTypeSingleSafeDoubleCheck &#123; private volatile static LazyTypeSingleSafeDoubleCheck lazyTypeSingleSafeDoubleCheck; public static LazyTypeSingleSafeDoubleCheck getInstance()&#123; if(lazyTypeSingleSafeDoubleCheck == null)&#123; synchronized (LazyTypeSingleSafeDoubleCheck.class)&#123; if(lazyTypeSingleSafeDoubleCheck == null)&#123; lazyTypeSingleSafeDoubleCheck = new LazyTypeSingleSafeDoubleCheck(); &#125; &#125; &#125; return lazyTypeSingleSafeDoubleCheck; &#125; private LazyTypeSingleSafeDoubleCheck()&#123;&#125;&#125; 单例模式的总结所以如果想要实现线程安全的单例模式，可以使用饿汉式、DoubleCheck（加volatile的版本），静态内部类，枚举等方式；想要使用懒加载策略就不能使用枚举了，只能DoubleCheck（加volatile的版本），静态内部类；如果想实现反射也无法破解的单例那么只能用枚举了，但是一般情况下不会去刻意排斥反射。所以比较推荐的方案还是静态内部类，简单实用而且线程安全。","updated":"2020-03-18T15:52:17.775Z","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://zouchanglin.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JVM","slug":"JVM","permalink":"https://zouchanglin.cn/tags/JVM/"}]},{"title":"Java的四种引用","date":"2020-03-15T06:01:35.000Z","path":"2020/03/15/Java的四种引用/","text":"在Java语言中，除了原始数据类型的变量，其他所有都是所谓的引用类型，指向各种不同的对象，理解引用对于掌握Java对象生命周期和JVM内部相关机制非常有帮助。本文讲述了强引用、软引用、弱引用、幻象引用的区别以及一些具体使用场景，而且是配合ReferenceQueue使用。 强引用我们平常典型编码0bject obj = new Object()中的obj就是强引用。通过关键字new创建的对象所关联用就是强引用。当IVM内存空间不足，JVM宁愿抛出OutOfMemoryError运行时错误(OOM)，使程序异常终止，也不会靠随意回收具有强引用的”存活”对象来解决内存不足的问题。对于个普通的对象，如果没有其他的引用关系，只要超过了引用的作用域或者显式地将相应(强)引用赋值为null，就是可以被垃圾收集的了，具体回收时机还是要看垃圾收集策略。 软引用软引用通过SoftReference类实现。软引 l用的生命周期比强引用短一些。只有当JVM认为内存不足时，才会去试图回收软引用指向的对象：即JVM 会确保在抛出OutOfMemoryError之前，清理软引用指向的对象。软用可以和一个引用队列(ReferenceQueue) 联合使用，如果软引用所引用的对象被垃圾回收器回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。后续，我们可以调用ReferenceQueue的poll()方法来检查是否有它所关心的对象被回收。如果队列为空，将返回一个null，该方法返回队列中前面的一个Reference对象。 应用场景：软引用通常用来实现内存敏感的缓存。如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。 12String str = new String(\"ABC\"); //强引用SoftReference&lt;String&gt; softReference = new SoftReference&lt;&gt;(str); //软引用 弱引用弱引用通过WeakReference类实现。弱用的生命周期比软引用短。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。于垃圾回收器是一个优先级很低的线程，因此不一定会很快回收弱引用的对象。弱引用可以和引用队列(ReferenceQueue) 联合使用，如果弱引用所引用的对象被垃圾回收，Java虛拟机就会把这个弱用加入到与之关联的引用队列中。 应用场景：弱应用同样可用于内存敏感的缓存。 在静态内部类中，经常会使用虚引用。例如：一个类发送网络请求，承担callback的静态内部类，则常以虚引用的方式来保存外部类(宿主类)的引用，当外部类需要被JVM回收时，不会因为网络请求没有及时回来，导致外部类不能被回收，引起内存泄漏 12String str = new String(\"ABC\"); //强引用WeakReference&lt;String&gt; weakReference = new WeakReference&lt;&gt;(str); //弱引用 虚引用特点：虚引用也叫幻象引用，通过PhantomReference类来实现。无法通过引用访问对象的任何属性或函数。幻象引用仅仅是提供了一种确保对象被finalize以后，做某些事情的机制。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。虚引用必须和引用队列(ReferenceQueue) 联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。 12ReferenceQueue queue = new ReferenceQueue();PhantomReference pr = new PhantomReference(object, queue); 程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取一些程序行动。 应用场景：可用来跟踪对象被垃圾回收器回收的活动，当一个引用关联的对象被垃圾收集器回收之前会收到一系统通知。 123String str = new String(\"ABC\"); //强引用ReferenceQueue&lt;String&gt; referenceQueue = new ReferenceQueue&lt;&gt;(); //引用队列PhantomReference&lt;String&gt; phantomReference = new PhantomReference&lt;String&gt;(str,referenceQueue); //虚引用 ReferenceQueue四种引用由强到弱分别是：强引用 &gt; 软引用 &gt; 弱引用 &gt; 虚引用。 ReferenceQueue无实际存储结构，存储逻辑依赖于内部节点之间的关系来表达。存储关联的且被GC的软引用，弱引用以及虚引用。下面可以看一个示例： NormalObject.java 12345678910111213package xpu.edu.tim;public class NormalObject &#123; public String name; public NormalObject(String name) &#123; this.name = name; &#125; @Override protected void finalize() throws Throwable &#123; System.out.println(\"Finalizing obj \" + name); &#125;&#125; NormalObjectWeakReference.java 12345678910111213141516171819package xpu.edu.tim;import java.lang.ref.ReferenceQueue;import java.lang.ref.WeakReference;public class NormalObjectWeakReference extends WeakReference&lt;NormalObject&gt; &#123; public String name; public NormalObjectWeakReference(NormalObject normalObject, ReferenceQueue&lt;NormalObject&gt; q) &#123; super(normalObject, q); this.name = normalObject.name; &#125; @Override protected void finalize() throws Throwable &#123; super.finalize(); System.out.println(\"Finalizing NormalObjectWeakReference \"+ name); &#125;&#125; ReferenceQueueTest.java 123456789101112131415161718192021222324252627282930313233343536package xpu.edu.tim;import java.lang.ref.Reference;import java.lang.ref.ReferenceQueue;import java.lang.ref.WeakReference;import java.util.ArrayList;import java.util.concurrent.TimeUnit;public class ReferenceQueueTest &#123; private static ReferenceQueue&lt;NormalObject&gt; rq = new ReferenceQueue&lt;&gt;(); private static void checkQueue()&#123; Reference&lt;NormalObject&gt; reference = null; while ((reference = (Reference&lt;NormalObject&gt;) rq.poll()) != null)&#123; if(reference != null)&#123; System.out.println(\"In queue: \"+ ((NormalObjectWeakReference)reference).name); System.out.println(\"reference object: \"+ reference.get()); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; ArrayList&lt;WeakReference&lt;NormalObject&gt;&gt; weakReferenceArrayList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 3; i++) &#123; weakReferenceArrayList.add(new NormalObjectWeakReference(new NormalObject(\"Weak \" + i), rq)); System.out.println(\"Created weak:\" + weakReferenceArrayList.get(i)); &#125; System.out.println(\"First time\"); checkQueue(); System.gc(); TimeUnit.SECONDS.sleep(1); System.out.println(\"Second time\"); checkQueue(); System.out.println(\"Third time\"); &#125;&#125; 上面的示例是否有点复杂呢？看看下面这个也行： 1234567891011121314151617181920212223242526package xpu.edu.tim;import java.lang.ref.PhantomReference;import java.lang.ref.Reference;import java.lang.ref.ReferenceQueue;import java.util.concurrent.TimeUnit;public class EasyReferenceQueueDemo &#123; public static void main(String[] args) &#123; Object object = new Object(); ReferenceQueue referenceQueue = new ReferenceQueue&lt;&gt;(); PhantomReference&lt;Object&gt; phantomReference = new PhantomReference&lt;&gt;(object, referenceQueue); object = null; System.gc(); try &#123; TimeUnit.SECONDS.sleep(1); //给GC足够时间回收 Reference&lt;Object&gt; reference = referenceQueue.remove(2000L); if(reference != null)&#123; //TODO something System.out.println(\"do something\"); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 最终打印出do something，remove是一个阻塞方法，可以指定timeout， 或者选择一直阻塞。","updated":"2020-04-14T03:09:44.209Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JVM","slug":"JVM","permalink":"https://zouchanglin.cn/tags/JVM/"}]},{"title":"请丢弃finalize","date":"2020-03-14T23:01:35.000Z","path":"2020/03/15/请丢弃finalize/","text":"用过JDK9的同学应该发现了，finalize方法在JDK9中已经被标记为deprecated，今天探讨一下finalize方法。如果没有特别的原因，不要实现finalize方法，也不要指望利用它来进行资源回收。因为你无法保证finalize什么时候执行，执行的是否符合预期。使用不当会影响性能，导致程序死锁、挂起等。 首先要明白的是为什么会出现finalize方法，因为早期一部分程序员是写C++的，仍然保留析构函数释放资源的行为，所以为了让他们平滑的过渡到Java并且适应Java，所以出现了finalize方法，用来对象被回收前的一次自我拯救，完成释放资源的操作。那么Object的finalize()方法的作用是否与C++的析构函数作用相同呢？其实finalize方法与C++的析构函数是有很大不同的，析构函数调用确定，而finalize是不确定的。重写了finalize的对象如果未被引用就会被放置于F-Queue队列，而且由一个优先级极底的线程成来执行finalize方法，而且方法执行随时可能会被终止。 finalize的执行是和垃圾收集关联在一起的，一实现了非空的finalize 方法，就会导致相应对象回收呈现数量级上的变慢，有人专做过benchmark，大概是40~ 50倍的下降。因为，finalize被设计成在对象被垃圾收集前调用，这就意味着实现了finalize方法的对象是个“特殊公民”， JVM要对它进行额外处理。finalize本质上成为了快速回收的阻碍者，可能导致你的对象经过多个垃圾收集周期才能被回收。 下面来看一个示例： 1234567891011121314151617181920212223public class Finalization &#123; public static Finalization finalization; @Override protected void finalize()&#123; System.out.println(\"Finalized\"); finalization = this; &#125; public static void main(String[] args) &#123; Finalization f = new Finalization(); System.out.println(\"First print: \" + f); f = null; System.gc(); try &#123; // 休息一段时间，让上面的垃圾回收线程执行完成 TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println(\"Second print: \" + f); System.out.println(f.finalization); &#125;&#125; 可见finalize会拖慢垃圾收集，导致对象堆积，容易导致OOM，而且使用finalize释放资源是不合理的，资源应该是用完立即释放的，或者采用资源池来解决这个问题，而不是依靠被动垃圾回收时触发finalize方法。","updated":"2020-04-14T03:09:58.959Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JVM","slug":"JVM","permalink":"https://zouchanglin.cn/tags/JVM/"},{"name":"异常处理","slug":"异常处理","permalink":"https://zouchanglin.cn/tags/%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"}]},{"title":"9种垃圾收集器","date":"2020-03-11T23:01:35.000Z","path":"2020/03/12/9种垃圾收集器/","text":"目前的垃圾收集器主要有7种，上图是他们的使用关系，连在一起的就可以配合使用。JDK11出现两种新的垃圾收集器，一个是Epsilon垃圾收集器，一个是ZGC垃圾收集器。垃圾收集器中很重要的两个概念：Stop-The-World和Safepoint。首先说说Stop-The-World：JVM由于要执行GC而停止了应用程序的执行，任何一种GC算法中都会发生。多数GC优化通过减少Stop-the -world发生的时间来提高程序性能。安全点 Safepoint：分析过程中对象引用关系不会发生变化的点，产生Safepoint的地方：方法调用、循环跳转、异常跳转等，安全点数量得适中。 各种收集器的关系JVM的两种运行模式，在[《HotSpot JVM类型以及编译模式》]( https://zouchanglin.cn/2019/10/25/HotSpot JVM类型以及编译模式) 一文中说道了，JVM有Server和Client型、Server型启动比较慢，属于做了重量级的优化，Client型启动较Server快。 下面来看看三个新生代的收集器：Serial收集器、Serial收集器和Parallel Scavenge收集器 1、Serial收集器Serial收集器（-XX:+UseSerialGC，复制算法）是Java中最基本的收集器，历史悠久，在JDK1.3.1之前是Java虚拟机新生代的唯一选择。单线程收集，进行垃圾收集时，必须暂停所有工作线程。由于Serial收集器采用复制算法，简单高效，Client模式下默认的年轻代收集器就是Serial。 2、ParNew收集器ParNew收集器（-XX:+UseParNewGC，复制算法），多线程收集， ParNew收集器其实是Serial收集器的多线程版本，它是许多运行在Server模式下的虚拟机中首选的新生代收集器，其余的行为特点和Serial收集器一样。单核执行效率不如Serial（因为存在线程切换的开销），在多核下执行才有优势。 同时ParNew也是一个很重要的垃圾收集器， 因为除了Serial收集器外，目前只有它能与CMS收集器配合工作。 Serial与ParNew收集器都是比较强调很短的停顿时间。 3、Parallel Scavenge收集器Parallel Scavenge收集器（-XX:+UseParallelGC，复制算法），多线程收集，比起关注用户线程停顿时间，更关注系统的吞吐量。在多核下执行才有优势，Server模式下默认的年轻代收集器 吞吐量即CPU用于运行用户代码的时间与CPU总消耗时间的比值，吞吐量=运行用户代码时间 /（运行用户代码时间+垃圾收集时间）， 假设虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，吞吐量就是99%。 对于需要与用户交互的程序来说停顿时间越短越好，良好的响应速度能提升用户的体验。 对于后台计算等任务，需要最高效率地利用CPU时间，尽快地完成程序的运算任务，这就是高吞吐量 上图与ParNew收集器的一致，因为都是多线程+复制算法收集，只不过关注点不同，ParNew收集器关注停顿时间，而Parallel Scavenge收集器关注吞吐量。下面看看相关参数： -XX:MaxGCPauseMillis 控制最大垃圾收集停顿时间（大于0的毫秒数）。停顿时间缩短是以牺牲吞吐量和新生代空间换取的。（新生代调的小，吞吐量跟着小，垃圾收集时间就短，停顿就小）。 -XX:GCTimeRatio 直接设置吞吐量大小，0&lt;x&lt;100 的整数，允许的最大GC时间=1/(1+x)。 -XX:+UseAdaptiveSizePolicy 一个开关参数，开启GC自适应调节策略（GC Ergonomics），将内存管理的调优任务（新生代大小-Xmn、Eden与Survivor区的比例-XX:SurvivorRatio、晋升老年代对象年龄-XX:PretenureSizeThreshold 、等细节参数）交给虚拟机完成。 下面来看看三个老年代的收集器：Serial Old收集器、Serial收集器和Parallel Scavenge收集器 4、Serial Old收集器Serial Old收集器（-XX:+UseSerialOldGC，标记-整理算法），单线程收集，进行垃圾收集时，必须暂停所有工作线程。简单高效，Client模式下默认的老年代收集器。 5、Parallel Old收集器Parallel Old收集器（-XX : +UseParallelOldGC，标记-整理算法）JDK6以后提供的。 Parallel Scavenge收集器其实在Parallel Old收集器没有出现之前一直处于比较尴尬的地位，因为能与之配合的老年代收集器只有Serial Old，Serial Old性能上拖累了Parallel Scavenge，所以即使在需要高吞吐量的情况下，虽然使用了Parallel Scavenge收集器，但是由于只能跟Serial Old收集器配合，结果性能还不如ParNew和CMS的组合收集器，Parallel Old收集器的出现总算是给Parallel Scavenge收集器搭配了一个好基友！ 所以在注重吞吐量及CPU资源敏感的场合，都可以优先考虑Parallel Scavenge+Parallel Old收集器 6、CMS收集器CMS收集器（-XX: +UseConcMarkSweepGC，标记-清除算法）， 以获取最短回收停顿时间为目标的收集器。优点：并发收集，低停顿。CMS收集器主要由以下6个步骤完成垃圾收集工作： 初始标记：stop-the-world，标记GC Roots能直接关联到的对象，虽然暂停了JVM，但是速度非常快 并发标记：在初始标记上并发追溯标记，程序不会停顿 并发预清理：查找执行并发标记阶段从年轻代晋升到老年代的对象，通过重新扫描减少下一个阶段的标记工作 重新标记：暂停虚拟机，扫描CMS堆中的剩余对象，速度较慢一些 并发清理：清理垃圾对象，程序不会停顿 并发重置：重置CMS收集器的数据结构 这6个步骤中初始标记和重新标记是需要暂停JVM的，是费时操作 CMS收集器对CPU资源非常敏感，面向并发设计的程序都会对CPU资源较敏感。CMS默认的回收线程数:(CPU数量+3)/4， 而且CMS收集器是基于标记-清除算法会产生大量空间碎片。 7、G1收集器G1收集器（-XX:+UseG1GC，复制算法 + 标记-整理算法），G1收集器全称是GarBage First。 G1收集器可以在几乎不牺牲吞吐量的前提下完成低停顿的内存回收，这是由于它能够极力避免全区域的垃圾收集，之前的收集器进行收集的范围都是整个新生代或老年代，而G1将整个Java堆（包括新生代、老年代）划分为多个大小固定的独立区域（Region），并且跟踪这些区域里面的垃圾堆积程度，在后台维护一个优先列表，每次根据允许的收集时间，优先回收垃圾最多的区域（这就是Garbage First名称的由来）。区域划分、有优先级的区域回收，保证了G1收集器在有限的时间内可以获得最高的收集效率。 G1收集器的特点：并行和并发、分代收集、空间整合、可预测的停顿。G1将整个Java堆（包括新生代、老年代）划分为多个大小固定的独立区域（Region），物理上可以不是连续的，年轻代和老年代空间大小动态可调整。 8、Epsilon垃圾收集器-XX: +UnlockExperimentalVMOptions、-XX:+UseEpsilonGC，Epsilon垃圾收集器是Java11出现的垃圾收集器， JDK上对这个特性的描述是：开发一个处理内存分配但不实现任何实际内存回收机制的GC，一旦可用堆内存用完，JVM就会退出。 Epsilon垃圾收集器提供完全被动的GC实现，具有有限的分配限制和尽可能低的延迟开销，但代价是内存占用和内存吞吐量。使用场景： 1、使用此GC来做性能测试(因为可以帮助过滤掉GC引起的性能假象) 还是不错的； 2、用来做内存压力测试也还行； 3、 如果是执行非常短的任务，根据不需要垃圾清理的话，使用此GC是合理的； 9、ZGC垃圾收集器ZGC的目标就是缩短Stop The World的时间：ZGC是一个并发，基于region（和G1对内存区域的管理方式一致）， 压缩型的垃圾收集器，只有root扫描阶段会Stop The World， 因此GC停顿时间不会随着堆的增长和存活对象的增长而变长。 ZGC垃圾收集器并且支持TB级内存容量，反正ZGC垃圾收集器也是Java11出现的垃圾收集器，目前开发中…","updated":"2020-04-14T03:10:16.177Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JVM","slug":"JVM","permalink":"https://zouchanglin.cn/tags/JVM/"}]},{"title":"垃圾标记与收集算法","date":"2020-03-10T23:01:35.000Z","path":"2020/03/11/垃圾标记与收集算法/","text":"本文主要是以通俗易懂的画图方式解释了标记清除算法和可达性分析算法，以及常用的回收算法（标记清除、标记整理、复制算法）以及整合百家之长的分代回收算法，另外还介绍了触发Full GC的几个场景。 标记算法 1、引用计数法 回收对象之前得判断那些是需要被清除的对象，首先出现的是引用计数算法，判断对象的引用数量，通过判断对象的引用数量来决定对象是否可以被回收，每个对象实例都有一个引用计数器，被引用则+1，完成引用则-1，任何引用计数为0的对象实例可以被当作垃圾收集。 优点：执行效率高，程序执行受影响较小 缺点：无法检测出循环引用的情况，导致内存泄露 12345678910public class MyObject&#123; MyObject object; public static void main(String[] args) &#123; MyObject myObject = new MyObject(); MyObject myObject2 = new MyObject(); myObject.object = myObject2; myObject2.object = myObject; &#125;&#125; 2、可达性分析算法 通过判断对象的引用链是否可达来决定对象是否可以被回收 可以作为GC Root的对象 1、虚拟机栈中引用的对象(栈帧中的本地变量表) 2、方法区中的常量引用的对象 3、方法区中的类静态属性引用的对象 4、本地方法栈中JNI ( Native方法)的引用对象 5、活跃线程的引用对象 回收算法 1、标记-清除（Mark and Sweep） 没有解决内存碎片化的问题 2、复制算法 （Copying） 分为对象面和空闲面，对象在对象面上创建，存活的对象被从对象面复制到空闲面，将对象面所有对象内存清除，解决了内存空间碎片化的问题，合适只有少数存活对象的情况，因为只复制一小部分对象就OK了 从图中可以看出复制算法能解决碎片化问题，顺序分配内存，简单高效。适用于对象存活率低的场景。 3、标记整理算法（Compacting） 标记：从根集合进行扫描，对存活的对象进行标记。清除：移动所有存活的对象，且按照内存地址次序依次排列，然后将末端内存地址以后的内存全部回收。 标记整理算法避免内存的不连续性，不用设置两块内存互换，但是效率成问题，如果第一个对象被标记为可回收，那么剩下的对象都要往前移动，损失性能。 4、分代收集算法（Generational Collector） 分代收集算法是垃圾回收算法的组合拳，按照对象生命周期的不同划分区域以采用不同的垃圾回收算法，目的在于提高JVM的回收效率。 在JDK6、JDK7中，Heap主要分为：新生代、老年代和永久代，JDK8以及以后的版本中去掉了永久代，而且使用MetaSpace替代，新生代对象存活率低，宜采用复制算法，老年代对象存活率高，采用标记清除算法或标记整理算法。 GC的分类Minor GC：发生在新生代中的垃圾收集动作，采用的是复制算法，新生代是垃圾收集比较频繁的区域 Full GC：一般对老年代的回收会伴随着新生代的垃圾收集 现在主要看看新生代，新生代主要分为一个Eden区和两个Survivor区（to区和from区，在垃圾收集中相互转换），对象一开始被创建的时候就是放在Eden区的，如果Eden去放不下，就会放到Survivor区甚至是老年代中。Eden区：两个Survivor区默认比例是8:1:1 对象年龄在默认情况下超过15岁（意思是经过15次Minor GC）就会晋升到老年代，Survivor去放不下的时候也会进入到老年代中，通过设置-XX:+PretenuserSizeThreshold 参数来指定如果对象的大小查过指定值对象一生成就直接放入老年代，常用的性能调优的参数： -XX:SurvivorRatio：Eden和Survivor的比值，默认8 : 1 -XX:NewRatio：老年代和新生代内存大小的比例 -XX:MaxTenuring Threshold：对象从新生代晋升到老生代经过GC次数的最大阈值 老年代使用的最多的是标记清除算法或者标记整理算法进行垃圾回收，通常触发老年代的垃圾回收的时候也会触发新生代的垃圾回收，这便是Full GC。Full GC比Minor GC慢，但是执行的频率很低。 触发Full GC的条件1、老年代空间不足 2、永久代空间不足 3、CMS GC时出现promotion failed , concurrent mode failure 当使用CMS垃圾收集器的时候，如果出现了promotion failed，那就说明在进行Minor GC时，Survivor空间不足，对象只嗯呢该放入老年代，但是此时老年代也出现了空间不足，就会出现promotion failed，就会触发Full GC； concurrent mode failure也是CMS垃圾收集器在执行的过程中，同时有对象要放入老年代，此时老年代空间不足，就会造成concurrent mode failure，从而触发Full GC 4、Minor GC晋升到老年代的平均大小大于老年代的剩余空间，Hotspot在设计时，在进行Minor GC的时候进行了判断，如果之前统计得到的Minor GC的平均大小已经达到了老年代的剩余空间大小，就会直接触发Full GC。比如第一次Minor GC剩余的对象大小为6M，如果此时老年代剩余空间小于6M，就会触发Full GC。 5、调用System.gc()，只是提醒一下虚拟机此时应该触发Full GC，至于回不回收由虚拟机自己决定，只是System.gc()增加了Full GC的可能。 6、使用RMI来进行RPC或管理的JDK应用，每小时执行1次Full GC。","updated":"2020-03-13T03:06:29.731Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JVM","slug":"JVM","permalink":"https://zouchanglin.cn/tags/JVM/"}]},{"title":"Java内存模型的回顾","date":"2020-03-09T23:01:35.000Z","path":"2020/03/10/Java内存模型的回顾/","text":"本篇文章主要讲述了Java内存模型中的程序计数器、虚拟机栈、本地方法栈、元空间与堆，以及堆中的常量池。前面通过javap反编译class文件得到int add(int a, int b)函数的栈帧，主要分析了栈帧中JVM指令对应的局部变量表、操作数栈、程序计数器的状态变化。以及JDK7以后出现了替代永久代的元数据区，并分析了元数据区替换了永久代有哪些好处，主要分析了给字符串常量池带来的影响，并通过代码验证了元数据区相比永久代的优越性。学习了JVM性能调优的三个参数的意义和普通用法，最后探讨了并验证了JDK1.6与JDK1.7+的版本String类的intern方法的不同表现结果，分析了出现不同结果的原因，其实主要是JDK1.6的版本是建立副本再放入字符串常量池，而JDK1.7+版本时直接把堆上的对象的引用入池。 线程私有的空间程序计数器( Program Counter Register ) 1、当前线程所执行的字节码行号指示器(逻辑) 2、改变计数器的值来选取下一条需要执行的字节码指令 3、和线程是一对一的关系即：线程私有 4、对Java方法计数，如果是Native方法则计数器值为Undefined 5、由于只是对指令行号进行计数，所以不会发生内存泄漏 Java虚拟机栈（Stack） Java虚拟机栈是Java方法执行的内存模型，包含多个栈帧，栈帧里面有哪些内容呢？ 局部变量表、操作栈、动态链接、返回地址等。局部变量表包含了方法执行过程中的所有变量。操作数栈主要是：入栈、出栈、复制、交换、产生消费变量。 1234567public class ByteCodeSimple &#123; public static int add(int a, int b)&#123; int c = 0; c = a + b; return c; &#125;&#125; 通过javac编译出class文件，再通过javap -verbose ByteCodeSimple.class编译出如下内容： 每一个蓝色的长方形就代表了一个栈帧的存在状态，对应上面的代码，总共有7个栈帧状态图。 iconst指令就是将操作数0压入操作数栈中，入参为1,2因此局部变量表就是1和2。istore指令的意思就是把操作数栈的元素给pop出来，放入局部变量表的第二个变量中（因为是istore_2），iload_0就是将局部变量表中第0个的元素压入到操作数栈当中，接着又把局部变量表中第1个元素压入到操作数栈中，接着执行iadd指令（将操作数栈中的元素取出来，执行相加的操作，将结果放回栈顶），接着把栈顶的结算结果弹栈，放到局部变量表中的第二个变量里面，然后再次把局部变量表中的第二个变量放到操作数栈当中。最后调用ireturn这个指令把栈顶的元素给返回，最后执行的是销毁栈帧。 所以可以看出调用一个Java方法就是需要建立栈帧，如果递归层数过多，超出虚拟机栈的深度限制，就会引发java.lang.StackOverFlowError异常。 另一个异常就是java.lang.OutOfMemoryError，虚拟机栈过多会引发java.lang.OutOfMemoryError异常，如要尝试可以使用如下代码，Windows测试之前请备份好重要文档 123456789public void stackLeakByThread()&#123; while (true)&#123; new Thread(() -&gt; &#123; while (true)&#123; &#125; &#125;).start(); &#125;&#125; 本地方法栈，与虚拟机栈相似，主要作用于标注了native的方法。 线程共享的空间先说说MetaSpace（元空间）与永久代的区别： 首先，得明白方法区只是JVM的一个规范，而MetaSpace和永久代均是方法区的实现，在Java7之后，原先位于方法区的字符串常量池已经被移动到了堆中，并且在JDK8以后，使用MetaSpace替代了永久代。不仅仅只是名字上的替代： 1、元空间使用本地内存，而永久代使用的是JVM的内存，使用本地内存有什么好处呢？那就是默认的类的元数据分配只受到本地内存大小的限制，解决了空间不足的问题， 所有的被intern的String被存储在PermGen的串常量池中，解决了以前在老版本的JDK中出现的OutOfMemoryError的问题，JVM默认在运行的时候会根据需要动态的设置其大小。 2、字符串常量池存在永久代中，容易出现性能问题和内存溢出 3、永久代会为GC带来不必要的复杂性 4、方便HotSpot与其他JVM如Jrockit的集成 堆这个空间其实就是存放对象实例用的（当然也包括数组和字符串常量池）。Java堆空间可以处于物理上不连续的空间，只要逻辑是连续的即可。 JVM三大性能调优参数-Xms、-Xmx、-Xss -Xss 规定了每个线程虚拟机栈的大小，一般来说256K足够用了，此配置将会影响并发线程数的大小 -Xms 堆的初始值，即进程刚创建时的堆的大小，一旦除过了堆的初始容量，将会自动扩容 -Xmx 堆能达到的最大值 通常情况下我们将 -Xms 和 -Xmx 设置成一样的，因为当堆内存不够用的时候，会发生扩容，此时会产生内存抖动，影响程序运行时的稳定性。 内存分配策略1、静态存储：编译时确定每个数据目标在运行时的存储空间需求 2、栈式存储：数据区需求在编译时未知，运行时模块入口前确定，如：虚拟机栈 3、堆式存储：编译时或运行时模块入口都无法确定，动态分配，如：对象的存储和销毁 Java内存模型中堆和栈的区别1、管理方式：栈自动释放，堆需要GC 2、空间大小：栈比堆小 3、碎片相关：栈产生的碎片远小于堆 4、分配方式：栈支持静态和动态分配，而堆仅支持动态分配 5、效率：栈的效率比堆高 不同JDK版本的intern()String对象的intern方法就是把字符串放入字符串常量池中。 JDK6：当调用intern方法时，如果字符串常量池先前已创建出该字符串对象，则返回池中的该字符串的引用。否则，将此字符串对象添加到字符串常量池中，并且返回该字符串对象的引用。 JDK6+：当调用intern 方法时，如果字符串常量池先前已创建出该字符串对象，则返回池中的该字符串的引用。否则，如果该字符串对象已经存在于Java堆中，则将堆中对此对象的引用添加到字符串常量池中，并且返回该引用;如果堆中不存在，则在池中创建该字符串并返回其引用。 先演示一个OutOfMemoryError：PermGen space的例子 下面是一个一直向常量池中放入随机字符串的程序示例： 12345678910111213141516171819public class PermGenErrTest &#123; public static void main(String[] args) &#123; for (int i = 0; i &lt; 1000; i++) &#123; getRandomString(1000000).intern(); &#125; System.out.println(\"success\"); &#125; private static String getRandomString(int length) &#123; String str = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"; Random random = new Random(); StringBuilder stringBuilder = new StringBuilder(); for (int i = 0; i &lt; length; i++) &#123; int num = random.nextInt(62); stringBuilder.append(str.charAt(num)); &#125; return stringBuilder.toString(); &#125;&#125; 我的JDK版本选择了JDK1.6，并且配置了永久代的初始大小为6M，而且最大为6M，然后一直往常量池里面放字符串，最终出现了OutOfMemoryError：PermGen space，那么如果是换成JDK1.7+呢？ 可以看到成功执行完毕，而且JDK1.7+是不受MaxPerSize这个参数的限制的，因为已经移除了永久代。 不同JDK版本的intern() 对于JDK1.6会输出什么？JDK1.7+呢？ 12345678910111213public class InternDifference &#123; public static void main(String[] args) &#123; String s1 = new String(\"a\"); s1.intern(); String s2 = \"a\"; System.out.println(s1 == s2); String s3 = new String(\"a\") + new String(\"a\"); s3.intern(); String s4 = \"aa\"; System.out.println(s3 == s4); &#125;&#125; 对于JDK1.7+，输出结果是false、true String s = new String(“a”)的时候， “a”会被首先创建，放入字符串常量池中，然后new出的对象放在堆中，在调用intern()的时候，会尝试将字符串对象放入字符串常量池中，但是发现字符串常量池中已经有了，就不能放了，在String s2 = “a”的时候，会先在常量池中寻找有没有对应的字符串，如果有，就直接返回它的引用。所以s1 == s2 比较的是字符串常量池中的”a” 的地址和堆中对象的地址，肯定是flase。在String s3 = new String(“a”) + new String(“a”)，字符串常量池是不会创建“aa”这个字符串的，因为“”中只有单个a，所以在调用intern()的时候，会尝试将“aa”也就是堆中的那个字符串对象的引用放入常量池中，并将该引用返回，由于这两个都是同一个地址引用，于是s3==s4是true。 对于JDK1.6，输出结果是false、false**，如下图 String s1 = new String(“a”)的时候， “a”会被首先创建，放入字符串常量池中，然后new出的对象放在堆中，在调用intern()的时候，会尝试将字符串对象放入字符串常量池中，但是发现字符串常量池中已经有了，就不能放了，在String s2 = “a”的时候，会先在常量池中寻找有没有对应的字符串，如果有，就直接返回它的引用。所以s1 == s2 比较的是字符串常量池中的”a” 的地址和堆中对象的地址，肯定是flase。 在String s3 = new String(“a”) + new String(“a”)，字符串常量池是不会创建“aa”这个字符串的，因为“”中只有单个a，所以在调用intern()的时候，会尝试将“aa”也就是堆中的那个字符串对象放入常量池中，并返回字符串常量池中“aa”的引用，但是由于常量池中放的相当于是一个对象副本，当返回它的引用时，地址是永久区的，因此s3和s4不会相等。","updated":"2020-03-13T03:06:29.664Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JVM","slug":"JVM","permalink":"https://zouchanglin.cn/tags/JVM/"},{"name":"字符串","slug":"字符串","permalink":"https://zouchanglin.cn/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"}]},{"title":"ClassLoader","date":"2020-03-07T23:01:35.000Z","path":"2020/03/08/ClassLoader/","text":"ClassLoader在Java中有着非常重要的作用，它主要工作在Class装载的加载阶段，其主要作用是从系统外部获得Class二进制数据流。它是Java的核心组件。所有的Class都是由ClassLoader 进行加载的。ClassLoader负责通过将Class文件里的二进制数据流装载进系统，然后交给Java虚拟机进行连接、初始化等操作。 ClassLoader的种类 BootStrapClassLoader：C++编写，这个类加载器负责将\\lib目录下的类库加载到虚拟机内存中，用来加载Java的核心库，此类加载器并不继承于java.lang.ClassLoader，不能被Java程序直接调用，代码是使用C++编写的，它虚拟机自身的一部分。 ExtClassLoader：Java编写，加载扩展库。 这个类加载器负责加载\\lib\\ext目录下的类库，用来加载Java的扩展库，开发者可以直接使用这个类加载器。 AppClassLoader：Java编写，加载程序所在目录的Class，这个类加载器负责加载用户类路径(CLASSPATH)下的类库，一般我们编写的Java类都是由这个类加载器加载，这个类加载器是CLassLoader中的getSystemClassLoader()方法的返回值，所以也称为系统类加载器。一般情况下这就是系统默认的类加载器。 自定义ClassLoader：Java编写，定制化加载 实现自己的ClassLoader如何实现自己的类加载器呢？其实无非是把class文件转化成二进制流加载到JVM中： 12345678910111213141516171819202122232425262728293031323334353637383940import java.io.*;public class MyClassLoader extends ClassLoader &#123; private String name; private String path; public MyClassLoader(String name, String path) &#123; this.name = name; this.path = path; &#125; @Override public Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; byte[] bytes = loadClassData(name); return defineClass(name, bytes, 0, bytes.length); &#125; private byte[] loadClassData(String name) &#123; name = path + name + \".class\"; InputStream in = null; ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); try &#123; in = new FileInputStream(new File(name)); int i; while ((i = in.read()) != -1)&#123; byteArrayOutputStream.write(i); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; in.close(); byteArrayOutputStream.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return byteArrayOutputStream.toByteArray(); &#125;&#125; 我在桌面编写了简单的Demo.java，并编译成了class文件，也放在桌面的： 12345public class Demo&#123; static&#123; System.out.println(\"Hello Demo!\"); &#125;&#125; 于是我们可以开始测试自己写的类加载器了： 1234567891011public class ClassLoaderTest &#123; public static void main(String[] args) throws Exception&#123; MyClassLoader classLoader = new MyClassLoader(\"Tim's ClassLoader\", \"C:\\\\Users\\\\15291\\\\Desktop\\\\\"); System.out.println(classLoader); //MyClassLoader@4554617c System.out.println(classLoader.getParent()); //sun.misc.Launcher$AppClassLoader@18b4aac2 System.out.println(classLoader.getParent().getParent()); //sun.misc.Launcher$ExtClassLoader@74a14482 System.out.println(classLoader.getParent().getParent().getParent()); //null Class&lt;?&gt; loaderClass = classLoader.loadClass(\"Demo\"); //Hello Demo! loaderClass.newInstance(); &#125;&#125; 由此也证明了ClassLoader之间的继承关系。 什么是双亲委派机制这么多类加载器，那么它们之间是如何配合的呢？下面是双亲委派机制的模型图： 双亲委派机制工作过程： 1、类加载器收到类加载的请求，如果已经加载过，则不需要再次加载； 2、如果未加载过，则把这个请求委托给父加载器去完成，一直向上委托，直到启动类加载器； 3、启动器加载器检查能不能加载（使用findClass()方法），能就加载（结束）；否则，抛出异常，通知子加载器进行加载。 4、重复步骤3； 其实也可以看到这个递归调用流程的代码，可以看到findBootstrapClass是一个native方法 12// return null if not foundprivate native Class&lt;?&gt; findBootstrapClass(String name); 那么这个native方法的实现在哪看呢？ http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/beb15266ba1a/src/share/native/java/lang/ClassLoader.c 最终调用了JVM_FindClassFromBootLoader这个方法，在这里暂且先不研究这个Native方法了。 为什么要使用双亲委派机制首先明确一点是JVM如何认定两个对象同属于一个类型，必须同时满足下面两个条件：1、都是用同名的类完成实例化的。 2、两个实例各自对应的同名的类的加载器必须是同一个。比如两个相同名字的类，一个是用系统加载器加载的，一个扩展类加载器加载的，两个类生成的对象将被JVM认定为不同类型的对象。 所以，为了系统类的安全，类似java.lang.Object这种核心类，JVM需要保证他们生成的对象都会被认定为同一种类型。即通过代理模式，对于 Java 核心库的类的加载工作由引导类加载器来统一完成，保证了 Java 应用所使用的都是同一个版本的 Java 核心库的类，是互相兼容的，同时也就避免了多分同样的字节码加载。 能不能自己写个类叫java.lang.System？当然可以，通过自定义类加载器的方式，只要让我们自己的类无法让父类加载器加载最终肯定就会让我们自定义的类加载器来加载这个类。 由于系统自带的三个类加载器都加载特定目录下的类，如果我们自己的类加载器加载一个特殊的目录，那么系统的加载器就无法加载，也就是最终还是由我们自己的加载器加载。 类的加载方式（loadClass与forName）1、new 使用new关键字去操作对象的时候会触发隐式加载，比如new Demo()，那么最终AppClassLoader就会加载这个Demo类 2、loadClass、forName 这种方式加做显式加载 那么loadClass与forName有什么区别呢？先来看看整个类的装载过程吧： 在loadClass方法中，我们注意到一个boolean类型的参数叫做resolve，false即表示不执行链接的过程，即不解析这个类。 再看看forName 所以上述表明，使用forName显式加载可以选择是否执行初始化，如果只是传类名称的话默认是要执行初始化的，但是loadClass却没有链接和初始化的步骤，这种行为加做延迟加载，IOC为了提高启动速度就选择了使用延迟加载，在IOC容器中被广泛使用。forName什么时候用呢？比如使用MySQL驱动的时候，就是直接初始化MySQL驱动的一些对象，而不是延迟加载。","updated":"2020-03-13T03:06:29.642Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JVM","slug":"JVM","permalink":"https://zouchanglin.cn/tags/JVM/"}]},{"title":"图解epoll原理","date":"2020-03-01T12:01:35.000Z","path":"2020/03/01/图解epoll原理/","text":"poll翻译过来是轮询的意思， 可以看到poll和epoll都有轮询的过程， 不同点在于：poll轮询的是所有的socket，而epoll只轮询就绪的socket。 epoll是开发linux高性能服务器的必备技术至，epoll本质，是服务端程序员的必须掌握的知识。 本文主要是利用图文讲述了select的原理和epoll相对于做出的优化，以及epoll的部分细节问题。 从硬件理解网卡接收数据过程下图是一个典型的计算机结构图，计算机由CPU、存储器（内存）、网络接口等部件组成。了解epoll本质的第一步，要从硬件的角度看计算机怎样接收网络数据。 下面这幅图在我的另一篇文章里也用到了，《冯诺依曼架构》 网卡收到网线传来的数据，通过硬件电路传输，网卡接收的数据存放到内存中，操作系统就可以去读取它们。 正确理解CPU中断一般而言，由硬件产生的信号需要CPU立马做出处理，不然数据可能就丢失，所以它的优先级很高。CPU理应中断掉正在执行的程序，去做出响应；当CPU完成对硬件的响应后，再重新执行用户程序。 比如我们按下键盘上的按键，会触发中断即（键盘会给cpu的中断引脚发出一个高电平，CPU能够捕获这个信号，然后执行键盘中断程序。）网卡也是硬件设备，和键盘不同的是我们用键盘可以主动触发中断，而网卡是收到数据之后向CPU发出信号，触发中断程序，即CPU去处理网卡设备的数据！ 什么是一切皆文件在Java的学习中，我们可以有深刻的体会，那就是一切皆对象；在Linux的学习中，一切皆文件的理念无处不在，文档、目录、磁盘驱动器、CD-ROM、调制解调器、键盘、打印机、显示器、终端，甚至是一些进程间通信和网络通信。所有这些资源拥有一个通用的抽象，在Linux中将其称为“文件”，其实Unix就是这种思想，所以Linux也借鉴了这个思想，因为每个“文件”都通过相同的 API 暴露出来，所以你可以使用同一组基本命令来读取和写入磁盘、键盘、文档或网络设备， “一切皆文件”的思想提供了一个强大而简单的抽象，那就是无论是硬件设备、还是网络连接、还是我们日常解除的文件，都是文件，这样使得API的设计可以化繁为简，用户可以使用通用的方式去访问任何资源，自有相应的中间件做好对底层的适配。 所以在Linux操作系统看来，一切都是文件，也就意味着，网卡设备也是文件，Socket连接也是文件，文件的统一抽象使得都有共同的属性，我们这里只谈最重要的文件属性——文件描述符（file descriptor，简写fd）！ 内核接收网络数据过程数据经由网卡传送到内存，然后网卡通过中断信号通知CPU有数据到达，CPU执行中断程序。此处的中断程序主要有两项功能，先将网络数据写入到对应socket的接收缓冲区里面，再唤醒进程A，重新将进程A放入工作队列中。 唤醒进程的过程如图所示： 操作系统如何知道网络数据对应于哪个socket呢？因为一个socket对应着一个端口号，而网络数据包中包含了ip和端口的信息，内核可以通过端口号找到对应的socket。当然，为了提高处理速度，操作系统会维护端口号到socket的索引结构，以快速读取。 select如何同时监视多个socket假如能够预先传入一个Socket列表，如果列表中的socket都没有数据，挂起进程，直到有一个socket收到数据，唤醒进程。这种方法很直接，也是select的设计思想。 先复习select的用法。在如下的代码中，先准备一个数组（下面代码中的fds），让fds存放着所有需要监视的socket。然后调用select，如果fds中的所有socket都没有数据，select会阻塞，直到有一个socket接收到数据，select返回，唤醒进程。用户可以遍历fds，通过FD_ISSET判断具体哪个socket收到数据，然后做出处理。 1234567891011121314int s = socket(AF_INET, SOCK_STREAM, 0); bind(s, ...)listen(s, ...)int fds[] = 存放需要监听的socketwhile(1)&#123; int n = select(..., fds, ...) for(int i=0; i &lt; fds.count; i++)&#123; if(FD_ISSET(fds[i], ...))&#123; //fds[i]的数据处理 &#125; &#125;&#125; select的实现思路很直接，假如程序同时监视如下图的sock1、sock2和sock3三个socket，那么在调用select之后，操作系统把进程A分别加入这三个socket的等待队列中。 当任何一个socket收到数据后，中断程序将唤起进程。下图展示了socket2接收到了数据的处理流程。 经由这些步骤，当进程A被唤醒后，它知道至少有一个socket接收了数据。程序只需遍历一遍socket列表，就可以得到就绪的socket。这种简单方式行之有效，在几乎所有操作系统都有对应的实现。 我们不难发现：每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个fds列表（即文件描述符列表，上图中写的是文件列表）传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，默认只能监视1024个socket。 进程被唤醒后，程序并不知道哪些socket收到数据，还需要遍历一次。 当程序调用select时，内核会先遍历一遍socket，如果有一个以上的socket接收缓冲区有数据，那么select直接返回文件描述符状态已改变的个数，不会阻塞； 如果没有socket有数据，且没有超过timeout时间，进程才会阻塞； 如果返回0，代表在描述符状态改变前已超过timeout时间； 如果有错误返回的是-1，错误原因存于errno。 epoll如何改进效率的措施一：功能分离 还记得select的输入输出型参数吗fd_set吗？fd_set是一个位图，使用位图中对应的位来表示要监视的文件描述符。所以select能监视的socket有上限，我的centos支持的描述符上限是1024个。 select低效的原因之一是将维护等待队列和阻塞进程两个步骤合二为一。如下图所示，每次调用select都需要这两步操作 如果上图看不懂，先复习本文中select用法的伪代码，然后复习下epoll的用法。如下的代码中，先用epoll_create创建一个epoll对象epfd，再通过epoll_ctl将需要监视的socket添加到epfd中，最后调用epoll_wait等待数据。 12345678910111213int s = socket(AF_INET, SOCK_STREAM, 0); bind(s, ...)listen(s, ...)int epfd = epoll_create(...);epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中while(1)&#123; int n = epoll_wait(...) for(接收到数据的socket)&#123; //处理 &#125;&#125; 通过以上代码是不是可以看出，对于select来说，每次都要传入文件描述符列表（虽然是位图，但是信息表达的就是一个文件描述符列表），而对于epoll来说，只传入了一次文件描述符列表； 功能分离，使得epoll有了优化的可能。 措施二： 就绪列表 select低效的另一个原因在于程序不知道哪些socket收到数据，只能一个个遍历。如果内核维护一个“就绪列表”，引用收到数据的socket，就能避免遍历。 如下图所示，计算机共有三个socket，收到数据的sock2和sock3被rdlist（就绪列表）所引用。当进程被唤醒后，只要获取rdlist的内容，就能够知道哪些socket收到数据。 epoll的原理的流程 创建epoll对象 如下图所示，当某个进程调用epoll_create方法时，内核会创建一个eventpoll对象（也就是程序中epfd所代表的对象）。eventpoll对象也是文件系统中的一员，和socket一样，它也会有等待队列。 创建一个代表该epoll的eventpoll对象是必须的，因为内核要维护就绪列表等数据，就绪列表作为eventpoll的成员。 维护监视列表 创建epoll对象后，可以用epoll_ctl添加或删除所要监听的socket。以添加socket为例，如下图，如果通过epoll_ctl添加sock1、sock2和sock3的监视，内核会将eventpoll添加到这三个socket的等待队列中。 当socket收到数据后，中断程序会操作eventpoll对象，而不是直接操作进程 接收数据 当socket收到数据后，中断程序会给eventpoll的就绪列表添加socket引用。如下图展示的是sock2和sock3收到数据后，中断程序让rdlist引用这两个socket eventpoll对象相当于是socket和进程之间的中介，socket的数据接收并不直接影响进程，而是通过改变eventpoll的就绪列表来改变进程状态。 当程序执行到epoll_wait时，如果rdlist已经引用了socket，那么epoll_wait直接返回，如果rdlist为空，阻塞进程。 阻塞和唤醒进程 假设计算机中正在运行进程A和进程B，在某时刻进程A运行到了epoll_wait语句。如下图所示，内核会将进程A放入eventpoll的等待队列中，阻塞进程。 当socket接收到数据，中断程序一方面修改rdlist，另一方面唤醒eventpoll等待队列中的进程，进程A再次进入运行状态（如下图）。也因为rdlist的存在，进程A可以知道哪些socket发生了变化。 epoll的实现细节 eventpoll的数据结构是什么样子？ 就绪队列应该应使用什么数据结构？eventpoll应使用什么数据结构来管理通过epoll_ctl添加或删除的socket？ 如下图所示，eventpoll包含了lock、mtx、wq（等待队列）、rdlist等成员。rdlist和rbr是我们所关心的。 就绪列表引用着就绪的socket，所以它应能够快速的插入数据。 程序可能随时调用epoll_ctl添加监视socket，也可能随时删除。当删除时，若该socket已经存放在就绪列表中，它也应该被移除。 所以就绪列表应是一种能够快速插入和删除的数据结构。双向链表就是这样一种数据结构，epoll使用双向链表来实现就绪队列（对应上图的rdllist） 既然epoll将维护监视队列和进程阻塞分离，也意味着需要有个数据结构来保存监视的socket。至少要方便的添加和移除，还要便于搜索，以避免重复添加。红黑树是一种自平衡二叉查找树，搜索、插入和删除时间复杂度都是O(log(N))，效率较好。epoll使用了红黑树作为索引结构（对应上图的rbr）。 因为操作系统要兼顾多种功能，以及由更多需要保存的数据，rdlist并非直接引用socket，而是通过epitem间接引用，红黑树的节点也是epitem对象。同样，文件系统也并非直接引用着socket。为方便理解，本文中省略了一些间接结构。 总结3种多路复用","updated":"2020-04-14T03:11:19.508Z","categories":[{"name":"高性能网络","slug":"高性能网络","permalink":"https://zouchanglin.cn/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"},{"name":"NIO","slug":"NIO","permalink":"https://zouchanglin.cn/tags/NIO/"}]},{"title":"select、poll和epoll多路复用","date":"2020-02-29T12:01:35.000Z","path":"2020/02/29/select、poll和epoll多路复用/","text":"Nginx和Redis中都用到了epoll多路复用模型，本节将讲述常见的多路复用模型：select、poll和epoll，以及部分示例代码，还是先回顾IO的两个重要过程： 任何IO过程中，都包含两个步骤：第一是等待，第二是拷贝。而且在实际的应用场景中，等待消耗的时间往往都远远高于拷贝的时间。让IO更高效，最核心的办法就是让等待的时间尽量少。 在以前的文章中介绍了五种IO模型，分别是阻塞式IO、非阻塞式IO、信号驱动IO、多路复用IO、异步IO；前四种都属于同步IO。今天重点介绍的是多路复用IO，多路复用IO通俗讲就是一次等待多个文件描述符，减少了等待时间，提高了IO过程的效率（此IO过程并不是只是从内核态到用户态数据的拷贝，而是从发起IO请求直到IO完成的过程），接下来将介绍Linux的三种多路复用模型。 非阻塞IO在介绍多路复用IO之前，先看看非阻塞IO吧：一个文件描述符，默认都是阻塞IO，我们可以通过fcntl函数来将文件描述符设置为非阻塞 传入的cmd的值不同，后面追加的参数也不相同。fcntl函数有5种功能： 复制一个现有的描述符（cmd=F_DUPFD） 获得/设置文件描述符标记(cmd=F_GETFD或F_SETFD). 获得/设置文件状态标记(cmd=F_GETFL或F_SETFL). 获得/设置异步I/O所有权(cmd=F_GETOWN或F_SETOWN). 获得/设置记录锁(cmd=F_GETLK,F_SETLK或F_SETLKW). 我们此处只是用第三种功能，获取/设置文件状态标记，就可以将一个文件描述符设置为非阻塞。对fcntl封装实现一个将文件描述符更改为非阻塞的功能： 使用F_GETFL将当前的文件描述符的属性取出来(这是一个位图)，然后再使用F_SETFL将文件描述符设置回去。设置回去的同时，加上一个O_NONBLOCK参数。下面使用轮询方式读取标准输入： select 多路复用系统提供select函数来实现多路复用输入/输出模型，select系统调用是用来让我们的程序监视多个文件描述符的状态变化的；程序会停在select这里等待，直到被监视的文件描述符有一个或多个发生了状态改变； select函数 参数nfds是需要监视的最大的文件描述符值+1；rdset、wrset、exset分别对应于需要检测的可读文件描述符的集合，可写文件描述符的集合及异常文件描述符的集合；参数timeout为结构timeval，用来设置select()的等待时间 NULL：则表示select()没有timeout，select将一直被阻塞，直到某个文件描述符上发生了事件; 0：仅检测描述符集合的状态，然后立即返回，并不等待外部事件的发生。 特定的时间值：如果在指定的时间段里没有事件发生，select将超时返回。 void FD_CLR(int fd, fd_set *set) 用来清除描述词组set中相关fd的位int FD_ISSET(int fd, fd_set *set) 用来测试描述词组set中相关fd的位是否为真void FD_SET(int fd, fd_set *set) 用来设置描述词组set中相关fd的位void FD_ZERO(fd_set *set) 用来清除描述词组set的全部位 select 函数返回值：执行成功则返回文件描述词状态已改变的个数；如果返回0代表在描述词状态改变前已超过timeout时间，没有返回；当有错误发生时则返回-1，错误原因存于errno，此时参数readfds，writefds，exceptfds和timeout的值变成不可预测。 错误值可能为： EBADF 文件描述词为无效的或该文件已关闭 EINTR 此调用被信号所中断 EINVAL 参数n为负值 ENOMEM 核心内存不足 select特点可监控的文件描述符个数取决与sizeof(fd_set)的值，我的服务器上sizeof(fd_set)＝128，每bit表示一个文件描述符，则我服务器上支持的最大文件描述符是128*8=1024。fd_set的大小可以调整，可能涉及重新编译内核。 1234int main()&#123; printf(\"%lu\\n\", sizeof(fd_set)); //128 return 0;&#125; 将fd加入select监控集的同时，还要再使用一个数据结构array保存放到select监控集中的fd，一是用于再select 返回后，array作为源数据和fd_set进行FD_ISSET判断。二是select返回后会把以前加入的但并无事件发生的fd清空，则每次开始select前都要重新从array取得fd逐一加入(FD_ZERO最先)，扫描array的同时取得fd最大值maxfd，用于select的第一个参数。 select缺点每次调用select, 都需要手动设置fd集合, 从接口使用角度来说也非常不便，每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大，同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大，select支持的文件描述符数量太小。 使用select监控stdin12345678910111213141516171819202122232425262728293031#include &lt;stdio.h&gt;#include &lt;stdbool.h&gt;#include &lt;unistd.h&gt;#include &lt;zconf.h&gt;int main()&#123; fd_set read_fds; //清除全部标志位 FD_ZERO(&amp;read_fds); FD_SET(0, &amp;read_fds); while (true)&#123; printf(\"&gt;\"); fflush(stdout); int ret = select(1, &amp;read_fds, NULL, NULL, NULL); if(ret &lt; 0)&#123; perror(\"select\"); continue; &#125; if(FD_ISSET(0, &amp;read_fds))&#123; char buf[1024] = &#123;0&#125;; read(0, buf, sizeof(buf) - 1); printf(\"input:%s\\n\", buf); &#125; else&#123; printf(\"error!\"); continue; &#125; FD_ZERO(&amp;read_fds); FD_SET(0, &amp;read_fds); &#125; return 0;&#125; poll 多路复用poll函数 对于poll这个系统函数来说，fds是一个poll函数监听的结构列表。每一个元素中，包含了三部分内容：文件描述符，监听的事件集合，返回的事件集合。nfds表示fds数组的长度，timeout表示poll函数的超时时间，单位是毫秒(ms)。 下面是events和revents的取值 返回值：返回值小于0，表示出错；返回值等于0，表示poll函数等待超时；返回值大于0，表示poll由于监听的文件描述符就绪而返回。 poll特点不同与select使用三个位图来表示三个fdset的方式，poll使用一个pollfd的指针实现，pollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式（因为select三个集合简直要命），接口使用比select更方便。poll并没有最大数量限制 (但是数量过大后性能也是会下降)，因为列表长度可以任意长，当然指的是内存充足的情况下； poll缺点poll中监听的文件描述符数目增多时，和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符，每次调用poll都需要把大量的pollfd结构从用户态拷贝到内核中。同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。 使用poll监控stdin12345678910111213141516171819202122232425262728293031#include &lt;stdio.h&gt;#include &lt;stdbool.h&gt;#include &lt;unistd.h&gt;#include &lt;zconf.h&gt;#include &lt;poll.h&gt;int main()&#123; struct pollfd poll_fd; poll_fd.fd = 0; poll_fd.events = POLLIN;//数据可读事件 while (true)&#123; int ret = poll(&amp;poll_fd, 1, 1000); if(ret &lt; 0)&#123; perror(\"poll\"); continue; &#125; if(ret == 0)&#123; printf(\"poll timeout\\n\"); continue; &#125; if(poll_fd.revents == POLLIN)&#123; char buf[1024] = &#123;0&#125;; read(0, buf, sizeof(buf) - 1); printf(\"stdin:%s\\n\", buf); &#125; &#125; return 0;&#125; epoll 多路复用按照man手册的说法: 是为处理大批量句柄而作了改进的poll，它是在2.5.44内核中被引进的，对于有大量的连接，但是却只有少数连接是活跃的这种情况非常适用。 epoll有三个系统调用： epoll_create创建一个epoll的句柄，自从linux2.6.8之后，size参数是被忽略的。用完之后，必须调用close()关闭。 epoll_ctlepoll_ctl是epoll的事件注册函数 它不同于select()是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型。 第一个参数是epoll_create()的返回值(epoll的句柄) 第二个参数表示动作，用三个宏来表示 第三个参数是需要监听的fd 第四个参数是告诉内核需要监听什么事 第二个参数的取值也如上图，分别是注册新的fd到epfd中；修改已经注册的fd的监听事件；从epfd中删除一个fd；第三个参数的取值如下图： events可以是以下几个宏的集合 EPOLLIN : 表示对应的文件描述符可以读 (包括对端SOCKET正常关闭) EPOLLOUT : 表示对应的文件描述符可以写 EPOLLPRI : 表示对应的文件描述符有紧急的数据可读 (这里应该表示有带外数据到来) EPOLLERR : 表示对应的文件描述符发生错误 EPOLLHUP : 表示对应的文件描述符被挂断 EPOLLET : 将EPOLL设为边缘触发(Edge Triggered)模式, 这是相对于水平触发(Level Triggered)来说的 EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里 epoll_wait收集在epoll监控的事件中已经发送的事件。 参数events是分配好的epoll_event结构体数组 epoll将会把发生的事件赋值到events数组中 (events不可以是空指针，内核只负责把数据复制到这个events数组中，不会去帮助我们在用户态中分配内存) maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size 参数timeout是超时时间 (毫秒，0会立即返回，-1是永久阻塞) 如果函数调用成功，返回对应I/O上已准备好的文件描述符数目，如返回0表示已超时, 返回小于0表示函数失败。 epoll工作原理 当某一进程调用epoll_create方法时，Linux内核会创建一个eventpoll结构体，这个结构体中有两个成员与epoll的使用方式密切相关。 1234567891011struct eventpoll&#123; .... /*红黑树的根节点，这颗树中存储着所有添加到epoll中的需要监控的事件*/ struct rb_root rbr; /*双链表中则存放着将要通过epoll_wait返回给用户的满足条件的事件*/ struct list_head rdlist; ....&#125;; 每一个epoll对象都有一个独立的eventpoll结构体，用于存放通过epoll_ctl方法向epoll对象中添加进来的事件。 这些事件都会挂载在红黑树中，如此，重复添加的事件就可以通过红黑树而高效的识别出来(红黑树的插入时间效率是lgN，其中n为树的高度)。 而所有添加到epoll中的事件都会与设备(网卡)驱动程序建立回调关系，也就是说，当响应的事件发生时会调用这个回调方法。 这个回调方法在内核中叫ep_poll_callback,它会将发生的事件添加到rdlist双链表中。 在epoll中，对于每一个事件，都会建立一个epitem结构体。 1234567struct epitem&#123; struct rb_node rbn;//红黑树节点 struct list_head rdllink;//双向链表节点 struct epoll_filefd ffd; //事件句柄信息 struct eventpoll *ep; //指向其所属的eventpoll对象 struct epoll_event event; //期待发生的事件类型&#125; 当调用epoll_wait检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可。如果rdlist不为空，则把发生的事件复制到用户态，同时将事件数量返回给用户。这个操作的时间复杂度是O(1)。 总结一下, epoll的使用过程就是三部曲：1、调用epoll_create创建一个epoll句柄；2、调用epoll_ctl，将要监控的文件描述符进行注册；3、调用epoll_wait，等待文件描述符就绪； epoll的优点1、接口使用方便：虽然拆分成了三个函数，但是反而使用起来更方便高效，不需要每次循环都设置关注的文件描述符，也做到了输入输出参数分离开； 2、数据拷贝轻量：只在合适的时候调用 EPOLL_CTL_ADD 将文件描述符结构拷贝到内核中，这个操作并不频繁，而select/poll都是每次循环都要进行拷贝； 3、事件回调机制：避免使用遍历，而是使用回调函数的方式，将就绪的文件描述符结构加入到就绪队列中，epoll_wait 返回直接访问就绪队列就知道哪些文件描述符就绪。这个操作时间复杂度O(1)，即使文件描述符数目很多，效率也不会受到影响； 4、没有数量限制：文件描述符数目无上限；","updated":"2020-04-14T03:11:33.081Z","categories":[{"name":"高性能网络","slug":"高性能网络","permalink":"https://zouchanglin.cn/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"},{"name":"NIO","slug":"NIO","permalink":"https://zouchanglin.cn/tags/NIO/"}]},{"title":"通俗理解五种IO模型","date":"2020-02-29T12:01:35.000Z","path":"2020/02/29/通俗理解五种IO模型/","text":"任何IO过程中，都包含两个步骤：第一是等待，第二是拷贝。而且在实际的应用场景中，等待消耗的时间往往都远远高于拷贝的时间。让IO更高效，最核心的办法就是让等待的时间尽量少。 理解五种IO模型先来看看五种IO模型： 1、阻塞式IO：在内核将数据准备好之前，系统调用会一直等待。所有的套接字，默认都是阻塞方式，阻塞式是最常见的IO模型，阻塞式IO属于同步IO模型 例子：A拿着一支鱼竿在河边钓鱼，并且一直在鱼竿前等，在等的时候不做其他的事情，十分专心。只有鱼上钩的时，才结束掉等的动作，把鱼钓上来。 2、非阻塞式IO：如果内核还未将数据准备好，系统调用仍然会直接返回，并且返回EWOULDBLOCK错误码，非阻塞IO往往需要程序员循环的方式反复尝试读写文件描述符，这个过程称为轮询。这对CPU来说是较大的浪费，一般只有特定场景下才使用，非阻塞式IO也属于同步IO模型 例子： B也在河边钓鱼，但是B不想将自己的所有时间都花费在钓鱼上，在等鱼上钩这个时间段中，B在做其他的事情（刷牛客或者刷知乎），但B在做这些事情的时候，每隔一个固定的时间检查鱼是否上钩。一旦检查到有鱼上钩，就停下手中的事情，把鱼钓上来。 3、信号驱动IO：内核将数据准备好的时候，使用SIGIO信号通知应用程序进行IO操作，信号驱动IO也属于同步IO，信号驱动是不是有点异步的感觉？ 但是在将数据从内核复制到用户空间这段时间内用户态进程是阻塞的，所以也属于同步IO 例子： C也在河边钓鱼，但与A、B不同的是，C比较聪明，他给鱼竿上挂一个铃铛，当有鱼上钩的时候，这个铃铛就会被碰响，C就会将鱼钓上来。 4、IO多路复用：虽然从流程图上看起来和阻塞IO类似，实际上最核心在于IO多路转接能够同时等待多个文件描述符的就绪状态，IO多路复用也属于同步IO 例子： D同样也在河边钓鱼，但是D生活水平比较好，D拿了很多的鱼竿，一次性有很多鱼竿在等，D不断的查看每个鱼竿是否有鱼上钩。增加了效率，减少了等待的时间。 5、异步IO：由内核在数据拷贝完成时，通知应用程序(而信号驱动是告诉应用程序何时可以开始拷贝数据)，异步IO就是异步IO，哈哈 例子：E也想钓鱼，但E有事情，于是他雇来了F，让F帮他等待鱼上钩，一旦有鱼上钩，F就打电话给E，E就会将鱼钓上去。 阻塞程度：阻塞IO&gt;非阻塞IO&gt;多路转接IO&gt;信号驱动IO&gt;异步IO，效率是由低到高 理解同步与异步通信所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了；换句话说，就是由调用者主动等待这个调用的结果； 异步则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果；换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果；而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用； 对于同步通信来说，都是由调用者主动等待这个调用的结果，无论是“干等”，还是轮询式的等，还是信号通知式的等，还是一次等多个的形式，都是需要主动去等的，对于异步通信来说，这个调用就直接返回了，所以没有返回结果，不需要调用者主动的等待。 理解阻塞与非阻塞阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态。 阻塞调用是指调用结果返回之前，当前线程会被挂起，比如阻塞式IO就是等待调用结果的时候会发生阻塞，啥也干不了，调用线程只有在得到结果之后才会返回； 非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程，比如非阻塞IO，进程在等待的同时，还可以做其他事情。包括信号驱动也属于非阻塞，但是在数据从内核拷贝到用户空间时也是阻塞的；","updated":"2020-04-14T03:11:56.427Z","categories":[{"name":"高性能网络","slug":"高性能网络","permalink":"https://zouchanglin.cn/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"NIO","slug":"NIO","permalink":"https://zouchanglin.cn/tags/NIO/"}]},{"title":"Redis的常见问题","date":"2020-02-28T12:01:35.000Z","path":"2020/02/28/Redis的常见问题/","text":"Redis最常用的数据类型有String类型、Hash、List、Set、SortedSet（分数控制的有序Set）。 Redis常用数据类型今天要介绍的是Redis的数据类型，顺便说说Redis的ping-pong： String类型：最基本的数据类型，二进制安全，最大存储长度为1G的字符串，String可以保存任何对象，无论是JPG图片，还是序列化的对象都是可以保存的！ 如果要统计用户对网站的日访问量应该如何统计呢？其实很简单，如图只要把UserId+日期当成Key，并赋值为0，用户每访问一次就把key对应的值+1，这样就可以轻松统计了： string类型的数据结构： 1234567891011//保存字符串对象的数据结构struct sdshdr&#123; //buf中已占用空间长度 int len; //buf中剩余空间 int free; //数据空间 char buf[];&#125; Hash：看看Hash数据类型，String元素组成的，适合用于存储对象 List：列表，按照String元素插入顺序排序，大约可以存储40亿成员，List可用于最新消息的展示，消息越新，越会立马展示 Set：String元素组成的无序集合，通过Hash表实现，不允许重复 Redis提供了求交集、并集、差集等操作，就可以很方便的实现如共同关注、共同喜好等功能 SortedSet：通过分数来为集合中的成员进行从小到大的排序 其实Redis还支持存储很多类型，用于计数的HyperLogLog，用于支持存储地理位置信息的Geo Redis海量数据里查询某固定前缀的Key首先看一段脚本，会向Redis插入2000万条数据： 1for((i=1;i&lt;=20000000;i++)); do echo \"set k$i v$i\" &gt;&gt; /tmp/redisTest.txt ;done; 首先生成2千万条redis批量设置kv的语句(key=kn,value=vn)写入到/tmp目录下的redisTest.txt文件中，去掉行尾的^M符号： 123vim /tmp/redisTest.txt:set fileformat=dos #设置文件的格式，通过这句话去掉每行结尾的^M符号:wq #保存退出 通过redis提供的管道–pipe形式，去跑redis，传入文件的指令批量灌数据，需要花10分钟左右: 1cat /tmp/redisTest.txt | redis-cli -h 主机ip -p 端口号 --pipe 如果使用keys指令效果是这样的： 使用keys对线上的业务的影响KEYS pattern：查找所有符合给定模式pattern的keyKEYS指令会一次性返回所有匹配的key，如果键的数量过大会使服务卡顿 那么应怎么做呢？ SCAN指令可以帮我们解决这个问题，命令用于迭代当前数据库中的数据库键， 它们每次执行都只会返回少量元素， 所以这些命令可以用于生产环境， 而不会出现像 KEYS命令带来的问题，当KEYS命令被用于处理一个大的数据库时，它们可能会阻塞服务器达数秒之久。 基于游标的迭代器，需要基于上一次的游标延续之前的迭代过程。以0作为游标开始一次新的迭代，直到命令返回游标0时完成一次遍历。不保证每次执行都返回某个给定数量的元素，支持模糊查询。一次返回的数量不可控，只能是大概率符合count参数。 虽然可能获取的Key是有重复的，但是只需要在应用中去重就好了。 Redis如何实现分布式锁这个问题其实在我之前的博客中已经说到过：《基于Redis实现分布式锁》 ，现在来回顾一下，其实主要就是几个指令：SETNX key value，如果Key不存在，则创建Key、并赋值，时间复杂度为O(1)，如果设置成功则会返回1，如果失败则返回0。 如果一个线程成功设置了Key，那么Key岂不是一直存在，别的线程根本不可能设置成功吗？是滴，所以要给Key加上一个过期时间，就要用到了EXPIRE key seconds这条指令了，当Key过期时（生存时间为0），会被自动删除 于是乎我们可以得出伪代码如下： 12345int status = redisService.setnx(key, \"1\");if(status == 1)&#123; redisService.expire(key, expire); //TODO...&#125; 但是这样就有一个问题就是，如果在执行了setnx后程序挂掉了，那么就并没有设置超时时间，就形成了死锁，所以这样的做法是不可取的，因为必须要保证sexnx和设置超时时间这两个操作是原子的，从Redis2.2.6以后，set命令可以把原来的set和expire命令合并在一起，成为一个原子操作： 1SET key value[EX seconds][PX millisecond][NX][XX] EX seconds：设置键的过期时间为seconds秒 PX millisecond：设置键的过期时间为millisecond毫秒 NX：只有在键不存在的时候，才对键进行设置操作 XX：只有在键已经存在的时候，才对键进行设置操作 SET：操作完成时才会返回OK，否则返回nil Redis 雪崩问题目前电商首页以及热点数据都会去做缓存，一般缓存都是定时任务去刷新，或者查不到之后去更新缓存的，定时任务刷新就有一个问题， 如果首页所有 Key 的失效时间都是 12 小时，中午 12 点刷新的，我零点有个大促活动大量用户涌入，假设每秒 6000 个请求，本来缓存可以抗住每秒 5000 个请求，但是缓存中所有 Key 都失效了。 此时6000 个/秒的请求全部落在了数据库上，数据库必然扛不住，真实情况可能 DBA 都没反应过来直接挂了。 此时，如果没什么特别的方案来处理，DBA 很着急，重启数据库，但是数据库立马又被新流量给打死了。这就是我理解的缓存雪崩。 简单来说就是在同一时间缓存大面积失效，瞬间 Redis 跟没有一样，那这个数量级别的请求直接打到数据库几乎是灾难性的。 那应该如何处理呢？ 在批量往 Redis 存数据的时候，把每个 Key 的失效时间都加个随机值就好了，这样可以保证数据不会再同一时间大面积失效。 1setRedis（key, value, time+Math.random()*10000）; 如果 Redis 是集群部署，将热点数据均匀分布在不同的 Redis 库中也能避免全部失效。 或者设置热点数据永不过期，有更新操作就更新缓存就好了（比如运维更新了首页商品，那你刷下缓存就好了，不要设置过期时间），电商首页的数据也可以用这个操作，比较保险的做法。 缓存穿透和击穿缓存穿透是指缓存和数据库中都没有的数据，而用户（黑客）不断发起请求。 例如我们数据库的 id 都是从1自增的，如果发起 id=-1 的数据或者 id 特别大不存在的数据，这样的不断攻击导致数据库压力很大，严重会击垮数据库。 缓存穿透的两种解决方式： 方法一：在接口层增加校验，比如用户鉴权，参数做校验，不合法的校验直接 return，比如 id 做基础校验，id&lt;=0 直接拦截； 方法二：Redis 里还有一个高级用法布隆过滤器（Bloom Filter）这个也能很好的预防缓存穿透的发生。它的原理也很简单，就是利用高效的数据结构和算法快速判断出你这个 Key 是否在数据库中存在，不存在你return 就好了，存在你就去查 DB 刷新 KV 再 return； 缓存击穿和缓存雪崩有点像，但是又有一点不一样，缓存雪崩是因为大面积的缓存失效，打崩了 DB。 而缓存击穿不同的是缓存击穿是指一个 Key 非常热点，在不停地扛着大量的请求，大并发集中对这一个点进行访问，当这个 Key 在失效的瞬间，持续的大并发直接落到了数据库上，就在这个 Key 的点上击穿了缓存。 缓存击穿的解决方式： 设置热点数据永不过期；或者加上互斥锁就搞定了，代码如下： 12345678910111213141516171819202122232425262728public static String getData(String key) throws InterruptedException &#123; //从Redis查询数据 String result = getDataByKV(key); //参数校验 if (StringUtils.isBlank(result)) &#123; try &#123; //获得锁 if (reenLock.tryLock()) &#123; //去数据库查询 result = getDataByDB(key); //校验 if (StringUtils.isNotBlank(result)) &#123; //插进缓存 setDataToKV(key, result); &#125; &#125; else &#123; //睡一会再拿 Thread.sleep(100L); result = getData(key); &#125; &#125; finally &#123; //释放锁 reenLock.unlock(); &#125; &#125; return result;&#125; Redis如何做异步队列使用list作为队列，RPUSH生产消息，LPOP消费消息。 如图所示，RPUSH生产消息，LPOP消费消息，但是当消息被消费完毕的时候LPOP不会等待，而是立即返回，通常的做法是让线程Sleep一会儿，再去尝试LPOP。有没有更好的办法呢？有的： BLPOP指令：阻塞直到队列有消息或者超时 1BLPOP key [key...] timeout 但是这样做也有一个缺点就是，只能提供一个消费者进行消费，那么怎么解决这个问题呢？ pub/sub 主题订阅者模式： 发送者（pub）发送消息，订阅者（sub）接收消息 订阅者可以订阅定义数量的频道 下面演示一下： pub/sub 订阅者模式的缺点：消息的发布是无状态的，无法保证可达，如果要解决这种问题就必须要使用专门的消息队列中间件来解决了，如Kafka等。 Redis如何做持久化Redis 为了保证效率，数据缓存在了内存中，但是会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件中，以保证数据的持久化。 Redis 的持久化策略有两种： RDB：快照形式是直接把内存中的数据保存到一个 dump 的文件中，定时保存，保存策略。 AOF：把所有的对 Redis 的服务器进行修改的命令都存到一个文件里，命令的集合。Redis 默认是快照 RDB 的持久化方式。 当Redis 重启的时候，它会优先使用 AOF 文件来还原数据集，因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。你甚至可以关闭持久化功能，让数据只在服务器运行时存储。 RDB 的工作方式： 默认 Redis 是会以快照”RDB”的形式将数据持久化到磁盘的一个二进制文件 dump.rdb。当 Redis 需要做持久化时，Redis 会 fork 一个子进程，子进程将数据写到磁盘上一个临时 RDB 文件中。 当子进程完成写临时文件后，将原来的 RDB 替换掉，这样的好处是可以 copy-on-write。RDB 非常适合灾难恢复。RDB 的缺点是：如果你需要尽量避免在服务器故障时丢失数据，那么RDB不合适你。而且RDB是做了内存数据的全量同步，数据量大的时候会由于IO而严重影响性能 AOF的工作方式： 123appendfsync yes appendfsync always #每次有数据修改发生时都会写入AOF文件。appendfsync everysec #每秒钟同步一次，该策略为AOF的缺省策略。 AOF可以做到全程持久化，只需要在配置中开启 appendonly yes。这样 Redis 每执行一个修改数据的命令，都会把它添加到 AOF 文件中，当 Redis 重启时，将会读取 AOF 文件进行重放，恢复到 Redis 关闭前的最后时刻。使用 AOF 的优点是会让 Redis 变得非常耐久。可以设置不同的 Fsync 策略，AOF的默认策略是每秒钟 Fsync 一次，在这种配置下，就算发生故障停机，也最多丢失一秒钟的数据。缺点是对于相同的数据集来说，AOF 的文件体积通常要大于 RDB 文件的体积。根据所使用的 Fsync 策略，AOF 的速度可能会慢于 RDB。 两种持久化的比较： 1、如果你非常关心你的数据，但仍然可以承受数分钟内的数据丢失，那么可以只使用 RDB 持久。 2、AOF 将 Redis 执行的每一条命令追加到磁盘中，处理巨大的写入会降低Redis的性能，不知道你是否可以接受 数据库备份和灾难恢复：定时生成 RDB 快照非常便于进行数据库备份，并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度快。Redis 支持同时开启 RDB 和 AOF，系统重启后，Redis 会优先使用 AOF 来恢复数据，这样丢失的数据会最少。 上面的配置文件中save那一块主要是自动触发备份的条件，主动触发RDB持久化的命令： SAVE指令: 阻塞Redis的服务器进程,直到RDB文件被创建完毕 BGSAVE指令：Fork出一-个子进程来创建RDB文件,不阻塞服务器进程 日志重写解决AOF文件大小不断增大的问题，原理如下 调用fork() ,创建一个子进程 子进程把新的AOF写到一个临时文件里,不依赖原来的AOF文件 主进程持续将新的变动同时写到内存和原来的AOF里 主进程获取子进程重写AOF的完成信号,往新AOF同步增量变动 使用新的AOF文件替换掉旧的AOF文件 RDB和AOF文件共存的情况下的恢复流程 Redis的默认持久化方式：RDB和AOF混合持久化方式的流程 BGSAVE做全量持久化，AOF做增量持久化。因为BGSAVE会耗费较长时间，不够实时，会导致大量丢失数据的问题，所以呢需要AOF来做增量持久化配合使用。 Pipeline以及主从同步Pipeline和Linux的管道是类似的，还记得我们之前做的插入2000万条数据吗？ 1cat /tmp/redisTest.txt | redis-cli -h 主机ip -p 端口号 --pipe Redis基于请求/响应模型，单个请求处理需要一一应答，所以如果需要批量操作数据的时候，每个数据操作都需要请求、应答的流程，那么IO负载将会变得非常高，为了提升效率，Pipeline会批量执行指令，即一次发送多条指令，节省多次IO往返时间（但是这样做的前提是批量指令之间没有依赖性）。 主从同步的原理： 首先将BGSAVE的镜像文件做同步，在把期间的增量数据做同步。 全同步的过程： 1、Salve发送sync命令到Master 2、Master启动一个后台进程,将Redis中的数据快照保存到文件中 3、Master将保存数据快照期间接收到的写命令缓存起来 4、Master完成写文件操作后，将该文件发送给Salve 5、使用新的AOF文件替换掉旧的AOF文件 6、Master将这期间收集的增量写命令发送给Salve端 增量同步过程： 1、Master接收到用户的操作指令，判断是否需要传播到Slave 2、将操作记录追加到AOF文件 3、将操作传播到其他Slave : ①对齐主从库；②往响应缓存写入指令 4、将缓存中的数据发送给Slave Redis Sentinel用来解决主从同步Master宕机后的主从切换问题：1、监控：检查主从服务器是否运行正常 2、提醒：通过API向管理员或者其他应用程序发送故障通知 3、自动故障迁移：主从切换 流言协议Gossip在杂乱无章中寻求一致 每个节点都随机地与对方通信,最终所有节点的状态达成一致 种子节点定期随机向其他节点发送节点列表以及需要传播的消息 不保证信息一定会传递给所有节点，但是最终会趋于一致 在区块链的去中心化实现方式中便用到了这种协议。 Redis集群与一致性Hash如何从海量数据里快速找到所需?分片：按照某种规则去划分数据，分散存储在多个节点上。常规的按照哈希划分无法实现节点的动态增减。 比如userId对2模，即可把用户数据分散到两台不同的数据库服务器，但是很容易出现数据分布不均匀的问题， 而且很难实现节点的动态增减。 什么是一致性Hash呢？其实就是对2^32取模，将哈希值空间组织成虚拟的圆环： 使用数据Key相同的函数Hash计算出Hash值 数据只需要进行Hash运算，然后顺时针找到最近的节点，就可以找到对应的服务： 那么这样做的好处是什么呢？ 现在我们假设Node C宕机了，那么如下图： 即使Node C宕机了，那么也会找到最近的Node D节点，最大化的止损。 如果是增加服务器又会是怎样的情况呢？ 如果是新增服务器，只会使一小部分数据发送改变，因为还是只需要找到最近的节点存储即可 接下来说说一致性Hash的缺点，Hash环的数据倾斜问题：此时，我们会引入虚拟节点来解决数据倾斜的问题","updated":"2020-04-14T03:12:16.332Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://zouchanglin.cn/tags/Redis/"},{"name":"缓存中间件","slug":"缓存中间件","permalink":"https://zouchanglin.cn/tags/%E7%BC%93%E5%AD%98%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"InnoDB秒级快照原理与当前读","date":"2020-02-27T12:01:35.000Z","path":"2020/02/27/InnoDB秒级快照原理与当前读/","text":"在之前为文章《分析事务隔离的实现》中我们提到：如果是可重复读隔离级别，事务T启动的时候会创建一个视图 read-view，之后事务 T 执行期间，即使有其他事务修改了数据，事务 T 看到的仍然跟在启动时看到的一样。也就是说，一个在可重复读隔离级别下执行的事务，好像与世无争，不受外界影响。但是，我在介绍MySQL锁机制的文章中《探究MySQL锁机制》 的时候，一个事务要更新一行，如果刚好有另外一个事务拥有这一行的行锁，它会被锁住，进入等待状态。问题是，既然进入了等待状态，那么等到这个事务自己获取到行锁要更新数据的时候，它读到的值又是什么呢？ 开始事务的两种方式需要注意的是：begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB 表的语句，事务才真正启动。 如果你想要马上启动一个事务，可以使用transaction with consistent snapshot 这个命令！ 第一种启动方式，一致性视图是在第执行第一个快照读语句时创建的； 第二种启动方式，一致性视图是在执行 start transaction with consistent snapshot 时创建的； 事务 C 没有显式地使用 begin/commit，表示这个 update 语句本身就是一个事务，语句完成的时候会自动提交。事务 B 在更新了行之后查询 ; 事务 A 在一个只读事务中查询，并且时间顺序上是在事务 B 的查询之后。但是结果却是：事务 B 查到的 k 的值是 3，而事务 A 查到的 k 的值是 1 按照我们想象的情况应该是如下所示： 最终得到的结果是事务A查询的结果为1，事务B查询的结果是2，但是为什么事务B查询后为3呢？？ 需要注意的是，在 MySQL 里，有两个视图的概念： 1、一个是view，它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。 2、另一个是 InnoDB 在实现 MVCC(Multi-Version Concurrency Control，多版本并发控制)时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。 InnoDB创建秒级快照原理本文的首图即是MVCC的实现原理，在可重复读隔离级别下，事务在启动的时候就”拍了个快照”。注意，这个快照是基于整库的。如果一个库有 100G，那么我启动一个事务，MySQL就要拷贝 100G 的数据出来，这个过程得多慢啊。可是平时的事务执行起来很快啊，实际上，我们并不需要拷贝出这 100G 的数据。我们先来看看这个快照是怎么实现的： InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。也就是说，数据表中的一行记录，其实可能有多个版本 (即row中的每个数据)，每个版本有自己的 row trx_id。 如所示，就是一个记录被多个事务连续更新后的状态。 图中的三个虚线箭头，就是 undo log（回滚日志）；而 V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。比如，需要 V2 的时候，就是通过 V4 依次执行 U3、U2 算出来。 按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。 因此，一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。如果是这个事务自己更新的数据，它自己还是要认的。 在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正处于启动了但还没提交的所有事务ID。数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加1记为高水位。这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）而数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。这个视图数组把所有的 row trx_id 分成了几种不同的情况: 这样，对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能： 1、如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的； 2、如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的； 3、如果落在黄色部分，那就包括两种情况 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见； 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。 重点要理解落在黄色部分的两种情况，一条数据被多个事物更新（事务还未提交），那么肯定在数组中含有row trx_id，即这个版本是由还没提交的事务生成的 所以InnoDB如何秒级创建快照的呢？我的总结如下： 1、事务开启时InnoDB会赋给事务一个ID，为了方便我们直接叫做事务ID 2、表中每行数据有多个版本，A事务更新数据的时候，都会生成一个新的数据版本，并且把A事务的事务ID赋值给这个数据版本的事务ID 3、通过回滚日志来计算事务ID对应的数据版本，比如A事务更新了 id=1的数据的value为1，那么就存在id=1那一条数据的新版本，(id=1，value=1)这一条数据对应的事务ID就是A事务的事务ID 4、如果一个数据版本的事务ID落在黄色部分并且还在事务数组中就判定此数据版本是其他未提交事务对数据修改产生的数据新版本；如果一个数据版本的事务ID落在黄色部分并且不在自己的事务数组中，那么说明事务已经提交，是合法数据 所以InnoDB的秒级快照的创建能力的原理无非和Linux的AUFS文件系统如出一撤，那就是你只要改了某一条数据，我就复制那一条让你改，并且不会影响其他人的数据： 都是”当前读”惹的祸我们在明白了InnoDB如何实现的创建秒级快照的原理后，开篇的疑惑其实很好解答： 我们不妨做如下假设，事务 A 开始前，系统里面只有一个活跃事务 ID 是 99；事务 A、B、C 的版本号分别是 100、101、102，且当前系统里只有这四个事务；三个事务开始前，(1,1）这一行数据的 row trx_id 是 90 这样，事务 A 的视图数组就是 [99,100]，事务 B 的视图数组是 [99,100,101]，事务 C 的视图数组是 [99,100,101,102]。 这其实就很容易理解了，事务C发现，此时的已开启事务但是未提交的有四个（99,100,101,102），由于其他事务还并未对id=1这条数据进行修改，所以此时只有一个版本那就是(id=1, k=1，事务ID=90)，并且90号事务ID处于低水位，说明事务ID为90的事务已经提交了，数据有效，随后事务C把k置为了2，并且提交了事务，所以此时生成了新版本（id=1，k=2，事务ID为102） 最后A事务去查询k的值，发现已经有三个版本了，当A事务看到最新版本（id=1，k=3，事务ID为101）的数据时，发现101在高水位，不能读到，接着读到版本为（id=1，k=2，事务ID为102）时候同样事务102也是高水位，不能读到，继续读，读到（id=1，k=1，事务ID为90）的版本的数据，事务ID为90的事务处于低水位，数据时可见的，于是得到的数据依旧是（id=1， k=1， 事务ID为90）的数据版本 问题来了？事务B是如何看到事务C的修改结果的？？对于事务B来说，事务C不是高水位吗？按道理应该不可见高水位事务的修改呀！！！ 事务 B 的视图数组是先生成的，之后事务 C 才提交，不是应该看不见 (1,2)吗，怎么能算出 (1,3) 来？ 是的，如果事务 B 在更新之前查询一次数据，这个查询返回的 k 的值确实是 1。但是，当它要去更新数据的时候，就不能再在历史版本上更新了，否则事务 C 的更新就丢失了。因此，事务 B 此时的 set k=k+1 是在（1,2）的基础上进行的操作。所以，这里就用到了这样一条规则： 更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。 因此，在更新的时候，当前读拿到的数据是 (1,2)，更新后生成了新版本的数据 (1,3)，这个新版本的 row trx_id 是 101。所以，在执行事务 B 查询语句的时候，一看自己的版本号是 101，最新数据的版本号也是101，是自己的更新，可以直接使用，所以查询得到的 k 的值是 3。 但是select的结果却是1，所以我们队开始的例子稍做修改，让更新的时候值不在是k=k+1，这样就避免了当前读，所以下面的例子就是避免的当前读所产生的结果： 因为在MYSQL中：不能在同一表中查询的数据作为同一表的更新数据，所以我这里使用了临时表 其实，除了 update 语句外，select 语句如果加锁，也是当前读。 现在假设事务 C 不是马上提交的，而是变成了下面的事务 C’，会怎么样呢？ 事务 C’的不同是，更新后并没有马上提交，在它提交前，事务 B 的更新语句先发起了。前面说过了，虽然事务 C’还没提交，但是 (1,2) 这个版本也已经生成了，并且是当前的最新版本。那么，事务 B 的更新语句会怎么处理呢？。 事务 C’没提交，也就是说 (1,2) 这个版本上的写锁还没释放。而事务 B 是当前读，必须要读最新版本，而且必须加锁，因此就被锁住了，必须等到事务 C’ 释放这个锁，才能继续它的当前读。 到这里，我们把一致性读、当前读和行锁就串起来了。 可重复读的实现可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。 而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。 InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。对于可重复读，查询只承认在事务启动前就已经提交完成的数据；对于读提交，查询只承认在语句启动前就已经提交完成的数据；而当前读，总是读取已经提交完成的最新版本。","updated":"2020-04-14T03:13:12.563Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"索引","slug":"索引","permalink":"https://zouchanglin.cn/tags/%E7%B4%A2%E5%BC%95/"},{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"},{"name":"文件系统","slug":"文件系统","permalink":"https://zouchanglin.cn/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"}]},{"title":"分析事务隔离的实现","date":"2020-02-27T00:01:35.000Z","path":"2020/02/27/分析事务隔离的实现/","text":"关于MySQL的锁机制问题，在之前的博客中有谈到《探究MySQL锁机制》 ，里面对锁的讲解比较详细，现在是在原来的基础上谈谈数据库事务相关的问题。简单来说事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在MySQL中，事务支持是在引擎层实现的。你现在知道，MySQL是一个支持多引擎的系统，但并不是所有的引|擎都支持事务。比如MySQL原生的MyISAM引擎就不支持事务，这也是MyISAM被InnoDB取代的重要原因之一。首先要知道的是数据库事务四大特性：ACID 事务四大特性ACID 原子性 Atomic：事务是一个原子操作单元， 其对数据的修改，要么全都执行，要么全都不执行。 一致性 Consistency：在事务开始和完成时， 数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改，以保持数据的完整性；事务结束时，所有的内部数据结构(如B树索引或双向链表)也都必须是正确的。 隔离性 Isolation：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的，反之亦然。 持久性 Durability：事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。 并发事务处理的问题1、更新丢失 当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题，这就是最后的更新覆盖了由其他事务所做的更新。 例如，两个程序员修改同一个Java文件。每程序员独立地更改其副本，然后保存更改后的副本，这样就覆盖了原始文档。最后保存其更改副本的编辑人员覆盖前一个程序员所做的更改。如果在一个程序员完成并提交事务之前，另一个程序员不能访问同一文件，则可避免此问题。 2、脏读 一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致状态，这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些 “脏” 数据，并据此做进一步的处理， 就会产生未提交的数据依赖关系。这种现象被形象地叫做”脏读”。 一句话：事务A读取到了事务B已修改但尚未提交的的数据，还在这个数据基础上做了操作。此时，如果B事务回滚，A读取的数据无效，不符合一致性要求。 3、不可重复读 一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或某些记录已经被删除了！这种现象就叫做“不可重复读”。一句话：事务A读取到了事务B已经提交的修改数据，不符合隔离性 4、幻读 一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”。 一句话：事务A读取到了事务B体提交的新增数据，不符合隔离性 注意：幻读和脏读有点类似，脏读是事务B里面修改了数据，幻读是事务B里面新增了数据。 四种事务隔离级别 读未提交：一个事务还没提交时，它做的变更就能被别的事务看到。 读提交：一个事务提交之后，它做的变更才会被其他事务看到。 可重复读： 一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。 当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化：顾名思义是对于同一行记录，“写” 会加写锁，”读” 会加读锁。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 这4种隔离级别，并行性能依次降低，安全性依次提高 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在可重复读隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在读提交隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。这里需要注意的是，读未提交隔离级别下直接返回记录上的最新值，没有视图概念；而串行化隔离级别下直接用加锁的方式来避免并行访问。 在不同的隔离级别下，数据库行为是有所不同的。Oracle 数据库的默认隔离级别其实就是读提交，因此对于一些从Oracle迁移到MySQL的应用为保证数据库隔离级别的一致，一定要记得将MySQL的隔离级别设置为读提交 配置的方式是，将启动参数 transaction-isolation 的值设置成 READ-COMMITTED。你可以用 show variables 来查看当前的值： 事务隔离的实现原理接下来我们看看事务隔离的实现原理，在这里我们可以简单说一下可重复读 在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值，假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录： 当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。 同时你会发现，即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的，回滚日志总不能一直保留吧，什么时候删除呢？ 答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除，什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。 通过上面的例子我们发现，尽量不要使用长事务，这是一条必须遵守的法则 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导占用大量的存储空间！ 这4种隔离级别，并行性能依次降低，安全性依次提高 总结如下表：","updated":"2020-04-14T03:13:47.640Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"},{"name":"异常处理","slug":"异常处理","permalink":"https://zouchanglin.cn/tags/%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"}]},{"title":"密集索引和稀疏索引","date":"2020-02-26T12:01:35.000Z","path":"2020/02/26/密集索引和稀疏索引/","text":"密集索引：文件中的每个搜索码值都对应一个索引值，就是叶子节点保存了整行，比如InnoDB 稀疏索引：文件只为索引码的某些值建立索引项，比如MyISAM 密集索引的表数据按顺序存储，即索引顺序和表记录物理存储顺序一致，所以一个表只能创建一个密集索引 InnoDB的密集索引的选取规则是怎么样的呢？ 1、若一个主键被定义，该主键则作为密集索引2、若没有主键被定义，该表的第一个唯一非空索引则作为密集索引3、若不满足以上条件，InnoDB内部会生成一个隐藏主键(密集索引)4、非主键索引存储相关键位和其对应的主键值，包含两次查找 如上图示意：InooDB的主键索引和数据是保存在同一个文件中的，所以在检索的时候，在加载叶子节点的主键进入内存的同时也加载了对应的数据，如果是使用主键查询，只需要根据B+Tree的规则找到主键，就可以获得对应的行数据，如果稀疏索引进行数据筛选就会经历两个步骤，第一个步骤是在稀疏索引B+Tree中检索该键，比如检索Ellison，就会获取到主键信息，获取到主键信息之后，需要经历第二个步骤：在主键的B+Tree中进行查找。 对于MyISAM，使用的均为稀疏索引，稀疏索引的两个B+Tree其实也没有什么不同，结构完全一致，只是存储的内容不一样而已，主键索引的B+Tree存储了主键，辅助键索的B+Tree引存储了辅助键，数据却存储在了独立的地方，也就是索引和数据是分开存储的，这两个B+Tree的叶子节点都存储了指向数据的地址，由于辅助键索是独立的，无需借助主键。 下面我们可以建表验证一下，建表语句如下和效果如下： 上面我们已经弄出清楚了密集索引和稀疏索引的区别，现在来思考一个问题：索引是越多越好吗？ 答案是当然是否定的，数据量小的表不需要建立索引，建立会增加额外的索引开销；数据变更需要维护索引，因此更多的索引意味着更多的维护成本，更多的索引意味着也需要更多的空间！","updated":"2020-03-13T03:06:29.735Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"索引","slug":"索引","permalink":"https://zouchanglin.cn/tags/%E7%B4%A2%E5%BC%95/"},{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"}]},{"title":"BTree与B+Tree","date":"2020-02-26T11:01:35.000Z","path":"2020/02/26/BTree与B+Tree/","text":"其实在之前的文章说说到过MySQL索引，只不过没细说，《JOIN查询与索引简介》 ，现在来看看回顾一下索引相关的数据结构，首先看看索引的定义： 索引(Index)是帮助MySQL高效获取数据的数据结构。 我不会索引就是类似于字典这样的话泛泛而谈，而是如何真正去理解索引。 根据列字段信息的特点，去用一种数据结构存储这些特点，这就是索引，如上图：我们需要把第二列的值通过二叉查找树存储起来，每一个节点都是以Key-Vlaue的形式存在，把值当做Key，把物理地址当做Value，那么只要根据二叉查找树的特性，很快就可以找到对应的物理地址，从而取出数据。 数据库常见的索引有Btree、B+tree、Hash索引等等，今天主要探讨的是BTree和B+Tree B Tree先看看Btree的定义： 根节点至少包括两个孩子 树中每个节点最多含有m个孩子( m&gt;=2 ) 除根节点和叶节点外，其他每个节点至少有ceil(m/2)个孩子，这里的ceil是取上限的意思 所有叶子节点位于同一层 假设每个非终端结点中包含有n个关键字信息，其中 Ki (i=1..n)为关键字，且关键字按顺序升序排序K(i-1) &lt; Ki 关键字的个数n必须满足: [ceil(m / 2) -1] &lt;= n &lt;= m-1 非叶子结点的指针: P[1]，P[2]，… P[M]；其中P[1]指向关键字小于K[1]的子树，P[M]指向关键字大于K[M-1]的子树，其它P[i]指向关键字属于(K[i-1]，K[i])的子树 看起来是非常绕的，通过下面的图示其实是蛮好懂的： 由于B-Tree的特性，在B-Tree中按key检索数据的算法非常直观：首先从根节点进行二分查找，如果找到则返回对应节点的data，否则对相应区间的指针指向的节点递归进行查找，直到找到节点或找到null指针，前者查找成功，后者查找失败。 但是由于插入删除新的数据记录会破坏B-Tree的性质，因此在插入删除时，需要对树进行一个分裂、合并、转移等操作以保持B-Tree性质，在这里先不讨论插入删除新的数据记录的问题。 B+TreeB+树是B树的变体， B-Tree有许多变种，其中最常见的是B+Tree，例如MySQL就普遍使用B+Tree实现其索引结构。 B+树的定义基本与B树相同，除了: 非叶子节点的子树指针与关键字个数相同 非叶子节点的子树指针P[i]，指向关键字值[K[i]，K[i+1])的子树 非叶子节点仅用来作为索引，数据都保存在叶子节点中 所有叶子节点均有一个链指针指向下一个叶子结点 B+Tree更适合做索引1、B+Tree的磁盘读写代价更低 B+Tree内部并没有指向关键字具体信息的指针，也就是不存放数据，只存放索引信息，所能容纳的关键字数量也就越多，一次性读入内存需要查找的关键字也就越多，相对来说IO读写次数也就降低了 2、B+Tree的查询效率更加稳定 由于B+Tree内部节点并不是直接指向文件内容的节点，而只是叶子节点中关键字的索引，所以任何关键字的查找必须走一条从根节点到叶子节点的路，所有关键字查询的路径长度相同，导致每一个数据的查询效率也几乎是相同的，所以查询效率更稳定 3、B+Tree更有利于对数据库的扫描 B Tree在提高了磁盘IO性能的同时，并没有解决元素遍历的效率底下的问题，而B+Tree只需要遍历叶子节点就可以解决对全部关键字信息的扫描，所以对于数据库频繁使用的范围查询是非常有利的 这也就是选择B+Tree作为主流索引数据结构的原因 Hash索引有些数据库存储引擎还支持Hash这个数据结构作为其索引哈希结构，想必大家已经非常熟悉了，就是根据哈希函数的运算，那只需经过一次定位便能找到需要的数据，相比起B+Tree索引，需要从根节点到非叶子节点，再到叶子节点才能访问到我们需要的数据，这样可能会经过多次的IO访问，所以呢，哈希索引的查询效率理论上要高于B+Tree索引。 但是Hash也是有非常明显的缺点的： 1、仅仅能满足“=” 、“IN”，不能使用范围查询 2、无法被用来避免数据的排序操作 3、不能利用部分索引建来查询 4、不能避免表扫描 5、遇到大量Hash值相等的情况后性能并不一定就会比B-Tree索引高","updated":"2020-03-13T03:06:29.631Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"索引","slug":"索引","permalink":"https://zouchanglin.cn/tags/%E7%B4%A2%E5%BC%95/"},{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"}]},{"title":"HTTPS协议实现原理","date":"2020-02-25T11:01:35.000Z","path":"2020/02/25/HTTPS协议实现原理/","text":"HTTP协议的信息传输完全以明文方式，不做任何加密，相当于是在网络上”裸奔”。意思就是如果我们以HTTP来传输数据的话由于是明文数据，则很容易发生不安全事故，比如我登陆一个网站，那么如果是HTTP明文传输肯定就会直接把用户名和密码放在明文中，这样是非常不安全的，处在同一个局域网下的小伙伴直接抓包就可以获取你的用户名和密码，那么应该怎么办呢？ 对称加密 事先约定一种对称加密方式，并且约定一个随机生成的密钥。后续的通信中，信息发送方都使用密钥对信息加密，而信息接收方通过同样的密钥对信息解密。 假设A和B通过网络进行对话，这样做是不是就绝对安全了呢？并不是。虽然我们在后续的通信中对明文进行了加密，但是第一次约定加密方式和密钥的通信仍然是明文，如果第一次通信就已经被拦截了，那么密钥就会泄露给中间人，中间人仍然可以解密后续所有的通信内容。 非对称加密 非对称加密的一组秘钥对中，包含一个公钥和一个私钥。明文既可以用公钥加密，用私钥解密；也可以用私钥加密，用公钥解密。 看看下面的例子，看下面的例子之前很有必要把非对称加密的特点再复习一下：明文既可以用公钥加密，用私钥解密；也可以用私钥加密，用公钥解密。 B同学和A同学想要通过非对称加密的方式进行通信，B准备好了自己的私钥和公钥，接下来把自己的公钥发给了A，A此时生成了自己的秘钥Akey，并且A同学利用B发过来的公钥加密自己的秘钥Akey，之后发给B同学，B同学通过自己的私钥对数据包解密，得到的A的秘钥Akey，这样接下来双方只需要用这个Akey对信息加密就可以万无一失了，因为别人不可能知道Akey 但是，这样做的风险同样在于第一次秘钥发送： 这就是为什么使用了非对称加密还是解决不了信息安全传输的原因， 难道再把公钥进行一次加密吗？ 很显然不可能，如果再把公钥进行一次加密，黑客要做的只是多解密一次罢了， 这时候，我们有必要引入第三方，一个权威的证书颁发机构（CA）来解决，什么是证书呢？下面是证书的组成： 下面还是B同学的A同学通信的例子： 那么这样做的话有没有可能出现黑客拦截证书，然后替换成自己的证书的情况呢？ 这是不可能的，因为证书的签名是由服务端网址等信息生成的，并且经过机构私钥加密，中间人也无法篡改。所以，黑客的假证书是无法验证通过的。","updated":"2020-03-13T03:06:29.659Z","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"网络安全","slug":"网络安全","permalink":"https://zouchanglin.cn/tags/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"name":"非对称加密","slug":"非对称加密","permalink":"https://zouchanglin.cn/tags/%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86/"}]},{"title":"TCP的高性能机制","date":"2020-02-25T04:01:35.000Z","path":"2020/02/25/TCP高性能的机制/","text":"之前介绍了TCP的报文格式（《TCP协议基本特性》），TCP的连接管理，学习了TCP如何建立连接，释放连接以及一些网络安装方面的问题，现在还剩下TCP的几个关键机制，主要是TPC的延迟应答和捎带应答、超时重传、快重传和快恢复、滑动窗口机制、拥塞避免算法；然后最后还记录了TCP的粘包问题和解决方案！ TCP实现的可靠传输其实依赖于确认应答机制，也就是接收方每次接收完数据之后会回发一个ack确认号，代表自己下次期望收到的序号，意思是告诉发送者, 我已经收到了哪些数据; 下一次你从哪里开始发。 TCP数据包中的序列号（Sequence Number）不是以报文段来进行编号的，而是将连接生存周期内传输的所有数据当作一个字节流，序列号就是整个字节 流中每个字节的编号。一个TCP数据包中包含多个字节流的数据（即数据段），而且每个TCP数据包中的数据大小不一定相同。在建立TCP连接的三次握手 过程中，通信双方各自已确定了初始的序号x和y，TCP每次传送的报文段中的序号字段值表示所要传送本报文中的第一个字节的序号。 TCP的确认应答TCP提供的确认机制，可以在通信过程中可以不对每一个TCP数据包发出单独的确认包（Delayed ACK机制），而是在传送数据时，顺便把确认信息传出， 这样可以大大提高网络的利用率和传输效率。同时，TCP的确认机制，也可以一次确认多个数据报，例如，接收方收到了201，301，401的数据报，则只 需要对401的数据包进行确认即可，对401的数据包的确认也意味着401之前的所有数据包都已经确认，这就是延迟应答，这样也可以提高系统的效率。 再说说捎带应答，TCP的确认应答和回执数据可以通过一个包发送。 TCP的超时重传若发送方在规定时间内没有收到接收方的确认信息，就要将未被确认的数据包重新发送。接收方如果收到一个有差错的报文，则丢弃此报文，并不向发送方 发送确认信息。因此，TCP报文的重传机制是由设置的超时定时器来决定的，在定时的时间内没有收到确认信息，则进行重传。这个定时的时间值的设定非常重要，太大会使包重传的延时比较大，太小则可能没有来得及收到对方的确认包发送方就再次重传，会使网络陷入无休止的重传过程中。接收方如果收到 了重复的报文，将会丢弃重复的报文，但是必须发回确认信息，否则对方会再次发送。 但是主机A未收到B发来的确认应答, 也可能是因为ACK丢失了；因此主机B会收到很多重复数据。那么TCP协议需要能够识别出那些包是重复的包,，并且把重复的丢弃掉。这时候我们可以利用前面提到的序列号, 就可以很容易做到去重的效果。 超时时间如何确定? TCP为了保证无论在任何环境下都能比较高性能的通信，因此会动态计算这个最大超时时间：Linux中(BSD Unix和Windows也是如此)，超时以500ms为一个单位进行控制，每次判定超时重发的超时时间都是500ms的整数倍，如果重发一次之后，仍然得不到应答，等待 2*500ms 后再进行重传如果仍然得不到应答，等待 4*500ms进行重传。依次类推以指数形式递增，累计到一定的重传次数，TCP认为网络或者对端主机出现异常，强制关闭连接 TCP快速重传（冗余ACK）有了超时重传机制为什么还出现了快速重传呢？其实这也是TCP为了效率的一种保障，每当比期望序号大的失序报文段到达时，发送一个冗余ACK，指明下一个期待字节的序号。 超时重传是底线，是功能性的，但是快重传是建立在超时重传上的，为了效率的提高 TCP流量控制接收端处理数据的速度是有限的。如果发送端发的太快，导致接收端的缓冲区被打满，这个时候如果发送端继续发送，就会造成丢包，继而引起丢包重传等等一系列连锁反应。因此TCP支持根据接收端的处理能力，来决定发送端的发送速度。这个机制就叫做流量控制(Flow Control)； 在通信过程中，接收方根据自己接收缓存的大小，动态地调整发送方的发送窗口大小，即接收窗口rwnd (接收方设置确认报文段的窗口字段来将rwnd通知给发送方)， 发送方的发送窗口取接收窗口rwnd和拥塞窗口cwnd的最小值。接收窗口也就是接收方的接收缓冲区，拥塞窗口简单来说就是网络堵塞了，那就是说明在这个网络中使用网络带宽的主机很多，或者占用了很多资源，导致发送缓慢，那么这就是一个网络拥塞的情况 TCP首部中，专门有一个窗口大小的字段用来通知窗口大小。接收主机将自己可以接收的缓冲区大小放人这个字段中通知给发送端。这个字段的值越大，说明网络的吞吐量越高。不过，接收端的这个缓冲区一旦面临数据溢出时，窗口大小的值也会随之被设置为一个更小的值通知给发送端，从而控制数据发送量。也就是说，发送端主机会根据接收端主机的指示，对发送数据的量进行控制。这也就形成了一个完整的TCP流控制(流量控制)。 从图中可以看到，如果过了重发时间还没有收到窗口的更新通知，会发送一个探测报文去探测接收方的窗口。 滑动窗口虽然只能往右滑，但是可能变大，也可能变小，也可能是0；收到第一个ACK后，滑动窗口向后移动，继续发送第五个段的数据，依次类推，操作系统内核为了维护这个滑动窗口，需要开辟发送缓冲区来记录当前还有哪些数据没有应答；只有确认应答过的数据，才能从缓冲区删掉；窗口越大，则网络的吞吐率就越高; TCP拥塞控制因为网络上有很多的计算机，可能当前的网络状态就已经比较拥堵。在不清楚当前网络状态下，贸然发送大量的数据，是很有可能引起雪上加霜的。一般来说，计算机网络都处在一个共享的环境。因此也有可能会因为其他主机之间的通信使得网络拥堵。在网络出现拥堵时，如果突然发送一个较大量的数据，极有可能会导致整个网络的瘫痪看看TCP是如何解决这个问题的呢？ TCP引入了慢启动机制：先发少量的数据，探探路，摸清当前的网络拥堵状态，再决定按照多大的速度传输数据； 那么慢启动和拥塞避免的算法如下图所示： cwnd指的是一个报文段（最大报文段长度MSS），最开始发一个报文段然后呈指数式增长到ssthresh初始值，这就是慢启动，然后执行加法增大，也就是拥塞避免阶段，达到阈值的时候变会触发快重传，此时降到新的ssthresh值，即为拥塞时的一半，这就是快恢复策略！当TCP通信开始以后，网络吞吐量会逐渐上升，但是随着网络拥堵的发生吞吐量也会急速下降。于是会再次进人吞吐量慢慢上升的过程。因此所谓TCP的吞吐量的特点就好像是在逐步占领网络带宽的感觉。 TCP粘包问题首先要明确，粘包问题中的”包”是指的应用层的数据包。 在TCP的协议头中，没有如同UDP一样的”报文长度”这样的字段，但是有一个序号这样的字段。站在传输层的角度，TCP是一个一个报文过来的，按照序号排好序放在缓冲区中。站在应用层的角度，看到的只是一串连续的字节数据，那么应用程序看到了这么一连串的字节数据，就不知道从哪个部分开始到哪个部分是一个完整的应用层数据包 那么如何避免粘包问题呢? 归根结底就是一句话，明确两个包之间的边界： 对于定长的包，保证每次都按固定大小读取即可；例如对于一个结构体struct，是固定大小的，那么就从缓冲区从头开始按sizeof(struct)依次读取即可； 对于变长的包，可以在包头的位置，约定一个包总长度的字段，从而就知道了包的结束位置； 对于变长的包，还可以在包和包之间使用明确的分隔符(应用层协议，是程序猿自己来定的，只要保证分隔符不和正文冲突即可); 对于UDP是否也存在”粘包问题”呢？对于UDP，如果还没有上层交付数据，UDP的报文长度仍然在。同时，UDP是一个一个把数据交付给应用层，就有很明确的数据边界。站在应用层的角度，使用UDP的时候，要么收到完整的UDP报文，要么不收，不会出现”半个”的情况。","updated":"2020-03-13T03:06:29.709Z","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"谈谈HTTP协议","date":"2020-02-25T04:01:35.000Z","path":"2020/02/25/谈谈HTTP协议/","text":"HTTP的全称 HyperText Transfer Protocol，即超文本传输协议，程序员自己发明的协议之一，基于TCP的应用层协议。 HTTP协议是基于请求-响应的模式： HTTP是一种无状态协议，HTTP协议自身不对请求和响应之间的通信状态进行保存。也就是说在HTTP这个级别，协议对于发送过的请求或响应都不做持久化处理。 HTTP是一种无连接协议， 无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间，并且可以提高并发性能，不能和每个用户建立长久的连接，请求一次相应一次，服务端和客户端就中断了。但是无连接有两种方式，早期的http协议是一个请求一个响应之后，直接就断开了，但是现在的http协议1.1版本不是直接就断开了，而是等几秒钟，这几秒钟是等什么呢，等着用户有后续的操作，如果用户在这几秒钟之内有新的请求，那么还是通过之前的连接通道来收发消息，如果过了这几秒钟用户没有发送新的请求，那么就会断开连接，这样可以提高效率，减少短时间内建立连接的次数，因为建立连接也是耗时的，默认的好像是3秒中现在，但是这个时间是可以通过咱们后端的代码来调整的，自己网站根据自己网站用户的行为来分析统计出一个最优的等待时间。 HTTP请求/响应格式 HTTP的方法 HTTP状态码 我经常遇到的状态码： 200 OK，代表请求正常处理完毕 202 OK，在文件下载断点续传的时候用过 304 使用缓存，响应体中无数据 131，302 重定向请求， 301表示旧地址A的资源已经被永久地移除了 ，302是暂时移除 400 请求参数有误 403 对请求资源的访问被服务器拒绝 404 服务器找不到请求资源 405 请求方式被拒绝 500 服务器端在执行请求时发生了错误 502 服务器超负载/在维护 HTTP常见的Header Content-Type: 数据类型(text/html等) Content-Length: Body的长度 Host:客户端告知服务器所请求的资源是在哪个主机的哪个端口上; User-Agent:声明用户的操作系统和浏览器版本信息; referer: 当前页面是从哪个页面跳转过来的; location:搭配3xx状态码使用,告诉客户端接下来要去哪里访问; Cookie:用于在客户端存储少量信息.通常用于实现会话(session)的功能; 实现简易的HTTP服务器12345678910111213141516public class HttpServer &#123; public static void main(String[] args) throws Exception&#123; ServerSocket serverSocket = new ServerSocket(80); Socket accept = serverSocket.accept(); OutputStream outputStream = accept.getOutputStream(); Writer writer = new BufferedWriter(new OutputStreamWriter(outputStream, \"UTF-8\")); writer.write(\"HTTP/1.0 200 OK\\r\\n\"); writer.write(\"Set-Cookie:CookieName=HelloCookie\\r\\n\"); writer.write(\"\\r\\n\"); writer.write(\"&lt;h1&gt;Hello, Http Server&lt;h1&gt;\"); writer.flush(); writer.close(); outputStream.close(); serverSocket.close(); &#125;&#125; 而且还携带了Cookie","updated":"2020-04-14T03:14:38.683Z","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"构造实现简单容器","date":"2020-02-24T14:38:52.000Z","path":"2020/02/24/构造实现简单容器/","text":"现在即将开始真正踏上构造自己的容器的道路。我们会基于当前的操作系统创建一个与宿主机隔离的容器环境，下面就开始吧。在开始之前我们需要先对Linux的proc文件系统做一个介绍： 如果你对这些基本知识已经很熟悉了，请直接略过。Linux下的/proc文件系统是由内核提供的，它其实不是一个真正的文件系统，只包含了系统运行时的信息（比如系统内存、mount设备信息、一些硬件配置等），它只存在于内存中，而不占用外存空间。它以文件系统的形式，为访问内核数据的操作提供接口。实际上，很多系统工具都是简单地去读取这个文件系统的某个文件内容，比如lsmod，其实就是cat /proc/modules。当遍历这个目录的时候，会发现很多数字，这些都是为每个进程创建的空间，数字就是它们的PID。 下面介绍几个比较重要的部分： 实现一个简单版本的run命令,类似docker run -ti [command] 代码目录结构如下：","updated":"2020-03-13T03:06:29.746Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"},{"name":"Golang","slug":"Golang","permalink":"https://zouchanglin.cn/tags/Golang/"},{"name":"文件系统","slug":"文件系统","permalink":"https://zouchanglin.cn/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"}]},{"title":"动手实现AUFS文件系统","date":"2020-02-23T03:37:40.000Z","path":"2020/02/23/动手实现AUFS文件系统/","text":"在之前的文章中我们探究了Docker是如何使用AUFS文件系统的，现在我们开始动手实践一下AUFS，用简单的命令来创建一个AUFS文件系统，感受下如何使用AUFS和COW实现文件管理。 在目录下创建一个aufs的文件夹，然后在aufs目录下创建一个mnt的文件夹作挂载点。接着在aufs目录下创建一个名为container-layer的文件夹，里面有一个名为container-layer.txt的文件，文件内容为”I am container layer”。同样地，继续在aufs目录下创建4个名为image-layern的文件夹(n取值分别为l和4)，里面有一个名为image-layer{n}.txt的文件，文件内容为”I am image layer${n}”。使用如下命令检查文件内容。 要联合的文件目录都己经准备好了。接下来把container-layer和4个名为image-layer${n}的文件夹用AUFS的方式挂载到刚刚创建的mnt目录下。在mount aufs的命令中，没有指定待挂载的5个文件夹的权限，默认的行为是dirs指定的左边起第一个目录是read-write权限，后续的都是read-only权限。 还是《Union File System在Docker中的应用》文中曾经在系统aufs目录下查看文件读写权限的做法，这里依然使用如下命令来确认新mount的文件系统中每个目录的权限。 接下来，执行一个有意思的操作，往mnt/image-layer1.txt的文件末尾添加一行文字 “write to mnt’s image-layer1.txt”。根据上面介绍的COW技术，大家猜想一下会产生什么样的行为。 也就是说，当尝试向mnt/image-layer4.txt文件进行写操作的时候，系统首先在mnt目录下查找名为image-layer4.txt的文件，将其拷贝到read-write层的container-layer目录中，接着对container-layer目录中的image-layer4.txt文件进行写操作。至此，我们成功地完成了一个小小的Demo，实现了自己的AUFS文件系统。 《虚拟化的基石——Namespace》、《通过Namespace实现隔离》介绍了Linux Namespace，一共有6种类别的Namespace，分别进行了简单介绍。然后，以Go语言为例实现了一个Demo,使大家能有一个直观的认识。而且对于这些Namespace的应用，会有更加复杂的例子《Linux Cgroups的资源控制》、《Cgroups在Docker中的应用》介绍了LinuxCgroups。通过LinuxCgroups的三种结构，可以随意定制对资源的限制及对资源做监控。最后使用Go语言实现了一个Cgroups限制资源的Demo，介绍了如何用Go语言去操控容器的Cgroups，进而实现限制容器资源的效果。《Union File System在Docker中的应用》介绍了Union File System，列举了其中的几个具体实现，并且讲解了Docker是如何使用分层文件系统来实现镜像不同分层的重复利用的。最后，以AUFS为例子介绍了如何构建，一个简单的分层文件系统。后面在开发自己的容器镜像的过程中就会使用这项技术。","updated":"2020-04-14T03:19:45.024Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"},{"name":"文件系统","slug":"文件系统","permalink":"https://zouchanglin.cn/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"}]},{"title":"Union File System在Docker中的应用","date":"2020-02-22T07:15:41.000Z","path":"2020/02/22/Union File System在Docker中的应用/","text":"Union File SystemUnion File System，简称UnionFS，关于联合文件系统我之前的一篇博客里也写过的《Docker镜像与数据容器卷》，是一种为Linux、FreeBSD 和NetBSD操作系统设计的，把其他文件系统联合到一个联合挂载点的文件系统服务。它使用branch把不同文件系统的文件和目录“透明地”覆盖，形成一个单一一致的文件系统。这些branch或者是read-only的，或者是read-write的，所以当对这个虚拟后的联合文件系统进行写操作的时候，系统是真正写到了一个新的文件中。看起来这个虚拟后的联合文件系统是可以对任何文件进行操作的，但是其实它并没有改变原来的文件,这是因为unionfs用到了一个重要的资源管理技术，叫写时拷贝。 关于写时拷贝技术用到的地方有很多，我在之前一篇转载的文章中也说到了写时拷贝技术，《谈谈写时拷贝》，可以参考Unix的fork函数与vfork函数，还是很容易明白的，调用fork函数时，内核只为新生成的子进程创建虚拟空间结构，它们来复制于父进程的虚拟究竟结构，但是不为这些段分配物理内存，它们共享父进程的物理空间，当父子进程中有更改相应段的行为发生时，再为子进程相应的段分配物理空间。 vfork函数调用时连内核连子进程的虚拟地址空间结构也不创建了，直接共享了父进程的虚拟空间，当然了，这种做法就顺水推舟的共享了父进程的物理空间。 高级多层统一文件系统 AUFSAUFS（全称：advanced multi-layered unification filesystem，高级多层统一文件系统）用于为Linux文件系统实现联合挂载。AUFS完全重写了早期的UnionFS 1.x，其主要目的是为了可靠性和性能，并且引入了一些新的功能，比如可写分支的负载均衡。AUFS的一些实现已经被纳入UnionFS 2.x 版本。同UnionFS类似，它能够将不同类型的文件系统透明地层叠在一起，实现一个高效的分层文件系统。说白了AUFS就是能将不同的目录挂载到某一目录下，并将各个源目录下的内容联合到目标目录下，这里每个源目录对应aufsAUFS一层，用户在目标目录读写时，感觉不到此目录是联合而来的。早期的Docker就是使用了AUFS作为第一种存储驱动。从Linux对AUFS支持版本可以看出，最新的内核已经支持到了AUFS 5， http://aufs.sourceforge.net/ Docker如何使用AUFS首先得看一下Docker的镜像层，每一个Docker image 都是由一系列只读镜像层(image layer)组成的。镜像层的内容都存储在 Docker hosts filesystem 的/var/lib/docker/aufs/diff 目录下。而/var/lib/docker/aufs/layers 目录，则存储着镜像层如何堆叠这些镜像的元数据，可以近似看成是镜像层之间的依赖关系！ 我在我的Ubuntu16.04上安装了Docker1.11.2，执行ls /var/lib/docker/aufs/diff/ 发现无任何文件 拉取镜像后，可以看到在docker pull中的结果显示ubuntu:l5.04 镜像一共有4个layer ，在执行命令的结果中也有4个对应的存储文件目录。自从Docker1.10 之后，diff目录下的存储镜像layer文件夹不再与镜像ID相同。 所以说，/var/lib/docker/aufs/layers 目录，则存储着镜像层如何堆叠这些镜像的元数据，可以近似看成是镜像层之间的依赖关系！ 接下来，以ubuntu:l5.04 镜像为基础镜像，创建一个名为changed-ubuntu 的镜像。这个镜像只是在镜像的/tmp文件夹中添加一个写了”Hello world”的文件。可以使用下面的DockerFile来实现。 使用如下命令，可以清楚地查看到changed-ubuntu 镜像使用了哪些image layer 从输出中可以看到83f8696997cf image layer 位于最上层，只有12B 的大小，由/bin/sh -c echo &quot;Hello World&quot; &gt; /tmp/newfile命令创建 也就是说，changed-ubuntu镜像只占用了12B的磁盘空间，这也证明了AUFS是如何高效使用磁盘空间的。而下面的四层image layer ，则是共享地构成ubuntu:15.04镜像的4个image layer 。”missing” 标记的layer，是自Docker1.10 之后，一个镜像的image layer 的image history 数据都存储在一个文件中导致的，这是Docker官方认为的正常行为，可以看到新的镜像层需要之前的四层来构建： 进一步探查/var/lib/docker/aufs/diff/f973657917d003daa420129cc1f1d802e2d31e34429d1ded178fa988d94f9182 文件夹，发现其中存储了一个/tmp/newfile文件，文件中只有一行”Hello world”，至此我们完整地分析出了image layer和AUFS是如何通过共享文件和文件夹来实现镜像存储的。 container layer与AUFSDocker使用AUFS的COW技术来实现image layer共享和减少磁盘空间占用。COW意味着一旦某个文件只有很小的部分有改动， AUFS也需要复制整个文件。这种设计会对容器性能产生一定的影响，尤其是在待复制的文件很大，或者位于很多image layer下方，又或者AUFS需要深度搜索目录结构树的时候。不过也不用过度担心，对于一个容器而言，每个image layer最多只需要复制一次。后续的改动都会在第一次拷贝的container layer上进行。 启动一个container的时候，Docker 会为其创建一个read-only的init layer，用来存储与这个容器内环境相关的内容：Docker还会为其创建一个read-write的layer来执行所有写操作。 container layer的mount目录也是/var/lib/docker/aufs/mnt。container的元数据和配置文件都存放在/var/lib/docker/containers/&lt;container-id＞目录中。container 的read-write layer存储在/var/lib/docker/aufs/diff/目录下。即使容器停止，这个可读写层仍然存在，因而重启容器不会丢失数据，只有当一个容器被删除的时候，这个可读写层才会一起删除。 接下来，仍然用实验来证明上面的结论。首先查询到现有的容器数目为0 ，而且在/var/lib/docker/containers目录下也没有查到任何数据。最后，查看一下系统的aufs mount 情况，发现只有一个config文件。 如上图，我们启动了一个changed-ubuntu容器，查看/var/lib/docker/aufs/diff目录发现，下面多了2个文件夹，fe0d1e8647d74a0f0a6eaf381ba135502be61e8eb323f31767e48fda4042fb01-init是Docker为容器创建的read-only的init layer，而fe0d1e8647d74a0f0a6eaf381ba135502be61e8eb323f31767e48fda4042fb01则是Docker为容器创建的read-write layer 在/var/lib/docker/containers目录下多了一个与containerid 相同的文件夹，存放着容器的metadata和config文件 接下来从系统AUFS来看mount的情况，在/sys/fs/aufs/目录下多了一个si_dca9c25b7188665e文件夹，执行如下命令 可以清楚地看到这就是刚刚创建的容器的layer权限， 只有最上面的fe0d1e8647d74a0f0a6eaf381ba135502be61e8eb323f31767e48fda4042fb01 layer是read-write权限 说一下AUFS如何为container删除一个文件。如果要删除file1时，AUFS会在container的read-write层生成一个.wh.file1的文件来隐藏所有read-only层的file1文件。至此，我们己清楚地描述和验证了Docker是如何使用AUFS来管理container layer的","updated":"2020-04-14T03:19:57.323Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"},{"name":"文件系统","slug":"文件系统","permalink":"https://zouchanglin.cn/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"}]},{"title":"Cgroups在Docker中的应用","date":"2020-02-22T04:20:19.000Z","path":"2020/02/22/Cgroups在Docker中的应用/","text":"之前几篇文章讲述了Linux的Cgroups技术，《Linux-Cgroups的资源控制》，主要是通过Cgroups实现了对进程的资源限制，那么在Docker中是如何运用Cgroups的呢？我们知道Docker是通过Cgroups去做的容器的资源限制和监控，我们下面就以一个实际的容器实例来看下Docker是如何配置Cgroups的：docker run -m 设置内存限制 可以看到Docker通过为每个容器创建Cgroup并通过Cgroup去配置的资源限制和资源监控。 现在通过Go语言来实现一个可以限制资源的容器，配合之前的Namespace使用嘛 通过对Cgroups虚拟文件系统的配置，我们让容器中的把stress进程的内存占用限制到了100m。","updated":"2020-03-13T03:06:29.640Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"}]},{"title":"Apache Freemarker的一个小坑","date":"2020-02-21T03:38:53.000Z","path":"2020/02/21/Apache-Freemarker的一个小坑/","text":"前段时间用到了Apache Freemarker来做前端页面，发现一个问题，就是JavaBean的Long类型和Integer类型向模板中注入的时候，只要是注入input类型，那么大于999的数字则不会被显示，这个问题我也问了Apache官方的论坛：https://issues.apache.org/jira/browse/FREEMARKER-132 ,果然是官方站点呀，反馈还是很快的，系统还自动发了邮件！ 给出的回复是在需要加上一个 ? c 于是我试了一下： 代码是这样的，非常简单的一个JavaBean 1234567package xpu.tim.freemarker_bug;@Datapublic class Person &#123; private String name; private Long age;&#125; 一个简单的Controller 12345678910111213141516171819package xpu.tim.freemarker_bug;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.servlet.ModelAndView;import java.util.Map;@Controller@RequestMapping(\"/\")public class IndexController &#123; @GetMapping public ModelAndView index(Map&lt;String, Object&gt; map)&#123; Person person = new Person(\"AAA\", 1001L); map.put(\"person\", person); return new ModelAndView(\"index\", map); &#125;&#125; 一个index.ftl 12&lt;input value=\"$&#123;person.age&#125;\" type=\"number\"&gt;&lt;input value=\"$&#123;person.name&#125;\" type=\"text\"&gt; 在person.age &lt;= 999 的时候一切正常，person.age &gt; 999了就无法在输入框显示，通过查看页面源代码发现如果数字是1000的话：源代码内容是1,000 而不是1000，根据解决办法修改index.ftl为： 12&lt;input value=\"$&#123;person.age?c&#125;\" type=\"number\"&gt;&lt;input value=\"$&#123;person.name&#125;\" type=\"text\"&gt; 果然修改后一切正常，最后查阅官方文档得到如下结论：https://freemarker.apache.org/docs/ref_builtins_number.html#ref_builtin_c​ This built-in converts a number to string for a “computer language” as opposed to for human audience. That is, it formats with the rules that programming languages used to use, which is independent of all the locale and number format settings of FreeMarker. It always uses dot as decimal separator, and it never uses grouping separators (like 3,000,000), nor exponential form (like 5E20), nor superfluous leading or trailing 0-s (like 03 or 1.0), nor + sign (like +1). It will print at most 16 digits after the decimal dot, and thus numbers whose absolute value is less than 1E-16 will be shown as 0. This built-in is crucial because be default (like with ${x}) numbers are converted to strings with the locale (language, country) specific number formatting, which is for human readers (like 3000000 is possibly printed as 3,000,000). When the number is printed not for human audience (e.g., for a database record ID used as the part of an URL, or as invisible field value in a HTML form, or for printing CSS/JavaScript numerical literals) this built-in must be used to print the number (i.e., use ${x?c} instead of ${x}), or else the output will be possibly broken depending on the current number formatting settings and locale (like the decimal point is not dot, but comma in many countries) and the value of the number (like big numbers are possibly “damaged” by grouping separators).If the incompatible_improvements FreeMarker configuration setting is set to 2.3.24 or higher (also if it’s set to 2.3.20 or higher and you are outside a string literal), this built-in will return “INF”, “-INF” and “NaN” for positive/negative infinity and IEEE floating point Not-a-Number, respectively. These are the XML Schema compatible representations of these special values. (Earlier it has returned what java.text.DecimalFormat did with US locale, none of which is understood by any (common) computer language.) 即在默认情况下，比如${x}数字被转换成具有地区语言、国家特定数字格式的字符串，这是针对我们人来说的，比如3000000可能打印为 3,000,000 ，但是显示成这样input标签就无法解析，所以还是需要显示成 3000000 就必须使用 ${x?c} 这种形式！","updated":"2020-03-13T03:06:29.630Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://zouchanglin.cn/tags/JavaWeb/"},{"name":"模板引擎","slug":"模板引擎","permalink":"https://zouchanglin.cn/tags/%E6%A8%A1%E6%9D%BF%E5%BC%95%E6%93%8E/"},{"name":"前端","slug":"前端","permalink":"https://zouchanglin.cn/tags/%E5%89%8D%E7%AB%AF/"}]},{"title":"Linux Cgroups的资源控制","date":"2020-02-20T12:03:27.000Z","path":"2020/02/20/Linux-Cgroups的资源控制/","text":"今天主要是说说Linux的Cgroup技术，之前介绍的是构建Linux容器的namespace技术，它能够很好的帮助进程隔离出自己单独的空间，但Docker又是怎么限制每个空间的大小，保证他们不会互相争抢呢？这就要用到Linux的Cgroups技术。 Linux Cgroups(Control Groups) 提供了对一组进程及将来的子进程的资源的限制 ，控制和统计的能力，这些资源包括CPU，内存，存储，网络等。通过Cgroups，可以方便的限制某个进程的资源占用，并且可以实时的监控进程的监控和统计信息。 Cgroups三个组件1、cgroupcgroup 是对进程分组管理的一种机制，一个cgroup包含一组进程，并可以在这个cgroup上增加Linux subsystem的各种参数的配置，将一组进程和一组subsystem的系统参数关联起来。亦即，管理“组进程”并关联参数 2、subsystemsubsystem 是一组资源控制的模块，一般包含有： blkio 设置对块设备（比如硬盘）的输入输出的访问控制（block/io） cpu 设置cgroup中的进程的CPU被调度的策略 cpuacct 可以统计cgroup中的进程的CPU占用(cpu account) cpuset 在多核机器上设置cgroup中的进程可以使用的CPU和内存（此处内存仅使用于NUMA架构） devices 控制cgroup中进程对设备的访问 freezer 用于挂起(suspends)和恢复(resumes) cgroup中的进程 memory 用于控制cgroup中进程的内存占用 net_cls 用于将cgroup中进程产生的网络包分类(classify)，以便Linux的tc(traffic controller) (net_classify) 可以根据分类(classid)区分出来自某个cgroup的包并做限流或监控。 net_prio 设置cgroup中进程产生的网络流量的优先级 ns 这个subsystem比较特殊，它的作用是cgroup中进程在新的namespace fork新进程(NEWNS)时，创建出一个新的cgroup，这个cgroup包含新的namespace中进程。 每个subsystem会关联到定义了相应限制的cgroup上，并对这个cgroup中的进程做相应的限制和控制，这些subsystem是逐步合并到内核中的 可以通过安装cgroup的命令行工具(apt-get install cgroup-bin)看到当前的内核支持哪些subsystem，然后通过lssubsys看到kernel支持的subsystem。 3、hierarchyhierarchy 的功能是把一组cgroup串成一个树状的结构，一个这样的树便是一个hierarchy，通过这种树状的结构，Cgroups可以做到继承。比如我的系统对一组定时的任务进程通过cgroup1限制了CPU的使用率，然后其中有一个定时dump日志的进程还需要限制磁盘IO，为了避免限制了影响到其他进程，就可以创建cgroup2继承于cgroup1并限制磁盘的IO，这样cgroup2便继承了cgroup1中的CPU的限制，并且又增加了磁盘IO的限制而不影响到cgroup1中的其他进程。 三个组件之间的关系Cgroups的是靠这三个组件的相互协作实现的，那么三个组件之间的关系是怎样的呢？ 系统在创建新的hierarchy之后，系统中所有的进程都会加入到这个hierarchy的根cgroup节点中，这个cgroup根节点是hierarchy默认创建，后面在这个hierarchy中创建cgroup都是这个根cgroup节点的子节点。 一个subsystem只能附加到一个hierarchy上面 一个hierarchy可以附加多个subsystem 一个进程可以作为多个cgroup的成员，但是这些cgroup必须是在不同的hierarchy中 一个进程fork出子进程的时候，子进程是和父进程在同一个cgroup中的，也可以根据需要将其移动到其他的cgroup中 其实上图很能说明问题，Cgroup包括了一组进程，subsystem(子系统)负责控制资源，资源控制包括CPU调度策略、块设备访问控制、内存占用、设备访问、进程的挂起和恢复等，很多Cgroup组成树状结构形成hierarchy，subsystem不但可以控制一个Cgroup，也可以控制多个Cgroup，Cgroup与Cgroup之间形树状，可以有继承关系，进程不但可以属于一个Cgroup、还可以属于多个Cgroup，但是肯定不是在同一个hierarchy中，Cgroup中的进程可以移动 调用Kernel配置Cgroups之前了解到Cgroups中的hierarchy是一种树状的组织结构，Kernel为了让对Cgroups的配置更直观，Cgroups通过一个虚拟的树状文件系统去做配置的，通过层级的目录虚拟出cgroup树，下面我们就以一个配置的例子来了解下如何操作Cgroups。 首先，我们要创建并挂载一个hierarchy(cgroup树)： 12root@ubuntu:~# mkdir cgroup-testroot@ubuntu:~# mount -t cgroup -o none,name=cgroup-test cgroup-test ./cgroup-test 如上图，接下来挂载后我们就可以看到系统在这个目录下生成了一些默认文件 ：cgroup.clone_children cgroup.procs cgroup.sane_behavior notify_on_release release_agent tasks 这些文件就是这个hierarchy中根节点cgroup配置项 ： cgroup.clone_children cpuset的subsystem会读取这个配置文件，如果这个的值是1(默认是0)，子cgroup才会继承父cgroup的cpuset的配置。 cgroup.procs是树中当前节点的cgroup中的进程组ID，现在我们在根节点，这个文件中会有现在系统中所有进程组ID。 notify_on_release和release_agent会一起使用，notify_on_release表示当这个cgroup最后一个进程退出的时候是否执行release_agent，release_agent则是一个路径，通常用作进程退出之后自动清理掉不再使用的cgroup。 tasks也是表示该cgroup下面的进程ID，如果把一个进程ID写到tasks文件中，便会将这个进程加入到这个cgroup中。 如上图，接下来我们创建在刚才创建的hierarchy的根cgroup中扩展出两个子cgroup，一个是cgroup-1，一个是cgroup-2，可以看到，在一个cgroup 的目录下创建文件夹时， Kernel 会把文件夹标记为这个cgroup的子cgroup ，它们会继承父cgroup 的属性。 接下来尝试一下在cgroup中添加和移动进程 ： 一个进程在一个Cgroups的hierarchy中只能存在在一个cgroup节点上， 系统的所有进程默认都会在根节点， 可以将进程在cgroup节点间移动，只需要将进程ID写到移动到的cgroup节点的tasks文件中。 可以看出我的当前进程是1449，通过把进程号写入task中的方式已经成功把当前进程，也就是Bash进程加入到了cgroup-test中，又开了一个Bash，把Bash进程加入到cgroup-1中： 接下来看看如何通过subsystem限制cgroup中进程的资源 ？ 上面我们创建hierarchy的时候，但这个hierarchy并没有关联到任何subsystem，所以没办法通过那个hierarchy中的cgroup限制进程的资源占用，其实系统默认就已经把每个subsystem创建了一个默认的hierarchy，比如memory的hierarchy: 可以看到，在/sys/fs/cgroup/memory目录便是挂在了memory subsystem的hierarchy。下面我们就通过在这个hierarchy中创建cgroup，限制下占用的进程占用的内存： 首先，我们不做限制启动一个占用内存的stress进程， stress 命令主要用来模拟系统负载较高时的场景，关于stress指令可以看看这篇文章 《Linux stress 命令》 可以看到通过cgroup，我们成功的将stress进程的最大内存占用限制到了100m以内。","updated":"2020-03-13T03:06:29.671Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"}]},{"title":"通过Namespace实现隔离","date":"2020-02-19T07:02:22.000Z","path":"2020/02/19/通过Namespace实现隔离/","text":"在上次的文章《虚拟化的基石——Namespace》 中主要讲述了Namespace的作用，却没有详细的拿代码举证，上次仅仅证明了UTS Namespace，剩下五个命名空间全在这篇文章了，主要是IPC Namespace、PID Namespace、Mount Namespace、User Namespace、Network Namespace的概念的理解以及使用代码证明！ IPC NamespaceIPC Namespace用来隔离System V IPC和POSIX message queues。每一个 IPC Namespace，都有自己的System V IPC和POSIX message queue。所谓System V IPC即系统IPC资源，其实主要是共享内存、信号量（ System V信号量 ）、消息队列；POSIX message queues其实就是随文件系统的消息队列 随进程持续的（Process-Persistent IPC）IPC对象一直存在，直到最后拥有他的进程被关闭为止，典型的IPC有pipes（管道）和FIFOs（先进先出对象） 随内核持续的（Kernel-persistent IPC）IPC对象一直存在直到内核被重启或者对象被显式关闭为止，在Unix中这种对象有System v 消息队列，信号量，共享内存。（注意Posix消息队列，信号量和共享内存被要求为至少是内核持续的，但是也有可能是文件持续的，这样看系统的具体实现）。 随文件系统持续的（FileSystem-persistent IPC）除非IPC对象被显式删除，否则IPC对象会一直保持（即使内核才重启了也是会留着的）。如果Posix消息队列，信号量，和共享内存都是用内存映射文件的方法，那么这些IPC都有着这样的属性。 还是昨天的UTS Namespace代码稍做修改 现在go run IPNamespace.go 通过以上实验，可以发现在新创建的Namespace里，看不到宿主机上已经创建的message queue，说明IPC Namespace创建成功，IPC 已经被隔离。 PID NamespacePID Namespace是用来隔离进程ID的。同样一个进程在不同的PID Namespace里可以拥有不同的PID。这样就可以理解，在docker container里面，使用ps -ef经常会发现，在容器内，前台运行的那个进程PID是1，但是在容器外，使用ps -ef会发现同样的进程却有不同的PID，这就是PID Namespace做的事情。 Mount NamespaceMount Namespace用来隔离各个进程看到的挂载点视图。在不同Namespace的进程中到的文件系统层次是不一样的。在Mount Namespace中调用mount()和umount()仅仅只会影当前Namespace内的文件系统，而对全局的文件系统是没有影响的。 看到这里，也许就会想到chroot()。它也是将某一个子目录变成根节点。但是，MountNamespace不仅能实现这个功能，而且能以更加灵活和安全的方式实现。MountNamespace是Linux第一个实现的Namespace类型，因此，它的系统调用参是NEWNS (New Namespace的缩写)。当时人们貌似没有意识到，以后还会有很多类型Namespace加入Linux大家庭。代码做了一点改动，增加了NEWNS标识，如下。 运行代码，然后查看一下／proc 的文件内容。proc 是一个文件系统，提供额外的机制，可以通过内核和内核模块将信息发送给进程。这里的／proc 还是宿主机的， 所以看到里面会比较乱，下面将／proc mount 到我们自己的Namespace 下面来。 Mount Namespace 中的mount 和外部空间是隔离的， mount 操作并没有影响到外部。Docker volume 也是利用了这个特性。 User NamespaceUser Namespace主要是隔离用户的用户组ID。也就是说，一个进程的UserID和GroupID在User Namespace内外可以是不同的。比较常用的是，在宿主机上以一个非root用户运行创建一个User Namespace，然后在User Namespace里面却映射成root用户。 这意味着，这个进程在User Namespace里面有root权限，但是在User Namespace外面却没有root的权限。从LinuxKernel3.8开始，非root进程也可以创建UserNamespace，并且此用户在Namespace里面可以被映射成root，且在Namespace内有root权限。 Network NamespaceNetwork Namespace是用来隔离网络设备、IP 地址端口等网络栈的Namespace。Network Namespace可以让每个容器拥有自己独立的(虚拟的)网络设备，而且容器内的应用可以绑定到自己的端口，每个Namespace内的端口都不会互相冲突。在宿主机上搭建网桥后，就能很方便地实现容器之间的通信，而且不同容器上的应用可以使用相同的端口。 在Namespace 里面什么网络设备都没有。这样就能断定Network Namespace 与宿主机之间的网络是处于隔离状态了，是不是离Docker的实现更进一步了呢？","updated":"2020-03-13T03:06:29.775Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"}]},{"title":"虚拟化的基石——Namespace","date":"2020-02-18T14:44:21.000Z","path":"2020/02/18/虚拟化的基石——Namespace/","text":"每每被人问到：“Docker技术到底是怎么实现的呢？”我只能粗粗浅浅地说：“Docker是使用Linux Kernel的Namespace 和 Cgroups实现的一种容器技术。”那么，什么是Namespace，什么是Cgroups，Docker是怎么使用它们的，容器到底是怎么一步步被创建出来的？问到这些，我就会支支吾吾地不知所以。由此可见，了解容器技术的底层技术，然后明白它们是如何工作的，尤为重要，这些才是整个容器技术的基石，掌握了这些基石才能更加容易地向上攀登。 那今天就先看看Namespace吧！ Linux NamespaceLinux Namespace 是Kernel 的一个功能，它可以隔离一系列的系统资源，比如PIO ( ProcessID ）、User ID 、Network 等。一般看到这里，很多人会想到一个命令chroot ，就像chroot 允许把当前目录变成根目录一样（被隔离开来的） , Namespace 也可以在一些资源上，将进程隔离起来，这些资源包括进程树、网络接口、挂载点等。 使用Namespace，就可以做到UID级别的隔离，也就是说，可以以UID为n的用户，虚拟化出来一个Namespace，在这个Namespace里面，用户是具有root权限的。但是，在真实的物理机器上，他还是那个以UID为n的用户，这样就解决了用户之间隔离的问题。当然这只是Namespace其中的一个简单功能。 当前Linux一共实现六种不同类型的namespace : Mount namespaces Linux 2.4.19 文件系统挂接点将一个文件系统的顶层目录挂到另一个文件系统的子目录上，使它们成为一个整体，称为挂载。把该子目录称为挂载点。 Mount namespace用来隔离文件系统的挂载点, 使得不同的mount namespace拥有自己独立的挂载点信息，不同的namespace之间不会相互影响，这对于构建用户或者容器自己的文件系统目录非常有用。 UTS namespaces Linux 2.6.19 nodename 和 domainnameUTS，UNIX Time-sharing System namespace提供了主机名和域名的隔离。能够使得子进程有独立的主机名和域名(hostname)，这一特性在Docker容器技术中被用到，使得docker容器在网络上被视作一个独立的节点，而不仅仅是宿主机上的一个进程。 IPC namespaces Linux 2.6.19特定的进程间通信资源，包括System V IPC 和 POSIX message queuesIPC全称 Inter-Process Communication，是Unix/Linux下进程间通信的一种方式，IPC有共享内存、信号量、消息队列等方法。所以，为了隔离，我们也需要把IPC给隔离开来，这样，只有在同一个Namespace下的进程才能相互通信。如果你熟悉IPC的原理的话，你会知道，IPC需要有一个全局的ID，即然是全局的，那么就意味着我们的Namespace需要对这个ID隔离，不能让别的Namespace的进程看到。 PID namespaces Linux 2.6.24进程 ID 数字空间 （process ID number space）PID namespaces用来隔离进程的ID空间，使得不同pid namespace里的进程ID可以重复且相互之间不影响。PID namespace可以嵌套，也就是说有父子关系，在当前namespace里面创建的所有新的namespace都是当前namespace的子namespace。父namespace里面可以看到所有子孙后代namespace里的进程信息，而子namespace里看不到祖先或者兄弟namespace里的进程信息。 Network namespaces始于Linux 2.6.24 完成于 Linux 2.6.29网络相关的系统资源每个容器用有其独立的网络设备，IP 地址，IP 路由表，/proc/net 目录，端口号等等。这也使得一个 host 上多个容器内的同一个应用都绑定到各自容器的 80 端口上。 User namespaces始于 Linux 2.6.23 完成于 Linux 3.8)用户和组 ID 空间User namespace用来隔离user权限相关的Linux资源，包括user IDs and group IDs。这是目前实现的namespace中最复杂的一个，因为user和权限息息相关，而权限又事关容器的安全，所以稍有不慎，就会出安全问题。 在不同的user namespace中，同样一个用户的user ID 和group ID可以不一样，换句话说，一个用户可以在父user namespace中是普通用户，在子user namespace中是超级用户 Namespace的API主要使用如下三个系统调用（可以在这里查询系统接口 http://man7.org/linux/man-pages/dir_all_alphabetic.html ）： 开发环境搭建操作系统：Ubuntu Server 14.04 内核版本：Linux version 4.4.0-142-generic（cat /proc/version命令可以查看） Golang版本：go version go1.7.1 linux/amd64 首先配置好GOATH，我的GOPATH是/go ，目录结构： 其中mydocker是我的工程路径！ UTS Namespace前面也说到过，UTS Namespace提供了主机名和域名的隔离。能够使得子进程有独立的主机名和域名(hostname)，下面我会使用Go来做一个UTS Namespace的例子： UTSNamespace.go: 123456789101112131415161718192021222324package mainimport ( \"log\" \"os\" \"os/exec\" \"syscall\")func main() &#123; cmd := exec.Command(\"sh\") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS, &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil&#123; log.Fatal(err) &#125;&#125; exec.Command (“sh”) 用来指定被fork出来的新进程内的初始命令，默认使用sh来执行。下面就是设置系统调用参数，使用CLONE_NEWUTS这个标识符去创建一个UTS Namespace。Go帮我们封装了对clone()函数的调用,这段代码执行后就会进入到一个sh运行环境中。 接下来运行它：go run UTSNamespace.go，然后打印一下PID 观察进程树，我们可以看到父进程的PID，于是可以通过readlink命令去看看父进程和子进程是否是不在同一个UTS namespace中： 可以看到它们确实不在同一个UTS Namespace中。由于UTS Namespace对hostname做了隔离，所以在这个环境内修改hostname应该不影响外部主机，下面来做一下实验。 可以看到，外部的hostname并没有被内部的修改所影响，由此可了解UTS Namespace的作用。","updated":"2020-03-13T03:06:29.763Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"}]},{"title":"构建可复用模块","date":"2020-02-18T02:27:22.000Z","path":"2020/02/18/构建可复用的模块/","text":"Golang的工程管理一直是初学者所不能很好的理解的事物，本篇主要讲述了在Golang中如何进行工程管理，如何构建可复用模块，从GitHub获取Go的包是如何操作的，以及Golang的包管理工具的基本使用，这样再进行项目开发的时候就可以模块化的管理自己的工程，首先还得从Package说起！ package1、基本复用模块单元，以首字母大写来表明可被包外代码访问2、代码的package可以和所在的目录不一致3、同一目录里的Go代码的package要保持一致4、通过go get来获取远程依赖 go get -u 强制从网络更新远程依赖 5、注意代码在Github上的组织形式，以适应go get 直接以代码路径开始，不要有src 注意GOPATH的配置 init函数1、在main被执行前，所有依赖的package的init方法都会被执行2、不同包的init函数按照包导入的依赖关系决定执行顺序3、每个包可以有多个init 函数4、包的每个源文件也可以有多个init函数，这点比较特殊 远程获取包就拿这个concurrentMap来说吧，这是一个Go语言的concurrentMap实现， https://github.com/easierway/concurrent_map 现在假设我们需要引入这个包，（在配置好GOPATH的情况下）直接使用命令搞定： 命令执行完毕就会在GOPATH的目录下直接把go源文件拉下来，接下来导入就好了 所以我们在提交自己的开源项目的时候，别把src目录放进去，只要把代码的路径放在相对根目录下就好了，比如： 依赖管理Go未解决的依赖问题：1、同一环境下，不同项目使用同一包的不同版本2、无法管理对包的特定版本的依赖 vender路径 随着Go 1.5 release版本的发布，vendor目录被添加到除了GOPATH和GOROOT之外的依赖目录查找的解决方案。在Go1.6之前，你需要手动的设置环境变量 查找依赖包路径的解决方案如下:1、当前包下的vendor目录2、向上级目录查找，直到找到src下的vendor目录3、在GOPATH下面查找依赖包4、在GOROOT目录下查找 常用的依赖管理工具： 接下来就简单演示一下glide在Windows下的使用： 1、首先安装glide，推荐直接通过go的源码安装 进入GOPATH路径：比如我的GOPATH路径是D:\\mycode\\practic_code\\go_learning\\ 1D:\\mycode\\practic_code\\go_learning&gt;go get -u github.com/Masterminds/glide 编译glide 1D:\\mycode\\practic_code\\go_learning\\src\\github.com\\Masterminds\\glide&gt;go build glide.go 如何配置正确glide.exe应该是在bin目录中，但是执行glide install 命令的时候有错误，需要更改 github.com\\Masterminds\\glide\\path\\winbug.go 好了，接下来重新编译出glide.exe，如果没在bin中，把它放到bin目录就好了 2、glide init命令 假设我的一个工程依赖与某个远程package 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647D:\\mycode\\practic_code\\go_learning\\src\\ch15\\remote&gt;glide init[INFO] Generating a YAML configuration file and guessing the dependencies[INFO] Attempting to import from other package managers (use --skip-import to skip)[INFO] Scanning code to look for dependencies[INFO] --&gt; Found test reference to github.com\\easierway\\concurrent_map[INFO] Writing configuration file (glide.yaml)[INFO] Would you like Glide to help you find ways to improve your glide.yaml configuration?[INFO] If you want to revisit this step you can use the config-wizard command at any time.[INFO] Yes (Y) or No (N)?Y[INFO] Looking for dependencies to make suggestions on[INFO] --&gt; Scanning for dependencies not using version ranges[INFO] --&gt; Scanning for dependencies using commit ids[INFO] Gathering information on each dependency[INFO] --&gt; This may take a moment. Especially on a codebase with many dependencies[INFO] --&gt; Gathering release information for dependencies[INFO] --&gt; Looking for dependency imports where versions are commit ids[INFO] Here are some suggestions...[INFO] The package github.com/easierway/concurrent_map appears to have Semantic Version releases (http://semver.org).[INFO] The latest release is 0.9.1. You are currently not using a release. Would you like[INFO] to use this release? Yes (Y) or No (N)Y[INFO] Would you like to remember the previous decision and apply it to future[INFO] dependencies? Yes (Y) or No (N)Y[INFO] Updating github.com/easierway/concurrent_map to use the release 0.9.1 instead of no release[INFO] The package github.com/easierway/concurrent_map appears to use semantic versions (http://semver.org).[INFO] Would you like to track the latest minor or patch releases (major.minor.patch)?[INFO] The choices are:[INFO] - Tracking minor version releases would use '&gt;= 0.9.1, &lt; 1.0.0' ('^0.9.1')[INFO] - Tracking patch version releases would use '&gt;= 0.9.1, &lt; 0.10.0' ('~0.9.1')[INFO] - Skip using ranges[INFO] For more information on Glide versions and ranges see https://glide.sh/docs/versions[INFO] Minor (M), Patch (P), or Skip Ranges (S)?S[INFO] Would you like to remember the previous decision and apply it to future[INFO] dependencies? Yes (Y) or No (N)Y[INFO] Configuration changes have been made. Would you like to write these[INFO] changes to your configuration file? Yes (Y) or No (N)Y[INFO] Writing updates to configuration file (glide.yaml)[INFO] You can now edit the glide.yaml file.:[INFO] --&gt; For more information on versions and ranges see https://glide.sh/docs/versions/[INFO] --&gt; For details on additional metadata see https://glide.sh/docs/glide.yaml/D:\\mycode\\practic_code\\go_learning\\src\\ch15\\remote&gt; 提示的意思其实很清楚，按照它的提示一步一步来就好了，会在目录下生成glide.yaml，就和Maven一样的： 12345package: ch15/remoteimport: []testImport:- package: github.com/easierway/concurrent_map version: 0.9.1 3、glide install命令 执行完毕后，发现依赖被弄到了vender文件夹底下，所以与其说是导包，不如说是直接把依赖的go文件下载下来了，放到vender目录下，这就是glide的管理方式：","updated":"2020-03-13T03:06:29.745Z","categories":[{"name":"工程构建","slug":"工程构建","permalink":"https://zouchanglin.cn/categories/%E5%B7%A5%E7%A8%8B%E6%9E%84%E5%BB%BA/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://zouchanglin.cn/tags/Golang/"}]},{"title":"在Golang中处理异常","date":"2020-02-17T12:04:22.000Z","path":"2020/02/17/在Golang中处理异常/","text":"通常我们需要编写好的错误处理方式，在了避免某些程序员滥用异常，于是Go这里直接把异常这一块给砍掉了，最终还是通过返回值来判断程序的异常情况，毕竟Go可是支持多返回值的语言，比如atoi、itoa等函数，就不能忽略它的第二个返回值，因为第二个返回值代表了转换是否成功！不过Golang还是提供了一些错误处理机制的 Go的错误机制1、没有异常机制 2、error类型实现了error接口 3、可以通过errors.New来快速创建错误实例 12345type error interface&#123; Error() string&#125;error.New(\"n must be in range[0, 1]\") 但是这样做只是给开发者标识，Code怎么认识呢？难道去string里判断吗？当然不是，可以预定义一些错误： 处理错误最佳实践1、预定义错误，code里判断 2、及早失败，避免嵌套 panic与recoverpanic用于不可恢复的错误，类似于Java的Error panic退出前会执行defer指定的内容 panic和os.Exit os.Exit退出时不会调用defer指定的函数 os.Exit退出时不会输出当前调用栈信息 接下来看看recover 在Java中如果你不知道要抛出什么错误，于是乎很多人直接来个Catch Throwable，作为一个异常无Fack说，C++中是直接catch…作为一个异常无Fack说 在Go语言中可以通过defer定义的函数去执行一些错误恢复的行为 很多人容易在defer处理错误时把recover获得的错误对象打印到日志文件中，这是一种非常危险的做法，一定要当心recover在做的事情，因为recover的时候并不去检测到底发生了什么错误，而是直接忽略了这个错误，那么如果这个时候系统的核心资源消耗完了，如果我们强制性的恢复的话系统仍然是不能正常工作的，还会导致我们的健康检查程序没办法检查出当前系统的问题，因为很多的health check只是检查进程在还是不在，因为我们的进程是在的，所以会导致僵尸服务进程，虽然活着但是无法正常提供服务。于是我们可以采用恢复设计模式中的一种叫做Let it Crash ，health check进程就会帮我们重启这个服务","updated":"2020-03-13T03:06:29.730Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://zouchanglin.cn/tags/Golang/"},{"name":"异常处理","slug":"异常处理","permalink":"https://zouchanglin.cn/tags/%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"}]},{"title":"实践Go的面向对象","date":"2020-02-17T09:22:21.000Z","path":"2020/02/17/实践Go的面向对象/","text":"Go是一种面向对象的语言吗？ 关于Go是一种面向对象的语言吗这种问题，官方的解释是难说，说不准，这回答就有意思了！在我看来尽管Go具有类型和方法，并允许使用面向对象的编程风格，但没有类型层次结构。 Go的接口完全提供了一种不同的方法，可以将类型嵌入其他类型，达到继承的效果，其实又完全不同于继承，因为Go其中一个特点是可以为任何类型的数据定义方法，所以再讨论Go是不是面向对象的语言的时候还是可以先体验体验Go的Functional Programming Functional Programming示例使用Go的 Functional Programming 实现一个装饰器模式，因为在Go语言中，方法也是一种类型，所以函数的返回值类型可以是一个方法，参数也可以是一个方法，所以接下来我们可以使用Go语言的这种特性实现一个装饰器模式： 上面的例子是一个记录函数运行时间的例子，把一个函数对象做了包装，然后在函数开始的时候开始计时，结束之后打印过了多少秒，在调用的时候是timeSpent，把slowFunc当参数穿进去的，返回包装之后的函数对象，这和函数指针是类似的 Golang的面向对象 属性的定义 行为的定义 接下来看这样一段代码 可以一看到，一个是传的指针、一个传值（传值存在一个Copy的过程），导致对象的属性地址不同： Golang的相关接口或者说是定义交互协议，因为接口就是对象之间的一种交互协议 看看下面的代码 由此可见，Go语言与其他编程语言的主要差别在于： 接口为非入侵性，实现不依赖于接口定义所以接口的定义可以包含在接口使用者包内 Dock Type式接口实现： Golang接口变量 如上面的例子，Coder是一个接口，GoProgrammer是实现，prog就是实现类的对象，初始化之后有两部分，一部分是类型，就是实现类的类型，第二部分是数据，就是GoProgrammer的一个实例。 扩展还是复用？扩展和复用一直是个很有争议性的话题，是继承呢？还是复用呢？ 其实Go是不支持继承的，但是可以看成是扩展，比如下面的例子： 多态的特性实现之前使用使用Functional Programming实现了一个简单版本的多态，见《Golang函数相关》 的简单多态的实现，现在是基于Golang的接口特性实现一个多态： 空接口与断言1、空接口可以表示任意类型 2、通过断言来将空接口转换为指定类型 1v, ok := p.(int) //ok=true 时为转换成功 可以看成是Java的Object类型，怎么用的呢？看下面这个示例 发现没有？是不是和C++的类型萃取很像？ Go接口的最佳实践","updated":"2020-03-13T03:06:29.735Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://zouchanglin.cn/tags/Golang/"},{"name":"面向对象","slug":"面向对象","permalink":"https://zouchanglin.cn/tags/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"}]},{"title":"Golang值得注意的点","date":"2020-02-16T08:52:02.000Z","path":"2020/02/16/Golang值得注意的点/","text":"记录了一下最近学习golang语言遗忘的和模糊的知识点，主要涵盖主函数退出返回值、如何编写测试程序、如何定义常量、golang的类型转换、golang中的预定义值、指针与其他语言的差异（主要是C语言），以及运算符等等！ 退出返回值Go中main函数不支持任何返回值，通过os.Exit来返回状态 1234567891011package mainimport ( \"fmt\" \"os\")func main() &#123; fmt.Println(\"Hello world\") os.Exit(0)&#125; 编写测试程序1、源文件以 _test 结尾：xxx_test.go 2、测试方法名以Test开头：func TestXXX(t *testing.T) {…} 比如我的一个first_test.go文件是这样的： 1234567package try_testimport \"testing\"func TestFirstTry(t *testing.T)&#123; t.Log(\"My First Try\")&#125; 常量定义12345678910111213141516171819202122232425package useimport \"testing\"func TestConstUser(t *testing.T) &#123; const( Monday = iota + 1 Tuesday Wednesday ) t.Logf(\"%d %d %d\", Monday, Tuesday, Wednesday)&#125;func TestConstant(t *testing.T) &#123; a := 1 //用位定义的常量标识 const ( Open = 1 &lt;&lt; iota Close Pending ) t.Log(a&amp;Open == Open, a&amp;Close == Close, a&amp;Pending == Pending)&#125; 关于类型转换1、Go语言不允许隐式类型转换 2、别名和原有类型也不能进行隐式类型转换，比如byte类型和uint8之间就不行 12345678910111213141516171819package userTypeimport \"testing\"type MyInt int64func TestUserType(t *testing.T) &#123; var a int = 10 var b int64 = 20 //a = b error //b = a error var c MyInt = 30 //c = b error c = MyInt(b) //OK t.Log(a, b, c)&#125; 预定义值123456func TestIncludeNum(t *testing.T) &#123; t.Log(math.MaxInt64) t.Log(math.MaxFloat64) t.Log(math.MaxUint32) t.Log(math.MaxInt8)&#125; 指针类型1、不支持指针运算 2、string是值类型，其默认初始化为空串，不是nil 123456789101112131415161718package pointimport ( \"testing\")func TestPoint_01(t *testing.T) &#123; a := 10 aPtr := &amp;a //aPtr++ error t.Log(\"a =\",*aPtr)&#125;func TestString_01(t *testing.T) &#123; var str string t.Logf(\"*%s*\", str) //** t.Log(len(str)) //0&#125; 运算符golang没有前置的++、前置的– 用==比较数组，相同维数且含有相同个数元素的数组才可以比较，每个元素都相同的才相等 1234567891011package array_testimport \"testing\"func TestArray(t *testing.T) &#123; a := [...]int&#123;1,2,3,4,5&#125; b := [...]int&#123;1,2,3,4,5&#125; t.Log(a) t.Log(a == b)&#125; 位运算符中有一个非常神奇的运算符 &amp;^ 按位置零 1 &amp;^ 0 – 1 1 &amp;^ 1 – 0 0 &amp;^ 1 – 0 0 &amp;^ 0 – 0 12345678910111213141516func TestBitOpt(t *testing.T) &#123; a := 7 //0111 //用位定义的常量标识 const ( Open = 1 &lt;&lt; iota Close Pending ) //清除Open状态 a = a &amp;^ Open t.Log(a&amp;Open == Open, a&amp;Close == Close, a&amp;Pending == Pending) //false true true //清除Close状态 a = a &amp;^ Close t.Log(a&amp;Open == Open, a&amp;Close == Close, a&amp;Pending == Pending) //false false true&#125;","updated":"2020-03-13T03:06:29.653Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://zouchanglin.cn/tags/Golang/"}]},{"title":"Golang的常用容器","date":"2020-02-15T16:00:00.000Z","path":"2020/02/16/Golang的常用容器/","text":"本篇主要是讲述了数组和切片、Map的初始化方式与基本使用、重点阐述了如何使用Map实现Set、用Map实现工厂模式，以及字符串的使用，字符串常用API、Unicode与UTF8的关系！ 数组和切片1234567891011121314151617181920212223func TestArray01(t *testing.T) &#123; //声明并初始化为默认值 var a [3]int a[0] = 1 b := [3] int &#123;11, 22, 33&#125; c := [2][2] int &#123;&#123;11, 22&#125;, &#123;33, 44&#125;&#125; for _,data := range b&#123; t.Log(data) &#125; for _,data := range c&#123; t.Log(data) &#125;&#125;func TestArraySection(t *testing.T)&#123; arr := [...]int&#123;11, 22, 33, 44&#125; arr_sec := arr[:3] //取前三个元素 t.Log(arr_sec) arr_sec = arr[3:] //取下标为3的后面的所有元素 t.Log(arr_sec)&#125; 其实切片的内部结构和Buffer的结构是差不多的，切片的使用: make函数相当于申请空间、并且可以控制多少空间初始化！ 1234567891011121314151617181920package slice_testimport \"testing\"func TestSlice(t *testing.T) &#123; var s0 []int //切片的声明 t.Log(len(s0), cap(s0)) //0 0 s0 = append(s0, 1) t.Log(len(s0), cap(s0)) //1 1 s1 := []int&#123;1, 2, 3, 4&#125; t.Log(len(s1), cap(s1)) //4 4 s2 := make([]int, 3, 5) t.Log(len(s2), cap(s2)) //4 4 s2 = append(s2, 1) t.Log(len(s2), cap(s2)) //4 4 t.Log(s2[0], s2[1], s2[2], s2[3])&#125; 切片如何扩容呢？ 1234567func TestSliceGrowing(t *testing.T) &#123; s := []int&#123;&#125; for i:=0; i&lt;10; i++&#123; s = append(s, i) t.Log(len(s), cap(s)) &#125;&#125; 可以看出来，每次增长二倍的关系，所以就解释了为什么每次append之后都要接收append的返回值，因为go的内部实现就是内存区域Copy的形式，所以随时可能返回新的内存地址，所以需要接收！ 1234567891011func TestSliceShareMemory(t *testing.T)&#123; year := []string&#123;\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"&#125; Q2 := year[3:6] t.Log(Q2, len(Q2), cap(Q2)) //[Apr May Jun] 3 9 summer := year[5:8] t.Log(summer, len(summer), cap(summer)) //[Jun Jul Aug] 3 7 summer[0] = \"UnKnow\" t.Log(Q2, len(Q2), cap(Q2)) //[Apr May UnKnow] 3 9&#125; 数组和切片的比较：数组不可伸缩、切片可以伸缩；相同维数且相同长度的数组才是可以比较的，切片是不可以比较的，下面的代码会编译不通过： 1234567func TestSliceCompare(t *testing.T)&#123; a := []int&#123;1, 2 , 3, 4&#125; b := []int&#123;1, 2 , 3, 4&#125; if a == b&#123; //error t.Log(\"equal\") &#125;&#125; Map的基本使用 123456789101112131415package map_testimport \"testing\"func TestInitMap(t *testing.T) &#123; m1 := map[string]int&#123;\"one\":1, \"two\":2, \"three\":3&#125; t.Log(m1, len(m1)) //map[one:1 three:3 two:2] 3 m2 := map[int]int&#123;&#125; m2[4] = 16 t.Log(m2, len(m2)) //map[4:16] 1 m3 := make(map[int]int, 10) t.Log(m3, len(m3)) //map[] 0&#125; Map的遍历 123456func TestForEachMap(t *testing.T) &#123; m1 := map[string]int&#123;\"one\":1, \"two\":2, \"three\":3&#125; for k, v := range m1&#123; t.Log(k, \"=\", v) &#125;&#125; Map与工厂模式Map的Value可以是一个方法，因为在Golang中，方法也是一种类型，可以当做参数传递的，与Dock type接口方式一起，可以方便的实现单一方法对象的工厂模式 123456789101112131415161718func TestMapWithFunValue(t *testing.T) &#123; //Key是整型、Value是方法类型 m := map[int] func(op int) int &#123;&#125; m[1] = func(op int) int &#123; return op &#125; m[2] = func(op int) int &#123; return op * op &#125; m[3] = func(op int) int &#123; return op * op * op &#125; t.Log(m[1](2 ), m[2](2), m[3](2)) //2 4 8&#125; Map实现SetGo内置集合没有Set实现，可以map[type]bool 1、元素的唯一性 2、基本操作 添加元素 判断元素是否存在 删除元素 元素个数 1234567891011121314151617181920212223242526272829func TestMapForSet(t *testing.T) &#123; mySet := map[int]bool&#123;&#125; //1、添加元素 mySet[1] = true //n := 3 //3 is not existing n := 1 //1 is existing //2、判断一个元素是否存在 if mySet[n]&#123; t.Logf(\"%d is existing\", n) //1 is existing &#125;else&#123; t.Logf(\"%d is not existing\", n) &#125; //3、获取Set的长度 mySet[2] = true mySet[3] = true t.Log(\"len =\", len(mySet)) //len = 3 //4、从Set删除元素 delete(mySet, 1) n = 1 if mySet[n]&#123; t.Logf(\"%d is existing\", n) &#125;else&#123; t.Logf(\"%d is not existing\", n) //1 is not existing &#125;&#125; 字符串与其他主要编程语言的差异： string 是数据类型，不是引用或指针类型 string 是只读的byte slice, len 函数可以它所包含的byte数 string 的byte数组可以存放任何数据 string实际上就是一个不可变的切片 123456789101112func TestString(t *testing.T) &#123; var s string t.Log(s) //初始化默认值\"\" s = \"hello\" t.Log(len(s)) //s[3] = 'c' error string不可变 s = \"\\xE4\\xB8\\xA5\" //可以存储任意二进制数据 t.Log(s, len(s)) //严 s = \"中国\" t.Log(len(s)) //len求的是字节数&#125; Unicode与UTF81、Unicode是一种字符集（code point） 2、UTF8是Unicode的存储实现（转为字节序列的规则） Unicode为世界上所有字符都分配了一个唯一的数字编号，这个编号范围从 0x000000 到 0x10FFFF(十六进制)，有110多万，每个字符都有一个唯一的Unicode编号，这个编号一般写成16进制， Unicode就相当于一张表，建立了字符与编号之间的联系，而UTF8是具体实现， 在UTF8中编号小的使用的字节就少，编号大的使用的字节就多。使用的字节个数从1到4个不等。 rune函数 在Go当中 string底层是用byte数组存的，并且是不可以改变的。fmt.Println(len(“Go编程”)) 输出结果应该是8因为中文字符是用3个字节存的， len(string(rune(‘编’)))的结果是3 ，如果想要获得我们想要的情况的话，需要先转换为rune切片再使用内置的len函数： fmt.Println(len([]rune(s))) 结果就是4了。 所以用string存储unicode的话，如果有中文，按下标是访问不到的，因为你只能得到一个byte。 要想访问中文的话，还是要用rune切片，这样就能按下标访问。 123456789func TestString(t *testing.T) &#123; s := \"中\" t.Log(len(s)) //3 c := []rune(s) t.Logf(\"中 unicode %x\", c[0]) //中 unicode 4e2d t.Logf(\"中 UTF8 %x\", s) //中 UTF8 e4b8ad&#125; 常用的字符串处理函数：strings包：https://golang.org/pkg/strings strconv包https://golang.org/pkg/strconv 123456789101112131415161718192021func TestStringFunc(t *testing.T) &#123; //切割字符串 s := \"A,B,C\" split := strings.Split(s, \",\") for _, ch := range split&#123; t.Log(ch) &#125; //连接字符串数组的元素 t.Log(strings.Join(split, \"-\")) //字符串与数字的转换 s = strconv.Itoa(250) t.Logf(\"s = %s\", s) num := \"100\" //返回两个值，err代表是否发生错误 if i, err := strconv.Atoi(num); err == nil&#123; t.Log(100 + i) &#125;&#125;","updated":"2020-03-13T03:06:29.657Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://zouchanglin.cn/tags/Golang/"}]},{"title":"IDEA插件推荐","date":"2020-02-14T16:00:00.000Z","path":"2020/02/15/IDEA插件推荐/","text":"Alibaba Java Coding Guidelines写代码怎么能没有规范呢？这款插件是伴随阿里巴巴编码约规出现的，自我感觉是必须要安装的插件 Lombok以前的Java项目中，充斥着太多不友好的代码：各种getter/setter/toString；异常处理；I/O流的关闭操作等等，这些样板代码既没有技术含量，又影响着代码的美观，Lombok必备！不要忘记引入Lombok依赖 GenerateSerialVersionUID需要序列化某个类时候我们只需要实现 Serializable 接口，eclipse点击黄色的警告即可生成相应的序列化ID，那么我们在idea 中没有了黄色警告该如何自动生成这个序列化ID呢，此插件帮你快速生成序列化ID EasyCode只需要配置好数据库，然后根据数据表可以直接生成Controller、Service、Dao层的所有代码，直接可以帮你省掉30%的代码，配合Lombok使用效果更佳 Codota支持智能代码自动提示，该功能可以增强 IDEA 的代码提示功能。支持 JDK 和知名第三方库的函数的使用方法搜索，可以看到其他知名开源项目对该函数的用法。当我们第一次使用某个类，对某个函数不够熟悉时，可以通过该插件搜索相关用法，快速模仿学习。 GsonFormat有时候接口返回的是JSON对象，想要用一个JavaBean去接收，以便于处理后续，此时，可以根据JSON一键生成对JavaBean对象，免去手写的烦恼 jclasslib bytecode viewer虽然有自带的强大的反汇编工具 javap，不需要敲命令，简单直接，在右侧方便和源代码进行对比学习，对我们学习虚拟机指令有极大的帮助 GenerateAllSetter想依次调用 Setter 函数对属性赋值，如果属性较多很容易遗漏或者重复，此插件可以一键调用一个对象的所有的set方法，避免遗漏 CodeGlance类似SublimeText的Mini Map插件，代码侧边小图，还是很实用的 Java Stream Debugger如果你是个经常使用JDK8的Stream特性的开发者，那么进行Debug的时候，可以将 Stream 的操作步骤可视化，非常有助于我们的学习 Maven Helper对于maven项目来说，jar包冲突非常常见，Maven Helper可以帮你快速找出插件冲突的 jar 包exclude掉 VisualVM Launcher有时在本地开发进行压力测试，性能测试之类的监控器，其他场景一般不推荐使用此模式启动，还会启动另外一个Visual vm窗口，这个窗口是JDK bin目录下的JvisualVM","updated":"2020-03-13T03:06:29.660Z","categories":[{"name":"开发工具","slug":"开发工具","permalink":"https://zouchanglin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"IDEA","slug":"IDEA","permalink":"https://zouchanglin.cn/tags/IDEA/"}]},{"title":"NIO网络通信","date":"2020-02-08T23:52:02.000Z","path":"2020/02/09/NIO网络通信/","text":"阻塞式IO与非阻塞式IO传统的IO流都是阻塞式的。也就是说，当一个线程调用read() 或write()时，该线程被阻塞，直到有一些数据被读取或写入，该线程在此期间不能执行其他任务。因此，在完成网络通信进行IO操作时，由于线程会阻塞，所以服务器端必须为每个客户端都提供一个独立的线程进行处理，当服务器端需要处理大量客户端时，性能急剧下降。 Java NIO是非阻塞模式的。当线程从某通道进行读写数据时，若没有数据可用时，该线程可以进行其他任务。线程通常将非阻塞IO的空闲时间用于在其他通道上执行I0操作，所以单独的线程可以管理多个输入和输出通道。因此，NIO可以让服务器端使用一个或有限几个线程来同时处理连接到服务器端的所有客户端。 选择器和通道的关系：通道注册到选择器上，选择器监控通道。当某一个通道上，某一个事件准备就绪时，那么选择器才会将这个通道分配到服务器端一个或多个线程上，再继续运行。比如说当客户端发送一些数据给服务器端，只有当客户端的所有数据都准备就绪时，选择器才会将这个注册的通道分配到服务器端的一个或者多个线程上。如果客户端的线程没有将数据准备就绪时，服务器端的线程可以执行其他任务，就不会阻塞在那里。 原先的传统的阻塞IO模式，相当于你没有手机去等快递，算准了EMS每天中午13:00会到你们公司门口，所以你12:50在那里等着他们来，在这10分钟里你被这件事情阻塞着，什么事情都做不了，真是浪费时间；而NIO的这种通道注册选择器，选择器监控通道，等到数据准备就绪才会占用服务器线程的非阻塞IO方式，更像是带着手机等外卖，我在饿了么注册了一个用户（通道在选择器上注册了），然后定完外卖就忙自己的去了，等到外卖送来之后我接到电话下去取就可以了。如果你学习过Linux内核的epoll多路复用模型，这一点应该不难理解，无非就是IO事件就绪通知！ 选择器 Selector选择器（ Selector） 是 SelectableChannle 对象的多路复用器， Selector 可以同时监控多个 SelectableChannel 的 IO 状况，也就是说，利用 Selector可使一个单独的线程管理多个 Channel。 Selector 是非阻塞 IO 的核心。 SelectableChannle 的结构如下图（注意：FileChannel不是可作为选择器复用的通道！FileChannel不能注册到选择器Selector！FileChannel不能切换到非阻塞模式！FileChannel不是SelectableChannel的子类！） 当调用 register(Selector sel, int ops) 将通道注册选择器时，选择器对通道的监听事件，需要通过第二个参数 ops 指定，可以监听的事件类型，可使用 SelectionKey 的四个常量表示： 若注册时不止监听一个事件，则可以使用位或操作符连接 SelectionKey： 表示 SelectableChannel 和 Selector 之间的注册关系。每次向选择器注册通道时就会选择一个事件(选择键)。 选择键包含两个表示为整数值的操作集。操作集的每一位都表示该键的通道所支持的一类可选择操作。 Selector的常用API 阻塞式IO示例阻塞式网络通信 有反馈的阻塞式通信 非阻塞式IO示例非阻塞式通信，其实与epoll的编程模式很像 上面的功能类似于一个群聊，由于IDEA不支持junit控制台，所以我直接拿eclipse试了一下 UDP使用NIO其实和上面的示例没啥区别， UDP非阻塞网络，NIO中信息的接收端和发送端都是直接用DatagramChannel的open方法生成一个数据报通道，发送的时候指明地址和端口；接收的一方需要先bind一个端口号，监听着。接收方可能监听到多个发送端向其发送数据，为了单个线程能够处理多个客户端发送的数据，并且在发送的过程中不被某个发送端全程阻塞，同样需要用到选择器。 使用选择器仍然需要将通道注册到选择器上，并指明需要监听的事件，即选择键。 一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道。换句话说，如果你对”读就绪”的通道感兴趣，select()方法会返回读事件已经就绪的那些通道。 int select():阻塞到至少有一个通道在你注册的事件上就绪了 int select(long timeout):select(long timeout)和select()一样，除了最长会阻塞timeout毫秒(参数) int selectNow():selectNow()不会阻塞，不管什么通道就绪都立刻返回，此方法执行非阻塞的选择操作。如果自从前一次选择操作后，没有通道变成可选择的，则此方法直接返回零。 管道 PipeJava NIO 管道是2个线程之间的单向数据连接。Pipe有一个source通道和一个sink通道。数据会被写到sink通道，从source通道读取 通过一个管道，在发送数据的源头线程内获取SinkChannel，将数据写入缓冲区，缓冲区切换成读模式，写入SinkChannel；通过该管道在接收数据的线程内获取SourceChannel，然后从SourceChannel读取数据放到缓冲区，然后切换缓冲区为读模式，将数据取出。 NIO2中的几个实用类随着 JDK 7 的发布， Java对NIO进行了极大的扩展，增强了对文件处理和文件系统特性的支持，以至于我们称他们为 NIO.2。因为 NIO 提供的一些功能， NIO已经成为文件处理中越来越重要的部分。 java.nio.file.Path 接口代表一个平台无关的平台路径，描述了目录结构中文件的位置。 java.nio.file.Files 用于操作文件或目录的工具类。 参考资料http://tutorials.jenkov.com/java-nio/index.html http://hg.openjdk.java.net/jdk/jdk/file/d8327f838b88/src/java.base/linux/classes/sun/nio/ch/EPollSelectorImpl.java","updated":"2020-03-13T03:06:29.700Z","categories":[{"name":"高性能网络","slug":"高性能网络","permalink":"https://zouchanglin.cn/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"NIO","slug":"NIO","permalink":"https://zouchanglin.cn/tags/NIO/"}]},{"title":"NIO与通道数据传输","date":"2020-02-08T11:24:22.000Z","path":"2020/02/08/NIO与通道数据传输/","text":"NIO的通道通道( Channel) ：由java.nio.channels包定义的。Channel 表示IO源与目标打开的连接。Channel类似于传统的“流”， 只不过Channel本身不能直接访问数据，Channel只能与Buffer进行交互。 应用程序与磁盘之间的数据写入或者读出，都需要由用户地址空间和内存地址空间之间来回复制数据，内存地址空间中的数据通过操作系统层面的IO接口，完成与磁盘的数据存取。在应用程序调用这些系统IO接口时，由CPU完成一系列调度、任务分配，在早期这些IO接口都是由CPU独立负责。所以当发生大规模读写请求时，CPU的占用率很高。 之后，操作系统为了避免CPU完全被各种IO接口调用占用，引入了DMA，也就是直接存储器。当应用程序对操作系统发出一个读写请求时，会由DMA先向CPU申请权限，申请到权限之后，内存地址空间与磁盘之间的IO操作就全由DMA来负责操作。这样，在读写请求的过程中，CPU不需要再参与，CPU去做其他事情。当然，DMA来独立完成数据在磁盘与内存空间中的复制，需要借助于DMA总线。当频繁发生IO操作时，会导致DMA总线增多，但是当DMA总线过多时，大量的IO操作也会造成总线冲突的问题，即也会影响最终的读写性能。 为了避免DMA总线冲突对性能的影响，后来便有了通道的方式。通道，它是一个完全独立的处理器。CPU是中央处理器，通道本身也是一个处理器，完全独立的处理器，但是也附属于CPU，专门负责IO操作。既然是处理器，通道有自己的IO命令，与CPU无关。它更适用于大型的IO操作，性能更高。 直接存储器DMA有独立总线。但在大量数据面前，可能会存在总线冲突，还是需要CPU来处理。通道是一个独立的处理器，DMA方式还是需要向CPU申请DMA总线的。通道有自己的处理器，适合与大量IO请求的场景，数据传输直接通过通道进行传输，不再需要请求CPU，这样便避免了大量IO导致CPU资源占用过高的问题！ NIO完成文件复制Channel 用于源节点与目标节点的连接，在JavaNIO中负责缓冲区数据的传输，本身不存储数据，所以需要配置缓冲区进行数据传输（铁路配合火车） 一、通道的主要实现类 实现的是java.nio.channels.Channel接口 FileChannel 文件IO SocketChannel TCP的IO ServerSocketChannel TCP的IO DatagramChannel UDP的IO 二、获取通道 1、对支持通道的类提供了 getChannel() 本地IO FileInputStream/FileOutputStream RandomAccessFile 网络IO Socket ServerSocket DatagramChannel 2、JDK1.7中的NIO.2针对各种通道提供了静态方法 3、JDK1.7中的NIO.2的Files工具类的newByteChannel() 1、FileInputStream打开的通道，通过通道+非直接缓冲区完成文件复制 2、这是通过把缓冲区建立在物理内存中的做法，使用了FileChannel的静态方法打开的通道 3、这是使用直接操作缓冲区完成的文件复制 4、这是不建立缓冲区，直接通道数据传输 我试了一下，复制1.3G的压缩包，四种方式分别用时为： 1、通道+非直接缓冲区完成文件复制：5956、 5645（内存有波动，不明显） 2、通道+直接缓冲区完成文件复制：5439、5302（内存有波动，不明显） 3、直接操作缓冲区完成的文件复制：1581、1560、1523（内存波动特别明显，见下图） 4、这是不建立缓冲区，直接通道数据传输：704、685、615 （内存几乎无波动） 得出结论：直接缓冲区肯定比非直接缓冲区快，但是我这次测试效果不是很明显，直接操作内存映射文件速度明显提升，但是也会导致内存突然爆涨（因为是直接操作的内存映射文件），方式4就更狠了，直接通道传输数据，压根与内存没关系，因为没有缓冲区的存在，所以也是最快的！ 分散读取和聚集写入分散读取( Scattering Reads)是指从Channel中读取的数据“分散”到多个Buffer中。 注意：按照缓冲区的顺序，从 Channel 中读取的数据依次将 Buffer 填满。 聚集写入（ Gathering Writes）是指将多个 Buffer 中的数据“聚集”到 Channel中，与分散读取是反的。 注意：按照缓冲区的顺序，写入 position 和 limit 之间的数据到 Channel 。 下面是一个分散读取&amp;聚集写入的示例： 分散读取（Scattering Reads）：将通道中的数据分散到多个缓冲区中 聚集写入（Gathering Writes）：将多个缓冲区中的数据聚集到通道中 字符集Charset只要是需要知道CharBuffer和ByteBuffer 的相互装换即可 FileChannel常用的API","updated":"2020-03-13T03:06:29.699Z","categories":[{"name":"高性能网络","slug":"高性能网络","permalink":"https://zouchanglin.cn/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JVM","slug":"JVM","permalink":"https://zouchanglin.cn/tags/JVM/"},{"name":"NIO","slug":"NIO","permalink":"https://zouchanglin.cn/tags/NIO/"}]},{"title":"NIO与缓冲区","date":"2020-02-08T05:54:00.000Z","path":"2020/02/08/NIO与缓冲区/","text":"Java NIO简介Java NIO (New IO/Non Blocking IO)，官方给的定义是New IO，但是我们也可以当作是Non Blocking IO，即非阻塞式IO，是从Java 1.4版本开始引入的一个新的IO API，可以替代标准的Java IO API。NIO与原来的IO有同样的作用和目的，但是使用的方式完全不同，NIO支 持面向缓冲区的、基于通道的IO操作。NIO将以更加高效的方式进行文件的读写操作。 传统IO与NIO区别 原来的IO是面向数据流的，包括网络流、文件流等等，要想双向传输只能分别建立输入流和输出流：输入输出流你可以把它当作管道，只能单向流动，一边作为输出，一边作为输入，这样我们操作的就是流，面向输入输出流的编程！ 现在看看NIO，NIO相当于建立的不是输入输出流，而是通道，通道只是负责连接，可以把通道看成是铁路，把缓冲区看成是火车，数据看作火车上的乘客，火车既能从应用程序到网络、扫描设备、文件，也能从网络、扫描设备、文件等到应用程序，是双向的IO，所以NIO是面向缓冲区的！ 阻塞式IO与非阻塞IO、选择器等区别主要是针对网络通信而言的 通道和缓冲区Java NIO系统的核心在于：通道(Channel)和缓冲区(Buffer)。通道表示打开到IO设备(例如:文件、套接字)的连接。若需要使用NIO系统，需要获取用于连接IO设备的通道以及用于容纳数据的缓冲区。然后操作缓冲区，对数据进行处理。简而言之，Channel负责传输、Buffer存储 缓冲区原理和操作缓冲区(Buffer)：一个用于特定基本数据类型的容器。由java.nio包定义的，所有缓冲区都是Buffer抽象类的子类。Java NIO中的Buffer主要用于与NIO通道进行交互，数据是从通道读入缓冲区，从缓冲区写入通道中的。 缓冲区中的四个核心属性 capacity：缓冲区中最大存储数据的容量，一旦声明不能改变 limit：界限，表示缓冲区中可以操作数据的大小（limit后面的数据是不能进行读写的） position：位置，表示缓冲区中正在操作数据的位置 mark：标记，表示记录当前position的位置，可以通过reset()恢复到mark的位置 关系：0 &lt;= mark &lt;= position &lt;= limit &lt;= capacity 下面是一段关于Buffer的联系代码，以ByteBuffer为例： 如何分析这一段程序呢，下面这个张图可以很容易的理解这几个属性 上述例子展示了其中position、capacity、limit的作用，没说到mark的作用，mark：标记，表示记录当前position的位置，可以通过reset()恢复到mark的位置 重点就是理解Buffer的四个属性，顺便总结一下API吧 allocate() 开辟缓冲区 put() 存数据 flip() 切换到读模式 get() 取数据 rewind() 重复度数据 clear() 清空缓冲区 mark() 标记此时position值 reset() 恢复到mark() 标记位置 hasRemaining() 判断缓冲区还有没有可以操作的数据 remaining() 缓冲区可操作数据的数量 直接缓冲区和非直接缓冲区字节缓冲区要么是直接的，要么是非直接的。如果为直接字节缓冲区，则Java虚拟机会尽最大努力直接在此缓冲区上执行本机I/0操作。也就是说，在每次调用基础操作系统的一个本机I/O操作之前(或之后)，虚拟机都会尽量避免将缓冲区的内容复制到中间缓冲区中(或从中间缓冲区中复制内容)。 直接字节缓冲区可以通过调用此类的allocateDirect()工厂方法来创建。此方法返回的缓冲区进行分配和取消分配所需成本通常高于非直接缓冲区。直接缓冲区的内容可以驻留在常规的垃圾回收堆之外，因此，它们对应用程序的内存需求量造成的影响可能并不明显。所以，建议将直接缓冲区主要分配给那些易受基础系统的本机I/O操作影响的大型、持久的缓冲区。一般情况下，最好仅在直接缓冲区能在程序性能方面带来明显好处时分配它们。 直接字节缓冲区还可以通过FileChannel的map()方法将文件区域直接映射到内存中来创建。该方法返回MappedByteBuffer。Java 平台的实现有助于通过JNI从本机代码创建直接字节缓冲区。如果以上这些缓冲区中的某个缓冲区实例指的是不可访问的内存区域，则试图访问该区域不会更改该缓冲区的内容，并且将会在访问期间或稍后的某个时间导致抛出不确定的异常。 字节缓冲区是直接缓冲区还是非直接缓冲区可通过调用其isDirect()方法来确定。提供此方法是为了能够在性能关键型代码中执行显式缓冲区管理。 非直接缓冲区 传统IO操作时，JVM在缓存在自己的JVM内存中，自己的JVM内存中的数据需要经过copy到内核缓存中，由操作系统决定何时把数据写入磁盘，但是一旦JVM把数据write到了缓存，操作系统把用户地址空间的缓存数据copy到自己的内核地址空间中，那么JVM时可以随时更改缓存中的数据的，但是程序员也没有操作JVM Gc的能力，这样应用程序IO就变得很低效，要经过copy到内核地址空间这一个步骤 直接缓冲区 直接缓冲区时直接在物理内存中开辟了一个缓冲区，那么这个缓冲区是直接建立在OS的物理内存中的。相当于省略了中的copy的过程，可以提高效率，但是这样做也是有缺点的，比如非常消耗资源，缓冲区直接建立在物理内存中是一个非常巨大的消耗（主要是分配和销毁），而且一旦JVM把数据写入到物理内存缓冲区，那么这里的数据就不归JVM管了，这里的数据何时写到磁盘中，完全是由操作系统控制的。而且这块资源什么时候销毁呢？那就是JVM Gc 发生的时候，但是JVM GC这不是我们能控制的！ 那么什么时候用这一块缓冲区比较好呢？比如有一些很大的数据长时间需要在内存中进行操作的话，就可以使用直接缓冲区！通过查看源码可以看到 123public native long allocateMemory(long var1); //建立直接缓冲区调用的方法return new HeapByteBuffer(capacity, capacity); //非直接缓冲区直接新建数组放在堆中","updated":"2020-03-13T03:06:29.698Z","categories":[{"name":"高性能网络","slug":"高性能网络","permalink":"https://zouchanglin.cn/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JVM","slug":"JVM","permalink":"https://zouchanglin.cn/tags/JVM/"},{"name":"NIO","slug":"NIO","permalink":"https://zouchanglin.cn/tags/NIO/"}]},{"title":"SYN Cookie","date":"2020-02-07T14:20:45.000Z","path":"2020/02/07/SYN Cookie/","text":"SYN泛洪攻击SYN攻击其实就是Server收到Client的SYN，Server向Client发送SYN-ACK之后未收到Client的ACK确认报文， 这样服务器就需要维护海量的半开连接 ，等待客户端的 ACK, 最终导致服务器资源耗尽(sync queue 满)而丢弃新的连接。 Server会不断重发SYN-ACK，Linux服务器默认直到63秒才断开连接！ SYN Cookie其中一种防护方式是SYN Cookie， SYN Cookies 的应用允许服务器当 SYN 队列被填满时避免丢弃连接。相反，服务器会表现得像 SYN 队列扩大了一样。服务器会返回适当的SYN+ACK响应，但会丢弃 SYN 队列条目。如果服务器接收到客户端随后的ACK响应，服务器能够使用编码在 TCP 序号内的信息重构 SYN 队列条目。 Linux内核实现服务器会构造一个 sequence number，根据 TCP 规范，由端点发送的第一个序号可以是由该端点决定的任何值。SYN Cookies 是根据以下规则构造的初始序号： 令t为一个缓慢递增的时间戳（通常为 time() &gt;&gt; 6，提供 64 秒的分辨率）； 令m为服务器会在 SYN 队列条目中存储的最大分段大小（maximum segment size，简称为 MSS）； 令s为一个加密散列函数对服务器和客户端各自的 IP 地址和端口号以及 t 进行运算的结果。返回得到的数值 s 必须是一个24位值，取低24位 为了达到最佳的传输效能，TCP协议在建立连接的时候通常要协商双方的MSS值，这个值TCP协议在实现的时候往往用MTU值代替（需要减去IP数据包包头的大小20Bytes和TCP数据段的包头20Bytes）所以一般MSS值1460 则初始序列号n为： 高 5 位为t mod 32（mod是一种同余运算） 接下来3位为m的编码值 低 24 位为s 下面是具体过程，主要是低24位s得计算过程，服务器收到一个SYN包，计算一个消息摘要mac。 mac = MAC(A, k); MAC是密码学中的一个消息认证码函数，也就是满足某种安全性质的带密钥的hash函数，它能够提供cookie计算中需要的安全性。在Linux实现中，MAC函数为SHA1 1A &#x3D; SOURCE_IP || SOURCE_PORT || DST_IP || DST_PORT || t || MSSIND k为服务器独有的密钥，实际上是一组随机数。t为系统启动时间，每60秒加1。MSSIND为MSS对应的索引。 其实就是根据源IP、源端口、目的IP、目的端口、系统启动时间，MSS索引通过哈希函数计算出的一个值 SYN Cookie在Linux内核中的实现 如果SYN Cookie功能有编译进内核(CONFIG_SYN_COOKIE)，且选项tcp_syncookies不为0，那么可使用SYN Cookie。同时设置SYN Flood标志(listen_opt-&gt;synflood_warned) SHA1 安全哈希算法(Secure HASH Algorithm)主要适用于数字签名。 对于长度小于2^64位的消息，SHA1会产生一个160位的消息摘要。当接收到消息的时候，这个消息摘要可以用来验证数据的完整性。在传输的过程中，数据可能会发生变化，那么这时候就会产生不同的消息摘要。SHA1有如下特性： 不可以从消息摘要中复原信息。 两个不同的消息不会产生同样的消息摘要。 当客户端收到此SYN+ACK 报文后，根据TCP标准，它会回复ACK报文，且报文中ack = n + 1，那么在服务器收到它时，将ack - 1就可以拿回当初发送的SYN+ACK报文中的序号了！服务器巧妙地通过这种方式间接保存了一部分SYN报文的信息。 看到这里在回顾一个这个序列号的组成 高 5 位为t mod 32（mod是一种同余运算） 接下来3位为m的编码值 低 24 位为s 接下来，服务器需要对ack - 1这个序号进行检查： 将高 5 位表示的t与当前之间比较，看其到达地时间是否能接受。 根据t和连接元组重新计算s，看是否和低 24 一致，若不一致，说明这个报文是被伪造的。 解码序号中隐藏的mss信息 如果成功匹配， 服务器就会为新的连接创建和初始化一个传输控制块，然后把完成三次握手的req和新sock关联起来，下面看看验证这段逻辑的代码 SYN Cookie技术可以让服务器在收到客户端的SYN报文时，不分配资源保存客户端信息，而是将这些信息保存在SYN+ACK的初始序号和时间戳中。对正常的连接，这些信息会随着ACK报文被带回来。 SYN Cookie总结由于cookie的计算只涉及到包头部分信息，在建立连接的过程。中不在服务器端保存任何信息，所以失去了协议的许多功能，比如超时重传。此外，由于计算cookie有一定的运算量，增加了连接建立的延迟时间，因此，SYN Cookie技术不能作为高性能服务器的防御手段。一些SYN攻击的防火墙也是基于SYN Cookie，只是把这个功能移动到内核之外的代理服务器上。 参考资料： https://lwn.net/Articles/277146/ https://zh.wikipedia.org/wiki/SYN_cookie","updated":"2020-03-13T03:06:29.706Z","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"康威法则与微服务","date":"2020-02-07T11:19:53.000Z","path":"2020/02/07/康威法则与微服务/","text":"什么是康威法则最开始是一个叫康威的人，他原来是一个程序员，在1967年的时候提出来。康威法则我们认为它是微服务的理论基础。康威法则的原话是这样讲的，设计系统的组织，其产生的设计和架构等价于组织的组织架构。 康威法则和微服务为什么这么说呢？今天主要就是介绍康威法则和微服务之间的一个关系。 一个互联公司在刚开始的时候业务量一般是不大的，刚开始的时候他是尝试业务模式能不运行能起来，所以他一开始的时候开发的系统一定是一个简单的单块系统，这个时候他的团队规模不大，一般就几十个人或者是有限的几个。随着业务规模越来越大，团队肯定也会扩大规模，需要两个、三个、甚至几十个团队来协同那么这个时候，如果你的系统架构仍然是单块，然后他就跟分散式的多团队之间产生了一种不匹配的情况。 就所谓的违反了我们的康威法则，该架构它没有反应组织的这个组织架构。这时候就会出现，矛盾协调成本很高，交付效率很低，假如有一个单块的一个大型的服务，但是那有好多个团队，可能有七八个甚至十几个团队围绕的这个单块这个架构进行开发，当其中一个团队对这个单块应用进行了一些升级，而引入一些新的功能往往，需要其他的团队来协作配合，做集成测试，才能够交付。以这个时候团队之间的协调成本很高，有的时候甚至还会产生摩擦。也就是说我们的单块应用和多团队之间产生了一种不匹配的情况，康威法则怎么来解决这个问题？ 怎么解决呢？微服务是解决的一个手段，我们把这个单块的这个应用把它拆解出来。拆分成若干个微服务。每个团队负责维护自己的服务，相互之间不干扰。当S1在这个团队在开发它自己的服务的时候，不需要其他的团队都得一起来，或者说这种配合的沟通协商成本比较小，还可以去独立的迭代和交付自己的微服务。那么这个时候就会发现，多团队和多模块之间的这个结构关系又能够映射起来，它符合了康威法则，然后整体的研发的效率，对业务的支持就更高效。 所以介绍的这个即将康威法则法则是这样描述的，设计系统的组织，其产生的设计和架构等价于组织的组织架构，架构师不仅仅要做好技术价格，同时要了解组织的组织架构！","updated":"2020-03-13T03:06:29.737Z","categories":[{"name":"微服务架构","slug":"微服务架构","permalink":"https://zouchanglin.cn/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://zouchanglin.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}]},{"title":"服务注册与发现","date":"2020-02-06T14:56:21.000Z","path":"2020/02/06/服务注册与发现/","text":"本篇主要讲述了微服务架构的组成，每个组件的作用是什么，重点是实现了Eureka Server和Eureka Client的实战演练，总体来说服务注册与发现就是微服务入门的基础，了解并掌握了在SpringCloud体系中的服务注册与发现的组件原理以及使用方法，由于start.spring.io的访问速度实在是不能忍，白天自己编译了整个SpringBoot的初始化器，我直接给出地址了，方便其他的朋友也可以用： http://139.159.234.67:8080 http://zouchanglin.cn:8080 就是上面的IP地址 如果还是很慢的话，可以下载我编译好的文件（下载地址），直接 1java -jar Spring-Initializr.jar 在IDEA自定义初始化器地址填写 http://localhost:8080 即可，注意这个版本是没有Web UI的，只有供三分初始化器使用的JSON串，Web UI几乎不咋用所以没考虑编译进来！ 简单的微服务架构微服务架构的基础框架/组件 服务注册与发现服务提供了肯定要注册上来，并且把自己的地址信息表现出来，然后服务的调用方才能从这个组件上发现目标服务 服务网关 Service GateWay服务不但要内部能访问到，而且肯定有一部分服务需要暴露给外界，所以这就是服务网关的通俗理解，服务网关是连接内外的大门，网关会屏蔽后台服务的一些细节，比如后台的程序要升级呀对外用户是无感知的，还有路由的功能，可以将外部的请求反向路由到内部的某个具体的微服务，还可以做一些限流和容错的功能，因为所有请求都会经过网关，所以可以控制流量，监控和日志也可以在这个里完成，用户认证、授权、反爬虫之类都是在服务网关完成的，作用是非常巨大的 后端通用服务也称为中间层服务 Middle Tier Service，把自己注册到服务注册表 前端服务也成为边缘服务 Edge Servcice，从服务注册于发现组件中调用自己想要的后端服务 微服务是一种架构，具体实现主要分为阿里系和SpringCloud系，阿里系主要使用Dubbo做服务治理，使用ZK做服务注册中心，在17年左右Dubbo死而复生之后就疯狂的更新，很多公司也在用Dubbo做服务治理；拎一个系列那就是SpringCloud系列，也就是SpringCloud全家桶，Spring Cloud是一个开发工具集，包含多个子项目，利用SpringBoot的开发便利，主要是对Netflix开源组件的进一步封装，基本囊括了微服务所需要的绝大部分功能组件，Spring Cloud简化了分布式应用的开发，需要更加深刻的理解分布式架构的特点！ 服务注册与发现SpringCloud Eureka，基于Netflix Eureka做的二次封装，主要有两个组件构成 Eureka Server 注册中心 Eureka Client 服务注册 服务端就是供服务注册的服务器，客户端用来简化与服务器的交互，作为轮询负载均衡器，并提供服务的自动切换功能，客户端连接到服务注册中心并维持心跳连接，Eureka就能够监控系统中的服务是否正常工作 Eureka ServerEureka这个单词的意思就是找到了，有了的意思 首先是吧SpringBoot和SpringCloud的版本对应起来，否则可能会造成很多意料之外的错误，比如eureka注册了结果找不到服务类啊，比如某些Jar导入不进来啊，等等这些错误，下图是对应关系，SpringCloud的版本名称是按照伦敦地铁站的名字命名的： 我这里选择的是Finchley的版本 接下来需要开启SpringBoot项目中的Eureka Server 只需要在启动类上注解 @EnableEurekaServer 即可，同时，一个Eureka作为Server也作为一个Client，所以需要自己注册自己，需要在配置文件中添加图如下配置（添加spring application name就是为了让注册实例的Name不是UNKOWN）： 12345678eureka: client: service-url: defaultZone: http://localhost:8080/eurekaspring: application: name: eureka 启动之后访问localhost:8080即可看到 但是作为注册中心，我又不想在页面上把它当作一个注册实例，那么添加如下配置 12345eureka: client: service-url: defaultZone: http://localhost:8080/eureka register-with-eureka: false 因为Eureka的默认端口是8761，所以这里也改一改 1234567891011eureka: client: service-url: defaultZone: http://localhost:8761/eureka register-with-eureka: falsespring: application: name: eureka server: port: 8761 Eureka Client首先通过SpringBoot创建一个工程，选择Eureka Discovery Client，我试过目前最新版只有spring-boot-starter-web也勾上才可以，不然直接启动失败！ 接下来在启动类配置@EnableDiscoveryClient，代表这是一个Eureka Client，然后配置文件如下 12345678eureka: client: service-url: defaultZone: http://localhost:8761/eureka/spring: application: name: client-01 打开我们之前配置的Eureka Server，因为Eureka Server的地址是 http://localhost:8761/eureka/ 所以在配置的时候需要配置这个地址，因为你要像Eureka Server去注册，然后给自己起个名字叫做 client-01 接下来先开启Eureka Server，我为了方便启动，直接打成了Jar包，并做成了脚本放在桌面上！ 然后启动client-01，然后打开localhost:8761 就看到了Eureka Client 如果频繁重启Client端，注意到会有这样的问题 Eureka Server 和 Eureka Client采用的是心跳检测连接，Server端不会停的检查Client是否存活，会在一定的时间统计Client端的上线率，这个上线率就是一个比例，当低于某个比例的时候就会爆出这样的警告，意思就是Client的上线率太低了，可能某个时间点就不知道你是上线还是下线，那么我就当作你是上线的，俗话讲就叫宁可信其有，不可信其无，实际上是它的一种自我保护模式，在开发环境可以把它关掉，避免显示的是在线但实际上却不在线的情况，所以在开发环境最好是把这种模式给关掉！现在来到Eureka Server端，加入配置： 12345678910111213eureka: client: service-url: defaultZone: http://localhost:8761/eureka register-with-eureka: false server: enable-self-preservation: false # 关闭这种自我保护机制spring: application: name: eurekaserver: port: 8761 记住生成环境不要做这样的配置，在关闭这种机制之后呢，又会出现这样的警告，无需关闭，它就是告诉我们不建议关闭这个机制，但是开发环境就这样配置很有必要的！","updated":"2020-05-14T03:29:26.968Z","categories":[{"name":"微服务架构","slug":"微服务架构","permalink":"https://zouchanglin.cn/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://zouchanglin.cn/tags/SpringCloud/"}]},{"title":"Gradle的使用和配置","date":"2020-02-05T10:57:21.000Z","path":"2020/02/05/Gradle的使用和配置/","text":"Java作为一门世界级主流编程语言，有一款高效易用的项目管理工具是Java开发者共同追求的心愿和目标。先是2000年Ant，后有2004年Maven两个工具的诞生，都在Java市场上取得了巨大的成功。但是二者都有一定的不足和局限性。 Gradle简介2012年基于Ant和Maven产生的Gradle，弥补了Ant和Maven的不足，带来了一些更高效的特点。它使用一种基于Groovy的特定领域语言(DSL)来声明项目设置，抛弃了基于XML的各种繁琐配置。面向Java应用为主。当前其支持的语言限于Java、Groovy和Scala，计划未来将支持更多的语言。 Gradle安装配置(Windows版)很简单，下载之后解压，需要把目录配置到环境变量中：配置 GRADLE_HOME，然后在PATH中添加配置：%GRADLE_HOME%\\bin，打开命令行： 出现上图即是配置成功 Gradle和idea集成对于IDEA 2019.3 无需集成，只要配置了GRADLE_HOME环境变量，一切都是自动的 Groovy语言简单介绍 先打开上面的命令行，接下来开始学习groovy编程语言 123456789101112131415161718192021222324//介绍groovy编程语言println(\"hello groovy\")println \"hello groovy!\"//groovy定义变量def i = 18println \"i = \" + i//定义集合类型def list = ['a', 'b', 'c']//往list里面添加元素list &lt;&lt; 'd'//取出第三个元素println \"list.get(3) = \" + list.get(3)//定义一个mapdef map = ['name': 'Tim', 'age': 18]//向map中添加 K-Vmap.sex = '男'println map 点击右上角的箭头即可执行 Groovy的闭包 1234567891011121314151617181920212223242526//介绍groovy的闭包//闭包的概念：闭包其实就是一段代码块在gradle中把闭包当作参数使用def b1 = &#123; println (\"Hello b1\")&#125;//定一个方法，方法里需要闭包类型的参数def method1(Closure closure)&#123; closure()&#125;//调用方法method1method1(b1) //输出：Hello b1//带参数的闭包def b2 = &#123; v -&gt; println(\"Hello $&#123;v&#125;\")&#125;//定一个方法，方法里需要闭包类型的参数def method2(Closure closure)&#123; closure(\"Tim\")&#125;method2(b2) //输出：Hello Tim Gradle仓库的配置123456789101112131415161718192021222324252627282930313233343536plugins &#123; id 'java'&#125;/** * 自己填写的一些信息 */group 'xpu.edu'version '1.0-SNAPSHOT'sourceCompatibility = 1.8/** * 指定所使用的参数仓库路径 * mavenCentral()表示使用中央仓库，从中央仓库下载到指定目录 */repositories &#123; //先从本地仓库寻找依赖，如果没有再从中央仓库下载 mavenLocal() mavenCentral() //如果只设置中央仓库，表示直接从中央仓库下载Jar包&#125;/** * Gradle工程的所有的Jar包的坐标都在dependencies属性内放置的 * 每个Jar包的坐标都有三个基本元素组成 * group、name、version 这和maven是一致的 * testCompile表示该Jar包在测试的时候使用，也就是Jar包的作用域 * 我们在Gradle里面添加坐标的时候都要添加作用域 */dependencies &#123; compile group: 'com.alibaba', name: 'fastjson', version: '1.2.58' compile group: 'mysql', name: 'mysql-connector-java', version: '8.0.19' testCompile group: 'junit', name: 'junit', version: '4.12'&#125; 如何设置本地仓库呢？ 只需要设置GRADLE_USER_HOME 这个环境变量即可，把GREADLE_USER_HOME环境变量设置到自己本地的Maven仓库路径即可！ 配置阿里云仓库12345repositories &#123; maven &#123; url \"http://maven.aliyun.com/nexus/content/groups/public/\" &#125;&#125; 这样配置是对当前工程生效，但是新建Gradle工程后又需要在配置一次 所以需要在Gradle的安装目录下的init.d文件夹里新建一个名称为init.gradle的文件，写入如下内容 12345678allprojects &#123; repositories &#123; maven &#123; name \"aliyunmaven\" url \"http://maven.aliyun.com/nexus/content/groups/public/\" &#125; &#125;&#125;","updated":"2020-03-13T03:06:29.658Z","categories":[{"name":"工程构建","slug":"工程构建","permalink":"https://zouchanglin.cn/categories/%E5%B7%A5%E7%A8%8B%E6%9E%84%E5%BB%BA/"}],"tags":[{"name":"Gradle","slug":"Gradle","permalink":"https://zouchanglin.cn/tags/Gradle/"}]},{"title":"Golang工程管理","date":"2020-02-04T06:06:00.000Z","path":"2020/02/04/Golang工程管理/","text":"工作区介绍Go代码必须放在工作区中。工作区其实就是一个对应于特定工程的目录，它应包含3个子目录: src目录、pkg目录和bin目录。 src目录：用于以代码包的形式组织并保存Go源码文件。 (比如: .go .c .h .s等) pkg目录：用于存放经由go install命令构建安装后的代码包(包含Go库源码文件)的.a归档文件 bin目录：与pkg目录类似，在通过go install命令完成安装后，保存由Go俞令源码文件生成的可执行文件 目录src用于包含所有的源代码，是Go命令行工具一个强制的规则，而pkg和bin则无需手动创建，如果必要Go命令行工具在构建过程中会自动创建这些目录 需要特别注意的是，只有当环境变量GOPATH中只包含一个工作区的目录路径时，go install命令才会把命令源码安装到当前工作区的bin目录下。若环境变量GOPATH中包含多个工作区的目录路径，像这样执行go install命令就会失效，此时必须设置环境变量GOBIN。 GOPATH设置为了能够构建这个工程，需要先把所需工程的根目录加入到环境变量GOPATH中。否则，即使处于同一工作目录(工作区)，代码之间也无法通过绝对代码包路径完成调用。 在实际开发环境中，工作目录往往有多个。这些工作目录的目录路径都需要添加至GOPATH。当有多个目录时，请注意分隔符，多个目录的时候Windows是分号，Linux 系统是冒号，当有多个GOPATH时，默认会将go get的内容放在第一个目录下。 自定义包所有Go语言的程序都会组织成若干组文件，每组文件被称为一个包。这样每个包的代码都可以作为很小的复用单元，被其他项目引用。 一个包的源代码保存在一个或多个以.go为文件后缀名的源文件中，通常一个包所在目录路径的后缀是包的导入路径。 对于一个较大的应用程序，我们应该将它的功能性分隔成逻辑的单元，分别在不同的包里实现。我们创建的的自定义包最好放在GOPATH的src目录下(或者GOPATH src的某个子目录)。 在Go语言中，代码包中的源码文件名可以是任意的。但是，这些任意名称的源码文件都必须以包声明语句作为文件中的第一行，每个包都对应一个独立的名字空间 1package calc 包中成员以名称首字母大小写决定访问权限 public 首字母大写，可被包外访问 private 首字母小写，仅包内成员可以访问 注意：同一个目录下不能定义不同的package 导入包的方式1234567891011121314151617181920package mainimport ( \"fmt\" \"os\")import \"os\"import \"fmt\"//推荐使用的导包方式import ( \"fmt\" \"os\")func main() &#123; fmt.Println(\"os.args =\", os.Args)&#125; 另一种导包方式（容易导致重名，非常不推荐） 12345678910package main//相当于静态导入import \"os\"import . \"fmt\"func main() &#123; //无需写fmt Println(\"os.args =\", os.Args)&#125; 给包起别名 123456789package main//导入包起别名import \"os\"import tim \"fmt\"func main() &#123; tim.Println(\"os.args =\", os.Args)&#125; 有时，用户可能需要导入一个包，但是不需要引用这个包的标识符。在这种情况，可以使用空白标识符_来重命名这个导入 123import ( _ \"fmt\") _ 操作其实是引入该包，而不直接使用包里面的函数，而是调用了该包里面的init函数。 main包在Go语言里，命名为main的包具有特殊的含义。Go语言的编译程序会试图把这种名字的包编译为二进制可执行文件。所有用Go语言编译的可执行程序都必须有一个名叫main的包。一个可执行程序有且仅有一个main 包。 当编译器发现某个包的名字为main 时，它一定也会发现名为main()的函数，否则不会创建可执行文件。main()函数是程序的入口， 所以，如果没有这个函数，程序就没有办法开始执行。程序编译时，会使用声明main包的代码所在的目录的目录名作为二进制可执行文件的文件名。 main函数和init函数Go里面有两个保留的函数：init函数(能够应用于所有的package)和main函数(只能应用于package main)。这两个函数在定义时不能有任何的参数和返回值。虽然一个package里面可以写任意多个init函数，但这无论是对于可读性还是以后的可维护性来说，我们都强烈建议用户在一个package中每个文件只写一个init函数。 Go程序会自动调用init(和main)，所以你不需要在任何地方调用这两个函数。每个package中的init函数都是可选的，但package main就必须包含一个main函数。 每个包可以包含任意多个init 函数，这些函数都会在程序执行开始的时候被调用。所有被编译器发现的init函数都会安排在main 函数之前执行。init 函数用在设置包、初始化变量或者其他要在程序运行前优先完成的引导工作。 程序的初始化和执行都起始于main包。如果main包还导入了其它的包，那么就会在编译时将它们依次导入。有时一个包会被多个包同时导入，那么它只会被导入一次( 例如很多包可能都会用到fmt包，但它只会被导入一次，因为没有必要导入多次)。 当一个包被导入时，如果该包还导入了其它的包，那么会先将其它包导入进来，然后再对这些包中的包级常量和变量进行初始化，接着执行init函数(如果有的话)，依次类推。等所有被导入的包都加载完毕了，就会开始对main包中的包级常量和变量进行初始化，然后执行main包中的init函数(如果存在的话)，最后执行main函数。下图详细地解释了整个执行过程: go install命令设置环境变量GOBIN，通过go install 命令可以自动生成pkg、bin目录 总结一下src：放源代码，如果有多个文件或多个包 1、配置GOPATH环境变量， 配置src同级目录的绝对路径2、自动生成bin或pkg目录，需要使用go install命令。除了要配置GOPATH环境变量，还要配置G0BIN环境变量工src：放源代码bin：放可执行程序pkg：放平台相关的库","updated":"2020-03-13T03:06:29.656Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://zouchanglin.cn/tags/Golang/"}]},{"title":"Golang函数相关","date":"2020-02-04T05:58:11.000Z","path":"2020/02/04/Golang函数相关/","text":"本文主要记录了Golang的函数定义，可变参数，函数返回值，函数类型，匿名函数和闭包， 延迟调用，变量的作用域以及如何获取命令行参数，Golang是可以有多个返回值的，这一点能干很多事情，函数类型的主要功能在于回调和多态的实现，就像函数指针一样，用起来也很方便！ 函数定义1234func FuncName(/* 参数列表 */)(o1 type, o2 type /* 返回值类型 */ )&#123; return v1, v2;&#125; 函数定义说明: func: 函数由关键字func开始声明 FumcName: 函数名称，根据约定，函数名首字母小写即为private，大写即为public 参数列表: 函数可以有0个或多个参数，参数格式为:变量名类型，如果有多个参数通过逗号分隔，不支持默认参数 返回类型:①上面返回值声明了两个变量名o1和o2(命名返回参数)，这个不是必须，可以只有类型没有变量名②如果只有一个返回值且不声明返回值变量，那么你可以省略，包括返回值的括号③如果没有返回值， 那么就直接省略最后的返回信息④如果有返回值， 那么 必须在函数的内部添加retun语句 123456789101112131415161718192021222324252627282930313233343536373839404142package mainimport \"fmt\"//无参无返回值函数定义func MyFunc()&#123; fmt.Println(\"MyFunc\")&#125;//带参无返回值func MyFunc3(a int)&#123; a = 999 fmt.Println(a)&#125;//带多个参数无返回值func MyFunc4(a int, b int) &#123; fmt.Printf(\"a=%d, b=%d\\n\", a, b)&#125;//参数一样时可以简写func MyFunc5(a, b int) &#123; fmt.Printf(\"a=%d, b=%d\\n\", a, b)&#125;//参数一样时可以简写,a、b都是int、e是float64、d、f都是stringfunc MyFunc6(a, b int, e float64, d, f string) &#123; fmt.Printf(\"a=%d, b=%d\\n\", a, b)&#125;func main() &#123; MyFunc() MyFunc2() MyFunc3(666) MyFunc4(666, 777) MyFunc5(666, 777)&#125;//无参无返回值函数定义，定义在main之后也OKfunc MyFunc2()&#123; fmt.Println(\"MyFunc2\")&#125; 可变参数和C语言一样，可变参数放最后，可传可不传 可变参数的参数传递方式，参考MyFunc04，如果是只传一部分，参考MyFunc05、MyFunc06 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package mainimport \"fmt\"//固定参数func MyFunc01(a, b int) &#123;&#125;//可变参数func MyFunc02(args ... int) &#123; for i, data := range args &#123; fmt.Printf(\"args[%d]=%d\\n\", i, data) &#125;&#125;//固定参数+可变参数//固定参数必传、可变参数可以为空//可变参数一定要放在最后func MyFunc03(a int, b string, args ... int) &#123; fmt.Println(\"a =\", a) fmt.Println(\"b =\", b) for i, data := range args&#123; fmt.Printf(\"args[%d]=%d\\n\", i, data) &#125;&#125;//可变参数的传递func MyFunc04(args ... int) &#123; MyFunc02(args ...)&#125;//可变参数的传递//特殊情况：比如只想把后面两个参数传递给另外一个函数使用func MyFunc05(args ... int) &#123; //从args[2]开始(包括本身)，把后面所有元素传过去 MyFunc02(args[2:] ...)&#125;func MyFunc06(args ... int) &#123; //从args[2]开始(不包括本身)，把前面所有元素传过去 MyFunc02(args[:2] ...)&#125;func main() &#123; //MyFunc02(11, 22, 33, 44) // OK //MyFunc02() // OK //MyFunc03(11, \"hello\", 22, 33) MyFunc04(11, 22, 33, 44) fmt.Printf(\"-----------------------------\\n\") MyFunc05(11, 22, 33, 44) fmt.Printf(\"-----------------------------\\n\") MyFunc06(11, 22, 33, 44)&#125; 返回值12345678910111213141516171819202122232425262728package mainimport \"fmt\"//无参有一个返回值func MyFunc01() int &#123; return 100&#125;//给返回值起一个变量名(golang的推荐写法)func MyFunc02() (result int) &#123; return 200&#125;//给返回值起一个变量名(golang的推荐写法)func MyFunc03() (result int) &#123; result = 300 return&#125;func main() &#123; a := MyFunc01() fmt.Printf(\"a=%d\\n\", a) b := MyFunc02() fmt.Printf(\"b=%d\\n\", b) c := MyFunc03() fmt.Printf(\"c=%d\\n\", c)&#125; 多个返回值的情况 123456789101112131415161718192021package mainimport \"fmt\"//多个返回值func MyFunc01() (int, int, int) &#123; return 11, 22, 33&#125;//官方推荐写法func MyFunc02() (a int, b int, c int) &#123; a, b, c = 11, 22, 33 return&#125;func main() &#123; _, _, c := MyFunc01() fmt.Printf(\"c=%d\\n\", c) _, b, _ := MyFunc02() fmt.Printf(\"b=%d\\n\", b)&#125; 多参数多个返回值 12345678910111213141516171819package mainimport \"fmt\"func MaxValue(a, b int) (max, min int) &#123; if a &gt; b &#123; max = a min = b &#125;else&#123; min = a max = b &#125; return&#125;func main() &#123; max, _ := MaxValue(11, 22) fmt.Printf(\"max=%d\\n\", max)&#125; 递归的例子 1234567891011121314151617package mainimport \"fmt\"func MyFunc01(num int) (ret int) &#123; if num == 1 &#123; return 1 &#125;else &#123; return num + MyFunc01(num - 1) &#125;&#125;func main() &#123; num := MyFunc01(100) fmt.Printf(\"num=%d\\n\", num)&#125; 函数类型函数也是一种数据类型，通过type可以定义它，它的类型就是所有拥有的相同的参数，相同的返回值的一种类型 1234567891011121314151617181920212223242526272829303132333435package mainimport \"fmt\"func Add(a, b int) (ret int) &#123; return a + b&#125;func Sub(a, b int) (ret int) &#123; return a - b&#125;//声明一个函数类型type FuncType func(int, int) int//多态：多种形态，调用一个接口不同的表现形态，这里相当于把函数类型当作参数传递了func MyCalc(calc FuncType, a, b int) int &#123; return calc(a, b)&#125;func main() &#123; //传统使用方式 ret := Add(10, 20) fmt.Printf(\"ret=%d\\n\", ret) //函数类型调用方式 var myFunc FuncType myFunc = Add ret = myFunc(20, 30) //等价于Add(20, 30) fmt.Printf(\"ret=%d\\n\", ret) fmt.Printf(\"Add ret=%d\\n\", MyCalc(Add, 10, 5)) //15 fmt.Printf(\"Sub ret=%d\\n\", MyCalc(Sub, 10, 5)) //5&#125; 函数类型的应用：回调函数，也就是通过这种传递函数参数的方式实现的 其实底层实现就是函数指针数组 匿名函数和闭包所谓闭包就是一个函数“捕获”了和它在同一作用域的其它常量和变量。这就意味着当闭包被调用的时候，不管在程序什么地方调用，闭包能够使用这些常量或者变量。它不关心这些捕获了的变量和常量是否已经超出了作用域，所以只有闭包还在使用它，这些变量就还会存在。 在Go语言里，所有的匿名函数 (Go语言规范中称之为函数字面量) 都是闭包。匿名函数是指不需要定义函数名的一种函数实现方式，它并不是一个新概念，最早可以回溯到1958年的Lisp语言 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package mainimport ( \"fmt\")func main() &#123; //匿名函数，无函数名 myFunc01 := func() &#123; fmt.Printf(\"hello\\n\") &#125; myFunc01() //给一个函数类型起别名 type FuncType func() //无参数无返回值 var myFunc02 FuncType = myFunc01 myFunc02() //定义匿名函数并调用 func() &#123; fmt.Printf(\"hello\\n\") &#125;() //()代表调用此匿名函数 //带参数的匿名函数 myFunc03 := func(a, b int) &#123; fmt.Printf(\"a=%d, b=%d\\n\", a, b) &#125; myFunc03(10, 20) //定义带参数的匿名函数并调用 func(a, b int) &#123; fmt.Printf(\"a=%d, b=%d\\n\", a, b) &#125;(10, 20) //定义匿名函数带参带返回值 x, y := func(i, j int)(max, min int)&#123; if i &gt; j &#123; max, min = i, j &#125;else &#123; max, min = j, i &#125; return &#125;(10, 20) fmt.Printf(\"x=%d, y=%d\\n\", x, y)&#125; 闭包以引用方式捕获外部变量 123456789101112131415161718package mainimport \"fmt\"func main() &#123; a := 10 s := \"hello\" func()&#123; //闭包以引用方式捕获外部变量 a = 666 s = \"world\" //内部：a=666, s=world fmt.Printf(\"内部：a=%d, s=%s\\n\", a, s) &#125;() //内部：a=666, s=world fmt.Printf(\"内部：a=%d, s=%s\\n\", a, s)&#125; 闭包的特点 12345678910111213141516171819202122232425262728293031323334353637package mainimport \"fmt\"//函数的返回值是一个匿名函数，返回一个函数类型func test02() func()int &#123; var x = 0 return func() int &#123; x++ return x * x &#125;&#125;func main() &#123; //函数的返回值是一个匿名函数，返回一个函数类型 //通过f来调用返回的匿名函数，即调用闭包 f := test02() fmt.Printf(\"%d\\n\", f()) // 1 fmt.Printf(\"%d\\n\", f()) // 4 fmt.Printf(\"%d\\n\", f()) // 9 fmt.Printf(\"%d\\n\", f()) // 12&#125;func test01() int &#123; //函数调用的时候，x才分配空间 var x = 0 x++ return x * x //调用完毕 x释放&#125;func main01() &#123; fmt.Printf(\"%d\\n\", test01()) // 1 fmt.Printf(\"%d\\n\", test01()) // 1 fmt.Printf(\"%d\\n\", test01()) // 1 fmt.Printf(\"%d\\n\", test01()) // 1&#125; 所以可以看出变量的声明周期不是由它的作用域决定的，如果有闭包在使用它，那么它的声明周期是由闭包是否还在继续调用决定的！ 延迟调用 defer关键字defer用于延迟一个函数或者方法（或者当前所创建的匿名函数）的执行，注意：defer语句只能出现在函数或者方法的内部， 文件操作时比较常用 下面看看多个defer的执行顺序 如果一个函数中有多个defer语句，它们会以LIFO (后进先出)的顺序执行。哪怕函数或某个延迟调用发生错误，这些调用依旧会被执行。 defer和匿名函数结合使用 与上面的例子做一个对比 所以很明显可以看出来，defer就是延迟调用，但是不代表延时传参数 获取命令行参数 作用域定义在{ } 里面的变量就是局部变量，注意if或者for语句里定义的变量也是属于if或者for语句独有的，执行到变量那句话的时候才分配内存空间，离开作用域内存自动释放 12345678910111213package mainimport \"fmt\"//定义全局变量var num = 20//定义全局变量不能使用类型推导//num := 30 errorfunc main() &#123; fmt.Println(\"num =\", num)&#125; 不同作用域允许定义同名变量，使用变量的原则，就近原则","updated":"2020-03-13T03:06:29.653Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://zouchanglin.cn/tags/Golang/"}]},{"title":"CentOS7防火墙操作","date":"2020-02-03T03:20:29.000Z","path":"2020/02/03/CentOS7防火墙操作/","text":"CentOS7使用的是firewall防火墙，不再是原来的iptables 1、查看firewall防火墙状态 1firewall-cmd --state 或者 1systemctl status firewalld 2、打开防火墙 1systemctl start firewalld 3、关闭防火墙 1systemctl stop firewalld 4、重启防火墙 1firewall-cmd --reload 或者 1systemctl reload firewalld 5、开机自启动防火墙 1systemctl enable firewalld 6、禁止开机启动防火墙 1systemctl disable firewalld 6、查看已打开的端口 1firewall-cmd --list-ports 7、打开端口（重要） 1firewall-cmd --permanent --zone=public --add-port=8080/tcp 其中permanent表示永久生效，public表示作用域，8080/tcp表示端口和类型 8、关闭端口 1firewall-cmd --permanent --zone=public --remove-port=8080/tcp","updated":"2020-03-17T10:53:57.908Z","categories":[{"name":"操作系统实战","slug":"操作系统实战","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"}]},{"title":"运行容器commit至云","date":"2020-02-02T14:39:00.000Z","path":"2020/02/02/运行容器commit至云/","text":"之前记载的commit镜像方式是有问题的，所以现在开始更正一下，总体的方式就是登录，然后先提交镜像到本地，然后打标签，最后push就好了！ 镜像站对开中国开发者来说可谓必备利器，受国际网络出口带宽的影响，大多数开源软件官网的速度慢，稳定性不足。阿里巴巴镜像站利用其在云服务上的优势，提供快速、稳定的镜像分发服务， 而且提供免费的个人私有镜像仓库存储服务。所以我也就是存储到了阿里云，貌似腾讯云好像也有免费的镜像存储服务，下载速度还可以！ 0、必须先登录， 登录的用户名为阿里云账号全名 1docker login --username=xxxx registry.cn-hangzhou.aliyuncs.com 1、查看运行的容器 1docker ps 2、提交镜像 1docker commit -m=\"my second commit\" -a=\"zouchanglin\" CONTAINER_ID mytest-release:v2.0 3、查看已提交镜像 1docker images 4、镜像打标 1docker tag IMAGE_ID registry.cn-hangzhou.aliyuncs.com/zouchanglin/mytest-release:v2.0 5、Push镜像文件 1docker push registry.cn-hangzhou.aliyuncs.com/zouchanglin/mytest-release:v2.0","updated":"2020-03-13T03:06:29.772Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"}]},{"title":"SpringBoot自定义配置文件","date":"2020-01-30T06:13:30.000Z","path":"2020/01/30/SpringBoot自定义配置文件/","text":"SpringBoot的配置文件老是出现警告，后面发现了这种解决方式，引入Maven依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 下面是官方给出的自定义配置的方式： 按照图示操作就行： 配置文件完美消除警告","updated":"2020-03-13T03:06:29.705Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://zouchanglin.cn/tags/SpringBoot/"}]},{"title":"Rancher的安装","date":"2020-01-30T06:04:30.000Z","path":"2020/01/30/Rancher的安装/","text":"控制端 1docker run -d --restart=unless-stopped -p 8080:8080 rancher/server:stable 服务端 12345docker run -e CATTLE_AGENT_IP=\"138.30.65.91\" --rm --privileged-v /var/run/docker.sock:/var/run/docker.sock -v /var/lib/rancher:/var/lib/rancher rancher/agent:v1.2.11 http://xxx.xxx:8080/v1/scripts/3A9E22EDA0BA91475:157700:3MnFfjZMBXnPrhZ8 注意Github授权访问，就可以实现权限控制了！","updated":"2020-03-13T03:06:29.702Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"},{"name":"集群","slug":"集群","permalink":"https://zouchanglin.cn/tags/%E9%9B%86%E7%BE%A4/"},{"name":"微服务","slug":"微服务","permalink":"https://zouchanglin.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}]},{"title":"使用Apache ab测压","date":"2020-01-30T05:56:39.000Z","path":"2020/01/30/使用Apacheab测压/","text":"一个在高压访问下，能承受很高峰值的访问并发才能称得上是一个好的网站，那么作为一个程序员，当你搭建好你的网站之后，你应该怎么测试你的网站并发访问量呢？ Apache Benchmark(简称ab) 是Apache安装包中自带的压力测试工具 ，简单易用。 使用起来非常的简单和方便。 不仅仅是可以apache服务器进行网站访问压力测试，还可以对其他类型的服务器进行压力测试。 比如nginx,tomcat,等 使用方式： ab -n 100 -c 100 http//:localhost:8080/xxxx -n 请求数 -c 并发数量 localhost/ 压测url 域名后面要带上一个斜杠否则会报url无效 ab命令会创建多个并发访问线程，模拟多个访问者同时对某一URL地址进行访问。它的测试目标是基于URL的，因此，它既可以用来测试apache的负载压力，也可以测试nginx、lighthttp、tomcat、IIS等其它Web服务器的压力。 ab命令对发出负载的计算机要求很低，它既不会占用很高CPU，也不会占用很多内存。但却会给目标服务器造成巨大的负载，其原理类似CC攻击。自己测试使用也需要注意，否则一次上太多的负载。可能造成目标服务器资源耗完，严重时甚至导致死机。 在Mac下可以直接使用，自带ab 在Linux下，如果是CentOS7 1yum -y install httpd-tools Windows下需要下载","updated":"2020-03-13T03:06:29.718Z","categories":[{"name":"软件测试技术","slug":"软件测试技术","permalink":"https://zouchanglin.cn/categories/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"压力测试","slug":"压力测试","permalink":"https://zouchanglin.cn/tags/%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95/"}]},{"title":"IDEA集成Docker实现一键打包部署","date":"2020-01-30T05:53:40.000Z","path":"2020/01/30/IDEA集成Docker实现一键打包部署/","text":"1、在IDEA工具中开发代码 2、代码打成jar包 3、部署到Linux服务器上 4、如果用Docker(编写Dockerfile文件) 5、构建镜像 6、运行容器 现在使用IDEA集成Docker实现一键打包部署 1、Docker开启远程访问 修改Docker服务文件 1vim /lib/systemd/system/docker.service 修改这一行 1/usr/bin/dockerd-current -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock \\ 2、重新加载配置文件 1systemctl daemon-reload 3、重启Docker服务 1systemctl restart docker 4、查看端口是否开启 1netstat -nlpt 5、关闭防火墙 1systemctl stop firewalld.service 6、禁止防火墙开机启动 1systemctl disable firewalld.service 7、直接curl看看是否生效 1curl http://127.0.0.1:2375/info 在IDEA中配置传统过程中,打包、部署、上传到Linux、 编写Dockerfile、 构建镜像、创建容器运行。 而在持续集成过程中,项目工程一般使用Maven编译打包,然后生成镜像,通过镜像上线,能够大大提供上线效率,同时能够快速动态扩容，快速回滚,着实很方便。docker-maven-plugin 插件就是为了帮助我们在Maven工程中,通过简单的配置,自动生成镜像并推送到仓库中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.2.2.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;edu.xpu&lt;/groupId&gt; &lt;artifactId&gt;docker&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;docker&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;!-- Docker镜像上传的用户名 --&gt; &lt;docker.image.prefix&gt;zouchanglin&lt;/docker.image.prefix&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt; &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;configuration&gt; &lt;!-- 镜像名称 --&gt; &lt;imageName&gt;$&#123;docker.image.prefix&#125;/$&#123;project.artifactId&#125;&lt;/imageName&gt; &lt;!-- 指定标签 --&gt; &lt;imageTags&gt; &lt;imageTag&gt;latest&lt;/imageTag&gt; &lt;/imageTags&gt; &lt;!-- 基础镜像版本JDK1.8 --&gt; &lt;baseImage&gt;java&lt;/baseImage&gt; &lt;!-- 作者本人信息 --&gt; &lt;maintainer&gt;zouchanglin zchanglin3@gmail.com&lt;/maintainer&gt; &lt;!-- 切换到ROOT目录 --&gt; &lt;workdir&gt;/ROOT&lt;/workdir&gt; &lt;cmd&gt;[\"java\", \"-version\"]&lt;/cmd&gt; &lt;entryPoint&gt;[\"java\", \"-jar\", \"$&#123;project.build.finalName&#125;.jar\"]&lt;/entryPoint&gt; &lt;!-- 指定 DockerFile路径 --&gt; &lt;!-- &lt;dockerDirectory&gt;$&#123;project.basedir&#125;/src/main/docker&lt;/dockerDirectory&gt;--&gt; &lt;!-- 指定远程 Docker api地址 --&gt; &lt;dockerHost&gt;http://192.168.70.128&lt;/dockerHost&gt; &lt;!-- 复制Jar包到指定的Docker容器 --&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/ROOT&lt;/targetPath&gt; &lt;!-- 用于指定用于复制的根目录 --&gt; &lt;directory&gt;$&#123;project.build.directory&#125;&lt;/directory&gt; &lt;!-- 用于指定需要复制的文件 --&gt; &lt;include&gt;$&#123;project.build.finalName&#125;.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;!-- 当执行maven package时执行maven clean package：build--&gt; &lt;execution&gt; &lt;id&gt;build-image&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;build&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 如果用上述maven插件则会构造出这样的Dockerfile文件 1234567891011FROM javaMAINTAINER zouchanglin zchanglin3@gmail.comWORKDIR /ROOTADD /ROOT/docker-0. 0.1-SNAPSHOT. jar /ROOT/ENTRYPOINT [\"java\", \"-jar\", \"docker-0.0. 1-SNAPSHOT. jar\"]CMD [\"java\", \"-version\"] 接下来使用命令构建镜像 mvn clean package docker:build 然后通过IDEA操作Docker去生成容器即可 绑定Docker命令道Maven各个阶段 我们可以绑定Docker命令到Maven各个阶段，我们可以把Docker分为buld、tag、 push ,然后分别绑定Maven的package、deploy 阶段，我们只需要执行mvn dep loy就可以完成整个build、tag、push操作了，当我们执行mvn build 就只完成build、tag 操作。 现在我们可以在mvn package的时候就完成镜像的打包 12345678910&lt;executions&gt; &lt;!-- 当执行maven package时，执行maven clean package：build --&gt; &lt;execution&gt; &lt;id&gt;build-image&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;build&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt;&lt;/executions&gt;","updated":"2020-05-06T03:55:45.946Z","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://zouchanglin.cn/categories/DevOps/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://zouchanglin.cn/tags/SpringBoot/"}]},{"title":"Golang学习第一天","date":"2020-01-30T05:41:25.000Z","path":"2020/01/30/Golang学习第一天/","text":"Golang的优势和特点 可直接编译成机器码，不依赖其他库，glibc 的版本有一-定要求，部署就是扔一个文件上去就完成了 静态类型语言，但是有动态语言的感觉，静态类型的语言就是可以在编译的时候检查出来隐藏的大多数问题，动态语言的感觉就是有很多的包可以使用，写起来的效率很高。 语言层面支持并发，这个就是Go最大的特色，天生的支持并发。Go就是基因里面支持的并发，可以充分的利用多核，很容易的使用并发。 内置runtime，支持垃圾回收，这属于动态语言的特性之一吧，虽然目前来说GC(内存垃圾回收机制)不算完美，但是足以应付我们所能遇到的大多数情况，特别是Go1.1之后的GC 简单易学，Go语言的作者都有C的基因，那么Go自然而然就有了C的基因，那么Go关键字是25个，但是表达能力很强大，几乎支持大多数你在其他语言见过的特性:继承、重载、对象等 丰富的标准库，Go目前已经内置了大量的库，特别是网络库非常强大。 内置强大的工具，Go语言里面内置了很多工具链，最好的应该是gofmt工具，自动化格式化代码，能够让团队review变得如此的简单，代码格式一模一样，想不一样都很困难 跨平台编译，如果你写的Go代码不包含cgo，那么就可以做到window系统编译linux的应用，如何做到的呢? Go引用了plan9的代码，这就是不依赖系统的信息。 内嵌C支持，Go里面也可以直接包含C代码，利用现有的丰富的C库。 Go适合用来做什么 服务器编程，以前你如果使用C或者C++做的那些事情，用Go来做很合适，例如处理日志、数据打包、虚拟机处理、文件系统等。 分布式系统，数据库代理器等。 网络编程，这一块目前应用最广，包括Web应用、API 应用、下载应用。 内存数据库，如google开发的groupcache, couchbase 的部分组件。 云平台，目前国外很多云平台在采用Go开发，CloudFoundy的部分组件，前VMare的技术总监自己出来搞的apcera云平台。 golang开发环境搭建下载 go1.13.6.windows-amd64.msi 下载地址是https://dl.google.com/go/go1.13.6.windows-amd64.msi一路Next安装别有中文路径即可 入门成功 另外还可以下载Go 的IDE，直接用LiteIDE就行了 https://studygolang.com/pkgdoc 这里是golang的文档 比如我们可以查找fmt包 golang简单语法 左括号和函数名称同行 go语言以包作为管理单位，调用函数大部分需要导包 每个文件必须先声明包 程序必须有一个main包才能运行 注释和Java相同，//和/**/ 导了包必须要使用，否则出错 golang的包管理方式一个包（文件夹）下之恩那个有一个main函数，在IDE的情况下 如果是非要放在一个包，可以直接go run **.go 如果需要一个可执行程序，那么可以go build xxx.go 关键字和常量 变量的使用声明了变量必须要使用，只声明，没有初始化的变量默认为0 同一个{ }里，变量名是唯一的 直接看这段代码吧，比较好懂一点 1234567891011121314151617181920212223242526272829303132333435363738394041424344package mainimport \"fmt\"//注释func main() &#123; //声明变量默认为0 var a int fmt.Println(\"a =\", a) //同时声明多个变量 var b, c int fmt.Println(\"b =\", b, \",c =\", c) //声明时赋值 var d int = 10 fmt.Println(\"d =\", d) //先声明，再赋值 var e int e = 20 fmt.Println(\"e =\", e) //类型自动推导 f := \"I' am string\" fmt.Println(\"f =\", f) //%T用于打印变量的类型 fmt.Println(\"Type is %T=\", f) fmt.Printf(\"Type is %T\\n\", f) fmt.Printf(\"Type is %T\\n\", a) //类型自动推导只能用于初始化那一次 g := 100 fmt.Println(\"g =\", g) //g:= \"a str\" -&gt; error //fmt.Println(\"g =\", g) g = 200 //g = \"a str\" 赋值时改变类型 -&gt; error //和C语言的printf()一样的 fmt.Printf(\"%d %d %s\", a, g, f) //fmt.Println()只是简单的拼接，不能使用%T去打印类型之类的信息，但是fmt.Printf()却可以&#125; 下面是一个变量交换的例子 123456789101112131415161718192021222324252627282930func main()&#123; //-------------01---------- //交换两个变量的值 var a0 int = 10 var b0 int = 20 var tmp int = a0 a0 = b0 b0 = tmp fmt.Printf(\"a0=%d, b0=%d\\n\", a0, b0) //-------------02---------- a1 := 10 b1 := 20 tmp1 := a1 a1 = b1 b1 = tmp1 fmt.Printf(\"a1=%d, b1=%d\\n\", a1, b1) //-------------03---------- a2, b2 := 10, 20 a2, b2 = b2, a2 fmt.Printf(\"a2=%d, b2=%d\\n\", a2, b2) //-------------04---------- a3, b3, c3 := 10, 20, 30 a3, b3, c3 = c3, a3, b3 fmt.Printf(\"a3=%d, b3=%d, c3=%d\\n\", a3, b3, c3)&#125; 匿名变量 比如下面这样的 常量的使用变量声明为var、常量声明为const 注意常量的类型自动推导不能使用 := 而且常量定义完了可以不使用，变量必须使用 多个变/常量的声明123456789101112131415161718192021222324252627282930313233343536373839package mainimport \"fmt\"func main() &#123; a, b := 10, 10.25 fmt.Printf(\"a = %d, b = %f \\n\", a, b) fmt.Println(\"b =\", b) //一次声明多个变量 var ( c int d float64 ) c = 10 d = 99.99 fmt.Printf(\"c = %d, d = %f \\n\", c, d) //一次声明多个变量并赋值 var ( e int = 10 f float64 = 30.00 ) fmt.Printf(\"e = %d, f = %f \\n\", e, f) //一次声明多个常量 const ( g int = 10 h int = 20 ) //一次声明多个常量并自动推导类型 const ( i = 10 j = 20 ) fmt.Printf(\"g = %d, h = %d \\n\", g, h)&#125; 枚举的使用123456789101112131415package mainimport \"fmt\"func main() &#123; //iota给常量赋值使用 const ( a = iota b = iota c = iota ) fmt.Printf(\"a=%d, b=%d, c=%d\", a, b, c) //a=0, b=1, c=2&#125; iota 是一个常量自动生成器，每隔一行自动加一 iota遇到const，则重置为0 123456789101112131415161718192021222324252627282930313233343536package mainimport ( \"fmt\")func main() &#123; const ( a = iota b = iota c = iota ) fmt.Printf(\"a=%d, b=%d, c=%d\\n\", a, b, c) // a=0, b=1, c=2 //iota遇到const重置为0 const d = iota fmt.Printf(\"d=%d\\n\", d) // d=0 //如果是同一行，值都一样 const e, f, g = iota, iota, iota fmt.Printf(\"e=%d, f=%d, g=%d\\n\", e, f, g) // e=0, f=0, g=0 const ( h, i, j = iota, iota, iota ) fmt.Printf(\"h=%d, i=%d, j=%d\\n\", h, i, j) // h=0, i=0, j=0 const ( k = iota l, m = iota, iota n = iota ) fmt.Printf(\"k=%d, l=%d, m=%d, n=%d\\n\", k, l, m, n) // k=0, l=1, m=1, n=2&#125; golang基本数据类型 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package mainimport \"fmt\"func main() &#123; //-----bool类型---------- var a = false fmt.Println(a) //false b := true fmt.Println(b) //true var c bool fmt.Println(c) //false //-----浮点类型---------- var d = 3.14 //这样赋值默认是float64 fmt.Println(d) fmt.Printf(\"%T\\n\", d) var e float32 fmt.Println(e) fmt.Printf(\"%T\\n\", e) f := 10.00 //这样赋值默认是float64 fmt.Println(f) fmt.Printf(\"%T\\n\", f) //-----字符类型---------- var a1 byte a1 = 'a' fmt.Println(a1) //97 fmt.Printf(\"a1=%d, a1=%c\\n\", a1, a1) //a1=97, a1=a fmt.Printf(\"%T\\n\", a1) b1 := 'A' fmt.Printf(\"b1 = %c\\n\", b1) fmt.Printf(\"b1 = %c\\n\", b1 + ('a'-'A')) //-----字符串类型---------- a2 := \"Tim\" fmt.Println(a2) // len()测字符串长度 fmt.Println(len(a2)) //打印字符串中的某一个字符 fmt.Printf(\"a2[2] = %c\\n\", a2[2]) //a2[2] = m //fmt.Printf(\"a2[3] = %c\", a2[3]) error 越界 //-----复数类型---------- a3 := 2.1 + 3i fmt.Println(a3) //(2.1+3i) var b3 complex128 = 2.5 + 3i fmt.Println(b3) //(2.5+3i) //通过内建函数取实部和虚部 fmt.Println(\"实部 real(b3) =\", real(b3), \"虚部 imag(b3) =\", imag(b3)) //实部 real(b3) = 2.5 虚部 imag(b3) = 3&#125; 格式化输出 键盘输入1234567891011package mainimport \"fmt\"func main() &#123; var name string; fmt.Scanf(\"%s\", &amp;name) //手动输入格式 fmt.Scan(&amp;name) //自动匹配格式 fmt.Printf(\"name=%s\\n\", name)&#125; 类型转换Go语言中不允许隐式转换，所有类型转换必须显式声明，而且转换只能发生在两种相互兼容的类型之间。 12345678910111213package mainimport \"fmt\"func main() &#123; var a byte = 'A' //类型转换 var b int = int(a) fmt.Printf(\"%T\\n\", a) //uint8 fmt.Printf(\"%T\\n\", b) //int&#125; 类型别名12345678910111213141516171819202122package mainimport \"fmt\"func main() &#123; type bigint int64 var a bigint = 100 fmt.Printf(\"%T\\n\", a) //main.bigint //一次取多个别名 type ( long int64 char byte ) var b char = 'A' var c long = 100 fmt.Printf(\"b=%c, type=%T\\n\", b, b) //b=A, type=main.char fmt.Printf(\"c=%d, type=%T\\n\", c, c) //c=100, type=main.long&#125; golang的运算符和c语言一样，*取值，&amp;取地址 在go语言中，一元运算符拥有最高的优先级，二元运算符的运算方向均是从左至右。 golang流程控制if if..else1234567891011121314151617181920212223242526272829303132333435363738package mainimport \"fmt\"func main() &#123; //简单的if语句判断 var name string name = \"ABC\" if name == \"ABC\" &#123; fmt.Println(\"相等\") &#125; //if支持一个初始化语句 if a := 10; a == 10&#123; fmt.Println(\"a==10\") &#125; //if多分支 name = \"AAA\" if name == \"ABC\" &#123; fmt.Println(\"相等\") &#125;else &#123; fmt.Println(\"不相等\") &#125; name = \"CCC\" if name == \"ABC\" &#123; fmt.Println(\"name=ABC\") &#125;else if name == \"AAA\" &#123; fmt.Println(\"name=AAA\") &#125;else if name == \"BBB\" &#123; fmt.Println(\"name=BBB\") &#125;else &#123; fmt.Println(\"Other\") &#125;&#125; switch1234567891011121314151617package mainimport \"fmt\"func main() &#123; a := 12 switch a &#123; case 10: fmt.Println(\"a=10\") case 20: fmt.Println(\"a=20\") case 30: fmt.Println(\"a=30\") default: fmt.Println(\"Default\") &#125;&#125; 可以看出没有写break，go语言保留了break关键字，不写break，默认也包含break fallthrough 关键字的作用：不跳出switch，还要执行紧随其后的一个分支 12345678910111213func main() &#123; //同样的，switch也是支持一个初始化语句的 switch a := 10; a &#123; case 10: fmt.Println(\"a=10\") case 20: fmt.Println(\"a=20\") case 30: fmt.Println(\"a=30\") default: fmt.Println(\"Default\") &#125;&#125; switch可以没有条件 1234567891011121314151617package mainimport \"fmt\"func main() &#123; var score int //switch可以没有条件 switch &#123; case score &gt; 90: fmt.Println(\"优秀\") case score &gt; 60 &amp;&amp; score &lt;= 90: fmt.Println(\"及格\") default: fmt.Println(\"不及格\") &#125;&#125; for12345678910111213141516package mainimport \"fmt\"func main() &#123; num := 0 for i:=1; i&lt;= 100; i++&#123; num += i &#125; fmt.Printf(\"num=%d\\n\", num) //5050 //死循环的写法 for&#123; //TDDO... &#125;&#125; range关键字range 会返回两个值，第一个返回值是元素的数组下标，第二个返回值是元素的值: 支持string、array、slice、map 12345678910111213141516171819202122232425package mainimport \"fmt\"func main() &#123; str := \"GoLand\" //01 传统写法 for i:=0; i&lt;len(str); i++&#123; fmt.Printf(\"%c \",str[i]) &#125; fmt.Println() //02 迭代写法 for i := range str&#123; fmt.Printf(\"%c \",str[i]) &#125; fmt.Println() //range返回两个值，一个是index、一个是元素本身 //支持string、array、slice、map for i, data := range str&#123; fmt.Printf(\"%d-%c\\n\",i, data) &#125;&#125; goto和C语言的一样的：","updated":"2020-03-13T03:06:29.655Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://zouchanglin.cn/tags/Golang/"}]},{"title":"CentOS7编译安装nginx","date":"2020-01-29T10:09:30.000Z","path":"2020/01/29/CentOS7编译安装Nginx/","text":"上图是Nginx成功安装后的访问图，下面记录一下如何编译安装Nginx，避免在安装时各种缺少依赖库的情况！ 安装Nginx1、安装gcc编译器 1yum -y install gcc 2、nginx的http模块使用pcre来解析正则表达式，所以需要安装pcre库 1yum install -y pcre pcre-devel 3、 nginx使用zlib对http包的内容进行gzip，所以需要安装zlib 1yum install -y zlib zlib-devel 4、openssl是nginx的https模块需要的，所以需要安装openssl 1yum install -y openssl openssl-devel 5、下载Nginx源码包 1wget http://nginx.org/download/nginx-1.9.9.tar.gz 6、把压缩包解压缩到 /usr/local下 12tar -zxvf nginx-1.9.9.tar.gzmv nginx-1.9.9 /usr/local/ 7、进入/usr/local/nginx-1.9.9 1cd /usr/local/nginx-1.9.9 8、编译安装 12345./configure make make install 9、安装完成不想配置环境变量的话可以建立软链接 1ln -s /usr/local/nginx/sbin/nginx /usr/local/bin/nginx 10、配置文件的修改 12cp /usr/local/nginx/conf/nginx.conf /usr/local/nginx/conf/nginx.conf.backvim /usr/local/nginx/conf/nginx.conf 启动/停止1、启动 1nginx 2、 快速停止 1nginx -s stop 3、正常停止 1nginx -s quit 4、重新加载配置文件 1nginx -s reload 配置HTTPS上面讲述了如何编译安装Nginx，要想用HTTPS， 我们只需要在原有的基础上添加ssl模块就行了 来到解压目录 1cd &#x2F;usr&#x2F;local&#x2F;nginx-1.9.9 123./configure --with-http_ssl_modulemake 停止Nginx 1nginx -s stop 把之前的nginx先备份一下，然后把新的程序复制过去覆盖之前的即可 123cp &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx.bakcp objs&#x2F;nginx &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx 接下来配置证书 解压后得到两个文件，放在服务器的任意目录 接下来就是修改Nginx的配置文件 1234567891011121314151617181920212223242526272829303132333435363738#user nobody;user root;worker_processes 2;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name zouchanglin.cn; return 301 https://zouchanglin.cn; &#125; # HTTPS server server &#123; listen 443; server_name zouchanglin.cn; ssl on; ssl_certificate /root/nginx.crt; ssl_certificate_key /root/nginx.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / &#123; root /root/hexo/public; index index.html index.htm; &#125; &#125;&#125; 注意 ssl on; 这个配置一定要加上！","updated":"2020-03-13T03:06:29.636Z","categories":[{"name":"高性能服务器","slug":"高性能服务器","permalink":"https://zouchanglin.cn/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://zouchanglin.cn/tags/Nginx/"}]},{"title":"解决跨域问题","date":"2020-01-28T05:25:09.000Z","path":"2020/01/28/解决跨域问题/","text":"0、浏览器跨域问题看到浏览器报Access-Control-Allow-Origin毫无疑问了，看来是跨域问题 CORS是一种访问机制，英文全称是Cross-Origin Resource Sharing，即我们常说的跨域资源共享，通过在服务器端设置响应头，把发起跨域的原始域名添加到Access-Control-Allow-Origin 即可 浏览器端：发送AJAX请求前需设置通信对象XHR的withCredentials 属性为true。 服务器端：设置Access-Control-Allow-Credentials为true。 两个条件缺一不可，否则即使服务器同意发送Cookie！ Access-Control-Allow-Methods 表示允许哪些跨域请求的提交方式。（例如GET/POST） Access-Control-Allow-Headers 表示跨域请求的头部的允许范围。 Access-Control-Expose-Headers 表示允许暴露哪些头部信息给客户端。 Access-Control-Max-Age 表示预检请求的最大缓存时间。 1、返回新的CorsFilter这是一种全局跨域的方式 在任意配置类，返回一个新的CorsFilter Bean，并添加映射路径和具体的CORS配置信息。 123456789101112131415161718192021222324252627package edu.xpu.cors.config;@Configurationpublic class GlobalCorsConfig &#123; @Bean public CorsFilter corsFilter() &#123; //1.添加CORS配置信息 CorsConfiguration config = new CorsConfiguration(); //放行哪些原始域 config.addAllowedOrigin(\"*\"); //是否发送Cookie信息 config.setAllowCredentials(true); //放行哪些原始域(请求方式) config.addAllowedMethod(\"*\"); //放行哪些原始域(头部信息) config.addAllowedHeader(\"*\"); //暴露哪些头部信息（因为跨域访问默认不能获取全部头部信息） config.addExposedHeader(\"*\"); //2.添加映射路径 UrlBasedCorsConfigurationSource configSource = new UrlBasedCorsConfigurationSource(); configSource.registerCorsConfiguration(\"/**\", config); //3.返回新的CorsFilter. return new CorsFilter(configSource); &#125;&#125; 2、重写WebMvcConfigurer在任意配置类，返回一个新的WebMvcConfigurer Bean，并重写其提供的跨域请求处理的接口，目的是添加映射路径和具体的CORS配置信息。 1234567891011121314151617181920212223242526package edu.xpu.cors.config;@Configurationpublic class GlobalCorsConfig &#123; @Bean public WebMvcConfigurer corsConfigurer() &#123; return new WebMvcConfigurer() &#123; @Override //重写父类提供的跨域请求处理的接口 public void addCorsMappings(CorsRegistry registry) &#123; //添加映射路径 registry.addMapping(\"/**\") //放行哪些原始域 .allowedOrigins(\"*\") //是否发送Cookie信息 .allowCredentials(true) //放行哪些原始域(请求方式) .allowedMethods(\"GET\",\"POST\", \"PUT\", \"DELETE\") //放行哪些原始域(头部信息) .allowedHeaders(\"*\") //暴露哪些头部信息（因为跨域访问默认不能获取全部头部信息） .exposedHeaders(\"Header1\", \"Header2\"); &#125; &#125;; &#125;&#125;","updated":"2020-03-13T03:06:29.765Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://zouchanglin.cn/tags/SpringBoot/"}]},{"title":"计算机网络常见面试题","date":"2020-01-25T11:01:35.000Z","path":"2020/01/25/计算机网络常见面试题/","text":"这篇文章主要是谈谈计算机网络常见的面试个考察点，记录一下，里面有很多问题之前面试也遇到过，特此记录一下！ 0、讲一下TCP三次握手0、TCP三次握手是一个TCP连接的建立过程，为了确认双方的收发能力正常，并初始化seq，为后续传输做准备 1、首先Client处于Closed状态，Server处于Listen状态，Client向Server发了SYN=1，seq=x，Server收到后确认了Server的接收能力正常，Client的发送能力正常； 2、Server回复SYN=1，ACK=1，ack=x+1，seq=y，Client收到后做出判断：Client和Server的收发能力都正常， 3、接着Client向Server发ACK=1，seq=x+1，ack=y+1，这就是第三次握手，可以携带数据了，Server收到后便可以确认Client接收能力正常，Server发送能力正常，并把链接从半连接队列移动到全连接队列 1、seq是固定的吗seq是随时间变化的，每个连接都有不同的seq，seq是一个32byte的计数器，每4ms加一，因此seq是动态生成的，为后续传输做准备 2、三次握手可以携带数据吗只有第三次可以携带数据，如果第一次就能携带数据的话服务器会花很多时间对数据进行存储，占用IO资源，非常危险，对于第三次握手来说Client已经知道双方的收发能力正常，所以携带数据也是没问题的 3、什么是半连接队列第一次握手后，Server发出ACK报文，等待Client向自己发送ACK报文，此时就会把请求放在半连接队列，半连接队列满了就会丢弃报文 4、谈谈SYN攻击Client向Server发SYN=1，seq=x，收到Server发来的ACK不回复ACK，或者直接丢弃Server的ACK报文，导致半连接队列满了，丢弃了其他真实需求的请求报文，导致服务器无法正常提供服务 5、第三次握手丢失，会怎样服务端会重传，如果收到就会停止，尝试时间间隔为1s、2s、4s、8s、16s、32s、64s，Linux最多尝试64s，之后若还是无应答就会移出半连接队列 6、Linux如何判断受到了SYN攻击1netstat -n -p TCP|grep SYN_RECV 7、Linux查看网络状态1netstat -n | awk '/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;' 它会显示例如下面的信息： TIME_WAIT 814CLOSE_WAIT 1FIN_WAIT1 1ESTABLISHED 634SYN_RECV 2LAST_ACK 1 ESTABLISHED 表示正在通信，TIME_WAIT 表示主动关闭，CLOSE_WAIT 表示被动关闭。 8、讲一下TCP四次挥手TCP四次挥手用户双方断开连接，TCP四次挥手由任意一方发起，这里假设A主机和B主机： 1、A主动发起释放连接请求，FIN=1，seq=u 2、B收到A的连接释放报文后，立即回复A主机一个ACK报文（ACK=1，ack=u+1，seq=v），表示自己收到了A主机的连接释放报文 3、B主机此时会通知ApplicationA主机要释放连接了，Application执行资源释放代码，完毕后回复A主机连接释放报文，ACK=1，FIN=1，ack=u+1，seq=w 4、A主机回复B主机连接释放报文，ACK=1，ack=w+1，seq=u+1，并且等待2MSL时间后关闭，B主机在收到A主机的ACK报文后也会关闭 9、为什么会有四次挥手因为B主机收到A主机连接释放报文后不会立即关闭Socket，只能先回复一个ACK报文，告诉A主机你的连接释放请求报文我已经收到了，等到B主机全部发完报文后，B主机才会向A主机发送FIN报文 10、什么是2MSL的等待时间MSL是TCP报文段的最大生存时间，2MSL是报文段在AB主机之间的往返时间，为了最后的ACK报文能到达B主机，B主机才能正常关闭连接。 如果B主机未收到A主机发的最后的ACK报文，B主机会重发FIN报文，一来一去正好是两个MSL；还有2MSL的等待时间也是为了避免新旧连接混淆，因为经过2MSL，上一次连接中所有的重复包都会消失 11、服务器出现大量CLOSE_WAIT的原因最有可能的就是资源释放代码有BUG，没有正确发出ACK=1、FIN=1的报文，导致大量连接不能正常关闭，也就是出现大量CLOSE_WAIT的原因 12、服务器出现大量TIME_WAIT的原因一些爬虫服务器（如果网管在安装的时候没有做内核参数优化的话）上经常会遇到这个问题 ，TIME_WAIT是主动关闭连接的一方保持的状态，对于爬虫服务器来说他本身就是“客户端”，在完成一个爬取任务之后，他就 会发起主动关闭连接，从而进入TIME_WAIT的状态，然后在保持这个状态2MSL（max segment lifetime）时间之后，彻底关闭回收资源。 解决思路很简单，就是让服务器能够快速回收和重用那些TIME_WAIT的资源，vim /etc/sysctl.conf 12345678910111213141516171819202122#对于一个新建连接，内核要发送多少个 SYN 连接请求才决定放弃,不应该大于255，默认值是5，对应于180秒左右时间 net.ipv4.tcp_syn_retries=2 #net.ipv4.tcp_synack_retries=2 #表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为300秒 net.ipv4.tcp_keepalive_time=1200 net.ipv4.tcp_orphan_retries=3 #表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间 net.ipv4.tcp_fin_timeout=30 #表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。 net.ipv4.tcp_max_syn_backlog = 4096 #表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭 net.ipv4.tcp_syncookies = 1 #表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭 net.ipv4.tcp_tw_reuse = 1 #表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭 net.ipv4.tcp_tw_recycle = 1 ##减少超时前的探测次数 net.ipv4.tcp_keepalive_probes=5 ##优化网络设备接收队列 net.core.netdev_max_backlog=3000 修改完之后执行/sbin/sysctl -p让参数生效。 12、TCP和UDP的区别UDP的主要特点是：（1）无连接；（2）尽最大努力交付；不保证可靠交付（3）面向报文，不对报文拆分或者合并；（4）无拥塞控制；（5）支持一对一、一对多、多对一和多对多的交互通信；（6）首部开销小（只有四个字段：源端口、目的端口、长度、检验和，即8个字节）。 TCP的主要特点是：（1）面向连接；（2）每一条TCP连接只能是点对点的（一对一）；（3）提供可靠交付的服务；（4）提供全双工通信；（5）面向字节流。 13、GET请求和POST请求的区别HTTP报文层面：GET将请求信息放在URL、POST放在报文体中。GET请求URL长度受浏览器限制， HTTP协议本身对URL长度并没有做任何规定。 数据层层面：GET符合幂等性和安全性， 反复读取不应该对访问的数据有副作用 ；POST不符合幂等性和安全性 其他层面：GET可以被缓存POST不幂等也就意味着不能随意多次执行，因此也就不能缓存。 GET和POST还有一个重大区别，简单的说：GET产生一个TCP数据包；POST产生两个TCP数据包。对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200（返回数据）； 而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok（返回数据）。 14、Session的两种实现方式1、使用Cookie来实现，也就是以SESSIONID为Cookie的Key，以SESSIONID的值为Cookie的Value，每次请求的时候都会通过Cookie机制携带上这个SESSIONID 2、如果客户端禁止使用Cookie，那么就使用URL回写来实现，就在参数 url 中加入 Session ID 信息，然后返回修改后的 url ， 通过这种机制，url中保存了sessionId，然后点击URL时又回传到服务器，来维持身份。 15、Cookie和Session的区别Cookie数据存放在客户的浏览器上，Session数据放在服务器上 Session相对Cookie更安全 如果考虑减轻服务器负担，应该使用Cookie 16、HTTPS真的很安全吗浏览器默认填充http://，请求需要进行跳转，又被劫持的风险，如果一开始输入的地址的https://… 那么这样还是可以保证安全性的，可以使用HSTS（HTTP Strict Transport Security）优化，目前还未开始推行 17、UDP如何实现可靠传输UDP要想可靠，就要接收方收到UDP之后回复个确认包，发送方有个机制，收不到确认包就要重新发送，每个包有递增的序号，接收方发现中间丢了包就要发重传请求，当网络太差时候频繁丢包，防止越丢包越重传的恶性循环，要有个发送窗口的限制，发送窗口的大小根据网络传输情况调整，调整算法要有一定自适应性。 恭喜你， 你在应用层重新实现了TCP！ 18、ARP地址解析协议，工作原理《NAT技术与ARP协议-ARP协议》 ARP不是一个单纯的数据链路层的协议，而是一个介于数据链路层和网络层之间的协议。ARP协议的作用：ARP协议建立了主机IP地址和MAC地址的映射关系。其实就是喊话的方式，大家都听见了，但是只有符合IP的主机才会回发自己的MAC地址。 19、 ICMP协议ICMP是InternetControl Message Protocol，因特网控制报文协议。它是TCP/IP协议族的一个子协议，用于在IP主机、路由器之间传递控制消息。控制消息是指网络通不通、主机是否可达、路由器是否可用等网络本身的消息。这些控制消息虽然并不传输用户数据，但是对于用户数据的传递起着重要的作用。ICMP报文有两种：差错报告报文和询问报文。 详情见《辅助IP的ICMP》 20、TTL是什么？作用是什么？TTL是指生存时间，简单来说，它表示了数据包在网络中的时间，经过一个路由器后TTL就减一，这样TTL最终会减为0，当TTL为0时，则将数据包丢弃，这样也就是因为两个路由器之间可能形成环，如果没有TTL的限制，则数据包将会在这个环上一直死转，由于有了TTL，最终TTL为0后，则将数据包丢弃。 21、TCP流量控制、拥塞控制关于TCP的流量控制、拥塞控制见《TCP的高性能机制》，点击链接 22、谈谈OSI七层模型与TCP/IP五层模型","updated":"2020-03-13T03:06:29.766Z","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"高效的Linux系统编程环境","date":"2019-12-13T12:01:35.000Z","path":"2019/12/13/高效的Linux系统编程环境/","text":"因为笔者的电脑是Windows，所以要进行Linux系统编程不得不准备一台虚拟机，当然子系统Ubuntu也是可以的，因为昨天解决了一个虚拟机固定IP的问题，《CentOS7虚拟机设置固定IP》所以一旦连接稳定，那么就可以开始开发了，哈哈，今天主要是通过Clion这个工具（和IDEA是亲兄弟）来进行高效的Linux系统编程环境搭建！ 一、准备Linux环境只要有一台装了Linux的虚拟机就可以，我使用的是CentOS7 Server版，所以，平时使用起来也是直接拿Xshell一连接就可以使用了 二、在Linux下准备开发工具Cmake 3.x 以上 + gdb 7.8.x 以上 + gcc + gcc-g++ 首先卸载预装Cmake 2 1yum remove cmake 安装必要环境 1yum install -y gcc g++ gcc-c++ make automake texinfo wget 安装Cmake 12345678910wget https://cmake.org/files/v3.14/cmake-3.14.0.tar.gztar -xf cmake-3.14.0.tar.gzcd cmake-3.14.0./configuremakemake install# 安装好后的Cmake位于 /usr/local/share/cmake-3.13/，可执行程序位于/usr/local/bin/cmake# 为了让CLion能够自动识别，构建软连接ln -s /usr/local/bin/cmake /usr/bin/cmake 如果wget很慢，推荐使用我的 1wget https://img.zouchanglin.cn/cmake-3.14.0.tar.gz 安装gdb 1234567# 首先安装依赖，termcapwget https://ftp.gnu.org/gnu/termcap/termcap-1.3.1.tar.gztar -xf termcap-1.3.1.tar.gzcd termcap-1.3.1./configuremakemake install 如果安装的依赖包下载很慢，推荐用我的 1wget https://img.zouchanglin.cn/termcap-1.3.1.tar.gz 如果依赖包安装完成，开始下一步 1234567891011121314# 卸载预装gdbyum remove -y gdb# 安装gbdwget http://mirrors.ustc.edu.cn/gnu/gdb/gdb-7.9.tar.xztar -xf gdb-7.9.tar.xzcd gdb-7.9./configuremakemake install# gdb将被安装到/usr/local/share/gdb目录，可执行程序位于/usr/local/bin/gdb# 为了让CLion能够自动识别，构建软连接。ln -s /usr/local/bin/gdb /usr/bin/gdb 如果gdb下载很慢，推荐用我的 1wget https://img.zouchanglin.cn/gdb-7.9.tar.xz 三、CLion配置remote主机 四、愉快的写代码吧 自动同步代码，超人性化提示，唯一的缺点就是对于不懂CMakeList的同学就Over了，下面复习一下CMakeList 五、CMakeList构建项目cmake 是一个跨平台、开源的构建系统。它是一个集软件构建、测试、打包于一身的软件。它使用与平台和编译器独立的配置文件来对软件编译过程进行控制。 在Windows下也可以使用Cmake进行大型项目的构建，比如我之前编译过OpenCV的源码，就是通过VisualStudio的编译器配合Cmake来完成的！ 1. 指定 cmake 的最小版本1cmake_minimum_required(VERSION 3.4.1) 这行命令是可选的，我们可以不写这句话，但在有些情况下，如果 CMakeLists.txt 文件中使用了一些高版本 cmake 特有的一些命令的时候，就需要加上这样一行，提醒用户升级到该版本之后再执行 cmake。 2. 设置项目名称1project(demo) 这个命令不是强制性的，但最好都加上。它会引入两个变量 demo_BINARY_DIR 和 demo_SOURCE_DIR，同时，cmake 自动定义了两个等价的变量 PROJECT_BINARY_DIR 和 PROJECT_SOURCE_DIR。 3. 设置编译类型123add_executable(demo demo.cpp) # 生成可执行文件add_library(common STATIC util.cpp) # 生成静态库add_library(common SHARED util.cpp) # 生成动态库或共享库 add_library 默认生成是静态库，通过以上命令生成文件名字， 在 Linux 下是：demolibcommon.alibcommon.so 在 Windows 下是：demo.execommon.libcommon.dll 4. 指定编译包含的源文件4.1 明确指定包含哪些源文件 1add_library(demo demo.cpp test.cpp util.cpp) 4.2 搜索所有的 cpp 文件 aux_source_directory(dir VAR) 发现一个目录下所有的源代码文件并将列表存储在一个变量中。 12aux_source_directory(. SRC_LIST) # 搜索当前目录下的所有.cpp文件add_library(demo $&#123;SRC_LIST&#125;) 自定义搜索规则 12345678910file(GLOB SRC_LIST \"*.cpp\" \"protocol/*.cpp\")add_library(demo $&#123;SRC_LIST&#125;)# 或者file(GLOB SRC_LIST \"*.cpp\")file(GLOB SRC_PROTOCOL_LIST \"protocol/*.cpp\")add_library(demo $&#123;SRC_LIST&#125; $&#123;SRC_PROTOCOL_LIST&#125;)# 或者aux_source_directory(. SRC_LIST)aux_source_directory(protocol SRC_PROTOCOL_LIST)add_library(demo $&#123;SRC_LIST&#125; $&#123;SRC_PROTOCOL_LIST&#125;) 5. 查找指定的库文件 find_library(VAR name path)查找到指定的预编译库，并将它的路径存储在变量中。默认的搜索路径为 cmake 包含的系统库，因此如果是 NDK 的公共库只需要指定库的 name 即可 123456find_library( # Sets the name of the path variable. log-lib # Specifies the name of the NDK library that # you want CMake to locate. log ) 6. 设置包含的目录12345include_directories( $&#123;CMAKE_CURRENT_SOURCE_DIR&#125; $&#123;CMAKE_CURRENT_BINARY_DIR&#125; $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/include) Linux 下还可以通过如下方式设置包含的目录 1set(CMAKE_CXX_FLAGS \"$&#123;CMAKE_CXX_FLAGS&#125; -I$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;\") 7. 设置链接库搜索目录123link_directories( $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/libs) Linux 下还可以通过如下方式设置包含的目录 1set(CMAKE_CXX_FLAGS \"$&#123;CMAKE_CXX_FLAGS&#125; -L$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/libs\") 8. 设置 target 需要链接的库123456target_link_libraries( # 目标库 demo # 目标库需要链接的库 # log-lib 是上面 find_library 指定的变量名 $&#123;log-lib&#125; ) 在 Windows 下，系统会根据链接库目录，搜索xxx.lib 文件，Linux 下会搜索 xxx.so 或者 xxx.a 文件，如果都存在会优先链接动态库（so 后缀） 8.1 指定链接动态库或静态库 12target_link_libraries(demo libface.a) # 链接libface.atarget_link_libraries(demo libface.so) # 链接libface.so 8.2 指定全路径 12target_link_libraries(demo $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/libs/libface.a)target_link_libraries(demo $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/libs/libface.so) 8.3 指定链接多个库 12345target_link_libraries(demo $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/libs/libface.a boost_system.a boost_thread pthread) 9. 设置变量9.1 set 直接设置变量的值 12set(SRC_LIST main.cpp test.cpp)add_executable(demo $&#123;SRC_LIST&#125;) 9.2 set 追加设置变量的值 123set(SRC_LIST main.cpp)set(SRC_LIST $&#123;SRC_LIST&#125; test.cpp)add_executable(demo $&#123;SRC_LIST&#125;) 9.3 list 追加或者删除变量的值 1234set(SRC_LIST main.cpp)list(APPEND SRC_LIST test.cpp)list(REMOVE_ITEM SRC_LIST main.cpp)add_executable(demo $&#123;SRC_LIST&#125;) 六、CMake项目示例1、简单项目新建文件 main.c，内容如下，新建文件 CMakeLists.txt（命名必须是 CMakeLists.txt，注意大小写） 一般我们采用 cmake 的 out-of-source 方式来构建（即生成的中间产物和源代码分离），这样做可以让生成的文件和源文件不会弄混，且目录结构看起来也会清晰明了。所以推荐使用这种方式，至于这个文件夹的命名并无限制，我们习惯命名为 build。 我们进入build文件夹，执行cmake..， .. 表示上一级目录，cmake 会在上一级目录下找到 CMakeLists.txt 文件并编译，并生成如下图所示的一些中间文件 直接执行 make命令，生成可执行程序，如下图： 2、复杂项目 CMakeList.txt 123456789cmake_minimum_required (VERSION 2.8)project(demo)aux_source_directory(. DIR_SRCS)# 添加math子目录add_subdirectory(math)# 指定生成目标add_executable(demo $&#123;DIR_SRCS&#125;)# 添加链接库target_link_libraries(demo MathFunctions) main.c 12345678#include &lt;stdio.h&gt;#include \"math/MathFunctions.h\"int main() &#123; printf(\"Hello World!\\n\"); printf(\"Add(1,2) ret = %d\\n\", add(1,2)); return 0;&#125; math里的CMakeList.txt 123aux_source_directory(. DIR_LIB_SRCS)# 生成链接库add_library(MathFunctions $&#123;DIR_LIB_SRCS&#125;) math里的MathFunctions.h 和MathFunctions.c 123456#ifndef define _MATHFUNCTIONS_H__#define _MATHFUNCTIONS_H__int add(int num, int num2);#endif //!define _MATHFUNCTIONS_H__ 123456#include &lt;stdio.h&gt;#include \"MathFunctions.h\"int add(int num, int num2)&#123; return num + num2;&#125; 同样的也只需要进入build目录，进行cmake ..","updated":"2020-04-14T03:00:20.566Z","categories":[{"name":"开发工具","slug":"开发工具","permalink":"https://zouchanglin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"},{"name":"IDEA","slug":"IDEA","permalink":"https://zouchanglin.cn/tags/IDEA/"}]},{"title":"CentOS7虚拟机设置固定IP","date":"2019-12-12T12:01:35.000Z","path":"2019/12/12/CentOS7虚拟机设置固定IP/","text":"虚拟机设置为固定IP，不管主机在什么网络环境下，是断网环境，还是连接任何网段访问外网的环境下，虚拟机的IP都固定不变，而且使用终端连接，始终不变，正常连接； 虚拟机的固定IP可以按照自己想设置的IP地址网段随意设置，比如我就想设置固定IP为192.168.2.2； 经过查询资料，终于得出一个完美解决方案：采用方式为NAT模式+固定IP的模式 。 配置环境说明：主机为Win10专业版，虚拟机为VMware Workstation 15 Pro，虚拟机中的Linux系统为CentOS 7.3_64位。 1、虚拟机设置NAT模式无论有几个网络适配器都设置成NAT模式！ 2、配置虚拟机NAT模式参数修改子网IP设置，实现自由设置固定IP，若你想设置固定IP为192.168.2.2-255，比如192.168.2.2，则子网IP为192.168.2.0；建议不要用：192.168.1.2-255，亲测1网段无法成功 3、配置物理主机VMnet8参数其中的IP地址随意设置，但是要保证不能跟你要设置虚拟机的固定IP一样。 4、配置Centos网络配置文件 5、重启CentOSreboot，ping一下baidu.com试试，SSH连接一下试试！","updated":"2020-04-14T03:01:15.596Z","categories":[{"name":"开发工具","slug":"开发工具","permalink":"https://zouchanglin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"重新认识volatile","date":"2019-12-11T10:09:30.000Z","path":"2019/12/11/重新认识volatile/","text":"1、CPU多核心缓存架构分析并发编程：肯定是为了更合理充分的利用多核CPU架构的性能 CPU 主频率远远高于主存，所以引入CPU缓存，CPU加载数据无法绕过缓存，而且对于多核CPU，一级缓存不是共享的，二级和三级缓存是线程共享的！CPU计算的数据并不是直接从主存中去拿，而且通过层层在缓存中寻找。 下图是一个单CPU多核心的架构图： 2、CPU缓存一致性协议先看这样一段代码： 加载过程就是ThreadA通过read指令读取到flag，在通过load指令加载到缓存，也就是JMM中的本地内存，CPU再通过寄存器去使用flag，ThreadB也是同样的流程，通过assign指令为本地内存中的flag赋值，再写入主存： 由于while(true)并没有释放时间片，所以在这里可以让它去进行上下文切换，则就会有时间清除缓存 两次的执行结果肯定很简单 什么情况下的上下文切换不会清除缓存呢？可以设置一个非常小的休眠时间 这个时候，设置500纳秒的Sleep和500000纳秒的等待会导致结果完全不一样！！ 这与CPU多核心架构有关系，CPU修改之后的值并不会立即刷新到主存，这便导致了缓存不一致的问题，这是属于CPU架构的问题，不仅仅存在于JVM层面，只要是这种的CPU架构都会出现这种问题，所以首先得靠加锁来解决这种问题，在早期有一种东西叫做总线锁： 一旦发生数据修改的回写操作，直接把总线加锁，这样别的线程在使用数据的时候就不得不重新从主存得到新的数据，而且存在严重的性能问题，如果存在大量IO操作时，还使用这种总线锁，那么肯定IO性能肯定会下降！ 于是出现了缓存一致性协议： 主要用到的缓存一致性协议时MESI协议（M修改、E独占、S共享、I无效四种状态，这四种状态记录的是缓存行的转态）。 这里还有一个机制叫做总线嗅探机制，嗅探通过总线的数据，如果只有一个核心用到，那么就会给这个缓存行一个状态叫做E状态（独占状态），当另一个线程也用到了这个变量的时候，CPU就会通过广播机制通知其他的核心把这个缓存行的状态修改为S状态（共享状态），接下来其中一个线程对这个变量进行了修改，那么对于这个线程来说，这个变量的缓存行变成了M状态（修改状态），回写的时候会通过总线，总线嗅探机制嗅探到这个变量的缓存行已经是M状态，它就会通过其他的核心，说缓存无效，则其他核心上的缓存行就会变成I状态（无效状态），变成无效状态后如果还需要使用这个变量，那么肯定只能重新从主存中去加载这个变量到缓存，而且必须等待修改核心修改（回写主存）完成！ 这里锁的缓存行最大值为 64 byte ，总线锁主要是用到了lock原语 3、内存模型JMM实现原理JMM描述的是一种抽象的概念，一组规则，通过这组规则控制程序中各个变量在共享数据区域和私有数据区域的访问方式，JMM是围绕原子性、有序性、可见性展开的 这个JMM模型只是一个抽象的概念，图上画出的也只是逻辑空间，通过对应到硬件内存架构是这样的： 4、Volatile关键字原理剖析在上面的例子中，我们明显可以通过volatile关键字来解决这个问题，那么volatile又是如何实现的呢？ 通过汇编指令来看看就知道了，运行时加虚拟机运行参数： 1-XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly -Xcomp 但是Mac环境可能会由于缺少部分包：hsdis-amd64.dylib，在这里下载就好了 https://github.com/evolvedmicrobe/benchmarks/blob/master/hsdis-amd64.dylib，下载完毕后放在：/Library/Java/JavaVirtualMachines/jdk1.8.0_231.jdk/Contents/Home/jre 就好了 先看看如何保证可见性的： 底层实现了 lock addl $0x0,(%rsp) ，触发了缓存一致性协议 什么是重排序呢？是指编译器生成了指令序列，处理乱序执行！ 接下来看看指令重排序的一个例子： 打印出来（0，0）（0，1）（1，0）（1，1）都有 这就好比单例模式，请看下面这个例子： myInstance = new SingletonFactory()，这句话主要是三个指令构成的，如果不能保证指令有序性的话，那么拿到的对象就是未被初始化的对象，是无效的！ 对于x86架构的CPU，加上volatile写后面的storeLoad内存屏障，我们也可以手动加上内存屏障： 这个加上内存屏障的方法定义是 public native void storeFence(); 因此是原生实现，可以看看OpenJDK的虚拟机实现，可以看到对于64位机器使用的是rsp寄存器，对于非64位用的是esp寄存器 5、可见性、有序性、原子性详解并发编程的三大特性： 可见性、有序性、原子性 volatile保证可见性与有序性，但是不能保证原子性，要保证原子性需要借助 synchronized、Lock锁机制，同理也能保证有序性与可见性，因为 synchronized和Lock能够保证任一时刻只有个线程访问该代码块。关于volatile不保证原子性这个其实很好证明： 因为线程进行了很多次无效计算，所以结果并不是100000，他们都是从主存中拿的值，而且值都是对的，但是计算过程却不是原子的，准确的说应该是资源并没有被锁定，导致自己修改的时候别人也在修改，所以正确理解volatile的作用是很重要的！ 最后看看CAS操作对应的汇编指令：","updated":"2020-03-13T03:06:29.776Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JVM","slug":"JVM","permalink":"https://zouchanglin.cn/tags/JVM/"}]},{"title":"基于Redis实现分布式锁","date":"2019-12-08T13:57:41.000Z","path":"2019/12/08/基于Redis实现分布式锁/","text":"在并发编程中，我们通过锁，来避免由于竞争而造成的数据不一致问题。通常，我们以synchronized 、Lock来使用它。 但是Java中的锁，只能保证在同一个JVM进程内中执行。如果在分布式集群环境下呢？ 分布式锁的实现有很多，比如基于数据库、memcached、Redis、系统文件、zookeeper等。它们的核心的理念跟上面的过程大致相同。 本篇文章，主要讲如何用Redis的形式实现分布式锁。 1、秒杀示例首先，下面的例子是一个特价商品得秒杀案例，直接看代码 服务层 单个电脑上直接拿鼠标点链接肯定不会有问题，因为是串行的操作，接下用使用Apache ab工具模拟并发的情况： http://httpd.apache.org/docs/2.0/programs/ab.html 这个是它的官网，使用说明里面也有，下面直接使用它进行模拟测压： 经过测试之后发现：卖超了，意料之中的事情 为什么会出现这种情况呢？ 没有做同步呗，很正常，首先要解决的是数据同步问题 使用synchronized对方法加锁： 加了synchronized之后肯定是数据同步了： 但是速度却太慢了，无法实现细粒度控制，而且系统根本无法实现水平拓展，问题很多，下面用分布式锁来解决这些问题。 2、Redis的几个命令http://www.redis.cn/commands/setnx.html 可以先看看这个文档： 第一个命令：SETNX Design pattern: Locking with !SETNX设计模式：使用!SETNX加锁Please note that:请注意：不鼓励以下模式来实现the Redlock algorithm ，该算法实现起来有一些复杂，但是提供了更好的保证并且具有容错性。无论如何，我们保留旧的模式，因为肯定存在一些已实现的方法链接到该页面作为引用。而且，这是一个有趣的例子说明Redis命令能够被用来作为编程原语的。无论如何，即使假设一个单例的加锁原语，但是从 2.6.12 开始，可以创建一个更加简单的加锁原语，相当于使用SET命令来获取锁，并且用一个简单的 Lua 脚本来释放锁。该模式被记录在SET命令的页面中。也就是说，SETNX能够被使用并且以前也在被使用去作为一个加锁原语。例如，获取键为foo的锁，客户端可以尝试一下操作：SETNX lock.foo &lt;current Unix time + lock timeout + 1&gt;如果客户端获得锁，SETNX返回1，那么将lock.foo键的Unix时间设置为不在被认为有效的时间。客户端随后会使用DEL lock.foo去释放该锁。如果SETNX返回0，那么该键已经被其他的客户端锁定。如果这是一个非阻塞的锁，才能立刻返回给调用者，或者尝试重新获取该锁，直到成功或者过期超时。 第二个命令：GETSET 自动将key对应到value并且返回原来key对应的value。如果key存在但是对应的value不是字符串，就返回错误。 设计模式 GETSET可以和INCR一起使用实现支持重置的计数功能。举个例子：每当有事件发生的时候，一段程序都会调用INCR给key mycounter加1，但是有时我们需要获取计数器的值，并且自动将其重置为0。这可以通过GETSET mycounter “0”来实现： 其实就是先GET、再SET，所以返回SET之前的值 3、使用Redis实现分布式锁首先搞清楚锁的是什么，即搞清楚什么是进行同步和互斥的数据，对于上面的秒杀系统示例程序来说，当然是商品，而且是同样的商品需要同步与互斥，假设现在是华为Mate30 Pro的秒杀活动，大家肯定买的都是这一款产品，所以其实需要关心的就是成交订单量和库存的关系，如果不进行同步与互斥，那么肯定会出现买超的情况，限量秒杀1000台，可能实际成交订单变成了1200或更多，所以需要进行数据的同步与互斥 我直接使用Docker跑了一个Redis实例 测试通过可以使用！ pom文件引入redis-starter 写入最基础的连接配置 RedisLock实现: 在秒杀系统中使用： 在设计的时候把商品的Id设置为Key，当前时间+超时时间为Value 通过代码注释可以看清楚实现流程： 如果锁未超时也是返回False，这说明别的线程已经持有锁！ 4、Redis分布式锁的好处保证了数据的同步与互斥，通过模拟10个线程模拟500次请求的结果，对了我还统计了一下未能成功获得锁的请求次数，本次实验是469次请求未能成功获得锁，所以订单成交了31个，仓库剩余99969份！ 另外分布式锁可以很好的支持分布式部署，本例中因为为了方便直接在一个服务中写死了，所以就不演示集群似的测试了。 效率上也是很大的进步： 使用Redis分布式锁： 使用synchronized：","updated":"2020-03-13T03:06:29.733Z","categories":[{"name":"分布式理论","slug":"分布式理论","permalink":"https://zouchanglin.cn/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JVM","slug":"JVM","permalink":"https://zouchanglin.cn/tags/JVM/"},{"name":"分布式","slug":"分布式","permalink":"https://zouchanglin.cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"TCP协议基本特性","date":"2019-11-21T14:32:17.000Z","path":"2019/11/21/TCP协议基本特性/","text":"TCP协议特点1、面向连接传输控制协议Transmission Control Protocol 要对数据的传输进行一个详细的控制，TCP是面向连接(虚连接)的传输层协议，但注意此处的连接是虚连接： 2、每个TCP连接只能是点对点TCP连接是点对点的连接，而不是向广播那样的形式 3、可靠有序，不丢不重TCP提供可靠交付的服务，无差错、不丢失、不重复、按序到达。可靠有序，不丢不重，后面会写到这些功能是如何实现的 4、TCP提供全双工通信发送缓存：准备发送的数据 + 已发送但尚未收到确认的数据接收缓存：按序到达但尚未被接受应用程序读取的数据 + 不按序到达的数据 5、面向字节流TCP面向字节流，TCP把应用程序交下来的数据看成仅仅是一连串的无结构的字节流。 TCP报文格式 1、填充字段首先看看填充字段，这个字段主要是为了保证添加选项字段后TCP首部是还是4字节的整数倍（通常填充的是全零字段）。 固定首部就是20字节（图中一行就是32位，8位是一个字节，所以图中的一行就是4字节，五行就是20字节） 2、源端口和目的端口源端口和目的端口分别占用4字节，总占用8字节 3、序号序号：在一个TCP连接中传送的字节流中的每一个字节都按顺序编号，本字段表示本报文段所发送数据的第一个字节的序号。如下图，如果发的是123号字节的数据，那么TCP报文中的头部序号就是1，如果发送的是456号字节的数据，那么TCP报文中的头部序号就是4： 4、确认号确认号：期望收到对方下一一个报文段的第一个数据字节的序号。若确认号为N，则证明到序号N-1为止的所有数据都已正确收到。如下图：接收端收到数据后要反馈给发送端，还是向发送端发一个确认报文，填写自己的确认号，发送端就知道了接收端收到了那些数据，没收到的超时要重发： 5、数据偏移数据偏移(首部长度) ： TCP报文段的数据起始处距离TCP报文段的起始处有多远，以4字节为单位，即1个数值是4字节，首部长度包含固定的20字节，还有选项字段和填充字段，所以需要知道首部的长度，就需要这个字段来说明首部长度。 由于数据偏移字段只有4位的存储空间，最大为 1111 ，就是最大值为15，15 * 4 = 60字节，所以TCP协议首部最长为60字节（20字节固定大小 + 40字节的选项和填充字段）。 6、6个控制位URG - 紧急位紧急位URG，URG=1时， 标明此报文段中有紧急数据，是高优先级的数据，应尽快传送，不用在缓存里排队，配合紧急指针字段使用 ACK - 确认位确认位ACK，ACK=1时确认号有效，在连接建立后所有传送的报文段都必须把ACK置为1 PSH - 推送位推送位PSH，PSH=1时， 接收方尽快交付接收应用进程，不再等到缓存填满再向上交付 RST - 复位复位RST，RST=1时， 表明TCP连接中出现严重差错，必须释放连接，然后再重新建立传输链接。 SYN - 同部位同步位SYN，SYN=1时，表明是一个连接请求/连接接受报文，建立连接的时候表示此报文段为请求/连接接受报文 FIN - 终止位终止位FIN，FIN=1时， 表明此报文段发送方数据已发完，要求释放放连接 不过在书中也有这样描述的： 这个会在后面的拥塞控制中讲到！ 7、窗口窗口指的是发送本报文段的一方的接收窗口，即现在允许对方发送的数据量，反映了自己可以缓存的字节流，那么对方就知道了你的接收能力怎么样，以此决定向你发送报文的数据量应该控制在多大的范围。举个简单的例子，如果接收方发来的报文确认号是701， 窗口是1000，那么发送方接下来要发的报文就是701~1700的数据。 8、校验和检验首部+数据，检验时要加上12字节的伪首部（IP首部）第四个字段为6（UDP中是17） 9、紧急指针当URG=1时，这个字段才有用，指出本报文段中紧急数据的字节数，假设紧急指针为50，那么数据部分的1-50的数据才是紧急数据，剩下的是普通数据 10、选项选项，例如最大报文段长度MSS、窗口扩大、时间戳、选择确认… TCP的连接管理建立连接 —-＞数据传送—-＞释放连接 TCP连接的建立采用客户服务器方式，主动发起连接建立的应用进程叫做客户，而被动等待连接建立的应用进程叫服务器。 1、建立连接过程step1、客户端发送连接请求报文段，无应用层数据， SYN = 1，seq = x(随机值，由客户端主机产生)，SYN为1很好理解，前面说到了SYN=1表明是一个连接请求/连接接受报文，建立连接的时候表示此报文段为请求/连接接受报文 step2、服务器端为该TCP连接分配缓存和变量，并向客户端返回确认报文段，允许连接，无应用层数据，SYN = 1，ACK = 1，seq=y(随机值)，ack=x+1。怎么理解呢？客户端首次发报文，这个报文是建立连接用的，所以客户端随机产生了一个序号seq=y，但是这个序号对于服务器是有用的，因为服务器通过客户端发来的seq就知道需要接收的下一个报文段的序号了，那就是seq的值再加一； step3、客户端为该TCP连接分配缓存和变量，并向服务器端返回确认的确认，并且可以携带数据 总结一下就是： 第一次发数据包 (客户端-&gt;服务器)：客户端知道自己的发送能力正常；服务器知道自己的接收能力正常，也知道客户端的发送能力正常 第二次发数据包 (服务器-&gt;客户端)：客户端知道服务器的接收、发送能力正常，客户端知道自己的接收能力正常；服务器知道自己的发送能力正常 那么现在的问题就是： 客户端知道服务器的发送、接收能力都正常，同时也知道自己的发送、接收都正常，那么是不是可以通信了呢？？？？ 不可以，因为服务器目前只知道：自己的发送、接收能力正常，客户端的发送能力正常 唯一不能确定的是客户端的接收能力是否正常，所以通过第三次握手，确定了客户端的接收能力也是正常的！ 补上一张加上状态的图 2、连接的释放过程 可以看出这是一个四次握手的过程 参与一条TCP连接的两个进程中的任何一个都能终止该连接，连接结束后，主机中的 资源 (缓存和变量) 将被释放。 step1、客户端发送连接释放报文段，停止发送数据，主动关闭TCP连接。 FIN=1，seq=u （u就等于前面已经传送过来的数据的最后一个字节的序号加1） step2、服务器端回送一个确认报文段，客户到服务器这个方向的连接就释放了，处于半关闭状态（close_wait），这个半关闭状态就是客户端停止发送数据，服务器端还可以发送数据。 TCP服务器通知高层的应用进程，客户端向服务器的方向就释放了，这时候处于半关闭状态，即客户端已经没有数据要发送了，但是服务器若发送数据，客户端依然要接受。这个状态还要持续一段时间，也就是整个CLOSE-WAIT状态持续的时间。 ACK=1，seq=v，ack=u+1 step3、服务器端发完数据，就发出来连接释放报文段，主动关闭TCP连接 FIN=1，ACK=1，seq=w，ack=u+1 step4、客户端回送确认报文段，再等待时间计时器设置的2MSL（最长报文段寿命）后，连接彻底关闭 ACK=1，seq=u+1，ack=w+1 TCP的其他问题1、TCP泛洪攻击SYN洪泛攻击，这种方式利用TCP协议的特性，就是三次握手。攻击者发送TCP SYN, SYN是TCP三次握手中的第-一个数据包，而当服务器返回ACK后，该攻击者就不对其进行再确认，那这个TCP连接就处于挂起状态，也就是所谓的半连接状态，服务器收不到再确认的话，还会重复发送ACK给攻击者。这样更加会浪费服务器的资源。攻击者就对服务器发送非常大量的这种TCP连接，由于每一个 都没法完成三次握手，所以在服务器上，这些TCP连接会因为挂起状态而消耗CPU和内存，最后服务器可能死机，就无法为正常用户提供服务了。 如何解决TCP泛洪攻击？ ① 无效连接的监视释放 监视系统的半开连接和不活动连接，当达到一定阈值时拆除这些连接，从而释放系统资源 ② SYN Cookie 它使用一种特殊的算法生成seq，这种算法考虑到了对方的IP、端口、己方IP、端口的固定信息，以及对方无法知道而己方比较固定的一些信息，如MSS(Maximum Segment Size，最大报文段大小，指的是TCP报文的最大数据报长度，其中不包括TCP首部长度)、时间等，在收到对方的ACK报文后，重新计算一遍，看其是否与对方回应报文中的（seq-1）相同，从而决定是否分配TCB资源 ③ SYN Cache 系统在收到一个SYN报文时，在一个专用HASH表中保存这种半连接信息，直到收到正确的回应ACK报文再分配TCB。这个开销远小于TCB的开销。当然还需要保存序列号。 ④ SYN Proxy防火墙 一种方式是防止墙dqywb连接的有效性后，防火墙才会向内部服务器发起SYN请求。防火墙代服务器发出的SYN ACK包使用的序列号为c, 而真正的服务器回应的序列号为c’, 这样，在每个数据报文经过防火墙的时候进行序列号的修改。另一种方式是防火墙确定了连接的安全后，会发出一个safe reset命令，client会进行重新连接，这时出现的syn报文会直接放行。这样不需要修改序列号了。但是，client需要发起两次握手过程，因此建立连接的时间将会延长。","updated":"2020-03-13T03:06:29.708Z","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[]},{"title":"辅助IP的ICMP","date":"2019-11-16T09:22:50.000Z","path":"2019/11/16/辅助IP的ICMP/","text":"1、ICMP协议概述ICMP主要是用于确认IP包是否成功发送至目标地址，通知在发送过程中，IP包被废弃的具体原因，改善网络设置等。有了这些功能就方便对网络进行诊断。 ICMP也是基于IP协议工作的，但是它并不是传输层的功能，因此人们仍然把它归结为网络层协议； 很好理解，如果主机A向B发送了报文，中途的路由器2却未发现主机B的存在，这个时候，路由器2便向A主机发送一个ICMP包，说明未能发往B主机： ICMP大概分为两类报文：一类是通知出错原因、一类是用于诊断查询 2、主要的ICMP消息1、不可达消息 IP路由器无法将IP数据包发送给目标地址时，会给发送端主机返回一个目标不可达( Destination Unreachable Message)的ICMP消息，并在这个消息中显示不可达的具体原因： 2、ICMP重定向消息如果路由器发现发送端主机使用了次优的路径发送数据，那么它会返回一个ICMP重定向(ICMP Redirect Message)的消息给这个主机。在这个消息中包含了最合适的路由信息和源数据。这主要发生在路由器持有更好的路由信息的情况下。路由器会通过这样的ICMP消息给发送端主机一个更合适的发送路由，但是这样做很容易成为引发问题的原因，一般不这么干。 3、超时消息 IP包中有一个字段叫做TTL (Time To Live，生存周期)，它的值随着每经过一次路由器就会减1，直到减到0时该IP包会被丢弃。此时IP 路由器将会发送一个ICMP超时的消息(ICMP Time Exceeded Message，错误号0 )给发送端主机，并通知该包已被丟弃。设置IP包生存周期的主要目的，是为了在路由控制遇到问题发生循环状况时，避免IP包无休止地在网络上被转发。此外，有时可以用TTL控制包的到达范围，例如设置一个较小的TTL值，如下图所示： 4、回送消息 ping命令同样也是回送消息的体现 有一款充分利用ICMP超时消息的应用叫做traceroute。它可以显示出由执行程序的主机到达特定主机之前历经多少路由器。它的原理就是利用IP包的生存期限从1开始按照顺序递增的同时发送UDP包，强制接收ICMP超时消息的一种方法。这样可以将所有路由器的IP地址逐一呈现。这个程序在网络上发生问题时，是问题诊断常用的一个强大工具。具体用法是在UNIX命令行里输入“traceroute目标主机地址”即可。","updated":"2020-03-13T03:06:29.771Z","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[]},{"title":"DNS域名解析","date":"2019-10-30T12:46:24.000Z","path":"2019/10/30/DNS域名解析/","text":"1、什么是DNS技术DNS(Domain Name System) DNS是一整套从域名映射到IP的系统 TCP/IP中使用IP地址和端口号来确定网络上的一台主机的一个程序。但是IP地址不方便记忆。于是人们发明了一种叫主机名的东西，是一个字符串，并且使用hosts文件来描述主机名和IP地址的关系。 最初，通过互连网信息中心(SRI-NIC)来管理这个hosts文件，如果一个新计算机要接入网络，或者某个计算机IP变更，都需要到信息中心申请变更hosts文件，而且其他计算机也需要定期下载更新新版本的hosts文件才能正确上网。这样就太麻烦了，于是产生了DNS系统，一个组织的系统管理机构，维护系统内的每个主机的IP和主机名的对应关系，如果新计算机接入网络，将这个信息注册到数据库；用户输入域名的时候，会自动查询DNS服务器，由DNS服务器检索数据库，得到对应的IP地址。 至今，我们的计算机上仍然保留了hosts文件，在域名解析的过程中仍然会优先查找hosts文件的内容。Windows和Linux上的host文件如图： 2、域名与根服务器域名的构成 域名是指为了识别主机名称和其他机构组织名称的一种具有分层的名称，比如我的网站的域名是 zouchanglin.cn ，zouchanglin表示我自己，cn则代表中国，在使用域名的时候，可以在每个机构后面追加上机构的域名，比如：我的博客网站域名可以是 blog.zouchanglin.cn，我的视频分享网站可以是 video.zouchanglin.cn ，我的线上小卖部的网站可以是 shop.zouchang.cn …… ，出现了带层次的域名之后，每个组织或者个人就可以自由的为主机命名了， DNS分成如下图，看起来是个树形结构，如果说顶点是树的根，那么底下就是树的各层枝叶，顶点的下一层叫做第一层域名，包括 cn (中国)、jp(日本)、uk(英国)…还包括edu(教育机构)、com(企业)… 域名服务器管理域名的主机和相应的软件，它可以管理所在分层的域的相关信息，所管理的分层叫做ZONE，每层都设有一个域名服务器。 各个域的分层上都设定有各自的域名服务器 各层域名服务器都了解该层以下分层中所有域名服务器的IP地址，因此葱根域名服务器开始呈现树状连接 由于所有域名服务器都知道根服务器的IP地址，所以如果从根开始按照顺序跟踪，可以访问世界所有的域名服务器的地址 一般为了提高容灾能力一般会设置至少两个以上的域名服务器，所有域名服务器都必须注册根域名服务器的IP地址，因为NDS根据IP地址进行检索的时候，需要从根域名服务器开始按顺序进行。 4、DNS查询过程 收到查询请求的服务器首先会在自己的数据控中进行查找，如果有该域名对应的IP地址就返回，如果没有，则域名服务器再想上一层根域名服务器进行查询处理，因此如图所示从根开始对这棵树进行遍历，直到找到指定的域名服务器，并由这个域名服务器返回想要的数据，一般查询到后会保存在缓存了里，减少每次都需要查询时的性能损耗。 5、DNS如同互联网的分布式数据库前面提到DNS是一种通过主机名检索IP地址的系统，然而它所管理的信息不仅仅是这些主机名跟IP地址之间的映射关系，它还要管理众多其他信息，参考下表： 主机名与IP地址对应的信息叫做A记录，反之，从IP地址检索主机名叫做PTR，此外，上层和下层域名服务器IP地址的映射叫做NS记录，特别需要指出的是MX记录，这类记录注册了电子邮件地址与邮件接收服务器的主机名 6、使用dig分析DNS过程安装 dig 工具 1yum install bind-utils 之后就可以使用 dig 指令查看域名解析过程了 开头位置是 dig 指令的版本号 第二部分是服务器返回的详情，重要的是 status 参数，NOERROR 表示查询成功 QUESTION SECTION 表示要查询的域名是什么 ANSWER SECTION 表示查询结果是什么 最下面是一些结果统计，包含查询时间和 DNS 服务器的地址等 默认情况下 dig 命令查询 A 记录，上图中显示的 A 即说明查询的记录类型为 A 记录。在尝试查询其它类型的记录前让我们先来了解一下常见的 DNS 记录类型。 除了 A 记录，常见的 DNS 记录还有 CNAME，我们可以在查询时指定要查询的 DNS 记录类型 这样结果中就只有 CNAME 的记录。其实我们可以在查询中指定任何DNS记录的类型 从指定的 DNS 服务器上查询，本次查询的 DNS 服务器为 8.8.8.8 如果不指定 DNS 服务器，dig 会依次使用 /etc/resolv.conf 里的地址作为 DNS 服务器 dig命令执行查询时都经历了哪些过程? +trace 选项就可以看到。它会输出从根域到最终结果的所有信息 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@localhost ~]# dig +trace zouchanglin.cn; &lt;&lt;&gt;&gt; DiG 9.11.4-P2-RedHat-9.11.4-9.P2.el7 &lt;&lt;&gt;&gt; +trace zouchanglin.cn;; global options: +cmd. 5 IN NS c.root-servers.net.. 5 IN NS g.root-servers.net.. 5 IN NS b.root-servers.net.. 5 IN NS e.root-servers.net.. 5 IN NS k.root-servers.net.. 5 IN NS m.root-servers.net.. 5 IN NS f.root-servers.net.. 5 IN NS a.root-servers.net.. 5 IN NS j.root-servers.net.. 5 IN NS i.root-servers.net.. 5 IN NS d.root-servers.net.. 5 IN NS l.root-servers.net.. 5 IN NS h.root-servers.net.;; Received 239 bytes from 192.168.45.2#53(192.168.45.2) in 10 mscn. 172800 IN NS a.dns.cn.cn. 172800 IN NS b.dns.cn.cn. 172800 IN NS c.dns.cn.cn. 172800 IN NS d.dns.cn.cn. 172800 IN NS e.dns.cn.cn. 172800 IN NS f.dns.cn.cn. 172800 IN NS g.dns.cn.cn. 172800 IN NS ns.cernet.net.cn. 86400 IN DS 57724 8 2 5D0423633EB24A499BE78AA22D1C0C9BA36218FF49FD95A4CDF1A4AD 97C67044cn. 86400 IN RRSIG DS 8 1 86400 20191111170000 20191029160000 22545 . d5pIvGPmRwXNYubnmNxTJ8V8CCz+gXsfhIa6r9CnL7e21vrU4gbf5vyp UcSbSDzzPYB4NkV2RShBLsh4xIV3eonb3HmxS/rIWMKDZavg9vMYAaEp BRB1ZYG4BS35cXeBNaeVs295wFQbRXq4jOT1MIwqeOsgIweG56gpeiKs 4aJdaZ4CSxzfpf0k5R4rntfPO7nJ/Dq/e/onnF7Xk4W3WJr0kTFY5a1G IBnT1VRPckGIphJ5yjNrRiar3qumG1A0y2Q1RZPpMhnbSIunT+Wdj5ZI EatiCeqDopMK3D6m82iBEZycB4yprwI/zVGJxL5xPJMAxFhgf9hI+AyS QnFW1w==;; Received 705 bytes from 192.58.128.30#53(j.root-servers.net) in 32 mszouchanglin.cn. 86400 IN NS ns2.bdydns.cn.zouchanglin.cn. 86400 IN NS ns1.bdydns.cn.3QDAQA092EE5BELP64A74EBNB8J53D7E.cn. 21600 IN NSEC3 1 1 10 AEF123AB 3QLMP0QRNQ96G5AFGOPNB7U7IJ4MBP4B NS SOA RRSIG DNSKEY NSEC3PARAM3QDAQA092EE5BELP64A74EBNB8J53D7E.cn. 21600 IN RRSIG NSEC3 8 2 21600 20191117072923 20191018072244 38388 cn. HXL4nUKND4ZJq6ZnjNKV0IyMCWh21KV9DA/hc/SBYpBNSE8fdq1Y7KlG O8DO+sgP/M68Zrkuml7cFTiFaJ1uYo7gHD55knKBrc1EbLRI3SehH6Q7 5iTGkZKEt70J2F4MnJ7gTdXUUhaJj5pEC8TiIB/Jebn5BV+FnyRh6XKJ s/c=SCLRQS9HBJL71295P4F8PH8S15CT2KFH.cn. 21600 IN NSEC3 1 1 10 AEF123AB SCPV80H0ATA0D0PS0V2HRUN4JOBNVAF5 CNAME RRSIGSCLRQS9HBJL71295P4F8PH8S15CT2KFH.cn. 21600 IN RRSIG NSEC3 8 2 21600 20191117074411 20191018065339 38388 cn. NdnRI6dmaFEFC0Qr/KbXIHQCXKt5jRLC+swFocjNI4BQCbvs3vhr37aZ 2VfW/lCxGPtvp5uV8juI7CY5X71w7MwCblJfZmsQ5n6Y9kfV4INqAW3E mOvoLcwX2mAwWQ4+pACdR5vdg7b6zdHBcdBvLJL2iCN8oU7826uaKNlb DR0=;; Received 609 bytes from 103.137.60.44#53(ns.cernet.net) in 51 mszouchanglin.cn. 300 IN A 139.159.234.67;; Received 59 bytes from 119.75.222.53#53(ns2.bdydns.cn) in 40 ms[root@localhost ~]#","updated":"2020-03-13T03:06:29.645Z","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[]},{"title":"NAT技术与ARP协议","date":"2019-10-29T11:36:35.000Z","path":"2019/10/29/NAT技术与ARP协议/","text":"私有IP地址之前我们讨论了， IPv4协议中，IP地址数量不充足的问题NAT技术当前解决IP地址不够用的主要手段，是路由器的一个重要功能；NAT能够将私有IP对外通信时转为全局IP。也就是就是一种将私有IP和全局IP相互转化的技术方法；很多学校, 家庭, 公司内部采用每个终端设置私有IP，而在路由器或必要的服务器上设置全局IP；全局IP要求唯一，但是私有IP不需要；在不同的局域网中出现相同的私有IP是完全不影响的； 路由器对目的地址是私有IP地址的数据报一律不进行转发。 网络地址转换 NAT网络地址转换NAT (Network Address Translation) ：在专用网连接到因特网的路由器上安装NAT软件，安装了NAT软件的路由器叫NAT路由器，它至少有一个有效的外部全球IP地址。 NAT路由器将源地址从10.0.0.10替换成全局的IP 202.244.174.37；NAT路由器收到外部的数据时，又会把目标IP从202.244.174.37替换回10.0.0.10；在NAT路由器内部，有一张自动生成的，用于地址转换的表；当 10.0.0.10 第一次向 163.221.120.9 发送数据时就会生成表中的映射关系； 网段划分IP地址分为两个部分，网络号和主机号 网络号：保证相互连接的两个网段具有不同的标识；主机号：同一网段内，主机之间具有相同的网络号，但是必须有不同的主机号； 不同的子网其实就是把网络号相同的主机放到一起，如果在子网中新增一台主机，则这台主机的网络号和这个子网的网络号一致，但是主机号必须不能和子网中的其他主机重复。通过合理设置主机号和网络号，就可以保证在相互连接的网络中，每台主机的IP地址都不相同。 手动管理子网内的IP，是一个相当麻烦的事情。有一种技术叫做DHCP，能够自动的给子网内新增主机节点分配IP地址，避免了手动管理IP的不便。一般的路由器都带有DHCP功能。因此路由器也可以看做一个DHCP服务器。 过去曾经提出一种划分网络号和主机号的方案，把所有IP 地址分为五类，如下图所示： A类 0.0.0.0到127.255.255.255B类 128.0.0.0到191.255.255.255C类 192.0.0.0到223.255.255.255D类 224.0.0.0到239.255.255.255E类 240.0.0.0到247.255.255.255 随着Internet的飞速发展，这种划分方案的局限性很快显现出来，大多数组织都申请B类网络地址，导致B类地址很快就分配完了，而A类却浪费了大量地址。 分类的IP地址的弱点:1、IP地址空间的利用率有时很低2、两级IP地址不够灵活 针对这种情况提出了新的划分方案，称为CIDR(Classless Interdomain Routing)：引入一个额外的子网掩码(subnet mask)来区分网络号和主机号；子网掩码也是一个32位的正整数，通常用一串 “0” 来结尾；将IP地址和子网掩码进行 “按位与” 操作, 得到的结果就是网络号；网络号和主机号的划分与这个IP地址是A类、B类还是C类无关； 子网掩码可以分离出IP地址中的网络地址和主机地址，那为什么要分离呢？因为两台计算机要通讯，首先要判断是否处于同一个广播域内，即网络地址是否相同。如果网络地址相同，表明接受方在本网络上，那么可以把数据包直接发送到目标主机，否则就需要路由网关将数据包转发送到目的地。 路由在复杂的网络结构中， 找出一条通往终点的路线； 当IP数据包，到达路由器时， 路由器会先查看目的IP；路由器决定这个数据包是能直接发送给目标主机，还是需要发送给下一个路由器；依次反复；一直到达目标IP地址； 那么如何判定当前这个数据包该发送到哪里呢？这个就依靠每个节点内部维护一个路由表；可以使用route查看路由表： 路由表的Destination是目的网络地址，Genmask是子网掩码，Gateway是下一跳地址，Iface是发送接口，也就是网卡，Flags中的U标志表示此条目有效(可以禁用某些条目)，G标志表示此条目的下一跳地址是某个路由器的地址，没有G标志的条目表示目的网络地址是与本机接口直接相连的网络，不必经路由器转发； 如果要发送的数据包的目的地址是192.168.45.3 跟第一行的子网掩码做与运算，与第一行的目的网络地址不符跟第二行的子网掩码做与运算，与第二行的目的网络地址不符跟第三行的子网掩码做与运算，得到192.168.45.0，正是第三行的目的网络地址，因此从eth1接口发送出去； 如果要发送的数据包的目的地址是202.10.1.2 依次和路由表前几项进行对比，发现都不匹配；按缺省路由条目，从eth34接口发出去，发往192.168.10.1路由器；由192.168.10.1路由器根据它的路由表决定下一跳地址； ARP协议ARP不是一个单纯的数据链路层的协议，而是一个介于数据链路层和网络层之间的协议 ARP协议的作用：ARP协议建立了主机IP地址和MAC地址的映射关系。 在网络通讯时，源主机的应用程序知道目的主机的IP地址和端口号，却不知道目的主机的硬件地址；数据包首先是被网卡接收到再去处理上层协议的，如果接收到的数据包的硬件地址与本机不符，则直接丢弃；因此在通讯前必须获得目的主机的硬件地址；不过ARP协议只能用于IPv4，不能够用于IPv6，IPv6中可以用ICMPv6替代ARP发送邻居探索消息！ 源主机发出ARP请求，询问 IP地址是172.20.1.2的主机的硬件地址是多少，并将这个请求广播到本地网段(以太网帧首部的硬件地址填FF:FF:FF:FF:FF:FF表示广播)；目的主机接收到广播的ARP请求，发现其中的IP地址与本机相符，则发送一个ARP应答数据包给源主机，将自己的硬件地址填写在应答包中；每台主机都维护一个ARP缓存表，可以用arp -a命令查看。缓存表中的表项有过期时间(一般为20分钟)，如果20分钟内没有再次使用某个表项，则该表项失效，下次还要发ARP请求来获得目的主机的硬件地址： 这种缓存可以有效减少ARP包的发送，反之，接受ARP包的主机也可以通过这个ARP包把发送方的MAC地址缓存起来，从而根据MAC地址发送ARP响应包给发送端主机，并作为响应，因此在接收主机缓存MAC地址也是一种提高效率的方式。 MAC地址的缓存是有期限的，超过这个期限，缓存的内容将被清除，这使得MAC地址与IP地址对应关系发生了变化，也依旧能够将IP数据报正确的发给目标地址。 ARP包格式： 硬件类型指链路层网络类型，1为以太网； 上层协议类型指要转换的地址类型，0x0800为IP地址； HLEN：MAC地址的长度 = 6 (字节)； PLEN：IP地址的长度 = 4 (字节)； 操作码 ： 该字段为1表示ARP请求，op字段为2表示ARP应答。 IP地址和MAC地址缺一不可如图所示，主机A想要发IP数据报给主机B的时候必须经过路由器c，即使主机A知道了主机B的MAC地址，，由于路由器会隔断两个网络，还是无法实现从主机A直接发送到主机B，此时主机A必须先把数据报发送给路由器C的MAC地址C1： 在以太网上发送IP数据包的时候，“下一次要经过那个路由发送数据包” 这一信息非常重要，而这里的“下一个路由器” 就是相应的MAC地址。由此看来IP地址和MAC地址缺一不可，将这两个地址关联起来就形成了ARP协议。","updated":"2020-03-13T03:06:29.695Z","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[]},{"title":"计算机网络性能指标","date":"2019-10-29T04:18:39.000Z","path":"2019/10/29/计算机网络性能指标/","text":"计算机网络性能指标1、速率速率即数据率或称数据传输率或比特率。连接在计算机网络上的主机在数字信道上传送数据位数的速率。单位是b/s，kb/s， Mb/s，Gb/s, Tb/s。下面是速率和存储容量的区别： 2、带宽原本指某个信号具有的频带宽度，即最高频率与最低频率之差，单位是赫兹(Hz)。计算机网络中，带宽用来表示网络的通信线路传送数据的能力，通常是指单位时间内从网络中的某一点到另一点所能通过的“最高数据率”。单位是”比特每秒”，b/s， kb/s, Mb/s， Gb/s。表示在单位时间内通 3、吞吐量过某个网络(或信道、接口)的数据量。单位b/s, kb/s, Mb/s等。吞吐量受网络的带宽或网络的额定速率的限制。 4、时延指数据(报文/分组/比特流)从网络(或链路)的一端传送到另一端所需的时间。也叫延迟或迟延。单位是s。 5、时延带宽积 6、往返时延 RTT从发送方发送数据开始，到发送方收到接收方的确认(接收方收到数据后立即发送确认)，总共经历的时延。 RTT越大，在收到确认之前，可以发送的数据越多。因为我等得越久，就有更多的时间发送数据。RTT包括往返传播时延，也就是传播时延的二倍，同时也包括了末端处理时间。 7、利用率信道利用率和网络利用率，信道利用率是有数据通过的时间 / (有+无)数据通过的时间 网络利用率就是信道利用率加权平均值 网络其他概念1、OSI 参考模型 2、三种通信方式① 单工通信 单工通信只有一个方向的通信 而没有反方向的交互，仅需要一条信道。 ② 半双工通信 半双工通信通信的双方都可以发送或接收信息，但任何一方都不能同时发送和接收，需要两条信道。 ③ 全双工通信 全双工通信通信双方可以同时发送和接受信息，也需要两条信道。 3、两种数据传输方式串行与并行","updated":"2020-03-13T03:06:29.766Z","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[]},{"title":"IP数据报格式及分片","date":"2019-10-29T04:10:41.000Z","path":"2019/10/29/IP数据报格式及分片/","text":"IP数据报格式 下面是首部的结构： 这里的长度单位都是位（比特位） 1、版本4位版本号(version)：指定IP协议的版本，对于IPv4来说，就是4 2、首部长度4位头部长度(header length)：IP头部的长度，单位是4比特，最小为5（因为固定部分为20字节，所以最小就是5），也就是说首部长度是 4×5=20到 4×15=60之间 3、区分服务8位服务类型(Type Of Service)：3位优先权字段(已经弃用)，4位TOS字段，和1位保留字段(必须置为0)。4位TOS分别表示：最小延时、 最大吞吐量、 最高可靠性、最小成本。 这四者相互冲突，只能选择一个。对于ssh/telnet这样的应用程序，最小延时比较重要；对于ftp这样的程序，最大吞吐量比较重要。 4、总长度16位总长度(total length)：IP数据报整体占多少个字节 5、标识16位标识(id)：唯一的标识主机发送的报文。如果IP报文在数据链路层被分片了，那么每一个片里面的这个id都是相同的。 6、标志3位标志字段：第一位保留（保留的意思是现在不用，但是还没想好说不定以后要用到）。第二位置为1表示禁止分片，这时候如果报文长度超过MTU，IP模块就会丢弃报文。第三位表示“更多分片”，如果分片了的话，最后一个分片置为1，其他是0。类似于一个结束标记。 7、片偏移13位分片偏移(framegament offset)：是分片相对于原始IP报文开始处的偏移。其实就是在表示当前分片在原报文中处在哪个位置，实际偏移的字节数是这个值 * 8 得到的。因此，除了最后一个报文之外，其他报文的长度必须是8的整数倍（否则报文就不连续了）。 8、生存时间8位生存时间(Time To Live, TTL)：数据报到达目的地的最大报文跳数，一般是64。每次经过一个路由，TTL就减一，一直减到0还没到达，那么就丢弃了。这个字段主要是用来防止出现路由循环 9、协议8位协议：表示上层协议的类型 10、首部校验和16位头部校验和：使用CRC进行校验，来鉴别头部是否损坏。也就是二进制的和！ 11、源地址和目的地址32位源地址和32位目标地址： 表示发送端和接收端 12、可选字段和填充用来支持排错、测量以及安全等措施 IP数据报分片1、最大传送单元MTUMTU全称是maximum transmission unit，是指链路层数据帧可封装数据的上限，以太网的MTU是1500字节。 2、IP数据报的标识回顾一下IP数据报中的标识：唯一的标识主机发送的报文。如果IP报文在数据链路层被分片了，那么每一个片里面的这个id都是相同的。 3、IP数据报的标识字段回顾一下IP数据报中的标志字段： 4、IP数据报的片偏移片偏移：指出较长分组分片后，某片在原分组中的相对位置，以8B为单位。除了最后一个分片，每个分片长度一定是8B的整数倍。 5、分片示例以太网帧中的数据长度规定最小46字节，最大1500字节，ARP数据包的长度不够46字节，要在后面补填充位；最大值1500称为以太网的最大传输单元(MTU)，不同的网络类型有不同的MTU； 如果一个数据包从以太网路由到拨号链路上，数据包长度大于拨号链路的MTU了，则需要对数据包进行分片(fragmentation)；不同的数据链路层标准的MTU是不同的； 下面以1420比特为最大值进行分片： 这个例子其实很好理解，首部必须占用了20位，数据划分为1400位+1400位+1000位，他们都来自同一个数据报，所以标识都是一样的（在这里假设都为12345），数据报1、2、3DF都是0则表示允许分片，数据报1、2MF都是1表示后面还有分片，数据报3的MF为0，代表自己是最后一个分片，后面没有分片了，至于片偏移这个额也是可以计算的，数据报1的片偏移为0，1400/8=175，所以数据报2的片偏移为175，数据报3的片偏移为350，通过片偏移就是为了数据重组或者合并后仍然是原来未分片时候的顺序！","updated":"2020-03-13T03:06:29.662Z","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zouchanglin.cn/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[]},{"title":"分布式架构基础","date":"2019-10-26T06:56:09.000Z","path":"2019/10/26/分布式架构基础/","text":"分布式的定义由多个计算机完成一系列任务， 一个业务拆分成多个子系统，部署在不同的服务器上 分布式系统需要解决的问题： 1、任务分解 2、节点通信 分布式与集群分布式： 一个业务拆分成多个子系统，部署在不同的服务器上 集群： 同一个业务，部署在多个服务器上 分布式与微服务 微服务与分布式的细微差别是，微服务的应用不一定是分散在多个服务器上，它也可以是同一个服务器，上面运行着不同的服务 架构演变过程1、第一版 2、第二版 单机负载越来越高，数据库服务器和应用服务器分离 3、第三版 应用服务器做集群 4、第四版 数据库的高性能操作 5、第五版 加入搜索集群 6、第六版 解决访问量持续增高，引入缓存机制 (页面缓存 + CDN) 7、第七版 数据库的水平/垂直拆分 8、第八版 服务拆分，微服务化","updated":"2020-03-13T03:06:29.723Z","categories":[{"name":"分布式理论","slug":"分布式理论","permalink":"https://zouchanglin.cn/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/"}],"tags":[{"name":"集群","slug":"集群","permalink":"https://zouchanglin.cn/tags/%E9%9B%86%E7%BE%A4/"},{"name":"微服务","slug":"微服务","permalink":"https://zouchanglin.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"分布式","slug":"分布式","permalink":"https://zouchanglin.cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"异常处理与资源释放","date":"2019-10-25T15:54:55.000Z","path":"2019/10/25/异常处理与资源释放/","text":"JVM每实例化一个 Exception，都会对当时的栈进行快照，这是一个相对比较重的操作。如果发生的非常频繁，这个开销可就不能被忽略了 。try-catch 代码段会产生额外的性能开销，或者换个角度说，它往往会影响 JVM 对代码进行优化，所以建议仅捕获有必要的代码段，尽量不要一个大的 try 包住整段的代码；与此同时，利用异常控制代码流程，也不是一个好主意，远比我们通常意义上的条件语句（if/else、switch）要低效。 1、Java异常体系图 2、从性能的角度看异常1、try-catch 代码段会产生额外的性能开销，或者换个角度说，它往往会影响 JVM 对代码进行优化，所以建议仅捕获有必要的代码段，尽量不要一个大的 try 包住整段的代码；与此同时，利用异常控制代码流程，也不是一个好主意，远比我们通常意义上的条件语句（if/else、switch）要低效。 2、Java 每实例化一个 Exception，都会对当时的栈进行快照，这是一个相对比较重的操作。如果发生的非常频繁，这个开销可就不能被忽略了。 3、受查异常的争论业界有一种争论（甚至可以算是某种程度的共识），Java 语言的 Checked Exception 也许是个设计错误，反对者列举了几点： ① Checked Exception 的假设是我们捕获了异常，然后恢复程序。但是，其实我们大多数情况下，根本就不可能恢复。Checked Exception 的使用，已经大大偏离了最初的设计目的。 ② Checked Exception 不兼容 functional 编程，如果写过 Lambda/Stream 代码，应该深有体会。异常的捕获，在生产环境中往往需要打印到日志中，不要直接打印到标准错误流， printStackTrace()的解释： public void printStackTrace()Prints this throwable and its backtrace to the standard error stream. This method prints a stack trace for this Throwable object on the error output stream that is the value of the field System.err. The first line of output contains the result of the toString() method for this object. Remaining lines represent data previously recorded by the method fillInStackTrace(). 我的看法，毕竟Java经受住了这么多考验，成为企业级应用程序开发的首选肯定有它的设计思想的。而且那些非受查异常很多都是程序员自己犯的错，可能只需要修改代码逻辑即可避免异常的出现，那些受查异常通常是由于环境因素导致的，并非程序设计本身的逻辑不严谨，不但使得程序更加高可用，而且代码更人性化。毕竟有时候在追求极致性能的同时也要兼顾一下程序的可读性！ 4、NoClassDefFoundError 与 ClassNotFoundExceptionNoClassDefFoundError 和 ClassNotFoundException 有什么区别呢？ 两者本质的区别：NoClassDefFoundError是一个错误(Error)，而ClassNOtFoundException是一个异常，在Java中错误和异常是有区别的，我们可以从异常中恢复程序但却不应该尝试从错误中恢复程序。 ClassNotFoundException的产生原因： 1、Java支持使用Class.forName方法来动态地加载类，任意一个类的类名如果被作为参数传递给这个方法都将导致该类被加载到JVM内存中，如果这个类在类路径中没有被找到，那么此时就会在运行时抛出ClassNotFoundException异常。要解决这个问题很容易，唯一需要做的就是要确保所需的类连同它依赖的包存在于类路径中。当Class.forName被调用的时候，类加载器会查找类路径中的类，如果找到了那么这个类就会被成功加载，如果没找到，那么就会抛出ClassNotFountException，除了Class.forName，ClassLoader.loadClass、ClassLOader.findSystemClass在动态加载类到内存中的时候也可能会抛出这个异常。 2、另外还有一个导致ClassNotFoundException的原因就是：当一个类已经某个类加载器加载到内存中了，此时另一个类加载器又尝试着动态地从同一个包中加载这个类。 NoClassDefFoundError产生的原因： 1、如果JVM或者ClassLoader实例尝试加载（可以通过正常的方法调用，也可能是使用new来创建新的对象）类的时候却找不到类的定义。要查找的类在编译的时候是存在的，运行的时候却找不到了。这个错误往往是你使用new操作符来创建一个新的对象但却找不到该对象对应的类。这个时候就会导致NoClassDefFoundError。由于NoClassDefFoundError是有JVM引起的，所以不应该尝试捕捉这个错误。 总结一下就是：加载时从外存储器找不到需要的class就出现ClassNotFoundException。连接时从内存找不到需要的class就出现NoClassDefFoundError。 5、JDK7 的Try-With-Resource12345678910111213141516171819202122232425public class TryWithResourceDemo &#123; public void usuallyTry(String[] args) &#123; FileInputStream fileInputStream = null; try &#123; fileInputStream = new FileInputStream(new File(\"filename\")); //TODO ... &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; fileInputStream.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public void tryWithResource(String[] args) &#123; try(FileInputStream fileInputStream = new FileInputStream(new File(\"filename\"))) &#123; //TODO ... &#125;catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 异常抑制：通过反编译的代码，代码中有一处对异常的特殊处理： 1var2.addSuppressed(var11); 这是try-with-resource语法涉及的另外一个知识点，叫做异常抑制。当对外部资源进行处理（例如读或写）时，如果遭遇了异常，且在随后的关闭外部资源过程中，又遭遇了异常，那么你catch到的将会是对外部资源进行处理时遭遇的异常，关闭资源时遭遇的异常将被 “抑制” 但不是丢弃，通过异常的getSuppressed方法，可以提取出被抑制的异常。需要明白一点：那就是这个try-with-resource 只是语法糖而已！ 最后说一种 try finally的特殊情况: 12345try &#123; System.exit(0);&#125;finally &#123; System.out.println(\"finally...\");&#125; 这种情况下finally是不会被执行的! 6、别指望 finalize() 释放资源非常不建议使用finalize，对于 finalize，我们要明确它是不推荐使用的，业界实践一再证明它不是个好的办法，在Java 9 中，甚至明确将 Object.finalize() 标记为 deprecated！如果没有特别的原因，不要实现 finalize 方法，也不要指望利用它来进行资源回收。为什么呢？ 简单说，你无法保证 finalize 什么时候执行，执行的是否符合预期。使用不当会影响性能，导致程序死锁、挂起等。利用 try-with-resources 或者 try-finally 机制，是非常好的回收资源的办法。finalize 的执行是和垃圾收集关联在一起的，一旦实现了非空的 finalize 方法，就会导致相应对象回收呈现数量级上的变慢，有人专门做过 benchmark，大概是 40~50 倍的下降。因为，finalize 被设计成在对象被垃圾收集前调用，这就意味着实现了 finalize 方法的对象是个“特殊公民”，JVM 要对它进行额外处理。finalize 本质上成为了快速回收的阻碍者，可能导致你的对象经过多个垃圾收集周期才能被回收。有人也许会问，我用 System.runFinalization () 告诉 JVM 积极一点，是不是就可以了？也许有点用，但是问题在于，这还是不可预测、不能保证的，所以本质上还是不能指望。实践中，因为 finalize 拖慢垃圾收集，导致大量对象堆积，也是一种典型的导致 OOM 的原因。从另一个角度，我们要确保回收资源就是因为资源都是有限的，垃圾收集时间的不可预测，可能会极大加剧资源占用。这意味着对于消耗非常高频的资源，千万不要指望 finalize 去承担资源释放的主要职责，最多让 finalize 作为最后的“守门员”，况且它已经暴露了如此多的问题。资源用完即显式释放，或者利用资源池来尽量重用。 finalize生吞异常，通过查看源代码 package java.lang.ref.Finalizer ;可以看出finalize方法根本没有做异常的处理， 这里的Throwable 是被生吞了的！也就意味着一旦出现异常或者出错，你得不到任何有效信息。况且，Java 在 finalize 阶段也没有好的方式处理任何信息，不然更加不可预测。 一旦在finally块中使用了return或者throw语句，会导致try块、catch块中的return、throw语句失效 在《深入理解Java虚拟机中》 已经明确告诉了我们finalize()方法的用途和初衷，已经对象通过finalize()方法进行自我拯救的过程，所以说把进行重写finalize方法的对象在JVM眼里就是特殊公民，但是由于GC的不确定性导致资源的释放并不是由我们自己决定的，而且finalize方法只会被调用一次，不会第二次执行，是非常不适合用来做资源释放的！ 7、finally与return这是初学者很容易发生疑惑的地方： try 中的 return 语句调用的函数先于 finally 中调用的函数执行，也就是说 try 中的 return 语句先执行，finally 语句后执行，但try中的 return 并不是让函数马上返回结果，而是 return 语句执行后，将把返回结果放置进函数栈中，此时函数并不是马上返回，它要执行 finally 语句后才真正开始返回。但此时会出现两种情况： ① 如果finally中也有return，则会直接返回finally中的return结果，并终止程序，函数栈中的return不会被完成 ② 如果finally中没有return，则在执行完finally中的代码之后，会将函数栈中保存的try return的内容返回并终止程序","updated":"2020-03-13T03:06:29.738Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JVM","slug":"JVM","permalink":"https://zouchanglin.cn/tags/JVM/"}]},{"title":"HotSpot JVM类型以及编译模式","date":"2019-10-25T09:31:15.000Z","path":"2019/10/25/HotSpotJVM类型以及编译模式/","text":"Java是编译型还是解释型语言关于这个问题我想表达一下自己的看法：Java 的源代码，通过Javac编译成为字节码，然后在运行时通过Java 虚拟机内嵌的解释器将字节码转换成为最终的机器码。但是常见的 JVM，比如我们大多数情况使用的 Oracle JDK 提供的 Hotspot JVM，都提供了 JIT（Just-In-Time）编译器，也就是通常所说的动态编译器，JIT 能够在运行时将热点代码编译成机器码，这种情况下部分热点代码就属于编译执行，而不是解释执行了。 那么什么样的代码算是热点代码呢？被多次调用的方法和被多次执行的循环体都属于热点代码！ 编译型语言：把做好的源程序全部编译成二进制代码的可运行程序。然后，可直接运行这个程序。如C/C++解释型语言：把做好的源程序翻译一句，然后执行一句，直至结束！ 如Python 结论：很显然，Java是解释型的语言，因为虽然Java也需要编译，编译成class文件，但是class文件并不是机器可以识别的语言，而是字节码，最终还是需要 JVM的解释，才能在各个平台执行，这同时也是Java跨平台的原因。所以直接从定义上来说Java属于解释型语言！ 三种编译模式Java 虚拟机启动时，可以指定不同的参数对运行模式进行选择。 比如，指定”-Xint”，就是告诉 JVM 只进行解释执行，不对代码进行编译，这种模式抛弃了JIT可能带来的性能优势。毕竟解释器（interpreter）是逐条读入，逐条解释运行的。与其相对应的，还有一个”-Xcomp”参数，这是告诉 JVM 关闭解释器，不要进行解释执行，或者叫作最大优化级别，但是这种模式未必是高效的。”-Xcomp”会导致JVM 启动变慢非常多，同时有些 JIT 编译器优化方式，比如分支预测，往往并不能进行有效优化。 1、只进行解释执行：-Xint-Xint标记会强制JVM执行所有的字节码，当然这会降低运行速度，通常低10倍或更多。 2、关闭解释器：-XcompJVM在第一次使用时会把所有的字节码编译成本地代码，从而带来最大程度的优化。这听起来不错，因为这完全绕开了缓慢的解释器。然而，很多应用在使用-Xcomp也会有一些性能损失，当然这比使用-Xint损失的少，原因是-xcomp没有让JVM启用JIT编译器的全部功能。JIT编译器在运行时创建方法使用文件，然后一步一步的优化每一个方法，有时候会主动的优化应用的行为。 3、混合模式：-Xmixed解释执行+JIT 通过下图也可以对比出这三种启动方式的速度： JDK9的AOT通过 AOT（Ahead-of-Time Compilation）， 编译代码加快应用程序启动，因为虽然这种代码通常比JIT 编译代码慢，但是却比解释代码快很多倍。此外，因为加载和绑定AOT编译代码的时间通常比检测和动态编译一个重要方法的时间少，所以能够在程序执行的早期达到那样的性能。类似地，交互式应用程序可以很快地从本地代码中获益，无需使用引起较差响应能力的动态编译。 只有在执行代码引用类的时候才加载该类。因为是在程序执行前进行AOT 编译的，所以编译器无法预测加载了哪些类。就是说编译器无法获知任何静态字段的地址、任何对象的任何实例字段的偏移量或任何调用的实际目标，甚至对直接调用（非虚调用）也是如此。虽然AOT是有好处的，但是缺少关于静态、字段、类和方法的信息意味着严重限制了 Java 编译器中优化框架的大部分功能。这实际上限制了Java 语言本身的动态特性，牺牲了平台无关性和代码！ JVM的server/client型现在的JVM运行Java程序（和其它的兼容性语言）时在高效性和稳定性方面做的非常出色。自适应内存管理、垃圾收集、及时编译、动态类加载、锁优化等等。在运行时，JVM会不断的计算并优化应用或者应用的某些部分。 有两种类型的 HotSpot JVM，即”server”和 “client”。服务端的VM中的默认为堆提供了一个更大的空间以及一个并行的垃圾收集器，并且在运行时可以更大程度地优化代码。客户端的VM更加保守一些，这样可以缩短JVM的启动时间和占用更少的内存。 从J2SE 5.0开始，当应用程序启动时，启动程序可以尝试检测该应用程序是否在”服务器级”计算机上运行，如果是，则使用JavaHotSpotServer虚拟机(服务器VM) 而不是JavaHotSpot客户端虚拟机（客户端VM）。目的是即使没有人配置VM来反映它正在运行的应用程序，也可以提高性能。通常，服务器VM的启动速度比客户端VM的启动速度慢，但随着时间的推移，运行速度会更快。 在Java SE 6中，如果在运行Solaris或Linux的i586或Sparc 32位计算机上启动应用程序时-server-client指定也未指定，则将进行服务器类型检测 。如下表所示，默认情况下，i586 Microsoft Windows平台使用客户端VM。其余的Sun支持的平台仅使用服务器VM：https://docs.oracle.com/javase/6/docs/technotes/guides/vm/server-class.html参考下表：x表示默认的虚拟机类型 - 表示 这个平台没有提供客户端VM 如何验证这个表呢？ 以windows版本的来看(Linux的JDK9是我刚才安装的)输出显示的是Java版本号(1.8.0_231)和JRE确切的build号(1.8.0_231-b11)。我们也可以看到JVM的名字(HotSpot)、类型(client)和build ID（25.231-b11)）。除此之外，我们还知道JVM以混合模式(mixed mode)在运行，这是HotSpot默认的运行模式，意味着JVM在运行时可以动态的把字节码编译为本地代码。","updated":"2020-03-13T03:06:29.658Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JVM","slug":"JVM","permalink":"https://zouchanglin.cn/tags/JVM/"}]},{"title":"探究MySQL锁机制","date":"2019-10-25T03:36:09.000Z","path":"2019/10/25/探究MySQL锁机制/","text":"锁的理解锁是计算机协调多个进程或线程并发访问某一资源的机制。 在数据库中，除传统的计算资源(如CPU、RAM、l/O等) 的争用以外，数据也是一种供许多用户共享的资源。如何保证数据并发访问的一致性、有效性是所有数据库必须解决的一个问题，锁冲突也是影响数据库并发访问性能的一一个重要因素。从这个角度来说锁对数据库而言显得尤其重要，也更加复杂。 最经典的例子莫过于抢购商品，如果还有一个库存的时候，这个时候如果还有另一个人买，那么如何解决是你买到还是另一个人买到的问的？这里肯定要用到事务，我们先从库存表中取出物品数量，然后插入订单，付款后插入付款表信息，然后更新商品数量。在这个过程中，使用锁可以对有限的资源进行保护，解决隔离和并发的矛盾。 锁的分类从数据操作的类型（读、写）分： 读锁（共享锁）：针对同一份数据，多个读操作可以同时进行而不会互相影响 写锁（排它锁）：当前写操作没有完成前，它会阻断其他写锁和读锁。 从对数据操作的颗粒度： 表锁、行锁 开销、加锁速度、死锁、粒度、并发性能，只能就具体应用的特点来说哪种锁更合适 表锁（偏读）特点：偏向MyISAM存储引擎，开销小，加锁快，无死锁，锁定粒度大，发生锁冲突的概率最高，并发最低 下面看看案例分析 建表插入数据语句 12345678910111213CREATE TABLE mylock(id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,NAME VARCHAR(20))ENGINE MYISAM;INSERT INTO mylock (NAME) VALUES ('a');INSERT INTO mylock (NAME) VALUES ('b');INSERT INTO mylock (NAME) VALUES ('c');INSERT INTO mylock (NAME) VALUES ('d');INSERT INTO mylock (NAME) VALUES ('e');SELECT * FROM mylock; 手动增加一个表锁 1lock table 表名称 read(write)，表名称2 read(write)，其他； 查看表上加过的锁 1show open tables; 现在呢，给mylock上读锁，给tbl_dept上写锁： 1lock table mylock read, tbl_dept write; 释放表锁 1unlock tables; 读阻塞写 - 示例好了，释放成功了接下来只给mylock加个读锁 1lock table mylock read; 可以看出，session1对mylock加了读锁，大家都是可以读的。下面看看其他的情况： session1 对mylock加读锁：1、大家都可以读2、自己不能修改、别人（session2）也不能修改3、自己也不能查询别的表4、别人（session2） 对mylock的表修改会发生阻塞5、自己（session1）解锁后，阻塞解除 现在session1解除对mylock表的读锁： session1对mylock表加的读锁，从session2想修改mylock表的数据到session1解锁，也就是个7分29秒，哈哈这很致命！ 写阻塞读 - 示例下面对mylock加写锁 session1 对mylock加写锁： 1、自己修改mylock表是ok的2、自己可以读mylock表3、自己不能读别的表4、别人（session2）可以读别的表，读mylock表发生阻塞5、别人（session2）更不能写mylock表了 session1解除写锁，session2才可以解除被阻塞的状态： 很显然，这次查询被session1阻塞了6分多钟才查询出结果。 注意：如果在session2中出现查询并未被阻塞的情况，那就是MySQL直接从缓存中取得数据，在MySQL5.7 以后的版本中不会发生此问题！ 得出结论MyISAM在执行查询语句(SELECT) 前，会自动给涉及的所有表加读锁，在执行增删改操作前，会自动给涉及的表加写锁。MySQL的表级锁有两种模式:表共享读锁 (Table Read Lock)表独占写锁 (Table Write Lock) 结合上表，所以对MyISAM表进行操作，会有以下情况:1、对MyISAM表的读操作(加读锁)，不会阻塞其他进程对同一表的读请求，但会阻塞对同一表的写请求。只有当读锁释放后，才会执行其它进程的写操作。2、对MyISAM表的写操作(加写锁)，会阻塞其他进程对同一表的读和写操作，只有当写锁释放后，才会执行其它进程的读写操作。 简而言之，就是读锁会阻塞写，但是不会堵塞读。而写锁则会把读和写都堵塞 表锁分析看看那些表被加锁了： 1show open tables; 0 表示未加锁，1 表示已经被锁定 如何分析表锁定 可以通过检查tables_locks_writed和table_locks_immediate 状态变量来分析系统上的表锁定： 1show status like 'table%'; 这里有两个状态变量记录MySQL内部表级锁定的情况，两个变量说明如下: Table_locks_immediate: 产生表级锁定的次数，表示可以立即获取锁的查询次数，每立即获取锁值加1 ;Table_locks_waited: 出现表级锁定争用而发生等待的次数(不能立即获取锁的次数，每等待一次锁值加1)，此值高则说明存在着较严重的表级锁争用情况； 所以，这也就是MyISAM引擎的缺陷：MyISAM的读写锁调度是写优先，这也是MyISAM不适合做写为主的表的引擎。因为写锁后，其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永远阻塞！ 行锁（偏写）偏向InnoDB存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 InnoDB与MyISAM的最大不同有两点：一是支持事务（TRANSACTION）；二是采用了行级锁 事物和ACID事务是由一组SQL语句组成的逻辑处理单元，事务具有以下4个属性，通常简称为事务的ACID属性。 1、原子性 (Atomicity) ：事务是一个原子操作单元， 其对数据的修改，要么全都执行，要么全都不执行。2、一致性(Consistent) : 在事 务开始和完成时， 数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改，以保持数据的完整性；事务结束时，所有的内部数据结构(如B树索引或双向链表)也都必须是正确的。3、隔离性(Isolation) : 数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的，反之亦然。4、持久性(Durable) : 事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。 并发事务处理带来的问题1、更新丢失 当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题，这就是最后的更新覆盖了由其他事务所做的更新。 例如，两个程序员修改同一个Java文件。每程序员独立地更改其副本，然后保存更改后的副本，这样就覆盖了原始文档。最后保存其更改副本的编辑人员覆盖前一个程序员所做的更改。如果在一个程序员完成并提交事务之前，另一个程序员不能访问同一文件，则可避免此问题。 2、脏读 一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致状态，这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些 “脏” 数据，并据此做进一步的处理， 就会产生未提交的数据依赖关系。这种现象被形象地叫做”脏读”。 一句话：事务A读取到了事务B已修改但尚未提交的的数据，还在这个数据基础上做了操作。此时，如果B事务回滚，A读取的数据无效，不符合一致性要求。 3、不可重复读 一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或某些记录已经被删除了！这种现象就叫做“不可重复读”。一句话：事务A读取到了事务B已经提交的修改数据，不符合隔离性 4、幻读 一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”。 一句话：事务A读取到了事务B体提交的新增数据，不符合隔离性 注意：幻读和脏读有点类似，脏读是事务B里面修改了数据，幻读是事务B里面新增了数据。 事物隔离级别脏读、不可重复读和幻读，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。 数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使事务在一定程度上 “串行化” 进行，这显然与“并发”是矛盾的。同时，不同的应用对读一致性和事务隔离程度的要求也是不同的，比如许多应用对 “不可重复读” 和 “幻读” 并不敏感，可能更关心数据并发访问的能力。 常看当前数据库的事务隔离级别: 1show variables like 'tx_isolation'; 案例分析1、建表SQL123456789101112131415CREATE TABLE test_innodb_lock (a INT(11), b VARCHAR(16))ENGINE=INNODB;INSERT INTO test_innodb_lock VALUES(1,'b2');INSERT INTO test_innodb_lock VALUES(3,'3');INSERT INTO test_innodb_lock VALUES(4, '4000');INSERT INTO test_innodb_lock VALUES(5, '5000');INSERT INTO test_innodb_lock VALUES(6,'6000');INSERT INTO test_innodb_lock VALUES(7,'7000');INSERT INTO test_innodb_lock VALUES(8,'8000');INSERT INTO test_innodb_lock VALUES(9,'9000');INSERT INTO test_innodb_lock VALUES(1,'b1');CREATE INDEX test_innodb_a_ind ON test_innodb_lock(a);CREATE INDEX test_innodb_lock_b_ind test_innodb_lock(b);SELECT * FROM test_innodb_lock; 2、行锁定基本演示 可以看出，session1加了行锁之后，修改一条数据，session1自己是可见的，但是session2查出来的数据仍然是原来未修改的数据！ 现在将session1的事物提交了，在看看session2查到的数据，还是4000，这是因为session2的事物还未提交： 所以接下来提交session2的事物，再次执行查询 证实了事物的隔离性，接下来试试修修改数据： session1 修改第四条数据，未提交session2 此时修改第四条数据，发生阻塞 此时，session1提交事务，session2解除被阻塞状态 对了，到这里顺吐槽一下datagrip，每次就算写连续的SQL它也要分成事物，比如我写了100行的Insert插入，没错，它会开100个事物…. 3、无索引行锁升级为表锁在建表的时候对a和b字段都建立了索引： 现在假设session1对b=’4000’ 这条记录做了修改，但是由于错误的写法导致索引失效： 很明显，虽然session1和session2操作的数据是同一条，但是session2被阻塞了，这就说明了如果修改数据的时候导致索引失效，那么行锁会变成表锁！ 同样的，变成表锁的话session2就需要等session1 commit之后才能提交事务！ 4、间隙锁的危害什么是间隙锁？当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙(GAP)” ，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁(Next-Key锁) 间隙锁的危害：因为Query执行过程中通过过范围查找的话，他会锁定整个范围内所有的索引键值，即使这个键值并不存在。间隙锁有一个比较致命的弱点， 就是当锁定一个范围键值之后， 即使某些不存在的键值也会被无辜的锁定，而造成在锁定的时候无法插入锁定键值范围内的任何数据，在某些场景下这可能会对性能造成很大的危害 好了，接下来看看这个现象 session1只是在更改一个范围的数据，session2只是在插入一条新的数据，但是同样发生了阻塞！ 同样的，需要等待session1 commit之后，session2才能插入成功！ 很显然，在InnoDB看来，就算没有2，但是你在我的范围里，那我也把你锁了，这也就是为什么session2会被是阻塞的原因！InnoDB想的就是宁可错杀一千不可放过一个，哈哈 5、如何锁定一行看看下面这个示例： 行锁结论Innodb存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面所带来的性能损耗可能比表级锁定会要更高一些，但是在整体并发处理能力方面要远远优于MyISAM的表级锁定的。当系统并发量较高的时候，Innodb的整体性能和MyISAM相比就会有比较明显的优势了。 但是，Innodb的行级锁定同样也有其脆弱的一面，当我们使用不当的时候，可能会让Innodb的整体性能表现不仅不能比MyISAM高，甚至可能会更差，也就是必须要避免行锁变表锁！ 1show status like 'innodb_row_lock%'; 对各个状态量的说明如下:Innodb_row_lock_current_waits: 当前正在等待锁定的数量Innodb_row_lock_ time: 从系统启动到现在锁定总时间长度Innodb_row_lock_time_avg: 每次等待所花平均时间Innodb_row_lock_time_max: 从系统启动到现在等待最常的一次所花的时间Innodb_row_lock_waits: 系统启动后到现在总共等待的次数 对于这5个状态变量，比较重要的主要是Innodb_row_lock_time_avg (等待平均时长)、Innodb_ row_lock_waits ( 等待总次数)、Innodb_row_lock_time (等待总时长) 这三项。尤其是当等待次数很高，而且每次等待时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手指定优化计划。 页锁开销和加锁时间界于表锁和行锁之间：会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 优化建议1、尽可能让所有数据检索都通过索引来完成，避免无索引行锁升级为表锁 2、合理设计索引，尽量缩小锁的范围 3、尽可能较少检索条件，避免间隙锁 4、尽量控制事务大小，减少锁定资源量和时间长度 5、尽可能低级别事务隔离 以下是2020-02-26 18:52补充的内容 MyISAM与InnoDB关于锁的区别 MyISAM默认用的是表级锁，不支持行级锁 InnoDB默认用的是行级锁，也支持表级锁 MyISAM与InnoDB的适用场景MyISAM适用于： 1、频繁执行全表count语句（因为MyISAM有一个字段用于统计全表的数据行数）2、对数据进行增删改的频率不高，查询非常频繁3、没有事务 InnoDB适用于： 1、数据增删改查都相当频繁的系统 2、可靠性要求比较高，要求支持事务 数据库锁的分类补充1、按锁的粒度划分：可分为表级锁、行级锁、页级锁（DBD支持页级锁）2、按锁级别划分：可分为共享锁、排它锁3、按加锁方式划分：可分为自动锁、显式锁4、按操作划分：可分为DML锁、DDL锁5、按使用方式划分：可分为乐观锁、悲观锁","updated":"2020-03-13T03:06:29.742Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"}]},{"title":"MySQL全局查询日志","date":"2019-10-23T13:54:42.000Z","path":"2019/10/23/MySQL全局查询日志/","text":"全局查询日志 全局查询日志用于保存所有的sql执行记录，该功能主要用于测试环境，在生产环境中永远不要开启该功能。 在MySQL的my.cn中，设置如下: 123456# 开启general_log=1# 记录日志文件的路径general_log_file=/path/logfile# 输出格式log_output=FILE 命令方式设置 12set global general_log=1;set global log_output='TABLE'; 此后，你编写的SQL语句，将会记录到MySQL库里的general_log表，可使用下面的命令查看 1select * from mysql.general_log; 通过以上配置，执行过的sql语句将会记录到MySQL库中general_log表中： 如果是通过命令方式开启的该功能，重启MYSQL后失效（这与其他配置是一致的），除非写在配置文件中。全局查询日志只用在测试环境，切记生产环境中永远不要开启该功能，这一点很重要！","updated":"2020-03-13T03:06:29.686Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"}]},{"title":"性能分析工具ShowProfile","date":"2019-10-23T10:36:47.000Z","path":"2019/10/23/性能分析工具ShowProfile/","text":"Show Profile是mysql提供的可以用来分析当前会话中sql语句执行的资源消耗情况的工具，可用于sql调优的测量。默认情况下处于关闭状态，并保存最近15次的运行结果。 通过show profiles查看sql语句的耗时时间，然后通过show profile命令对耗时时间长的sql语句进行诊断 。注意show profile诊断结果中出现相关字段的含义，判断是否需要优化sql语句 SQL批量插入1、建库建表123456789101112131415161718CREATE TABLE dept( id INT UNSIGNED PRIMARY KEY AUTO_INCREMENT, deptno MEDIUMINT UNSIGNED NOT NULL DEFAULT 0, dname VARCHAR(20) NOT NULL DEFAULT \"\", loc VARCHAR(13) NOT NULL DEFAULT \"\")ENGINE=INNODB DEFAULT CHARSET=GBK;CREATE TABLE emp( id INT UNSIGNED PRIMARY KEY AUTO_INCREMENT, empno MEDIUMINT UNSIGNED NOT NULL DEFAULT 0, /*编号*/ ename VARCHAR(20) NOT NULL DEFAULT \"\",/*名字*/ job VARCHAR(9) NOT NULL DEFAULT \"\",/*工作*/ mgr MEDIUMINT UNSIGNED NOT NULL DEFAULT 0,/*上级编号*/ hiredate DATE NOT NULL,/*入职时间*/ sal DECIMAL(7,2) NOT NULL,/* 薪水*/ comm DECIMAL(7,2) NOT NULL,/*红利*/ deptno MEDIUMINT UNSIGNED NOT NULL DEFAULT 0 /*部门编号*/)ENGINE=INNODB DEFAULT CHARSET=GBK; 2、设置参数创建函数，假如报错: This function has none of DETERMINIST…， 由于开启过慢查询日志，因为我们开启了bin-log，我们就必须为我们的function指定一个参数。 但是这样设置会导致的问题是：如果MySQL重启，上述参数又会丢失，所以到达到永久配置的效果，需要修改配置文件，在/etc/my.cnf[mysqld]下加上 global log_bin_trust_function_creators=1; 3、创建函数，保证数据的随机性随机产生字符串 123456789101112DELIMITER $$CREATE FUNCTION rand_string(n INT) RETURNS VARCHAR(255)BEGIN DECLARE chars_str VARCHAR(100) DEFAULT 'abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ'; DECLARE return_str VARCHAR(255) DEFAULT ''; DECLARE i INT DEFAULT 0; WHILE i&lt; n DO SET return_str = CONCAT(return_str, SUBSTRING(chars_str,FLOOR(1+RAND()*52),1)); SET i = i + 1; END WHILE; RETURN return_str;END $$ 随机产生数字编号 1234567DELIMITER $$CREATE FUNCTION rand_num( ) RETURNS INT(5)BEGIN DECLARE i INT DEFAULT 0; SET i = FLOOR(100 + RAND()*10);RETURN i;END $$ 4、创建存储过程向emp表存储数据的存储过程 1234567891011121314/* 建立存储过程（插入数据emp）*/DELIMITER $$CREATE PROCEDURE insert_emp(IN START INT(10) ,IN max_num INT(10) )BEGINDECLARE i INT DEFAULT 0;#set autocommit =0把autocommit设置成0SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO emp(empno, ename, job, mgr, hiredate, sal, comm, deptno) VALUES ((START+i), rand_string(6) , 'SALESMAN', 0001, CURDATE(), 2000, 400, rand_num()); UNTIL i = max_num END REPEAT; COMMIT;END $$ 向dept表存储数据的存储过程 12345678910111213/* 向dept表存储数据的存储过程 */DELIMITER $$CREATE PROCEDURE insert_dept(IN START INT(10),IN max_num INT(10))BEGINDECLARE i INT DEFAULT 0;SET autocommit = 0; REPEATSET i = i+ 1;INSERT INTO dept(deptno, dname, loc) VALUES ((START+i) ,rand_string(10), rand_string(8));UNTIL i = max_numEND REPEAT;COMMIT; END $$ 5、调用存储过程由于定义函数的时候是以$$ 这个符号作为结束符，现在要更换为普通语句： 1DELIMITER; 下面开始调用： 1CALL insert_dept(100, 10); 试试向emp表添加50万条数据： 1CALL insert_emp(100001, 500000); 哈哈，虚拟机还可以，OK 妥妥的50万条数据！ Show profiles1、Show profiles是什么Show profiles是什么：是mysql提供可以用来分析当前会话中语句执行的资源消耗情况，可以用于SQL的调优测量 这就好比去超市买东西，买什么东西花了多少钱都是有明确的记录的，Show profiles也是一样，记录SQL执行步骤耗时，每一步都做了记录。默认情况下，参数处于关闭状态，并保存最近15次的运行结果 2、Show profiles分析步骤① 查看当前版本是否支持 ② 开启功能，默认是关闭，使用前需要开启 1show variables like 'profiling%' 把之前的测试数据放过来 1234567891011121314151617181920212223242526272829303132create table tbl_dept( id int(11) not NULL auto_increment, deptName varchar(30) default NULL, locAdd varchar(40) default NULL, primary key (id))engine=INNODB auto_increment=1 default charset=utf8;create table tbl_emp( id int(11) not null auto_increment, name varchar(20) default null, deptId int(11) default null, primary key (id), key fk_dept_id(deptId) #constraint fk_dept_id foregin key('deptId') references tbl_dept(id))engine=innodb auto_increment=1 default charset=utf8;insert into tbl_dept(deptName, locAdd) values ('RD', 11);insert into tbl_dept(deptName, locAdd) values ('HR', 12);insert into tbl_dept(deptName, locAdd) values ('MK', 13);insert into tbl_dept(deptName, locAdd) values ('MIS', 14);insert into tbl_dept(deptName, locAdd) values ('FD', 15);insert into tbl_emp(name, deptId) VALUES ('z3', 1);insert into tbl_emp(name, deptId) VALUES ('z4', 1);insert into tbl_emp(name, deptId) VALUES ('z5', 1);insert into tbl_emp(name, deptId) VALUES ('z3', 1);insert into tbl_emp(name, deptId) VALUES ('w5', 2);insert into tbl_emp(name, deptId) VALUES ('w6', 2);insert into tbl_emp(name, deptId) values ('s7', 3);insert into tbl_emp(name, deptId) values ('s8', 4);insert into tbl_emp(name, deptId) values ('s9', 51); 然后开启Show profile，进行了几条查询再show profile 下面开始选取一条开始分析。 3、分析执行过程现在假设分析的是语句3 ，也就是select * from tbl_dept: 从上面可以看出，通过show profile 的分析，完整的呈现了一条SQL执行的全流程，配合着MySQL架构模型，其实很容易看出，先进行权限检查，打开表，初始化，优化器优化等等一系列的执行流程… 只能查看CPU和IO吗？当然不是，下面给出了常用的查询字段： ①ALL：显示所有的开销信息。 ②BLOCK IO：显示块IO开销。 ③CONTEXT SWITCHES：上下文切换开销。 ④CPU：显示CPU开销信息。 ⑤IPC：显示发送和接收开销信息。 ⑥MEMORY：显示内存开销信息。 ⑦PAGE FAULTS：显示页面错误开销信息。 ⑧SOURCE：显示和Source_function，Source_file，Source_line相关的开销信息。 ⑨SWAPS：显示交换次数开销信息 4、日常开发需要注意的结论Status里面出现的字段： ① converting HEAP to MyISAM 查询结果太大，内存都不够用了往磁盘上搬了。 ② creating tmp table 创建临时表：说说创建临时表为什么这么费事呢？首先需要新建临时表，然后需要拷贝数据到临时表，数据推送后需要删除数据，这也就是为什么创建临时表非常损耗性能的原因 ③ copying to tmp table on disk 把内存中临时表复制到磁盘，危险！！！这说明临时表都存不下了，只能往磁盘丢 ④ locked 锁定了 如果在show profile诊断结果中出现了以上4条结果中的任何一条，则sql语句需要优化。 下面看看临时表的处理过程： 可以看出，拷贝数据到临时表是非常消耗时间的！","updated":"2020-03-13T03:06:29.741Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"}]},{"title":"SSH免密登录","date":"2019-10-20T06:12:46.000Z","path":"2019/10/20/SSH免密登录/","text":"putty这个软件是个轻量且好用的ssh工具，Xshell、Xftp这些需要收费，有学生家庭版但是要申请，而且有一定的期限的，不是很方便。putty这个工具倒是不错，可是每次登陆需要输入密码，于是下面挤在一个无需登录密码的方式，那就是SSH密钥登录， 这种方式需要生成一组对应的公钥和密钥，然后把公钥放到Linux，私钥提供给putty。putty仍然不知道你的密码，而是通过与服务器核对密钥而核实身份。 第一步：配置用户名 第二步： 生成公钥/私钥对 按照提示：不断移动鼠标这个进度条才会更新 第四步：保存生成功的密钥： 点击保存私钥(Save Private Key)，保存一个.ppk文件。 第五步：关联私钥文件 请先用putty，输用户名密码登录Linux， vim ~/.ssh/authorized_keys 把刚才copy下来的公钥粘到这个文件里去，直接右键即可粘贴，这样便实现了免密登录。 接下来还有个技巧: 创建putty.exe的快捷方式到桌面；然后运行putty，输入host name、port点击保存名称为 xxx 1&quot;C:\\Program Files\\PuTTY\\putty.exe&quot; -load &quot;xxx&quot; -ssh -l root -pw lhl123456an+","updated":"2020-03-13T03:06:29.706Z","categories":[{"name":"开发工具","slug":"开发工具","permalink":"https://zouchanglin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"SSH","slug":"SSH","permalink":"https://zouchanglin.cn/tags/SSH/"}]},{"title":"MySQL慢查询日志","date":"2019-10-16T13:43:17.000Z","path":"2019/10/16/MySQL慢查询日志/","text":"慢查询日志什么是慢查询日志MySQL的慢查询日志是 MySQL提供的一种日志记录，它用来记录在 MySQL 中响应时间超过阀值的语句，具体指运行时间超过long_query_time 值的 SQL，则会被记录到慢查询日志中 具体指运行时间超过 long_query_time 值的 SQL，则会被记录到慢查询日志中。long_query_time 的默认值为 10, 意思是运行 10 秒以上的语句 由它来查看哪些 SQL 超出了我们的最大忍耐时间值，比如一条SQL执行超过 5 秒钟，我们就算慢 SQL，希望能收集超过 5 秒的SQL，结合之前explain进行全面分析 默认情况下，MySQL 数据库没有开启慢査询日志，需要我们手动来设置这个参数。当然，如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。慢査询日志支持将日志记录写入文件！ 如何开启慢查询查看开启状态 1SHOW VARIABLES LIKE '%slow_query_log%' 开启慢查询 1set global slow_query_log = 1 使用 set global_slow_query_log = 1 开启了慢查询日志只对当前数据库生，如果 MYSQL 重启后则会失效。 如果要永久生效，就必须修改配置文件 my.cnf（其它系统变量也是如此）修改 my.cnf 文件，[mysqld]下增加或修改参数 slow_query_log 和 slow_query_log_file 后，然后重启 MySQL 服务器。也即将如下两行配置进my.cnf文件 123slow_query_log =1slow_query_log_file=/var/lib/mysql/tim-slow.log 关于慢查询的参数 slow_query_log_fie，它指定慢查询日志文件的存放路径，系统默认会给一个缺省的文件host_name-slow.log（如果没有指定参数 slow_query_log_file的话） 那么开启慢查询日志后，什么样的SQL参会记录到慢查询里面？ 通过 show variables like ‘long_query_time%’ 来查看默认时间长度，单位是秒： 同样的，可以使用命令修改，也可以在my.cnf里面配置。假如运行时间正好等于 long_query_time 的情况，并不会被记录下来。也就是说，在MySQL源码里是判断大于 long_query_time，而非大于等于! 设置记录的阈值： 1set global long_query_time=3; 设置了但是还是没有发生更改？为什么？此时需要重新开启一个会话才可以： 接下来实行一个较慢的查询，如下图，但是记得要在配置文件中做如下配置： 接着去日志文件中查看存在哪些超过阈值的SQL就好了： 查询当前系统中有多少条慢查询记录： 记载一下我的配置文件 1234slow_query_log=1;slow_query_log_file=/var/lib/mysql/tim-slow.log;long_query_time=3;log_output=FILE 日志分析工具mysqldumpshow 在生产环境中，如果要手工分析日志，查找、分析 SQL，显然是个体力活，MYSQL 提供了日志分析工具mysqldumpshow s:是表示按何种方式排序 c:访问次数 l:锁定时间 r:返回记录 t:查询时间 al:平均锁定时间 ar:平均返回记录数 at:平均查询时间 t:即为返回前面多少条的数据 g:后边搭配一个正则匹配模式，大小写不敏感的 下面是使用示例： 得到返回记录集最多的 10 个 SQL 1mysqldumpslow -s r -t 10 /var/lib/mysql/tim-slowlog 得到访问次数最多的 10 个 SQL 1mysqldumpslow -s c-t 10 /var/lib/mysql/tim-slow log 得到按照时间排序的前 10 条里面含有左连接的查询语句 1mysqldumpslow -s t -t 10 -g \"left join\" /var/lib/mysql/tim-slowlog 另外建议在使用这些命令时结合和 more 使用，否则有可能出现爆屏情況 1mysqldumpslow -s r -t 10 /var/lib/mysq/tim-slow.log | more","updated":"2020-03-13T03:06:29.691Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"}]},{"title":"MySQL的docker容器中安装vim","date":"2019-10-16T13:09:49.000Z","path":"2019/10/16/MySQL的docker容器中安装vim/","text":"如何在MySQL的docker容器中安装vim以及其他的工具？ 先进入MySQL容器： 1docker exec -it &lt;mysql容器id&gt; bash 配置网易的镜像源 12345678910mv /etc/apt/sources.list /etc/apt/sources.list.bakecho \"deb http://mirrors.163.com/debian/ jessie main non-free contrib\" &gt; /etc/apt/sources.listecho \"deb http://mirrors.163.com/debian/ jessie-proposed-updates main non-free contrib\" &gt;&gt;/etc/apt/sources.listecho \"deb-src http://mirrors.163.com/debian/ jessie main non-free contrib\" &gt;&gt;/etc/apt/sources.listecho \"deb-src http://mirrors.163.com/debian/ jessie-proposed-updates main non-free contrib\" &gt;&gt;/etc/apt/sources.listapt-get update 安装vim编辑器 1apt-get install vim 安装其他的工具亦是如此！","updated":"2020-03-13T03:06:29.692Z","categories":[{"name":"开发工具","slug":"开发工具","permalink":"https://zouchanglin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"}]},{"title":"order by与group by的优化","date":"2019-10-15T10:28:00.000Z","path":"2019/10/15/orderby与groupby的优化/","text":"性能常见分析步骤1、慢查询的开启并捕获 2、explain+慢 SQL 分析 3、show profile 查询 SQL 在 MySQL 服务器里面的执行细节和生命周期情况 4、SQL数据库服务器的参数调优。 小表驱动大表1234567891011for(int i = 0; i &lt; 5; i++)&#123; for(int j = 0; j &lt; 1000; j++)&#123; ... &#125;&#125;for(int i = 0; i &lt; 1000; i++)&#123; for(int j = 0; j &lt; 5; j++)&#123; ... &#125;&#125; 用上面的代码举例子，如果是第一种for，那么就比如我们只建立了5次链接，而第二个for建立了1000次链接，毫无疑问，建立少的链接次数才是更优的选择！ 优化原则：小表驱动大表，即小的数据集驱动大的数据集。 原理： 1234567select * from A where id in (select id from B)等价于for select id from Bfor select * from A where A.id = B.id 当B表的数据集必须小于A表的数据集的时候，用in优先于exists 1234567select * from A where exists (select 1 from B where B.id = A.id)等价于for select id from Afor select * from B where B.id = A.id 当A表的数据集小于B表的数据集的时候，用exists 优于in（A、B两表都应建立索引） exists 用法：select … from table where exists (subquery) 该语法可以理解为：将主查询的数据，放到子查询中做条件验证，根据验证结果（TRUE 或 FALSE）来决定主查询的数据结果是否得以保留。 1、exists（subquery）只返回true或者false，因此子査询中的 select * 也可以是 select 1 或其他，官方说法是实际执行时会忽略 SELECT 清单，因此没有区别 2、exists 子查询的实际执行过程可能经过了忧化而不是我们理解上的逐条对比，如果担忧效率问題，可进行实际检验以确定是否有效率问题 3、exists 子査询往往也可以用条件表达式、其他子査询或者 JOIN 来替代，何种最优需要具体回题具体分析 order by优化ORDER BY子句，尽量使用Index方式排序，避免使用FileSort方式排序 建表 + 插入数据 + 新建索引示例： 1234567891011121314use day_07;create table tblA( #id int primary key not null auto_increment, age int, birth TIMESTAMP not null);insert into tblA(age, birth) VALUES (22, NOW());insert into tblA(age, birth) VALUES (23, NOW());insert into tblA(age, birth) VALUES (24, NOW());create index idx_A_ageBirth ON tblA(age, birth);select * from tblA; MySQL支持二种方式的排序，FileSort和Index，Index效率高。 它指MySQL扫描索引本身完成排序。FileSort方式效率较低。 order by满足两情况，会使用Index方式排序： 1、order by语句使用索引最左前列，这其实就是最佳左前缀法则 2、使用where子句与order by子句条件列组合满足索引最左前列，这同样也是符合最佳左前缀法则 所以，尽可能在索引列上完成排序操作，遵照索引建的最佳左前缀 FileSort两种排序如果不在索引列上，filesort有两种算法：双路排序和单路排序 双路排序：MySQL4.1 之前是使用双路排序，字面意思是两次扫描磁盘，最终得到数据。读取行指针和orderby列，对他们进行排序，然后扫描已经排序好的列表，按照列表中的值重新从列表中读取对应的数据传输。磁盘IO是非常耗时的，从磁盘取排序字段，在buffer进行排序，再从磁盘取其他字段。所以这就是两次磁盘扫描，也就是双路排序。 取一批数据，要对磁盘进行两次扫描，IO很耗时的，所以在mysql4.1之后，出现了第二张改进的算法，就是单路排序。 单路排序：从磁盘读取查询需要的所有列，按照orderby列在buffer对它们进行排序，然后扫描排序后的列表进行输出，它的效率更快一些，避免了第二次读取数据，并且把随机IO变成顺序IO，但是它会使用更多的空间，因为它把每一行都保存在内存中了。但是这种情况就害怕遇到一次拿数据拿不完的情况，那这样的话需要拿取多次数据，也就是多次IO，还不如以前的双路算法！ 两种排序的坑在 sort_buffer 中，方法B比方法A要多占用很多空间，因为方法B是把所有字段都取出，所以有可能取出的数据的总大小超出了sort_buffer的容量，导致每次只能取 sort_buffer容量大小的数据，进行排序（创建 tmp 文件，多路合并），排完再去取sort_buffer容量大小， 再排…从而多次IO，本来想省一次IO操作，反而导致了大量的O 操作，反而得不偿失 order by优化策略优化策略：增大sort_buffer_size参数的设置、增大max_length_for_sort_data参数的设置 1、order by时 select * 是一个大忌，只Query 需要的字段，这点非常重要。在这里的影响是： 当 Query 的字段大小总和小于 max_ length_for_ sort data 而且排序字段不是 TEXT|BLOB 类型时，会用改进后的算法——单路排序，否则用老算法一一多路排序 两种算法的数据都有可能超出 sort_buffer的容量，超出之后，会创建tmp文件进行合并排序，导致多次IO，但是用单路排序算法的风险会更大ー些，所以要提高 sort_buffer_size 2、尝试提高 sort_buffer_size 不管用哪种算法，提高这个参数都会提高效率，当然，要根据系统的能力去提高，因为这个参数是针对每个进程的 3、尝试提高 max_ length_for_sort_data 提高这个参数，会增加用改进算法的概率。但是如果设的太高，数据总容量超出 sort_buffer_size 的概率就增大，明显症状是高的磁盘IO活动和低的处理器使用率。 order by优化总结Mysq 两种排序方式：文件排序或扫描有序索引排序， Mysql 能为排序与查询使用相同的索引 KEY a_b_c (a, b, c） Order by 能使用索引最左前缀 1234567order by aorder by a, border by a, b, corder by a DESC, b DESC, c DESC 如果 where 使用索引的最左前缀定义为常量，则 order by 能使用索引 12345where a = const order by b, cwhere a = const and b = const order by cwhere a = const and b &gt; const order by c 不能使用索引进行排序 123456789order by a ASC, b DESC, c DESC # 排序方式不一致where g = const order by b, c # 丢失a索引where a = const order by c # 丢失b索引where a = const order by d # d不是索引的一部分where a in (...) order by b, c # 对于排序来说，多个相等条件也是范围查询 group by优化group by的优化和order by如出一辙， group by实质是先排序后进行分组，遵照索引建的最佳左前缀。当无法使用索引列，增大max_length_for_sort_data参数的设置或者增大sort_buffer_size参数的设置。where高于having，能写在where限定的条件就不要去having限定了。","updated":"2020-03-13T03:06:29.702Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"}]},{"title":"synchronized底层实现与优化","date":"2019-10-14T09:42:55.000Z","path":"2019/10/14/synchronized底层实现与优化/","text":"线程的同步与死锁是多线程里面最需要重点理解的概念。这种操作的核心问题在于：每一个线程对象轮番强占资源带来的问题。 同步问题的引出需求：多个线程同时卖票(经典的卖票问题) 一、问题引出线程的同步与死锁是多线程里面最需要重点理解的概念。这种操作的核心问题在于：每一个线程对象轮番强占资源带来的问题。 同步问题的引出需求：多个线程同时卖票(经典的卖票问题) 123456789101112131415161718192021222324252627package com.xpu.demo_03;class B implements Runnable&#123; private int ticket = 100; @Override public void run() &#123; while(ticket &gt; 0)&#123; try &#123; //模拟网络延迟 Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"剩余：\"+ticket--+\"张票\"); &#125; &#125;&#125;public class Demo2 &#123; public static void main(String[] args) &#123; B b = new B(); Thread t1 = new Thread(b, \"黄牛A\"); Thread t2 = new Thread(b, \"黄牛B\"); Thread t3 = new Thread(b, \"黄牛C\"); t1.start(); t2.start(); t3.start(); &#125;&#125; 二、问题分析明明写的是ticket&gt;0才进入并进行ticket–，为啥出现了-1这种情况呢？明显是不合常理的，这种问题我们称之为不同步操作。不同步唯一的好处就是处理速度快！ 其实本质上就是最后一张票被大家卖了三次，所以出现了这种问题！ 三、问题解决本次将采用synchronized处理同步问题，synchronized处理同步问题有两种方式，同步代码块和同步方法： 1234567891011121314151617181920class B implements Runnable&#123; private int ticket = 1000; @Override public void run() &#123; for (int i = 0; i &lt; 1000; i++) &#123; synchronized (this)&#123; if(this.ticket&gt;0) &#123; try &#123; //模拟网络延迟 Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \"剩余：\" + ticket-- + \"张票\"); &#125; &#125; &#125; &#125;&#125; 上面使用了同步代码块 : 如果要使用同步代码块必须设置一个要锁定的对象，所以一般可以锁定当前对象：this，接下来试一下同步方法：同步方法标识此方法只有一个线程可以进入，包含一个隐式锁对象：this 12345678910111213141516class B implements Runnable&#123; private int ticket = 1000; @Override public void run() &#123; for (int i = 0; i &lt; 1000; i++) &#123; sell(); &#125; &#125; private synchronized void sell()&#123; if(ticket &gt; 0)&#123; System.out.println(Thread.currentThread().getName() + \"剩余：\" + ticket-- + \"张票\"); &#125; &#125;&#125; 四、synchronized锁的是什么？接下来看看这样的一种情况： 1234567891011121314151617181920212223242526272829303132class Sync&#123; public synchronized void fun()&#123; System.out.println(Thread.currentThread().getName()+\"fun方法开始\"); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+\"fun方法结束\"); &#125;&#125;class MyThread implements Runnable&#123; @Override public void run() &#123; Sync sync = new Sync(); sync.fun(); &#125;&#125;public class Demo &#123; public static void main(String[] args) &#123; MyThread myThread = new MyThread(); Thread thread1 = new Thread(myThread, \"线程A\"); Thread thread2 = new Thread(myThread, \"线程B\"); Thread thread3 = new Thread(myThread, \"线程C\"); thread1.start(); thread2.start(); thread3.start(); &#125;&#125; 按照道理来说，对于Sync的fun方法，应该同一时刻只会有一个线程进入，但是其实这里的同步方法的锁对象却不是同一个对象，三个线程new了三个对象，大家都抱着自己的锁，所以想要锁住必须是同一个对象锁！！！ 在同步方法上面加上static就好了，为什么呢？ 12345678910111213class Sync&#123; public static synchronized void fun()&#123; System.out.println(Thread.currentThread().getName() +\"fun方法开始\"); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() +\"fun方法结束\"); &#125;&#125; static声明的方法是类方法，与对象无关了，所以只要是这个类的对象都可以锁住，此时用的锁对象是Sync.class对象，Sync.class对象在虚拟机中只有一份！ 123456789101112131415161718192021222324252627282930313233343536373839class Sync&#123; public synchronized void fun()&#123; System.out.println(Thread.currentThread() .getName()+\"fun方法开始\"); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread() .getName()+\"fun方法结束\"); &#125;&#125;class MyThread implements Runnable&#123; private Sync sync; public MyThread(Sync sync) &#123; this.sync = sync; &#125; @Override public void run() &#123; sync.fun(); &#125;&#125;public class Demo &#123; public static void main(String[] args) &#123; Sync sync = new Sync(); MyThread myThread = new MyThread(sync); Thread thread1 = new Thread(myThread, \"线程A\"); Thread thread2 = new Thread(myThread, \"线程B\"); Thread thread3 = new Thread(myThread, \"线程C\"); thread1.start(); thread2.start(); thread3.start(); &#125;&#125; 再思考两个问题(前提是线程中是同一个对象)： 123456789class Sync&#123; public synchronized void funA()&#123; //问题一：线程1进入funA方法了,线程2能否进入funB()? while(true)&#123; //问题二：线程1在这里是否可以调用funB()? &#125; &#125; public synchronized void funB()&#123;&#125;&#125; 对于问题一：很显然不能，同步方法采用的是对象锁，如果是同一个对象的话，如果A线程获取到了锁，而且进入死循环，那么其他的线程是获取不到锁的，自然其他的线程也就无法进入线程B！ 对于问题二：很显然可以，对象已经获得锁了，那么在这期间它干什么都是可以的，直到释放锁之前，一切畅通无阻！ 五、synchronized底层分析首先，我们归纳一下同步的方式，以及相应的方式应锁的是什么：一、同步代码块：1、锁类的实例对象 1synchronized(this)&#123;...&#125; 2、锁类对象（class对象），又叫做全局锁，可以理解为把代码锁住了，无论多少对象产生，一定能锁住 1synchronized(XXXX.class)&#123;...&#125; 3、锁任意实例对象 1synchronized(new String()) 二、同步方法1、普通方法+synchronized：锁的是当前对象2、静态方法+synchronized：锁的是类，也是全局锁，效果等同于同步代码块的锁类对象 对象锁（monitor）机制 —— JDK1.6的重量级锁一、同步代码块：12345678public class Demo &#123; public static void main(String[] args) &#123; Object object = new Object(); synchronized (object)&#123; System.out.println(\"hello synchronized\"); &#125; &#125;&#125; 使用javap -V 查看反编译后的输出信息： 1234567891011121314151617181920212223242526272829public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=4, args_size=1 0: new #2 // class java/lang/Object 3: dup 4: invokespecial #1 // Method java/lang/Object.\"&lt;init&gt;\":()V 7: astore_1 8: aload_1 9: dup 10: astore_2 11: monitorenter 12: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 15: ldc #4 // String hello synchronized 17: invokevirtual #5 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 20: aload_2 21: monitorexit 22: goto 30 25: astore_3 26: aload_2 27: monitorexit 28: aload_3 29: athrow 30: return Exception table: from to target type 12 22 25 any 25 28 25 any 执行同步代码块后首先要执行monitorenter指令，退出时要执行monitorexit指令。 使用内建锁synchronized进行同步，关键在于要获取指定锁对象的monitor对象，当线程获取到monitor后才能向下执行，否则就只能等待，这个获取的过程是互斥的，即同一时刻只有一个线程能够获取到对象monitor。 通常一个monitorenter指令会包含若干个monitorexit指令，原因在于JVM需要确保锁在正常执行路径以及异常执行路径都能正确的解锁！ 二、同步方法123456public class Demo &#123; public static void main(String[] args) &#123; fun(); &#125; public static synchronized void fun()&#123;&#125;&#125; 一：当使用synchronized标记方法时，编译后字节码中的方法访问标记多了一个ACC_SYNCHRONIZED。该标记表示：进入该方法时，JVM需要进行monitorenter操作，退出方法时，无论是否正确返回，JVM均需要进行monitorexit操作。 二、当执行monitorenter时，如果目标锁对象的monitor计数器为0，表示此对象没有被任何其他对象所持有，此时JVM会将该锁对象的持有线程设置为当前线程，并将计数器+1； 三、如果目标锁对象的计数器不为0，判断目标锁对象的持有持有线程是不是当前线程，如果是再次将计数器+1（锁的可重入性），如果锁对象的持有线程不是当前线程，当前线程需要等待，直到持有线程释放锁。 之所以采用这种计数器的方式，是为了允许同一个线程重复获取同一把锁。举个例子，如果一个 Java 类中拥有多个 synchronized 方法，那么这些方法之间的相互调用，不管是直接的还是间接的，都会涉及对同一把锁的重复加锁操作。因此，我们需要设计这么一个可重入的特性，来避免编程里的隐式约束。 四、当执行monitorexit指令时，JVM会将锁对象的计数器-1，当计数器减为0时，表示该锁对象已经被释放。 可重入性的证明： 12345678910111213141516171819202122232425class MyThread implements Runnable&#123; @Override public void run() &#123; test1(); &#125; public synchronized void test1()&#123; System.out.println(\"A线程进入test1()...\"); test2(); &#125; public synchronized void test2()&#123; System.out.println(Thread.currentThread().getName()+ \"线程进入test2()...\"); &#125; &#125;&#125;public class Demo &#123; public static void main(String[] args)&#123; MyThread my = new MyThread(); Thread threadA = new Thread(my, \"A\"); threadA.start(); &#125;&#125; 互斥的证明： 123456789101112131415161718192021222324252627282930313233343536class MyThread implements Runnable&#123; @Override public void run() &#123; test1(); test2(); &#125; public synchronized void test1()&#123; System.out.println(\"A线程进入test1()...\"); if(Thread.currentThread().getName().equals(\"A\"))&#123; while (true)&#123;&#125; &#125; &#125; public synchronized void test2()&#123; if(Thread.currentThread().getName().equals(\"B\"))&#123; System.out.println(\"B线程进入该同步方法test2()..\"); &#125;else&#123; System.out.println(Thread.currentThread().getName()+\"线程进入test2()...\"); &#125; &#125;&#125;public class Demo &#123; public static void main(String[] args)&#123; MyThread my = new MyThread(); Thread threadA = new Thread(my, \"A\"); threadA.start(); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; Thread threadB = new Thread(my, \"B\"); threadB.start(); &#125;&#125; 好了，JDK6之前的对象锁（monitor）机制已经讲述完毕了，这个其实是重量级锁，为什么是重量级锁呢？因为这个锁是阻塞式的，这也就意味着效率非常低，没有成功获取锁的线程会阻塞，会从用户态切换到内核态，再次切换回去又是一笔重大的开销，所以JDK6之前的对象锁是重量级的锁，效率很低，于是在JDK5出现了Lock体系，synchronized对象锁也得到了优化。 interrupt():1、线程中没有调用wait()、sleep()、join()，isInterrupt()：true2、线程调用了wait()、sleep()、join()，调用interrupt引发中断异常，catch中捕获异常，退出，中断状态设置为false JDk1.5提供的Lock锁12345678910111213141516171819202122232425class MyThread implements Runnable &#123; private int ticket = 100; private Lock lock = new ReentrantLock(); @Override public void run() &#123; for (int i = 0; i &lt; 100; i++) &#123; try &#123; lock.lock(); try &#123; Thread.sleep(20); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if(ticket &gt; 0)&#123; System.out.println(Thread.currentThread().getName() +\"剩下\"+ticket--+\"张票\"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;&#125; 在JDK1.5中，synchronized是性能低效的。因为这是一个重量级操作，它对性能最大的影响是阻塞的是实现，挂起线程和恢复线程的操作都需要转入内核态中完成，这些操作给系统的并发性带来了很大的压力。相比之下使用Java提供的Lock对象，性能更高一些。 到了JDK1.6，发生了变化，对synchronize加入了很多优化措施，有自适应自旋，锁消除，锁粗化，轻量级锁，偏向锁等等。导致在JDK1.6上synchronize的性能并不比Lock差。官方也表示，他们也更支持synchronized，在未来的版本中还有优化余地，所以还是提倡在synchronized能实现需求的情况下，优先考虑使用synchronized来进行同步。 synchronized优化现在我们对Synchronized应该有所印象了，它最大的特征就是在同一时刻只有一个线程能够获得对象的监视器（monitor），从而进入到同步代码块或者同步方法之中，即表现为互斥性（排它性）。这种方式肯定效率低下，每次只能通过一个线程，既然每次只能通过一个，这种形式不能改变的话，那么我们能不能让每次通过的速度变快一点呢？ 打个比方，去收银台付款，之前的方式是，大家都去排队，然后取纸币付款收银员找零，有的时候付款的时候在包里拿出钱包再去拿出钱，这个过程是比较耗时的，然后，支付宝解放了大家去钱包找钱的过程，现在只需要扫描下就可以完成付款了，也省去了收银员跟你找零的时间的了。同样是需要排队，但整个付款的时间大大缩短，是不是整体的效率变高速率变快了？这种优化方式同样可以引申到锁优化上，缩短获取锁的时间。 CAS操作CAS操作概念悲观锁（JDK1.6之前的内建锁）：使用锁时，线程获取锁是一种悲观锁策略，即假设每一次执行临界区代码都会产生冲突，所以当前线程获取到锁的时候同时也会阻塞其他线程获取该锁。乐观锁（LOCK）：假设所有线程访问共享资源时不会出现冲突，既然不会出现冲突自然就不会阻塞其他线程，线程不会出现被阻塞状态。 CAS操作（又称为无锁操作）是一种乐观锁策略，那么，如果出现冲突了怎么办？无锁操作是使用CAS(compare and swap)又叫做比较交换来鉴别线程是否出现冲突，出现冲突就重试当前操作直到没有冲突为止。 CAS操作过程一般来讲，CAS交换过程分三个阶段（V，O，N）V:内存中地址存放的实际值O:预期值（旧值）N:更新后的值 当执行CAS后，当V==O，即旧值和内存中实际值相等，表示上次修改后没有任何线程再次修改此值，因此可将N替换到内存中，如果V!=O，表示内存中的值已经被其他线程修改，所以无法将N替换，返回最新的V值。 当多个CAS操作同一个变量时，只有一个线程会成功，并成功更新变量值，其余线程均会失败，失败线程会重新尝试或将线程挂起(阻塞) 元老级内建锁（Synchronized）最主要的问题：当存在线程竞争情况下会出现线程阻塞以及唤醒带来的性能问题，对应互斥同步（阻塞同步），效率降低。而CAS并不是武断地将线程挂起，而是会尝试若干次CAS操作，并非进行耗时的挂起与唤醒操作，因此非阻塞式同步。 CAS的实现需要硬件指令集的支撑，在JDK1.5后虚拟机才可以使用处理器提供的CMPXCHG指令实现！ CAS操作的问题1、A-B-A问题：比如线程1检测到变量值为：A，但是线程2修改为B了，线程3把B修改为A了，A去检查结果还是A，以为没有人更改过！ 解决方案：沿用数据库的乐观锁机制，添加版本号1A-&gt;2B-&gt;3AJDK1.5提供atomic包下AtomicStampedeReference来解决CAS的ABA问题 2、自旋（CAS）会浪费大量的处理器资源阻塞不会浪费CPU资源，与线程阻塞相对比，自旋会浪费大量CPU资源，因为此时线程仍处于运行状态，只不过跑的是无用指令，期望在无用指令时，锁能被释放出来。 解决方案：自适应自旋。根据以往自旋等待时能否获取到锁来动态调整自旋时间（循环尝试的数量），如果在上一次自旋时获取到锁，则此次自旋时间长一点，如果上一次自旋结束还没有获取到锁，此次自旋时间短一点。打个比方：如果你上一次看到十字路口是红灯，你是选择刹车呢（自旋）？还是直接熄火（线程挂起）？结果你等了好长时间也没变绿灯，那么你下次再次遇到红灯的时候会选择刹车时间短一点，如果在你的刹车时间内还没变绿灯，那么就选择熄火；如果你等了很短一段时间就变成了绿灯，那么下次你肯定踩刹车的时间会更长一点，因为你相信很短的时间就可以等到绿灯，车就不用熄火了！ 3、公平性问题很显然，如果刚好显示绿灯了，之前踩刹车的人肯定比重新打火的人更快的跑起来，所以自旋的线程其实相对于挂起的线程更容易获取到锁，这就导致了不公平的问题。处于阻塞状态的线程无法立刻竞争被释放的锁，而处于自旋状态的线程很可能先获取到锁。内建锁无法实现公平性。lock体系可以实现公平锁（一定会让等待时间最长的线程最先获取到锁）。 Java对象头JDK1.6之后对内建锁做了优化（新增偏向、轻量级锁），下面是锁的四种状态，锁状态在对象头的mark word中 无锁状态 0 01 偏向锁 1 01 轻量级锁 00 重量级锁（JDK1.6之前）10 这四种状态随着竞争情况逐渐升级，锁可以升级不能降级，为了提高获得锁与释放锁的效率， 在同步的时候是获取对象的monitor,即获取到对象的锁。那么对象的锁怎么理解？无非就是类似对对象的一个标志，那么这个标志就是存放在Java对象的对象头。Java对象头里的Mark Word里默认的存放的对象的Hashcode，分代年龄和锁标记位。32位JVM Mark Word默认存储结构为： 锁状态 25bit 4bit 1bit是否偏向锁 2bit是否标志位 无锁状态 对象hashCode 对象分代年龄 0 01 如图在Mark Word会默认存放hasdcode，年龄值以及锁标志位等信息。 锁一共有4种状态，级别从低到高依次是：无锁状态、偏向锁状态、轻量级锁状态和重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率。对象的MarkWord变化为下图： 偏向锁偏向锁概念：最乐观的锁，从始至终只有一个线程请求一把锁 HotSpot的作者经过研究发现，大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。偏向锁是四种状态中最乐观的一种锁:从始至终只有一个线程请求某一把锁。 这就好比你在私家庄园里装了个红绿灯，并且庄园里只有你在开车。偏向锁的做法便是在红绿灯处识别来车的车牌号。如果匹配到你的车牌号，那么直接亮绿灯，线程ID用来标识线程： 偏向锁的获取(1) 当一个线程访问同步代码块并获取锁时，会在对象头和栈帧中的锁记录中记录存储偏向锁的线程ID，以后该线程再次进入同步块时，不需要 CAS来加锁和解锁，只需简单测试一下对象头的mark word中偏向线程ID是否是当前线程的ID，如果成功，表示线程已经获取到锁直接进入代码块运行。 (2) 如果测试失败（不是当前线程ID），检查当前偏向锁字段是否为0（无锁状态）： 如果为0，将偏向锁字段设置为1（采用CAS操作），并且更新自己的线程ID到mark word字段中。 如果为1，表示此事偏向锁已经被别的线程获取，则次线程需要不断尝试使用CAS获取偏向锁，或者将偏向锁撤销，升级为轻量级锁，一般情况下后者（升级）概率较大。 偏向锁的撤销偏向锁撤销：偏向锁使用一种等待竞争出现才释放锁的机制，当有其他线程尝试竞争偏向锁时，持有偏向锁的线程才会撤销偏向锁。 但是偏向锁的撤销开销很大，需要等待线程进入全局安全点safepoint，全局安全点safepoint简单理解就是当前线程在CPU上没有执行任何有用字节码！ Epoch字段标识偏向锁的撤销次数，每撤销一个Epoch就加一，加到40 的时候升级为轻量级锁，此值是可以设置的！ 偏向锁从JDK6后默认开启，但是在应用程序启动几秒后在激活-XX：BiasedLockingStartupDelay = 0，将延迟关闭，JVM一启动就激活偏向锁。 -XX：-UserBiasedLocking = false，关闭偏向锁，程序默认进入轻量级锁。 轻量级锁多个线程在不同的时间段请求同一把锁，也就是说没有锁竞争。针对这种情况，JVM采用了轻量级锁，来避免线程的阻塞以及唤醒。类比于生活中的例子：比如现在是深夜，车辆很少，红绿灯处只有一个车在等待红灯结束，很显然，此时没有其他的车，这样的等待是一种时间的浪费，于是便出现了轻量级锁来解决这种问题！ 加锁：线程在执行同步代码块之前，JVM先在当前线程的栈帧中创建用于存储锁记录的空间，并将对象头的Mark Word字段直接复制到此空间中。然后线程尝试使用CAS将对象头的Mark Word替换为指向锁记录的指针（指向当前线程），如果成功表示获取到轻量级锁，如果失败，表示其他线程竞争轻量级锁，当前线程便采用自旋来不断尝试。 总结拿个上厕所的例子所为结尾：A和B都需要上厕所 重量级锁：重量级锁会阻塞、唤醒请求加锁的进程，针对的是多个线程同一时刻竞争同一把锁的情况，JVM采用自适应自旋来避免线程在面对非常小的同步块时，仍会被阻塞以及唤醒。比如：A和B同时都要上厕所，这个时候就要重量级锁！ 轻量级锁：轻量级锁采用CAS操作，将锁对象的标记字段替换为指向线程的指针，存储着锁对象原本的标记字段。针对的是多个线程在不同时间段申请同一把锁的情况。比如：A总是在早晨上厕所，B总是在晚上才上厕所！ 偏向锁：偏向锁只会在第一次请求时采用CAS操作，在锁对象的Mark Word字段中记录下当前线程ID，此后运行中持有偏向锁的线程不再有加锁过程，针对的锁仅会被同一线程持有。比如：这个厕所只有A上，B根本不会来这个厕所！","updated":"2020-03-13T03:06:29.707Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"}]},{"title":"谈谈两次比赛的感想","date":"2019-10-14T09:08:58.000Z","path":"2019/10/14/谈谈两次比赛的感想/","text":"这篇文章呢，其实也没必要非得用Markdown写，但是用习惯了哈，没办法都改不了。差点跑题，今天谈谈今年这几次比赛的心得体会之类的吧。 其实我也是看见了队友的感言，链接在这里，所以只能看到他之前的博客备份了。今天是2019的10月份，转眼就到2020年了。其实也就当时回顾了一下自己上半年的成长吧，算是一份小小的总结。比赛是从去年冬天开始筹划的吧，不能算是筹划（应该是去年冬天听说的有这个比赛，其实就是服务外包大赛）。今年上半年开始着手准备选题，记得是3-4月份期间的选的题目。然后就选了 “智慧教室“ 这个项目，当时我和其中一个队友判断这个题目稍微简单一点，其实就是Android+、JavaEE+前端、Python控制的硬件端，比赛之前就认识两个队友：村支书和大力，其他两个队友是最后才接触了才认识的。 由于之前写过一点Android，最后也就做出来了，在开发过程中也发现了些像XUI这样的框架，由于是原生开发，而且整个组就5个人。我负责的模块就只有Android开发，然后陆陆续续完成了项目主要功能点，而且在西部赛区获得了一等奖，感觉努力了还是有结果的，虽然当时想的是只要能拿奖就行。接下来是国赛，大家申请了一间计算机学院楼的会议室，这样能够在一起 ”敏捷开发“ 。出了BUG能够很快的解决，于是到了国赛的时候作品完成度已经达到很高的地步了，甚至我们自己还加了些额外的功能，另外把说明文档也丰富了一下。为了能够在国赛拿奖，演讲PPT排练了很多次，都是演讲+现场演示功能的模式，最后经过大家的九九六般的努力得了一等奖（所以最后也就有九九六工作室这个团队了），比赛在无锡，这次的无锡之行给人感觉还是不错的，起码有高铁呀，而且带队老师都是同龄人，不尴尬。 然后回来大家一起和指导老师吃了饭，接着是计算机设计大赛，把之前的比赛的东西改变了一些东西。在暑假期间也接了个小项目，然后忙着做一下项目，计算机设计大赛接踵而至，好吧，那看来是真的没啥时间做太大的修改了，主要是把体现设计，重心不一样了，以前的是按照赛题的要求的去完成项目，现在是体现创意的时候，开始的时候倒是想了一些，但是完成度不太理想，最终还是参加比赛去了，不过只能报名三个人，我和其中两个队友参加了，省赛是二等奖国赛也是。可能还是没有认真准备太多的东西，当时不知道为啥连百度云提交作品的地址都搞错了，原来大家都理解错了，等到答辩的时候才发现没有PPT，只能找组委会商量这个事情，最后还是让我们拷贝了PPT，然后答辩结束。暑假期间的比赛，我一个人坐火车到的安徽芜湖，17个小时真心难受呀，不过我到芜湖了还精神抖擞，奇怪得很··· 老师更奇葩，第二天才到，结果到了才知道根本不是比赛的那个校区，相距十多公里。反正是女老师和我们在一起也不方便，从比赛开始到结束就见了一面。然后随便转了转，安师大的荷花池倒是真心不错，看得人赏心悦目，夏天的荷花池有清香是真的。 说了这么多，总结一下比赛的经验，设计类的比赛要看创意好不好，创新点在哪里。像服务外包这种的呢，技术选型要对，完成度要高。自己的算法还是不太好，应该多练练。其实有人说这是一场PPT大赛，这么说肯定是错的，没有实物展示 == 造假，但是呢PPT的制作也确实很重要的，毕竟影响答辩时屏幕的美观度，包括说明文档的制作，别人都能看懂的文档才是优质的文档。还有一个体会就是其实基础组件开发也很重要，这次设计大赛的特等奖是一队伍的作品，居然是一种编程语言叫做爱丽丝什么的，GitHub上面有他们的作品，看完后感觉确实很震撼的一个作品，暂且不说这个编程语言应用领域在哪里，光凭着打造编程语言的创造力他们就值得被肯定，一群对编译原理了如指掌的大佬不得不令人佩服。","updated":"2020-03-13T03:06:29.768Z","categories":[{"name":"生活","slug":"生活","permalink":"https://zouchanglin.cn/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"生活","slug":"生活","permalink":"https://zouchanglin.cn/tags/%E7%94%9F%E6%B4%BB/"}]},{"title":"【面试】MySQL索引失效分析","date":"2019-09-22T13:30:22.000Z","path":"2019/09/22/面试MySQL索引失效分析/","text":"本文主要向大家介绍了MySQL数据库之索引会失效的原因分析及解决索引失效的方法 ，通过具体的内容向大家展现，希望对大家学习MySQL数据库有所帮助。 先看下面的一个示例： 123456789101112131415161718192021create table test_03( id int primary key not null auto_increment, c1 char(10), c2 char(10), c3 char(10), c4 char(10), c5 char(10));insert into test_03(c1, c2, c3, c4, c5) values ('a1', 'a2', 'a3', 'a4', 'a5');insert into test_03(c1, c2, c3, c4, c5) values ('b1', 'b2', 'b3', 'b4', 'b5');insert into test_03(c1, c2, c3, c4, c5) values ('c1', 'c2', 'c3', 'c4', 'c5');insert into test_03(c1, c2, c3, c4, c5) values ('d1', 'd2', 'd3', 'd4', 'd5');insert into test_03(c1, c2, c3, c4, c5) values ('e1', 'e2', 'e3', 'e4', 'e5');create index idx_test03_c1234 on test_03(c1, c2, c3, c4);show index from test_03;explain select * from test_03 where c1='a1' and c2='a2' and c3='a3' and c4='a4';explain select * from test_03 where c4='a1' and c3='a2' and c1='a3' and c2='a4'; 很显然最下面的SQL语句并没有按照索引的建立顺序来查询，但是由于MySQL优化器对这条SQL做了优化，使得优化后的语句和上面的是一样的顺序，这样就能顺利用到索引了！但是由于这样写SQL语句还是会让底层做一次SQL优化，所以还不如一开始就按照建立索引的顺序来写SQL。 上面的SQL会导致c4这个索引失效，那么如果是c4的范围条件写前面呢？同样的道理，SQL优化器会对SQL语句做优化，优化后的语句成了explain select * from test_03 where c1=&#39;a1&#39; and c2=&#39;a2&#39; and c3=&#39;a3&#39; and c4&gt;&#39;c4&#39;; 所以自然会用到四个索引了！ 再看看下面的order by的例子，很显然用于查找的索引有两个就是c1、c2。c3用于排序了而不是查找，c4这个索引没有用到： 所以上面的SQL和explain select * from test_03 where c1=&#39;a1&#39; and c2=&#39;a2&#39; order by c3; 一样的，和c4根本没有关系了 上面这个例子呢主要是说中间兄弟没了，也就是你都没有给我第三层梯子，我怎么通过第四层排序，所以只能硬着头皮完成任务，那就Using filesort 呗！ 用到了一个c1索引，但是c2、c3用于排序，无filesort 用了c1、c2两个字段索引，但是c2、c3用于排序，无filesort。看看下面这一种情况，为什么explain select * from test_03 where c1=&#39;a1&#39; and c2=&#39;a2&#39; and c5=&#39;a5&#39; order by c3, c2; 并没有产生文件内排序呢？ 因为c2已经是常量了，在已知字段是常量的情况下需要排序吗？肯定不需要。所以即使你写成了order by c3,c2 但是实际上只是对c3排序！ 由上面的例子可以看出，用到了c1来查找，c4由于中间兄弟的消失是失效的索引，c2、c3用于分组，但是c2、c3如果不按照顺序分组的话就基本是是死掉了，出现了using temporary，这个是什么意思呢？看这个Explain中的Using temporary group 表面上是分组，分组之前必排序，所以说和order by排序的法则和索引优化的原则是一致的！ 定值、范围还是排序，一般order by是给个范围。group by 基本上都需要进行排序，会有临时表产生 一般性建议： 对于单键索引，尽量选择针对当前query过滤性更好的索引 在选择组合索引的时候，当前Query中过滤性最好的字段在索引字段顺序中，位置越靠前越好。 在选择组合索引的时候，尽量选择可以能包含当前query中的where子句中更多字段的索引 尽可能通过分析统计信息和调整query的写法来达到选择合适索引的目的","updated":"2020-03-13T03:06:29.778Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"}]},{"title":"如何避免MySQL索引失效与三个优化实例","date":"2019-09-21T15:25:35.000Z","path":"2019/09/21/如何避免MySQL索引失效与三个优化实例/","text":"最好是查找的值都是建立索引的字段，要遵守最佳左前缀匹配法则，第一个索引没用上其他的都用不上，中间的索引没用上后面的也用不上。索引字段不要函数计算、自动或手动的类型转换。凡是在范围条件之后的索引全部失效，like的百分号写在最后边，实在是需要两边都是百分号那么建立索引，并且别查找其他非索引字段，也就是尽量别写select * 。尽量不使用不等于、大于、小于等条件，尽量不要使用or进行连接，否则会导致索引失效。对于varchar类型的字段不要忘记写引号，避免发生隐式类型转换。 索引分析案例一 单表分析假设这样一张表，下面是建表语句 12345678910111213141516171819CREATE TABLE `article` ( `id` int(10) NOT NULL, `author_id` int(10) NOT NULL, `category_id` int(10) NOT NULL, `views` int(10) NOT NULL, `comments` int(10) NOT NULL, `title` varchar(255) NOT NULL, `content` text NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;-- ------------------------------ Records of article-- ----------------------------BEGIN;INSERT INTO `article` VALUES (1, 1, 1, 1, 1, '1', '1');INSERT INTO `article` VALUES (2, 2, 2, 2, 2, '2', '2');INSERT INTO `article` VALUES (3, 1, 1, 3, 3, '3', '3');COMMIT; 查询 category_id为1且 comments大于1的情况下，views最多的 article_id： 1explain select id, author_id from article where category_id = 1 and comments &gt; 1 order by views desc limit 1; 很显然，type是ALL，即最坏的情况。Exta里还出现了 Using filesort 也是最坏的情况。优化是必须的，先来看看这张表的索引： 1234567mysql&gt; show index from article;+---------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+---------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| article | 0 | PRIMARY | 1 | id | A | 2 | NULL | NULL | | BTREE | | |+---------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+1 row in set (0.00 sec) 很显然，只有主键索引。那么接下来开始建索引吧，有两种方式： 123alter table article add index idx_article_ccv(category_id, comments, views);create index idx_article_ccv on article(category_id, comments, views); 建好索引后 关于Using index condition https://blog.csdn.net/z69183787/article/details/53393153 。 建立索引后，也可以明显看出来用到了索引，避免了全表扫描，但是还是存在文件排序： 这是为什么呢？原因是我们SQL语句中的&gt;1， &gt;1是个范围，如果改成=1呢？ 很显然，这是两个常量字段都是const，也就不需要文件内排序，但是这样是属于更改题目。所以这也说明了一个问题，就是范围后的索引会失效，也就是把索引放到一个范围后面，这个索引成了失效索引！所以尽量给定具体值，不要给范围。 所以如果非要在给定范围内查找，这个索引是不合适的，我们需要重建索引，先把之前的索引删除： 1drop index idx_article_ccv on article; 当我们建立索引的后，type变成了 range，这是可以忍受的。但是 extra里使用 Using files仍是无法接受的 但是我们已经建立了索引，为啥没用呢？这是因为按照 BTree索引的工作原理，先排序 category_id，如果遇到相同的 category_id则再排序 comments，如果遇到相同的 comments则再排序views。当 comments字段在联合索引里处于中间位置时，因 comments &gt; 1条件是一个范围值(所谓 range)，MySQL无法利用索引再对后面的views部分进行检索，即 range类型查询字段后面的索引无效。 好了，接下来开始重新建立索引： 123alter table article add index idx_article_cv(category_id, views);create index idx_article_cv on article(category_id, views); 案例二 两表分析如下两张表，分别插入20条数据： 1234567891011121314create table if not exists class( id int(10) unsigned not null auto_increment, card int(10) unsigned not null, primary key (id));create table if not exists book( bookid int(10) unsigned not null auto_increment, card int(10) unsigned not null, primary key (bookid));insert into class(card)values (FLOOR(1 + RAND() * 20));insert into book(card)values (FLOOR(1 + RAND() * 20)); 下面开始explain分析： 两个type都是ALL，那么究竟是在class加索引还是book加索引呢？ 实验一：左连接 + 索引加在右表 实验二：接下来删除book的card索引，对右表建立索引。也就是：左连接 + 索引加在左表 通过实验一和实验二的对比，可以看到 *左连接 + 索引加在右表 * 第二行的type变为了 ref ，rows也变成了优化比较明显。这是由左连接特性决定的。 left join 条件用于确定如何从右表搜索行，左表一定都有，所以右表是我们的关键点，一定需要建立索引。 所以说有时对于DBA建好的索引也无需修改，我们需要修改SQL语句中表的位置即可！比如对于上述例子： 1select * from book right join class on class.card = book.card; 同样的道理， right join 条件用于确定如何从左表搜索行，右表一定都有，所以左表是我们的关键点，一定需要建立索引。 案例三 三表分析还是依照上面的两张表，再新建一张表，并插入20条数据： 1234567create table if not exists phone( phoneid int(10) unsigned not null auto_increment, card int(10) unsigned not null, primary key (phoneid));insert into phone(card)values (FLOOR(1 + RAND() * 20)); 1explain select * from class left join book on class.card = book.card left join phone on book.card = phone.card; 现在对phone和book表的card字段建立索引： 后2行的type都是ref且总rows优化很好效果不错。因此索引最好设置在需要经常查询的字段中。 JOIN语句的优化： 尽可能减少JOIN语句中的NestedLoop的循环总次数：”永远用小结果集驱动大的结果集” 优先优化NestedLoop的循环的内层 保证JOIN语句中被驱动表上Join条件字段已经被索引 当无法保证被驱动表的JOIN条件字段被索引且内存资源充足的前提下，不要太吝惜 JoinBuffer的设置 索引失效先建示例表与该表的索引： 12345678910111213141516create table staffs( id int primary key auto_increment, name varchar(24) not null default '' comment '姓名', age int not null default 0 comment '年龄', pos varchar(20) not null default '' comment '职位', add_time timestamp not null default current_timestamp comment '入职时间')charset utf8 comment '员工记录表';INSERT INTO staffs(NAME, age, pos, add_time)VALUES('z3', 22, 'manager', NOW());INSERT INTO staffs(NAME, age, pos, add_time) VALUES('July', 23, 'dev', NOW());INSERT INTO staffs(NAME, age, pos, add_time) VALUES('2000', 23, 'dev', NOW());SELECT * FROM staffs;alter table staffs add index idx_staffs_nameAgePos(name, age, pos);show index from staffs; 1、全值匹配我最爱 2、最佳左前缀法则 3、不在索引列上做任何操作(计算、函数、(自动or手动)类型转换)，会导致索引失效而转向全表扫描 4、存储引擎不能使用索引中范围条件右边的列 5、尽量使用覆盖索引(只访问索引的查询(索引列和查询列一致)，减少 select 6、mysql在使用不等于(=或者&lt;&gt;)的时候无法使用索引会导致全表扫描 7、is null, is not null也无法使用索引 8、like以通配符开头(‘%abc…’)mysq索引失效会变成全表扫描的操作 9、字符串不加单引号索引失效 10、少用or，用它来连接时会索引失效 1、全值匹配我最爱123explain select * from staffs where name = 'July';explain select * from staffs where name = 'July' and age = 23;explain select * from staffs where name = 'July' and age = 23 and pos = 'dev'; 2、最佳左前缀法则但是请看下面这种情况 很明显建的索引没用到，但是如果只是根据name字段来查询却又可以用到索引。如果索引了多列，要遵守最佳左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的列。否则会引起索引失效！这也是最常用的法则！很形象的一个例子：带头大哥不能死，name索引相当于火车头，age、pos是车厢，所以没有name火车肯定动不了，但是如果只有火车头，那么也是能动的，单独的车厢不能运动而已！ 那么这样呢？ 1explain select * from staffs where name = 'July' and pos = 'dev'; 上面这条SQL很显然用到了索引，但是key_len还是74没变，其实这条SQL只用到了name索引，并未用到pos索引。这违反了最佳左前缀法则，因为age索引还没用到就用pos，肯定pos会无效的。拿上面的火车举例子，如果中间的车厢断开了，后面的车厢肯定也动不了了。所以最佳左前缀法则还是很重要的！ 3、不在索引列上做任何操作不在索引列上做任何操作（计算、函数、（自动or手动）类型转换），会导致索引失效而转向全表扫描 接下来分别看看他们的执行计划： 很明显，在索引列上做任何操作（计算、函数、（自动or手动）类型转换），会导致索引失效而转向全表扫描！所以这一点是非常需要注意的！ 4、存储引擎不能使用索引中范围条件右边的列 如上图所示，索引一旦出现范围条件，那么后面的索引会失效。出现范围的索引呢也是会用到，比如这里的age字段排序，但是级别从ref降到了range级别，在数据量非常大的情况下还是很损伤性能的！ 5、尽量使用覆盖索引尽量使用覆盖索引（只访问索引的查询，即索引列和查询列一致），减少使用select* 可以看出如果只是要检索字段，那就尽量明确写出需要查询的字段，不要写select * ，只查询索引字段的话就会使用Using index，而不是Using where。 6、在使用不等于的时候无法使用索引mysql在使用不等于（!=或者&lt;&gt;）的时候无法使用索引会导致全表扫描 7、is null、is not null 也无法使用索引 关键字段尽量避免null值，最好设置默认值！ 8、like以通配符开头索引失效会变成全表扫描like以通配符开头（’$abc…’）mysql索引失效会变成全表扫描操作 通过上面的例子可以看出，like的%尽量加在右边。而且like查询是一个范围查询！ 解决like’%字符串%’索引不被使用的方法？？看看下面的示例： 123456789101112create table tbl_user( id int(11) not null auto_increment, name varchar(20) default null, age int(11) default null, email varchar(20) default null, primary key(id))engine=INNODB auto_increment=1 default charset=utf8;insert into tbl_user (name, age, email) values ('1aa1', 21, 'b@163.com');insert into tbl_user (name, age, email) values ('2aa2', 222, 'a@163.com');insert into tbl_user (name, age, email) values ('3aa3', 265, 'c@163.com');insert into tbl_user (name, age, email) values ('4aa4', 21, 'd@163.com'); 123create index idx_user_nameAge on tbl_user(name, age);show index from tbl_user; 通过建立覆盖索引来避免全表扫描，从下图可以看出类型为index： 所以说：like百分写右边，复合索引解决两边都是百分号的问题！需要时建立了索引，查询的字段是建立了索引的字段，那就没问题。如果包含了其他字段，就会造成索引失效！ 9、字符串不加单引号索引失效varchar类型必须加单引号，下面的示例可以说明问题： 这是为什么呢？因为MySQL自动识别name是一个varchar类型，所以如果没有在SQL语句中没加单引号，MySQL会自动发生隐式类型转换，所以参考第三条，这必然导致索引失效，在开中应该尽量避免 10、少用or，用or来连接时索引失效 优化总结全值匹配我最爱，最左前缀要遵守； 带头大哥不能死，中间兄弟不能断； 索引列上少计算，范围之后全失效； like 百分写最右，覆盖素引不写星； 不等空值还有or，索引失效要少用； varchar引号不可丢，SQL高级也不难！ 打油诗解读：通俗来讲，长话短说，最好是查找的值都是建立索引的字段，要遵守最佳左前缀匹配法则，第一个索引没用上其他的都用不上，中间的索引没用上后面的也用不上。索引字段不要函数计算、自动或手动的类型转换。凡是在范围条件之后的索引全部失效，like的百分号写在最后边，实在是需要两边都是百分号那么建立索引，并且别查找其他非索引字段，也就是尽量别写select * 。尽量不使用不等于、大于、小于等条件，尽量不要使用or进行连接，否则会导致索引失效。对于varchar类型的字段不要忘记写引号，避免发生隐式类型转换。","updated":"2020-03-13T03:06:29.734Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"}]},{"title":"MySQL性能分析","date":"2019-09-17T11:44:51.000Z","path":"2019/09/17/MySQL性能分析/","text":"前提知识Mysql中有专门负责优化 SELECT语句的优化器模块，主要功能：通过计算分析系统中收集到的统计信息，为客户端请求的 Query提供他认为最优的执行计划(他认为最优的数据检索方式，但不见得是DBA认为是最优的) 当客户端向 MySQL请求一条 Query，命令解析器模块完成请求分类，区别出是 SELECT并转发给 MySQL Query Optimizer时， MySQL Query Optimizer首先会对整条 Query进行优化，处理掉一些常量表达式的预算直接换算成常量值。并对 Query中的查询条件进行简化和转换，如去掉一些无用或显而易见的条件、结构调整等。然后分析 Query中的Hint信息(如果有)，看显示Hint信息是否可以完全确定该 Query的执行计划。如果没有Hint或Hint信息还不足以完全确定执行计划，则会读取所涉及对象的统计信息，根据 Query进行写相应的计算分析，然后再得出最后的执行计划。 MySQL常见性能瓶颈CPU：CPU在饱和的时候一般发生在数据装入内存或从磁盘上读取数据时候 IO：磁盘I/O瓶颈发生在装入数据远大于内存容量的时候 服务器硬件的性能瓶颈：top、free、 iostat 和 vmstat来查看系统的性能状态 Explain-执行计划用 EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道 MySQL是如何处理你的sqL语句的。分析你的査询语句或是表结构的性能瓶颈！ 用法：explain + SQL explain能干啥？表的读取顺序 数据读取操作的操作类型 哪些索引可以使用 哪些索引被实际使用 表之间的引用 每张表有多少行被优化器查询 执行计划包含的信息 下面解释一下这些字段是干啥的： 1、idselect查询的序列号，包含一组数字，表示查询中执行select子句或操作表的顺序。有三种情况： 1、id相同，执行顺序由上至下 2、id不同，如果是子査询，id的序号会递増，id值越大优先级越高，越先被执行 PRIMARY就是主查询，id越大越先被执行，则毫无疑问在这样的子查询中，肯定最先要查询的表就是table_03，所以这是MySQL自己约定的顺序，就和运算符优先级一样！SUBQUERY 代表最外层查询。 3、id相同不同，同时存在 id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行，衍生=DERIVED，所以这里就体现了explain的第一个用处，那就是它能看出表的读取和加载顺序！ 2、select_typeSIMPLE、PRIMARY、SUBQUERY、DERIVED、UNION、UNION RESULT、 查询的类型，主要是用于区别普通査询、联合査询、子査询等的复杂查询 SIMPLE：简单的 select查询，查询中不包含子查询或者 UNION PRIMARY：查询中若包含任何复杂的子部分，最外层查询则被标记为PRIMARY SUBQUERY：在 SELECT或 WHERE列表中包含了子查询 DERIVED：在FROM列表中包含的子查询被标记为 DERIVED(衍生)，MySQL会递归执行这些子查询，把结果放在临时表里。 UNION：若第二个 SELECT出现在UNION之后，则被标记为UNION；若UNON包含在FRON子句的子查询中，外层 SELECT将被标记为: DERIVED UNION RESULT：从 UNION表获取结果的 SELECT 所以这样就看到了数据读取操作的操作类型 3、table这个基本不用说，显示这一行的数据是关于哪张表的 4、typetype显示的是访问类型，是较为重要的一个指标，结果值从最好到最坏依次是: system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt;unique_subquery &gt;index _subquery &gt; range &gt; index &gt; ALL 一般来说，得保证查询至少达到 range级别，最好能达到ref 这样查询的类型就是ALL，如果数据达到百万级别的一个全表扫描，那么性能肯定会下降的！ 常见的type就这些：system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL system 表只有一行记录(等于系统表)，这是 const类型的特例，平时不会出现，这个也可以忽略不计 const 表示通过索引一次就找到了，const用于比较primary key或者unique索引。因为只匹配一行数据，所以很快。如将主键至于where列表中，MySQL就能将该查询转换为一个常量，比如where id = 1就被当成是常量 eq_ref 唯一性索引，对于每个索引键，表中只有一条记录与之匹配，常见于主键或唯一索引扫描 ref 非唯一性索引扫描，返回匹配某个单独值的所有行本质上也是一种索引访问，它返回所有匹配某个单独值的行。然而，能会找到多个符合条找和扫描的混合，与eq_ref对比，其实可以发现ref是对非唯一索引进行扫描，其实也就是对数据表的非主键字段建立索引，然后通过这个索引进行扫描，结果可想而知肯定是非唯一性的！ range 只检索给定范围的行，使用一个索引来选择行。key列显示使用了哪个索引，一般就是在你的 where语句中出现 between、&lt;、&gt;、in等的查询，这种范围扫描索引比全表扫描要好，因为它只需要开始于索引的某一点，而结束于另一点，不用扫描全部索引。 index Full Index scan，index与ALL区别为index类型只遍历索引树。这通常比ALL快，因为索引文件通常比数据文件小 (也就是说虽然叫all和index都是读全表,但 index是从索引中读取的，而all是从硬盘中读的 all FulITable scan，将遍历全表以找到匹配的行 5、possible_keys显示可能应用在这张表中的索引，一个或多个。 查询涉及的字段上若存在索引，则该索引将被列出，但不一定被查询实际使用 6、key实际使用的索引。如果为null则没有使用索引 查询中若使用了覆盖索引，则索引和查询的select字段重叠。 所以possible_keys和key实际上是告诉了使用者，MySQL理论上会用到哪些索引，实际上会用到哪些索引。 这就是覆盖索引，也就是如果你要查询的字段顺序正好与索引建立的顺序相等，那么查询类型那直接变为index查询，而不是ref类型！ 7、key_len表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度。在不损失精确性的情况下，长度越短越好 key_len显示的值为索引最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的 这个其实比较容易理解，精度越高，需要的索引字节数也变长了，很显然，更高精度的查询付出的代价就是key_len变长了！ 8、ref显示索引那一列被使用了，如果可能的话，是一个常数。那些列或常量被用于查找索引列上的值，比如: 所以通过这里我们已经知道哪些索引可以使用，哪些索引被实际使用！ 9、rows显示索引那一列被使用了，如果可能的话，是一个常数。那些列或常量被用于查找索引列上的值，越少越好！ 通过上面的例子进行分析，在没建立索引前，扫描了641行，先加载t2，再加载t1，查询t2表的时候是ALL，也就是逐行扫描，并且本来可以使用主键索引，但是依旧没什么意义，所以还是以逐行扫描的方式进行查询，这样会有640行结果。接下来对t2表建立复合索引，所以可供选择的索引就有主见索引、自己新建的复合索引。很明显，新建复合索引之后呢，查询的时候不再采用逐行扫描的方式，而是选择了复合索引，行数降到了142行，总共143行就可以搞定这个查询问题！ 10、Extra额外的，扩展的。包含不适合在其他列中显示但十分重要的额外信息！ ① Using filesort 说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。MySQL中无法利用索引完成排序操作成为”文件排序”，这种文件排序是需要尽量避免的。 ② Using temporary 使用了临时表保存中间结果，MySQL在对查询结果排序时使用临时表。常见于排序order by 和分组查询 group by。这个动作更耗费时间，如果说Using filesort 是九死一生的话那么出现Using temporary是十死无生了 通过上面的例子我们应该明白，如果建立了索引，那么在进行group by的时候应该按照建立索引的顺序使用到索引，否则会造成Using filesort 和 Using temporary。 ③ Using index 表示相应的select操作中使用了覆盖索引（Coveing Index）,避免访问了表的数据行，效率不错！如果同时出现using where，表明索引被用来执行索引键值的查找；如果没有同时出现using where，表面索引用来读取数据而非执行查找动作。 索引覆盖 覆盖索引( Covering Index)，一说为索引覆盖。 理解方式一：就是 select的数据列只用从索引中就能够取得，不必读取数据行，MySQL可以利用索引返回 select列表中的字段，而不必根据索引再次读取数据文件。换句话说查询列要被所建的索引覆盖！ 理解方式二：索引是高效找到行的一个方法，但是一般数据库也能使用索引找到一个列的数据，因此它不必读取整个行。毕竟索引叶子节点存储了它们索引的数据；当能通过读取索引就可以得到想要的数据，那就不需要读取行了。一个素引包含了(或覆盖了)满足查询结果的数据就叫做覆盖索引 注意： 如果要使用覆盖索引，一定要注意 select列表中只取出需要的列，不可 select *，因为如果将所有字段一起做索引会导致索引文件过大，查询性能下降 ④ Using where 表明使用了where过滤 ⑤ Using join buffer 使用了连接缓存，Join查询动作平凡，数据量大的时候就需要增大join buffer的容量 ⑥ impossible where where子句的值总是false，不能用来获取任何元组 比如这样的MySQL不能理解的SQL。 ⑦ select tables optimized away 在没有GROUPBY子句的情况下，基于索引优化MIN/MAX操作或者，对于MyISAM存储引擎优化COUNT(*) 操作，不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化。 ⑧ distinct 优化distinct，在找到第一匹配的元组后即停止找同样值的工作 小练习 执行顺序是： 第一行(执行顺序4)：id列为1，表示是 union里的第一个 select，select type列的primary表示该查询为外层查询。tabe列被标记为&lt;derived3&gt;，表示查询结果来自一个衍生表，其中 derived3中3代表该查询衍生自第三个select查询，即id为3的 select【select d1 name……】 第二行(执行顺序2)：id为3，是整个查询中第三个selec的一部分。因查询包含在from中，所以为 derived【 select id, name from t1 where other_column=”】 第三行(执行顺序3)：select列表中的子查询select type为subquery，为整个查询中的第二个 select【 select id from t3】 第四行(执行顺序1)：select type为unon，说明第四个select是unon里的第二个select，最先执行【 select name, id from t2】 第五行(执行顺序5)：代表从union的临时表中读取行的阶段，table列的&lt; unIon1,4&gt;表示用第一个和第四个select的结果进行union操作。【两个结果union操作】","updated":"2020-03-13T03:06:29.690Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"}]},{"title":"JOIN查询与索引简介","date":"2019-09-12T01:29:32.000Z","path":"2019/09/12/JOIN查询与索引简介/","text":"MySQL索引优化本文主要讲述了索引的概念，索引的作用是什么，MySQL索引结构，哪些情况下需要建立索引，哪些情况下需要建立索引！MySQL索引的建立对于MySQL的高效运行是很重要的，索引可以大大提高MySQL的检索速度。如果合理的设计且使用索引的MySQL是一辆兰博基尼的话，那么没有设计和使用索引的MySQL就是一个人力三轮车。 索引分单列索引和组合索引。单列索引，即一个索引只包含单个列，一个表可以有多个单列索引，但这不是组合索引。组合索引，即一个索引包含多个列。 一、性能下降主要原因往往体现在执行时间长 / 等待时间长 1、SQL语句写的很烂2、索引失效（单值索引、复合索引）单值索引的建立： 假设一张表(user)的字段是id、name、email、wechat_num，如果要执行查询select * from user where name = &#39;&#39;，但是这样做在数据量很大的情况下会很慢，所以根据name字段建立索引，create index idx_user_name on user(name) idx_user_name是索引名称，user是表名，name是索引字段 复合索引的建立： 与上面的例子是相似的，假设要执行查询select * from user where name=&#39;&#39; and email=&#39;&#39;，那么如果要建立复合索引，create index idx_user_nameEmail on user(name, email) 比如： 3、关联查询太多join设计缺陷或者不得已的需求，大多发生在老旧的系统中 4、服务器调优及各个参数设置比如缓冲区大小、线程数、连接数等等 二、常见通用的Join查询1、SQL执行顺序手写顺序 机读顺序 所以整个SQL的解析流程为： Join总结 2、SQL Join的练习建表插入数据，做练习准备 123456789101112131415161718192021222324252627282930313233use day01;create table tbl_dept( id int(11) not NULL auto_increment, deptName varchar(30) default NULL, locAdd varchar(40) default NULL, primary key (id))engine=INNODB auto_increment=1 default charset=utf8;create table tbl_emp( id int(11) not null auto_increment, name varchar(20) default null, deptId int(11) default null, primary key (id), key fk_dept_id(deptId) #constraint fk_dept_id foregin key('deptId') references tbl_dept(id))engine=innodb auto_increment=1 default charset=utf8;insert into tbl_dept(deptName, locAdd) values ('RD', 11);insert into tbl_dept(deptName, locAdd) values ('HR', 12);insert into tbl_dept(deptName, locAdd) values ('MK', 13);insert into tbl_dept(deptName, locAdd) values ('MIS', 14);insert into tbl_dept(deptName, locAdd) values ('FD', 15);insert into tbl_emp(name, deptId) VALUES ('z3', 1);insert into tbl_emp(name, deptId) VALUES ('z4', 1);insert into tbl_emp(name, deptId) VALUES ('z5', 1);insert into tbl_emp(name, deptId) VALUES ('z3', 1);insert into tbl_emp(name, deptId) VALUES ('w5', 2);insert into tbl_emp(name, deptId) VALUES ('w6', 2);insert into tbl_emp(name, deptId) values ('s7', 3);insert into tbl_emp(name, deptId) values ('s8', 4);insert into tbl_emp(name, deptId) values ('s9', 51); 下面是INNER JOIN的查询示例，可以看出查询结果是AB的共有部分： A独有+AB共有 B独有+AB共有 A排除AB共有 B排除AB共有 AB全有，下面是full join 但是却报错了，是因为MySQL是不支持这种语法的，但是Orcal数据库却是支持的，所以这么干：A的独有+B的独有+AB共有，所以需要合并去重的操作，使用union AB 的独有 三、索引简介1、什么是索引MySQL官方对索引的定义为：索引(Index)是帮助MySQL高效获取数据的数据结构。 可以得到索引的本质：索引是数据结构 很简单，如果对于一个无顺字典，我要查找Mysql这个单词，那么唯一的方法是遍历， 如果词典按照A-Z排序的话，只需要从M开始的页就好了！这就是索引，所以索引有两大功能：排序和快速查找。则索引就是排好序的快速查找数据结构 在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向数据），这样就可以在这些数据结枃上实现高级査找算法。这种数据结构，就是索引，如下图： 为了加快 Col2 的查找，可以维护一个右边所示的二叉査找树，每个节点分别包含素引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找在一定的复杂度内获取到相应数据，从而快速的检索出符合条件的记录！ 数据本身之外，数据库还维护着一个满足特定査找算法的数据结构，这些数据结构以某种方式指向数据，这样就可以在这些数据结构的基础上实现高级查找算法，这种数据结构就是索引。 一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上，我们平常所说的素引，如果没有特别指明，都是指 B 树（多路搜素树，并不一定是二又的）结构组织的索引。其中聚集索引，次要索引复合索引，前缀索引，唯一索引默认都是使用 B+树索引，统称索引。当然，除了 B+树这种类型的索引之外，还有哈稀索引（hash index等） 2、索引的优缺点优点一：类似大学图书馆建书目索引，提高数据检索的效率，降低数据库的 IO 成本 优点二：通过索引列对数据进行排序，降低数据排序的成本，降低了 CPU 的消耗 所以在多个条件查询的情况下常常建立复合索引 缺点一：实际上索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录，所以索引列也是要占用空间的 缺点二：虽然索引大大提高了査询速度，同时却会降低更新表的速度，如对表进行 INSERT、UPDATE 和 DELETE 因为更新表时，MSQL 不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段都会调整因为更新所带来的键值变化后的索引信息 缺点三：索引只是提高效率的一个因素，如果你的 MYSQL 有大数据量的表，就需要花时间研究建立最优秀的素引，或优化查询 3、索引的分类单值索引：即一个索引只包含单个列，一个表可以有多个单列索引（建议一张表索引不要超过5个，优先考虑复合索引） 唯一索引：索引列的值必须唯一，但允许有空值 复合索引：即一个索引包含多个列 4、索引建立基本语法创建索引 如果是CHAR,VARCHAR类型，length可以小于字段实际长度；如果是BLOB和TEXT类型，必须指定length。 1CREATE [UNIQUE] INDEX indexName ON mytable(columnname(length)); 如果加上[UNIQUE] 就是唯一索引，下面这种方式也可以建立索引 1ALTER mytable ADD [UNIQUE] INDEX [indexName] ON(columnname(length)); 删除索引 1DROP INDEX [indexName] ON mytable; 查看索引 1SHOW INDEX FROM table_name\\G 四种方式来添加数据表的索引 ALTER TABLE tbl_nme ADD PRIMARY KEY (column_list) 该语句添加一个主键，这意味着索引值必须是唯一的，且不能为 NULL ALTER TABLE tbl_name ADD UNIQUE index_name (column_list） 这条语句创建索引的值必须是唯一的（除了 NULL 外，NULL 可能会出现多次） ALTER TABLE tbl_name ADD INDEX index_name (column_list） 添加普通索引，索引值可出现多次 ALTER TABLE tbl_name ADD FULLTEXT index_name (column_list）该语句指定了索引为 FULLTEXT，用于全文索引 5、MySQL索引结构BTree索引、Hash索引、full-text全文索引、R-Tree索引 我们主要看看BTree索引： 颗 b+树，浅蓝色的块我们称之为一个磁盘块，可以看到每个磁盘块包含几个数据项（深蓝色所示）和指针（黄色所示）如磁盘块 1 包合数据项 17 和 35, 包含指针 P1、P2、P3, P1 表示小于 17 的磁盘块，P2 表示在 17 和 35 之间的磁盘块，P3 表示大于 35 的磁盘块 真实的数据存在于叶子点即 3、5、9、10、13、15、28、29、36、60、75、79、90、99 非叶子节点只不存储真实的数据，只存储指引搜索方向的数据项，如 17、35 并不真实存在于数据表中 如果要查找数据项29，那么首先会把磁盘块1由磁盘加载到内存，此时发生一次O，在内存中用二分查找确定29在17和35之间，锁定磁盘块1的P2指针，内存时间因为非常短(相比磁盘的IO)可以忽略不计，通过磁盘块1的P2指针的磁盘地址把磁盘块3由磁盘加载到内存，发生第二次IO，29在26和30之间，锁定磁盘块3的P2指针，通过指针加载磁盘块8到内存，发生第三次IO，同时内存中做二分查找找到29，结束查询，总计三次IO。 真实的情况是，3层的B+树可以表示上百万的数据，如果上百万的数据查找只需要三次O，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次IO，那么总共需要百万次的IO，显然成本非常非常高 6、哪些情况下需要建立索引（1）主键自动建立唯一索引 （2）频繁作为查询条件的字段应该创建索引 （3）查询中与其它表关联的字段,外键关系建立索引 （4）频繁更新的字段不适合创建索引，因为每次更新不单单是更新了记录还会更新索引，加重IO负担 （5）Where条件里用不到的字段不创建索引 （6）单键/组合索引的选择问题，who?(在高并发下倾向创建组合索引) （7）查询中排序的字段，排序字段若通过索引去访问将大大提高排序速度 （8）查询中统计或者分组字段 7、那些情况下不需要建立索引（1）表记录太少 （2）经常增删改的表 Why？提高了查询速度，同时却会降低更新表的速度，如对表进行 INSERT、 UPDATE和 DELETE，因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件 （3）数据重复且分布平均的表字段，因此应该只为最经常查询和最经常排序的数据列建立索引。注意，如果某个数据列包含许多重复的内容，为它建立索引就没有太大的实际效果 假如一个表有10万行记录，有一个字段A只有T和F两种值，且每个值的分布概率大约为50%，那么对这种表A字段建索引一般不会提高数据库的査询速度。索引的选择性是指索引列中不同值的数目与表中记录数的比。如果一个表中有2000条记录，表索引列有1980个不同的值，那么这个索引的选择性就是1980/2000=0.99。一个索引的选择性越接近于1，这个索引的效率就越高。","updated":"2020-03-13T03:06:29.668Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"}]},{"title":"MySQL配置与存储引擎介绍","date":"2019-09-04T12:36:35.000Z","path":"2019/09/04/MySQL配置与存储引擎介绍/","text":"现在开始学习MySQL高级部分的知识，比如MySQL内核、SQL优化、SQL服务器的优化、各种参数常亮的设定、查询语句优化、主从复制、软硬件升级、容灾备份、SQL编程之类的东西。不过接下来学到的呢只是作为一个后台Java开发者应该掌握的知识，很完整的SQL优化需要很深的功底，大公司甚至有专门的DBA写上面这些东西，从IT时代到互联网时代，不断增长的是数据，所以学习一下MySQL优化还是很重要的！ 一、Linux下安装MySQL的目录说明 路径 解释 备注 /var/lib/mysql mysql数据库文件的存放路径 /var/lib/mysql/数据库名称 /usr/share/mysql 配置文件目录 mysql.server 命令及配置文件` /usr/bin 命令相关目录 mysqladmin、mysqldump等命令 /etc/init.d/mysql 启停相关脚本 二、MySQL配置文件说明MySQL的配置文件，在5.6的版本下MySQL配置文件在/usr/share/mysql 下名称叫做my-default.cnf，可是在MySQL5.7中就没有这个配置文件了，其实是从5.7.18开始不在二进制包中提供my-default.cnf文件，my.cnf文件就是把在命令行上启动MySQL时后面的参数用cnf文件配置好，那么下载启动时就不再需要在命令上加如参数。这个my.cnf文件可以是自定义位置，也可以使用如下默认的位置，只要放在默认位置，MySQL自动识别（初始位置在下方列表），官方说明是这样的 文件名 目的 /etc/my.cnf 全局选项 /etc/mysql/my.cnf 全局选项 SYSCONFDIR/my.cnf 全局选项 $MYSQL_HOME/my.cnf 服务器特定选项（仅限服务器） defaults-extra-file 指定的文件 --defaults-extra-file，如果有的话 ~/.my.cnf 用户特定选项 ~/.mylogin.cnf 用户特定的登录路径选项（仅限客户端） 在上表中，〜表示当前用户的主目录（$ HOME的值）。 SYSCONFDIR表示在构建MySQL时使用CMake的SYSCONFDIR选项指定的目录。默认情况下，这是位于已编译安装目录下的etc目录 MYSQL_HOME是一个环境变量，包含特定于服务器的my.cnf文件所在目录的路径。如果未设置MYSQL_HOME并使用mysqld_safe程序启动服务器，则mysqld_safe将其设置为BASEDIR（MySQL基本安装目录）。 DATADIR通常是/usr/local/mysql/data，尽管这可能因平台或安装方法而异。该值是编译MySQL时内置的数据目录位置，而不是mysqld启动时使用--datadir选项指定的位置。在运行时使用--datadir不会影响服务器在处理任何选项之前查找其读取的选项文件的位置。 如果找到给定选项的多个实例，则最后一个实例优先，但有一个例外：对于mysqld， --user选项的第一个实例用作安全预防措施，以防止在选项文件中指定的用户被在命令行上覆盖。 这些配置有关的参数说明也在官网可以找到，https://dev.mysql.com/doc/refman/5.7/en/option-files.html 下面给出一些常用的配置选项： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216# MySql5.7配置文件my.cnf设置[client]port &#x3D; 3306socket &#x3D; &#x2F;tmp&#x2F;mysql.sock [mysqld]############################### 基础设置 ###################################### Mysql服务的唯一编号 每个mysql服务Id需唯一server-id &#x3D; 1 # 服务端口号 默认3306port &#x3D; 3306 # mysql安装根目录basedir &#x3D; &#x2F;opt&#x2F;mysql # mysql数据文件所在位置datadir &#x3D; &#x2F;opt&#x2F;mysql&#x2F;data # 临时目录 比如load data infile会用到tmpdir &#x3D; &#x2F;tmp # 设置socke文件所在目录socket &#x3D; &#x2F;tmp&#x2F;mysql.sock # 主要用于MyISAM存储引擎,如果多台服务器连接一个数据库则建议注释下面内容skip-external-locking # 只能用IP地址检查客户端的登录，不用主机名skip_name_resolve &#x3D; 1# 数据库默认字符集,主流字符集支持一些特殊表情符号（特殊表情符占用4个字节）character-set-server &#x3D; utf8mb4 # 数据库字符集对应一些排序等规则，注意要和character-set-server对应collation-server &#x3D; utf8mb4_general_ci # 设置client连接mysql时的字符集,防止乱码init_connect&#x3D;&#39;SET NAMES utf8mb4&#39; # 是否对sql语句大小写敏感，1表示不敏感lower_case_table_names &#x3D; 1 # 最大连接数max_connections &#x3D; 400# 最大错误连接数max_connect_errors &#x3D; 1000 # TIMESTAMP如果没有显示声明NOT NULL，允许NULL值explicit_defaults_for_timestamp &#x3D; true # SQL数据包发送的大小，如果有BLOB对象建议修改成1Gmax_allowed_packet &#x3D; 128M # MySQL连接闲置超过一定时间后(单位：秒)将会被强行关闭# MySQL默认的wait_timeout 值为8个小时, interactive_timeout参数需要同时配置才能生效interactive_timeout &#x3D; 1800wait_timeout &#x3D; 1800# 内部内存临时表的最大值 ，设置成128M。# 比如大数据量的group by ,order by时可能用到临时表，# 超过了这个值将写入磁盘，系统IO压力增大tmp_table_size &#x3D; 134217728max_heap_table_size &#x3D; 134217728 ########################## 用户进程分配到的内存设置BEGIN ########################### 每个session将会分配参数设置的内存大小# 用于表的顺序扫描，读出的数据暂存于read_buffer_size中，当buff满时或读完，将数据返回上层调用者# 一般在128kb ~ 256kb,用于MyISAM# read_buffer_size &#x3D; 131072# 用于表的随机读取，当按照一个非索引字段排序读取时会用到，# 一般在128kb ~ 256kb,用于MyISAM# read_rnd_buffer_size &#x3D; 262144# order by或group by时用到# 建议先调整为2M，后期观察调整sort_buffer_size &#x3D; 2097152# 一般数据库中没什么大的事务，设成1~2M，默认32kbbinlog_cache_size &#x3D; 524288 ############################ 日志设置 ########################################### 数据库错误日志文件log_error &#x3D; error.log # 慢查询sql日志设置slow_query_log &#x3D; 1slow_query_log_file &#x3D; slow.log# 检查未使用到索引的sqllog_queries_not_using_indexes &#x3D; 1# 针对log_queries_not_using_indexes开启后，记录慢sql的频次、每分钟记录的条数log_throttle_queries_not_using_indexes &#x3D; 5# 作为从库时生效,从库复制中如何有慢sql也将被记录log_slow_slave_statements &#x3D; 1# 慢查询执行的秒数，必须达到此值可被记录long_query_time &#x3D; 2# 检索的行数必须达到此值才可被记为慢查询min_examined_row_limit &#x3D; 100 # mysql binlog日志文件保存的过期时间，过期后自动删除expire_logs_days &#x3D; 5 ############################主从复制 设置######################################### 开启mysql binlog功能log-bin&#x3D;mysql-bin# binlog记录内容的方式，记录被操作的每一行binlog_format &#x3D; ROW# 作为从库时生效,想进行级联复制，则需要此参数log_slave_updates # 作为从库时生效,中继日志relay-log可以自我修复relay_log_recovery &#x3D; 1 # 作为从库时生效,主从复制时忽略的错误slave_skip_errors &#x3D; ddl_exist_errors ##---redo log和binlog的关系设置BEGIN---###(步骤1) prepare dml相关的SQL操作，然后将redo log buff中的缓存持久化到磁盘#(步骤2)如果前面prepare成功，那么再继续将事务日志持久化到binlog#(步骤3)如果前面成功，那么在redo log里面写上一个commit记录#当innodb_flush_log_at_trx_commit和sync_binlog都为1时是最安全的，#在mysqld服务崩溃或者服务器主机crash的情况下，binary log只有可能丢失最多一个语句或者一个事务。#但是都设置为1时会导致频繁的io操作，因此该模式也是最慢的一种方式。#当innodb_flush_log_at_trx_commit设置为0，mysqld进程的崩溃会导致上一秒钟所有事务数据的丢失。#当innodb_flush_log_at_trx_commit设置为2，只有在操作系统崩溃或者系统掉电的情况下，上一秒钟所有事务数据才可能丢失。#commit事务时,控制redo log buff持久化磁盘的模式 默认为1innodb_flush_log_at_trx_commit &#x3D; 2#commit事务时,控制写入mysql binlog日志的模式 默认为 0#innodb_flush_log_at_trx_commit和sync_binlog都为1时，mysql最为安全但性能上压力也是最大sync_binlog &#x3D; 1##---redo log 和 binlog的关系设置END---## ############################ Innodb设置 ########################################### 数据块的单位8k，默认是16k，16kCPU压力稍小，8k对select的吞吐量大# innodb_page_size的参数值也影响最大索引长度，8k比16k的最大索引长度小# innodb_page_size &#x3D; 8192# 一般设置物理存储的60% ~ 70%innodb_buffer_pool_size &#x3D; 1G #5.7.6之后默认16M# innodb_log_buffer_size &#x3D; 16777216# 该参数针对unix、linux，window上直接注释该参数.默认值为NULL# O_DIRECT减少操作系统级别VFS的缓存和Innodb本身的buffer缓存之间的冲突innodb_flush_method &#x3D; O_DIRECT # 此格式支持压缩, 5.7.7之后为默认值innodb_file_format &#x3D; Barracuda # CPU多核处理能力设置，假设CPU是2颗4核的，设置如下# 读多，写少可以设成2:6的比例innodb_write_io_threads &#x3D; 4innodb_read_io_threads &#x3D; 4 # 提高刷新脏页数量和合并插入数量，改善磁盘I&#x2F;O处理能力# 默认值200（单位：页）# 可根据磁盘近期的IOPS确定该值innodb_io_capacity &#x3D; 500 # 为了获取被锁定的资源最大等待时间，默认50秒，超过该时间会报如下错误:# ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactioninnodb_lock_wait_timeout &#x3D; 30 # 调整buffer pool中最近使用的页读取并dump的百分比,通过设置该参数可以减少转储的page数innodb_buffer_pool_dump_pct &#x3D; 40 # 设置redoLog文件所在目录, redoLog记录事务具体操作内容innodb_log_group_home_dir &#x3D; &#x2F;opt&#x2F;mysql&#x2F;redolog&#x2F; # 设置undoLog文件所在目录, undoLog用于事务回滚操作innodb_undo_directory &#x3D; &#x2F;opt&#x2F;mysql&#x2F;undolog&#x2F; # 在innodb_log_group_home_dir中的redoLog文件数, redoLog文件内容是循环覆盖写入。innodb_log_files_in_group &#x3D; 3 # MySql5.7官方建议尽量设置的大些，可以接近innodb_buffer_pool_size的大小# 之前设置该值较大时可能导致mysql宕机恢复时间过长，现在恢复已经加快很多了# 该值减少脏数据刷新到磁盘的频次# 最大值innodb_log_file_size * innodb_log_files_in_group &lt;&#x3D; 512GB,单文件&lt;&#x3D;256GBinnodb_log_file_size &#x3D; 1024M # 设置undoLog文件所占空间可以回收# 5.7之前的MySql的undoLog文件一直增大无法回收innodb_undo_log_truncate &#x3D; 1innodb_undo_tablespaces &#x3D; 3innodb_undo_logs &#x3D; 128 # 5.7.7默认开启该参数 控制单列索引长度最大达到3072# innodb_large_prefix &#x3D; 1 #5.7.8默认为4个, Inodb后台清理工作的线程数# innodb_purge_threads &#x3D; 4 # 通过设置配置参数innodb_thread_concurrency来限制并发线程的数量，# 一旦执行线程的数量达到这个限制，额外的线程在被放置到对队列中之前，会睡眠数微秒，# 可以通过设定参数innodb_thread_sleep_delay来配置睡眠时间# 该值默认为0,在官方doc上，对于innodb_thread_concurrency的使用，也给出了一些建议:# (1)如果一个工作负载中，并发用户线程的数量小于64，建议设置innodb_thread_concurrency&#x3D;0；# (2)如果工作负载一直较为严重甚至偶尔达到顶峰，建议先设置innodb_thread_concurrency&#x3D;128,### 并通过不断的降低这个参数，96, 80, 64等等，直到发现能够提供最佳性能的线程数# innodb_thread_concurrency &#x3D; 0############################ 其他内容设置 ##########################################[mysqldump]quickmax_allowed_packet &#x3D; 128M[mysql]no-auto-rehash[myisamchk]key_buffer_size &#x3D; 20Msort_buffer_size &#x3D; 256kread_buffer &#x3D; 2Mwrite_buffer &#x3D; 2M[mysqlhotcopy]interactive-timeout[mysqld_safe]# 增加每个进程的可打开文件数量.open-files-limit &#x3D; 28192 注意在修改MySQL的字符集编码的时候，即使修改了配置文件，在修改之前创建的数据库和表都不会发生更改，而是需要新建数据库才行。 二进制日志文件 log-bin主要用于主从复制，log-bin=mysql-bin 错误日志 log-error错误日志，用于记录MySQL出现的错误信息 查询日志 log默认是关闭的，记录查询的SQL语句，如果开启会降低MySQL的整体性能，因为记录日志也需要消耗系统资源 数据文件在windows下，安装MySQL的目录/data 目录可以挑选很多数据库文件；在Linux下，这个默认路径是/var/lib/mysql 下，使用ls -1F|grep ^d即可过滤 frm文件frm文件代表存放表结构 myd文件myd文件存放的是表数据 myi文件myi文件存放的是表的索引 三、MySQL逻辑架构介绍MySQL 是一个可移植的数据库，几乎能在当前所有的操作系统上运行，如 Unix/Linux、Windows、Mac 和 Solaris。各种系统在底层实现方面各有不同，但是 MySQL 基本上能保证在各个平台上的物理体系结构的一致性： 说说这张图： Client Connectors 是客户端链接，这个不用细说，就是应用程序与Mysql交互的接口，毕竟Mysql是要为程序提供数据存储服务的，所以必须将操作接口暴露出来，假如你是一个Java开发者，那么JDBC可以轻松链接上Mysql服务，就可以让你的Java程序使用上Mysql提供的服务 Connection Pool这个是连接池，Mysql与外界可能不止有一个连接，多次链接和断开会造成非常大的性能消耗，于是用使用连接池来管理这些链接，这就如Java的线程池来管理线程一样，通过连接池来避免性能损耗 Management Serveices &amp; Utilities是管理服务和工具组件，例如备份恢复、Mysql复制、安全性验证、集群、分区工作台等，下面会演示一个Mysql备份的例子 SQL Interface 就是SQL接口，存储过程、触发器、视图等，接受用户的SQL命令，并且返回用户需要查询的结果。接收DML(data manipulation language)数据操纵语言、DDL(data definition language数据库定义语言、比如select from就是调用SQL Interface Parser 是解析器，SQL命令传递到解析器的时候会被解析器验证和解析。解析器是由Lex和YACC实现的，是一个很长的脚本，将SQL语句分解成数据结构，并将这个结构传递到后续步骤，以后SQL语句的传递和处理就是基于这个结构的，如果在分解构成中遇到错误，那么就说明这个sql语句是不合理的 Optimizer 是查询优化器，SQL语句在查询之前会使用查询优化器对查询进行优化，这个不难理解，假如你有一张info 表中的字段是年龄(很显然这个额字段值是大于0的)，如果你在查询的时候的SQL语句是select * form info where age=-10，那么这条语句经过优化器之后不会再被执行，这就好像优化器知道不可能存在年龄小于0的条目 Caches 是高速缓存， 查询缓存，如果查询缓存有命中的查询结果，查询语句就可以直接去查询缓存中取数据。 通过LRU算法将数据的冷端溢出，未来得及时刷新到磁盘的数据页，叫脏页。 这个缓存机制是由一系列小缓存组成的。比如表缓存，记录缓存，key缓存，权限缓存等 Pluggable Storage Engines 是存储引擎，图中的圆柱体都是存储引擎，Mysql默认的存储引擎是InnoDB，后面谈论存储引擎 FileSystem 就是文件系统，Mysql数据库的数据最终还是要存放到文件中，所以我们可以理解为数据库就是一种帮我们管理数据的软件，处于文件系统的应用程序之间专门提供数据管理的软件，把数据的增删改查以及他的功能做了完美的封装，使用起来安全性更高，更方便我们队数据进行操作 1、连接层 最上层是一些客户端和连接服务，包含本地sock通信和大多数基于客户端/服务端工具实现的类似于tcp/ip的通信。主要完成一些类似于连接处理、授权认证、及相关的安全方案。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限 2、服务层 第二层架构主要完成大多少的核心服务功能，如SQL接口，并完成缓存的查询，SQL的分析和优化及部分内置函数的执行。所有跨存储引擎的功能也在这一层实现，如过程、函数等。在该层，服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化如确定查询表的顺序，是否利用索引等，最后生成相应的执行操作。如果是select语句，服务器还会查询内部的缓存。如果缓存空间足够大，这样在解决大量读操作的环境中能够很好的提升系统的性能。 3、引擎层 存储引擎层，存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API与存储引擎进行通信。不同的存储引擎具有的功能不同，这样我们可以根据自己的实际需要进行选取。后面我会在博客中介绍MyISAM和InnoDB。 MySQL存储引擎存储引擎是：数据库管理理系统如何存储数据、如何为存储的数据建立索引和如何更新、查询数据等技术的实现方法。MySQL的核心就是插件式存储引擎，支持多种存储引擎，所以你可以看到在Mysql的架构图上存储引擎的小插头，存储引擎是插拔式的，默认是InnoDB（从MySQL5.5.8开始，之前是MyISAM），当然也可以选择其他的存储引擎使用show engines;命令可以查看支持的存储引擎： 查看MySQL当前默认的存储引擎 123456789mysql&gt; show variables like '%storage_engine%';+----------------------------+--------+| Variable_name | Value |+----------------------------+--------+| default_storage_engine | InnoDB || default_tmp_storage_engine | InnoDB || storage_engine | InnoDB |+----------------------------+--------+3 rows in set (0.00 sec) 接下来说说他们的区别： 特点 Myisam BDB Memory InnoDB Archive 存储限制 没有 没有 有 64TB 没有 事物安全 支持 支持 锁机制 表锁 页锁 表锁 行锁 行锁 B树索引 支持 支持 支持 支持 哈希索引 支持 支持 全文索引 支持 集群索引 支持 数据缓存 支持 支持 索引缓存 支持 支持 支持 数据可压缩 支持 支持 空间使用 低 低 中等 高 低 批量插入速度 高 高 高 低 非常高 支持外键 支持 MyISAM存储引擎MyISAM是MySQL官方提供默认的存储引擎，其特点是不支持事务、表锁和全文索引，对于一些OLAP系统(OLAP 系统强调数据分析，强调SQL执行市场，强调磁盘I/O，强调分区等)，操作速度快。关于《OLAP、OLTP的介绍和比较》 每个MyISAM在磁盘上存储成三个文件。文件名都和表名相同，扩展名分别是.frm（存储表定义）、.MYD (MYData，存储数据)、.MYI (MYIndex，存储索引)。这里特别要注意的是MyISAM不缓存数据文件，只缓存索引文件。 InnoDB存储引擎InnoDB存储引擎支持事务，主要面向OLTP方面的应用，其特点是行锁设置、支持外键，并支持类似于Oracle的非锁定读，即默认情况下读不产生锁。InnoDB将数据放在一个逻辑表空间中。InnoDB通过多版本并发控制来获得高并发性，实现了ANSI标准的4种隔离级别，默认为Repeatable，使用一种被称为next-key locking的策略避免幻读。 对于表中数据的存储，InnoDB采用类似Oracle索引组织表Clustered的方式进行存储。 InnoDB 存储引擎提供了具有提交、回滚和崩溃恢复能力的事务安全。但是对比Myisam的存储引擎，InnoDB 写的处理效率差一些并且会占用更多的磁盘空间以保留数据和索引 NDB存储引擎NDB存储引擎是一个集群存储引擎，类似于Oracle的RAC，但它是Share Nothing的架构，因此能提供更高级别的高可用性和可扩展性。NDB的特点是数据全部放在内存中，因此通过主键查找非常快。 关于NDB，有一个问题需要注意，它的连接(join)操作是在MySQL数据库层完成，不是在存储引擎层完成，这意味着，复杂的join操作需要巨大的网络开销，查询速度会很慢。 Memory (Heap) 存储引擎Memory存储引擎（之前称为Heap）将表中数据存放在内存中，如果数据库重启或崩溃，数据丢失，因此它非常适合存储临时数据。 Archive存储引擎正如其名称所示，Archive非常适合存储归档数据，如日志信息。它只支持INSERT和SELECT操作，其设计的主要目的是提供高速的插入和压缩功能。 Federated存储引擎Federated存储引擎不存放数据，它至少指向一台远程MySQL数据库服务器上的表，非常类似于Oracle的透明网关。 InnoDB与MyISAM应用场景参考：《InnoDB与MyISAM两者的区别》MyISAM管理非事务表。它提供高速存储和检索，以及全文搜索能力。如果应用中需要执行大量的SELECT查询，那么MyISAM是更好的选择。 InnoDB用于事务处理应用程序，具有众多特性，包括ACID事务支持。如果应用中需要执行大量的INSERT或UPDATE操作，则应该使用InnoDB，这样可以提高多用户并发操作的性能 所以MySQL的特点可以总结为： 和其它数据库相比，MySQL有点与众不同，它的架构可以在多种不同场景中应用并发挥良好作用。主要体现在存储引擎的架构上，插件式的存储引擎架构将查询处理和其它的系统任务以及数据的存储提取相分离。这种架构可以根据业务的需求和实际需要选择合适的存储引擎。 Alibaba用哪个？Percona为MySQL数据库服务器进行了改进，在功能和性能上较MySQL有着很显著的提升。该版本提升了在高负载情况下的InnoDB的性能、为DBA提供一些非常有用的性能诊断工具；另外有更多的参数和命令来控制服务器行为。该公司新建了一款存储引擎叫XtraDB完全可以替代innodb,并且在性能和并发上做得更好，阿里巴巴大部分mysql数据库其实使用的percona的原型加以修改。","updated":"2020-03-13T03:06:29.693Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"}]},{"title":"基于Docker的MySQL主从复制","date":"2019-08-18T06:03:35.000Z","path":"2019/08/18/基于Docker的MySQL主从复制/","text":"再谈谈数据库优化如图，MySQL优化呢无非就是从这几个方面入手，第一是数据库设计，第二是SQL优化！但是随着数据表的增长，数据会越来越多，通过MySQL优化是可以解决部分性能问题的，但是一台服务器的资源是有限的，无论怎么优化始终无法解决MySQL的性能瓶颈问题。 举一个经典的例子，一个卡车无论再怎么优化发动机，优化传动结构，优化材料设计都没办法拉着金字塔这种沉重的负载，这种情况就属于MySQL的瓶颈，无论如何优化，只要数据量达到一定的规模，一台MySQL肯定是撑不住的： 如何用架构进行优化对于千万级的表如何进行优化呢？见下图，加索引，优化SQL都是之前的优化方法，至于不解决问题，把问题留给后面的程序员这种做法最好还是放弃吧，啊哈哈，其中加缓存和数据库拆分是很常见的解决方案，下面说一下如何通过MySQL主从复制来解决MySQL的性能瓶颈问题： 主从复制（又叫做读写分离），主从复制的目的：分散压力 为什么要读写分离呢？如果对数据库的读操作和写操作都在同一个数据库服务器中进行，业务系统性能会降低，所以需要进行读写分离，通常情况下遵循二八原则，即20%的时候进行写操作，80%的时候进行读操作： 生活中有很形象的例子，比如你在自助咖啡厅买咖啡（如果只有一台咖啡机）： 如果有多台咖啡机，很明显大家买咖啡的效率就上去了： 所以主从复制的简单原理图如下： 一台MySQL作为写服务器，另外几台MySQL作为读服务器，这样便完成了分散压力，为了几台服务器之间的数据一致，所以需要做一个数据库主从复制，即读写分离！ Docker实现MySQL主从复制那么如何使用Docker实现MySQL主从复制呢？我先在Docker里面跑了两个MySQL： Docker命令是：docker run -p 3306:3306 --restart=always --name mysql_master -v /root/mysql/conf:/etc/mysql/conf.d -v /root/mysql/logs:/logs -v /root/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql 接着查看一下两台机器的IP地址 首先进入到master机器，使用vi编辑器编辑/etc/my.cnf，我直接使用docker pull mysql:5.7 ，里面默认是Ubuntu系的内核，所以没有vim编辑器，可以使用apt包管理器下载，下载命令是apt install vim ，如果无法下载那应该是没有更新，使用apt update即可更新！但是我发现其实我的配置文件地址不一样，我的在/etc/mysql/mysql.conf.d/mysqld.cnf ，编辑这个配置文件就好了，主机的配置文件： 编辑完成后重启mysql，CentOS下直接systemctl restart mysql 即可，但是在Ubuntu要使用的命令是service mysql restart （另外，在Ubuntu下重启MySQL会导致用户退出Docker容器，Docker关闭，所以需要再次开启MySQL的容器，使用docker exec -it mysql_master /bin/bash 进入容器，然而在CentOS下却不需要） 接下来给主机添加一个用户，并设置密码，然后重启容器： 接下来进入主机，使用如下命令即可看到binaryLog，以及binaryLog文件的偏移量： 接着需要配置从机，和编辑主机的配置文件是一样的，只不过从机的配置只需要指定 server_id 即可，我们指定从机的server_id=2，然后停止同步线程， 并做主从配置，完成后开启同步线程： 检查是否开启成功： 查看主从复制的状态 接下来测试一下，主机新建数据库，从机自动同步，主机新建表，从机自动同步，主机插入数据，从机自动同步： 主从复制的原理上面已经完成了MySQL的主从复制，接下来看看主从复制究竟是怎么实现的？ 上面的图其实很能说明问题，主机的数据发生更改的时候会产生一个BinaryLog文件，然后从机的IO线程会去取这个二进制文件，取回来之后会将主机的BinaryLog拷贝到中继日志中，SQL线程直接通过中继日志来改变自身的数据。那么说到这里可能很多人会有疑问，不是说主机用来写，从机用来读吗？为啥还是需要从机的写操作，事实上这并不是写操作，而且直接修改数据的操作，举一个很简单的例子，如果是普通的修改数据，会首先找到要修改的数据的二进制位置，这样才能发生更改，如果直接告诉你在哪个位置要修改成什么数据，会不会更快呢？很明显，主从复制是非常能提交数据库系统的效率的。 一个小坑：如果主机上面的并发量特别高的话，从机同步数据的能力会下降，甚至一个数据插入到主机几小时后才能同步到从机，这种情况的根本原因就是从机的IO线程是单线程的，如果配置为多线程就能解决这种问题！","updated":"2020-03-13T03:06:29.732Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"},{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"}]},{"title":"Thymeleaf对date类型的input格式化支持","date":"2019-08-05T10:09:30.000Z","path":"2019/08/05/Thymeleaf对date类型的input格式化支持/","text":"一、解决问题最近在使用Thymeleaf模板引擎，但是遇到的问题就是我现在有这样一个标签：输入类型是date，我现在要把JavaBean中的java.sql.Date数据放置到这个额输入框中，但是如你上图所见，这个输入框根本不是单纯的文本框，而且一个日期选择框，好吧….我尝试过th:datetime 但是不行，即使是按照格式化的方式也是不行，就像这样的： 123&lt;input type=\"date\" th:value=\"$&#123;#dates.format(company.companyRegdate, 'yyyy/MM/dd')&#125;\"/&gt;&lt;input type=\"date\" th:value=\"$&#123;company.companyRegdate&#125;\"/&gt; 果然还是不行，于是卡了半天的stackoverflow终于出来了： 12345678@DateTimeFormat(pattern = \"yyyy-MM-dd\")private Date companyRegdate;@DateTimeFormat(pattern = \"yyyy-MM-dd\")private Date companyUnregdate;@DateTimeFormat(pattern = \"yyyy-MM-dd\")private Date companyUpdatedate; 这里推荐的方式就是给JavaBean的属性上注解一个时间日期格式化器，对的，这个很容易理解，我们所看到的日期不过是像2018/01/01这样的字符串，或者说像2018年1月1日 这样的字符串，我们和老外对时间的格式表示当然会不一样，但是这个世界上统一的时间就是时间戳，所有的时间表示都是通过时间戳转换而来的。所以我们在存储Date时其实保存的是时间戳的数值，至于具体显示出来时间是怎么样的，要看格式化串，就好比一个模板，所以这个解决方式还是很靠谱的！果断改成@DateTimeFormat，哈哈，还是经验不足呀！ 二、util.Date与sql.Date后面呢我又发现出了问题，其实@DatetimeFormat是将String转换成Date，一般前台给后台传值时用@JsonFormat(pattern=”yyyy-MM-dd”) 将Date转换成String 一般后台传值给前台时使用，总结一下其实就是这样！对了顺便提一下，这个想要使用这些注解的前提是必须使用java.util.Date类，而不是java.sql.Date类，否则是格式化注解是会报错的，关于java.util.Date类和java.sql.Date类之前在用的时候还真没注意过这两者的区别，现在还是可以总结一下的： 两者都有getTime方法返回毫秒数，可以直接构建对象，java.sql.Date是针对SQL语句使用的，它只包含日期而没有时间部分。java.util.Date 是 java.sql.Date 的父类，java.sql.Date转为java.util.Date示例： 12java.sql.Date date=new java.sql.Date();java.util.Date d=new java.util.Date (date.getTime()); java.util.Date转为java.sql.Date示例： 1234java.util.Date utilDate = new Date();java.sql.Date sqlDate = new java.sql.Date(utilDate.getTime());java.sql.Time sTime = new java.sql.Time(utilDate.getTime());java.sql.Timestamp stp = new java.sql.Timestamp(utilDate.getTime()); 又遇到一个问题，现在输入类型成了datetime-local，那么如何把Date类型注入到这个标签中呢？下面的方式亲测可用！ datetime-local赋值时间格式：yyyy-MM-d + &#39;T&#39; + HH:mm:ss 年月日和时分秒分别格式化然后拼接大写”T” 123$&#123;#dates.format(productInfo.overTime, &#39;yyyy-MM-dd&#39;)&#125; + &#39;T&#39; + $&#123;#dates.format(productInfo.overTime, &#39;HH:mm:ss&#39;)&#125; 三、总结还是用long时间戳好使！","updated":"2020-03-13T03:06:29.710Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"模板引擎","slug":"模板引擎","permalink":"https://zouchanglin.cn/tags/%E6%A8%A1%E6%9D%BF%E5%BC%95%E6%93%8E/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://zouchanglin.cn/tags/SpringBoot/"}]},{"title":"Docker常用软件安装与镜像发布","date":"2019-08-04T03:02:35.000Z","path":"2019/08/04/Docker常用软件安装与镜像发布/","text":"Docker安装MySQL 下面将使用Docker来安装Mysql，先搜索，然后拉取： 1docker pull mysql:5.7 12345docker run -p 3306:3306--name mysql \\-v /root/mysql/conf:/etc/mysql/conf.d \\-v /root/mysql/logs:/logs \\-v /root/mysql/data:/var/lib/mysql \\-e MYSQL_ROOT_PASSWORD=123456 -d mysql 123docker ps # 查看运行容器docker exec -it 容器ID /bin/bash # 进入容器 解决外部工具连接不上的问题 1234567ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39; PASSWORD EXPIRE NEVER;Query OK, 0 rows affected (0.02 sec) FLUSH PRIVILEGES; Query OK, 0 rows affected (0.01 sec) Docker安装MySQL需要注意的问题： 1、搜索时在 https://hub.docker.com/ 这个网站进行搜索，比如搜索MySQL 下面就有版本号，默认是最新的，也就是latest 2、这条命令的意思 12345docker run -p 3306:3306--name mysql \\-v /root/mysql/conf:/etc/mysql/conf.d \\-v /root/mysql/logs:/logs \\-v /root/mysql/data:/var/lib/mysql \\-e MYSQL_ROOT_PASSWORD=123456 -d mysql 其实很近简单–restart=always是让容器在Docker服务启动后也跟随启动，我的Docker设置的是开机启动，所以开机后一旦Docker服务启动，mysql容器也会启动，通过-v命令添加了三个容器卷，分别是/mysql/conf、/mysql/logs、~/mysql/data ，这样的话我们在宿主机上就可以配置MySQL、查看log、以及备份数据！ Docker安装Tomcat同样的安装Tomcat也是先查找，然后直接根据自己的版本号直接docker pull tomcat:标签 1234docker run -it -p 8080:8080 --name tomcat \\-v /root/tomcat/webapps:/usr/local/tomcat/webapps \\-v /root/tomcat/logs:/usr/local/tomcat/logs \\tomcat /bin/bash 这个Tomcat需要手动启动，所以使用 -it 模式进入bin目录start.sh才可以开启Tomcat，另外，如果像上面这个路径去添加容器卷的话，webapp和logs目录会清空，我是直接把webapps的东西copy进去就好了！ Docker安装Redis同样的安装redis也是先查找，然后直接根据自己的版本号直接`docker pull redis:标签 1234docker run -p 6379:6379 -v /root/redis/data:/data \\-v /root/redis/conf/redis.conf:/usr/local/etc/redis/redis.conf -d redis:3.2 redis-server /usr/local/etc/redis/redis.conf --appendonly yes 在主机/root/redis/conf/redis.conf目录下新建redis.conf文件 vim /root/redis/conf/redis.conf/redis.conf 测试redis-cli连接和测试持久化文件生成 docker exec -it 容器编号 redis-cli Docker镜像发布到阿里云 镜像的生成方法，编写DockerFile，这个在上一篇文章DockerFile已经说过了。或者从容器生成一个新的镜像： 1docker commit [OPTIONS] 容器ID [REPOSITORY[:TAG]] 登录到阿里云的镜像服务：https://cr.console.aliyun.com/cn-hangzhou/instances/repositories 创建一个命名空间，创建本地仓库: 将镜像推送到阿里云，推送方式阿里云文档里面就有 按照提示操作，先登录 登录成功后，由于我之前没还没制作镜像，现在开始制作镜像并且推送镜像 出现下面这种，即说明推送完毕（其实这和Pull是一样的，镜像分层都是一层一层的结构） 在阿里云执行搜索，一定要是公有仓库才能搜到，下面看看我的Redis 这样就和那些mysql镜像一样了，可以直接下载，先删除本地的镜像，然后再拉取","updated":"2020-03-13T03:06:29.648Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"}]},{"title":"DockerFile快速构建自定义镜像","date":"2019-08-03T10:31:28.000Z","path":"2019/08/03/DockerFile快速构建自定义镜像/","text":"Dockerfile什么是DockerfileDockerfile是用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本。通俗的理解，和Maven一样，只需要写好pom文件就可以定义整个工程的构建，DockerFile是一样的，通过这个脚本可以构建出自己想要的容器！ Dockerfile构建步骤 编写Dockerfile文件 docker build docker run 以我们熟悉的CentOS为例 ，看看CentOS的Dockerfile的内容： 12345678910FROM scratch (scratch相当于Object，是所有镜像的祖先镜像)MAINTAINER The CentOS Project &lt;cloud-ops@centos.org&gt;ADD c68-docker.tar.xz /LABEL name=\"CentOS Base Image\" \\ vendor=\"CentOS\" \\ license=\"GPLv2\" \\ build-date=\"2016-06-02\" # Default commandCMD [\"/bin/bash\"] DockerFile基础语法 1：每条保留字指令都必须为大写字母且后面要跟随至少一个参数 2：指令按照从上到下，顺序执行 3：#表示注释 4：每条指令都会创建一个新的镜像层，并对镜像进行提交 Docker执行Dockerfile的大致流程 （1）docker从基础镜像运行一个容器 （2）执行一条指令并对容器作出修改 （3）执行类似docker commit的操作提交一个新的镜像层 （4）docker再基于刚提交的镜像运行一个新容器 （5）执行dockerfile中的下一条指令直到所有指令都执行完成 从应用软件的角度来看，Dockerfile、Docker镜像与Docker容器分别代表软件的三个不同阶段： 1、Dockerfile 是软件的原材料2、Docker 镜像是软件的交付品3、Docker 容器则可以认为是软件的运行态。 Dockerfile面向开发，Docker镜像成为交付标准 1 Dockerfile，需要定义一个Dockerfile，Dockerfile定义了进程需要的一切东西。Dockerfile涉及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程(当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计namespace的权限控制)等等; 2 Docker镜像，在用Dockerfile定义一个文件之后，docker build时会产生一个Docker镜像，当运行 Docker镜像时，会真正开始提供服务; 3 Docker容器，容器是直接提供服务的。 DockerFile体系结构FROM 基础镜像，当前新镜像是基于哪个镜像的 MAINTAINER 镜像维护者的姓名和邮箱地址 RUN 容器构建时需要运行的命令 EXPOSE 当前容器对外暴露出的端口 WORKDIR 指定在创建容器后，终端默认登陆的进来工作目录，一个落脚点 ENV 用来在构建镜像过程中设置环境变量，比如ENV MY_PATH /usr/mytest，这个环境变量可以在后续的任何RUN指令中使用，这就如同在命令前面指定了环境变量前缀一样；也可以在其它指令中直接使用这些环境变量，比如：WORKDIR $MY_PATH ADD 将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar压缩包 COPY 类似ADD，拷贝文件和目录到镜像中。将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置 COPY的写法有两种：COPY src dest 或者 COPY [&quot;src&quot;, &quot;dest&quot;] VOLUME 容器数据卷，用于数据保存和持久化工作 CMD 指定一个容器启动时要运行的命令 CMD命令也是两种格式，一种是Shell脚本 CMD&lt; 命令&gt;，另一种是 CMD [&quot;可执行文件&quot;, &quot;参数一&quot;, &quot;参数二&quot;]， Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换 ENTRYPOINT 指定一个容器启动时要运行的命令 ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数 ONBUILD 当构建一个被继承的Dockerfile时运行命令，父镜像在被子继承后父镜像的onbuild被触发 Dockerfile案例Docker Hub 中 99% 的镜像都是通过在 base 镜像中安装和配置需要的软件构建出来的 一、自定义一个CentOS 现在我们需要自定义一个镜像来支持vim、ifconfig、并且登录后的默认路径改做修改 编写Dockerfile： 123456789101112131415161718192021# 从标准centos构建FROM centos# 定义作者信息MAINTAINER tim&lt;15291418231@163.com&gt;# 定义一个变量ENV mypath /tmp# 设置登录后的落脚点WORKDIR $mypath# 安装vim和net-tools工具RUN yum -y install vimRUN yum -y install net-toolsEXPOSE 80CMD echo $mypathCMD echo \"success----------ok\"CMD /bin/bash 接下来开始构建 1docker build -f &#x2F;mydocker&#x2F;Dockerfile -t mycentos:1.3 . 然后启动镜像，测试一下： 看看构建过程是否是如前面所说，这也证实了镜像的分层： 二、制作可以查询IP信息的镜像Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换，这是什么意思呢？ Tomcat 并没有运行起来，就是因为Docker的run命令后面加了参数！所以ENTRYPOINT命令就更好用了，ENTRYPOINT不会覆盖，只是追加命令！ 首先解释一下curl命令 curl命令可以用来执行下载、发送各种HTTP请求，指定HTTP头部等操作。如果系统没有curl可以使用yum install curl安装，也可以下载安装。curl是将下载文件输出到stdout使用命令：curl http://www.baidu.com，执行后，www.baidu.com的html页面就会以文本的形式显示在屏幕上了。 这是最简单的使用方法。用这个命令获得了http://curl.haxx.se指向的页面，同样，如果这里的URL指向的是一个文件或者一幅图都可以直接下载到本地。如果下载的是HTML文档，那么缺省的将只显示文件头部，即HTML的header。要全部显示，请加参数-i 12345FROM centosRUN yum install -y curlCMD [\"curl\", \"-s\", \"https://ip.cn\"] 使用curl这个工具就可以查看IP，现在使用构件好的镜像run一下： 哈哈，居然给我识别成渭南的，这个工具不是很准呀！如果要显示HTML的header需要加参数-i，下面我们来试一下： 很显然不好使了，这就是为什么CMD指令不生效的原因，就是因为后面加的参数，所以出现了ENTRYPOINT指令，接下来使用ENTRYPOINT指令构建一下镜像： 12345FROM centosRUN yum install -y curlENTRYPOINT [\"curl\", \"-s\", \"https://ip.cn\"] 果然，在ENTRYPOINT指令下加参数是可用的 三、ONBUILD指令的使用1234567FROM centosRUN yum install -y curlENTRYPOINT [\"curl\", \"-s\", \"https://ip.cn\"]ONBUILD RUN echo \"father build finished! 886\" 在父镜像构建完成的时候执行ONBUILD的指令，执行构建，构建出父镜像为mycentos:1.6 接下来编写子镜像的Dockerfile 123456# 由于需要从父镜像构建，所以FROM写成mycentos1.6FROM mycentos:1.6RUN yum install -y curlENTRYPOINT [\"curl\", \"-s\", \"https://ip.cn\"] 可以看到，在构建父镜像完成后执行了ONBUILD的后面的内容！ 四、自定义构建Tomcat新建一个文件夹，里面包含（apache-tomcat-9.0.8.tar.gz、jdk-8u171-linux-x64.tar.gz、a.txt） 编写Dockerfile 123456789101112131415161718192021222324FROM centosMAINTAINER tim&lt;15291418231@163.com&gt;#把宿主机当前上下文的a.txt拷贝到容器/usr/local/路径下COPY a.txt /usr/local/cincontainer.txt#把java与tomcat添加到容器中ADD jdk-8u171-linux-x64.tar.gz /usr/local/ADD apache-tomcat-9.0.8.tar.gz /usr/local/#安装vim编辑器RUN yum -y install vim#设置工作访问时候的WORKDIR路径，登录落脚点ENV MYPATH /usr/localWORKDIR $MYPATH#配置java与tomcat环境变量ENV JAVA_HOME /usr/local/jdk1.8.0_171ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarENV CATALINA_HOME /usr/local/apache-tomcat-9.0.8ENV CATALINA_BASE /usr/local/apache-tomcat-9.0.8ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin#容器运行时监听的端口EXPOSE 8080#启动时运行tomcat# ENTRYPOINT [\"/usr/local/apache-tomcat-9.0.8/bin/startup.sh\" ]# CMD [\"/usr/local/apache-tomcat-9.0.8/bin/catalina.sh\",\"run\"]CMD /usr/local/apache-tomcat-9.0.8/bin/startup.sh &amp;&amp; tail -F /usr/local/apache-tomcat-9.0.8/bin/logs/catalina.out 开始构建 1docker build -t mytomcat:1.1 开始运行 1docker run -d -p 8080:8080 --name mytomcat9 -v /tim/mydockerfile/tomcat9/test:/usr/local/apache-tomcat-9.0.8/webapps/test -v /tim/mydockerfile/tomcat9/tomcat9logs/:/usr/local/apache-tomcat-9.0.8/logs --privileged=true mytomcat9 其实这条命令虽然看起来很长，但是无非是启动镜像并且添加了两个数据卷，privileged 是Docker挂载主机目录Docker访问出现cannot open directory .: Permission denied的时候需要加上的参数！ Dockerfile的总结","updated":"2020-03-13T03:06:29.647Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"}]},{"title":"Docker镜像与数据容器卷","date":"2019-08-03T03:47:37.000Z","path":"2019/08/03/Docker镜像与数据容器卷/","text":"Docker镜像镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的所有内容，包括代码、运行时、库、环境变量和配置文件。 UnionFS（联合文件系统）UnionFS（联合文件系统）：Union文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)。Union 文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。 特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录 Docker镜像加载原理docker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS。bootfs(boot file system)主要包含bootloader和kernel, bootloader主要是引导加载kernel, Linux刚启动时会加载bootfs文件系统，在Docker镜像的最底层是bootfs。这一层与我们典型的Linux/Unix系统是一样的，包含boot加载器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载bootfs。 rootfs (root file system) ，在bootfs之上。包含的就是典型 Linux 系统中的 /dev, /proc, /bin, /etc 等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Ubuntu，Centos等 这也就解释了为什么我们开的虚拟机很大，但是Docker里面的镜像却很小： 对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用Host的kernel，自己只需要提供 rootfs 就行了。由此可见对于不同的linux发行版, bootfs基本是一致的, rootfs会有差别, 因此不同的发行版可以公用bootfs。 分层的镜像以pull为例，在下载的过程中可以看到docker的镜像好像是在一层一层的在下载，这也就解释了为什么Tomcat的镜像那么大 最大的一个好处就是 - 共享资源 比如：有多个镜像都从相同的 base 镜像构建而来，那么宿主机只需在磁盘上保存一份base镜像，同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了，而且镜像的每一层都可以被共享。 镜像的特点Docker镜像都是只读的，当容器启动时，一个新的可写层被加载到镜像的顶部。这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。 Docker镜像commit操作docker commit 提交容器副本使之成为一个新的镜像 docker commit -m=&quot;提交的描述信息&quot; -a=&quot;作者&quot; 容器ID 要创建的目标镜像名:[标签名] 演示操作 1、先下载一个Tomcat镜像，并且运行Tomcat docker pull tomcat docker run -it -p 8888:8080 tomcat (使用-P是随机分配端口，分配的是Docker对外暴露的端口) 2、删除Tomcat的docs部分 进入到这个容器中，删除webapps下的docs文件夹 此时访问docs就会404，说明我们删除成功 也即当前的tomcat运行实例是一个没有文档内容的容器，以它为模板commit一个没有doc的tomcat新镜像xpu/tomcat_02 （xpu是命名空间，就相当于类的包名） 3、命令打包 123[root@localhost ~]# docker commit -m&#x3D;&quot;This is my Tomcat&quot; -a&#x3D;&quot;Tim&quot; 4f2161bbdb42 xpu&#x2F;tomcat_02:1.2sha256:b29a3c2a5e09af550d3bee9b6ff3cd9cf8e2b2b2397dc53278f4c495607f748f[root@localhost ~]# 1docker rm -f $(docker ps -q) 删除正在运行的所有容器 4、后台方式启动Tomcat 1docker run -d -p 8080:8080 tomcat Docker容器数据卷先来看看Docker的理念： 1、将运用与运行的环境打包形成容器运行 ，运行可以伴随着容器，但是我们对数据的要求希望是持久化的 2、容器之间希望有可能共享数据 Docker容器产生的数据，如果不通过docker commit生成新的镜像，使得数据做为镜像的一部分保存下来，那么当容器删除后，数据自然也就没有了。 为了能保存数据在Docker中我们使用卷，也就是容器数据卷！ Docker容器数据卷有点类似我们Redis里面的rdb和aof文件，也就是把运行时的数据持久化在硬盘上 卷就是目录或文件，存在于一个或多个容器中，由docker挂载到容器，但不属于联合文件系统，因此能够绕过Union File System提供一些用于持续存储或共享数据的特性： 卷的设计目的就是数据的持久化，完全独立于容器的生存周期，因此Docker不会在容器删除时删除其挂载的数据卷 特点：1：数据卷可在容器之间共享或重用数据2：卷中的更改可以直接生效3：数据卷中的更改不会包含在镜像的更新中4：数据卷的生命周期一直持续到没有容器使用它为止 添加数据卷 _使用-v命令 docker run -it -v /宿主机绝对路径目录:/容器内目录 镜像名 1docker run -it -v &#x2F;myDataVolume:&#x2F;dataVolumeContainer centos 这样便添加了数据卷 同样的，使用docker inspect 容器名称 便可以查看JSON形式描述的容器： 测试一下两者的文件通信共享(容器和宿主机之间数据共享) 容器停止退出后，主机修改后数据是否同步 带有写保护权限的数据卷 1docker run -it -v &#x2F;宿主机绝对路径目录:&#x2F;容器内目录:ro 镜像名 ro即是ReadOnly，只读，不允许容器修改数据卷，而宿主机才可以！ 添加数据卷_使用Dockerfile在Linux下写项目很多时候用到makefile来构建工程，或者是通过Shell脚本把一系列的操作封装起来，所以理解Dockerfile就不难，之前说过镜像是层层包裹的，就比如：Tomcat镜像肯定是依赖于JDK镜像的，所以Dockerfile还是很重要的 先编写Dockerfile (注意VOLUME后面有空格) 1234567[root@localhost mydocker]# cat Dockerfile # volume testFROM centosVOLUME [\"/dataVolume1\",\"/dataVolume2\"]CMD echo \"finished, ------success\"CMD /bin/bash[root@localhost mydocker]# 由上图输出也可以看出，这个镜像是分层构建的！接下来检测一下构建结果： 那么对应的宿主机器目录在哪呢？虽然我们没有手动指定，但是dockerfile有默认的路径，使用docker inspect 容器ID查看容器细节： 如果Docker挂载主机目录Docker访问出现cannot open directory .: Permission denied，解决办法：在挂载目录后多加一个--privileged=true参数即可 所以到目前为止可以吧数据卷简单的理解为虚拟机和物理机的共享文件夹，上面讲述了两种创建容器数据卷的方式，方式一不是很常用，使用Dockerfile的方式是更适合使用的！ 数据卷容器命名的容器挂载数据卷，其它容器通过挂载这个(父容器)实现数据共享，挂载数据卷的容器，称之为数据卷容器 先见了三个容器，dc01是之前建好的，里面有容器卷dataVolume1，dataVolume2，剩下两个容器分别是dc02，dc03，然后通过 1docker run -it --name dc02 --volumes-from dc01 tim&#x2F;centos 这样的命令去新建一个dc02容器继承父容器dc01，同样的，使用此命令新建dc03容器继承父容器dc01，于是dc02和dc03都含有一两个容器卷dataVolume1，dataVolume2，于是dc01、dc02、dc03都是数据共享的： 接下来删除dc01，然后dc02新建一个dc02_update.txt，虽然dc03是继承自dc01的，但是dc03仍然可以看到dc02_update.txt，看下图： 由此得出一个重要结论： 容器之间配置信息的传递，数据卷的生命周期一直持续到没有容器使用它为止","updated":"2020-03-13T03:06:29.651Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"}]},{"title":"Docker底层原理与命令详解","date":"2019-08-02T06:27:48.000Z","path":"2019/08/02/Docker底层原理与命令详解/","text":"Docker 是如何工作的Docker是一个Client-Server结构的系统，Docker守护进程运行在主机上， 然后通过Socket连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。 容器，是一个运行时环境，就是我们前面说到的集装箱。 docker有着比虚拟机更少的抽象层。由于docker不需要Hypervisor实现硬件资源虚拟化，运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。 docker利用的是宿主机的内核,而不需要Guest OS。因此,当新建一个容器时，docker不需要和虚拟机一样重新加载一个操作系统内核。仍而避免引寻、加载操作系统内核返个比较费时费资源的过程，当新建一个虚拟机时，虚拟机软件需要加载Guest OS，返个新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统,则省略了返个过程，因此新建一个docker容器只需要几秒钟。 所以，把两者对比来看 所以结合之前的理解，镜像与容器的关系更清楚了 Docker常用命令Usage: docker [OPTIONS] COMMAND 1、帮助命令docker version 查看版本 docker info 查看详细信息 docker –help 帮助命令 2、镜像命令① docker images 列出本地主机上的镜像 选项参数 -a :列出本地所有的镜像（含中间映像层） -q:只显示镜像ID –digests :显示镜像的摘要信息 –no-trunc :显示完整的镜像信息 同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。 如果你不指定一个镜像的版本标签，例如你只使用 ubuntu，docker 将默认使用 ubuntu:latest 镜像（也就是最新的） ② docker search 某个XXX镜像名字 从https://hub.docker.com 这个网站去查找，从阿里云下载，和Maven是一样的 选项参数 –no-trunc : 显示完整的镜像描述 -s : 列出收藏数不小于指定值的镜像 –automated : 只列出 automated build类型的镜像 ③ docker pull 某个XXX镜像名字 下载镜像 docker pull 镜像名字[:TAG] 接下来看看Tomcat的镜像大小，居然是506M，为什么？ ④ docker rmi 某个XXX镜像名字ID 删除单个 docker rmi -f 镜像ID 删除多个 docker rmi -f 镜像名1:TAG 镜像名2:TAG 删除全部 docker rmi -f $(docker images -qa) 3、容器命令新建并且启动容器有镜像才能创建容器，这是根本前提(下载一个CentOS镜像演示) 新建并启动容器 docker run [OPTIONS] IMAGE [COMMAND] [ARG…] OPTIONS参数说明：有些是一个减号，有些是两个减号 –name=”容器新名字”: 为容器指定一个名称； -d 后台运行容器，并返回容器ID，也即启动守护式容器； -i 以交互模式运行容器，通常与 -t 同时使用； -t 为容器重新分配一个伪输入终端，通常与 -i 同时使用； -P 随机端口映射； -p 指定端口映射，有以下四种格式 ip:hostPort:containerPort ip::containerPort hostPort:containerPort containerPort 所以下面的 -it 就是交互模式并且登录伪终端 查看运行中和最近运行容器查看正在运行的容器 docker ps [OPTIONS] OPTIONS说明（常用）： -a : 列出当前所有正在运行的容器+历史上运行过的 -l : 显示最近创建的容器 -n：显示最近n个创建的容器 -q : 静默模式，只显示容器编号 –no-trunc : 不截断输出 退出容器两种退出容器的方式： exit 容器停止并退出 Ctrl + P + Q 容器不停止退出 启动容器docker start 容器ID或者容器名 重启容器docker restart 容器ID或者容器名 停止容器docker stop 容器ID或者容器名 强制停止容器docker kill 容器ID或者容器名 删除已停止的容器docker rm 容器ID 一次性删除多个容器 docker rm -f $(docker ps -a -q) docker ps -a -q | xargs docker rm 4、重要的容器命令启动守护式容器docker run -d 容器名称 很显然启动了一个守护式容器centos，但是启动完成后容器却立马退出了，为什么？ 很重要的要说明的一点: Docker容器后台运行,就必须有一个前台进程. 容器运行的命令如果不是那些一直挂起的命令（比如运行top，tail），就是会自动退出的。 这个是docker的机制问题,比如你的web容器,我们以nginx为例，正常情况下,我们配置启动服务只需要启动响应的service即可。例如service nginx start。但是这样做，nginx为后台进程模式运行，就导致docker前台没有运行的应用，这样的容器后台启动后，会立即自杀，因为他觉得他没事可做了。所以，最佳的解决方案是，将你要运行的程序以前台进程的形式运行 查看容器日志docker logs -f -t –tail 容器ID​-t 是加入时间戳​-f 跟随最新的日志打印​–tail 数字 显示最后多少条 比如在我们启动守护式容器的时候，可以这样启动： docker run -d centos /bin/sh -c “while true; do each hello tim; sleep 2; done” 这样保持运行而且一直打印日志 查看容器内运行的进程docker top 容器ID 查看容器内部细节docker inspect 容器ID 进入正在运行的容器并以命令行交互docker exec -it 容器ID bashShell​重新进入docker attach 容器ID 两种方式的区别： attach 直接进入容器启动命令的终端，不会启动新的进程 exec 是在容器中打开新的终端，并且可以启动新的进程 所以如果是要在容器中执行很多命令的时候可以进入容器执行，那么就用attach，退出时用Ctrl + Q + P，如果只是一条命令，那么直接使用exec即可！ 从容器内拷贝文件到主机上docker cp 容器ID:容器内路径 目的主机路径 5、常用命令的总结","updated":"2020-03-13T03:06:29.649Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"}]},{"title":"Docker简介与安装","date":"2019-08-02T03:09:47.000Z","path":"2019/08/02/Docker简介与安装/","text":"1、前提知识 熟悉Linux命令和相关背景 建议了解Git、Maven相关知识和概念 2、Docker方向选择 JavaEE研发方向：掌握Docker基本用法和相关概念 Docker研发方向：主要开发语言是go，Swarm、Compose、Machine、K8S等 3、Docker是什么一款产品从开发到上线，从操作系统，到运行环境，再到应用配置。作为开发+运维之间的协作我们需要关心很多东西，这也是很多互联网公司都不得不面对的问题，特别是各种版本的迭代之后，不同版本环境的兼容，对运维人员都是考验 Docker之所以发展如此迅速，也是因为它对此给出了一个标准化的解决方案。 环境配置如此麻烦，换一台机器，就要重来一次，费力费时。很多人想到，能不能从根本上解决问题，软件可以带环境安装？也就是说，安装的时候，把原始环境一模一样地复制过来。开发人员利用 Docker 可以消除协作编码时在我的机器上可正常工作的问题。 这就好比，我安装了Windows操作系统，里面安装了各种开发环境，某天我重装了系统，各种开发工具和环境我都要重新配置一次，即使是这样也不能保证和以前完全一致。所以，现在我改为在虚拟机里面做开发，把我整个操作系统打包成一个虚拟机镜像文件，下次就可以通过镜像把整个虚拟机转移，里面的东西丝毫不会改变，Docker就是把应用程序、配置文件、各种需要的环境打包，这样便可以方便运维人员移植！所以对于Docker来说：镜像就是应用程序 4、Docker的理念Docker是基于Go语言实现的云开源项目。 Docker的主要目标是“Build，Ship and Run Any App,Anywhere”，也就是通过对应用组件的封装、分发、部署、运行等生命周期的管理，使用户的APP（可以是一个WEB应用或数据库应用等等）及其运行环境能够做到“一次封装，到处运行”。 Linux 容器技术的出现就解决了这样一个问题，而 Docker 就是在它的基础上发展过来的。将应用运行在 Docker 容器上面，而 Docker 容器在任何操作系统上都是一致的，这就实现了跨平台、跨服务器。只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作. 一句话解释Docker就是：解决了运行环境和配置问题软件容器，方便做持续集成并有助于整体发布的容器虚拟化技术。 其实我们很容易发现这和Java的初衷有点像，Java的特性之一就是跨平台，但是只是针对Java虚拟机，针对Java程序的一次编译到处运行，所以Docker是针对所有应用程序的！ 5、Docker能干啥虚拟机技术（virtual machine）虚拟机（virtual machine）就是带环境安装的一种解决方案。 它可以在一种操作系统里面运行另一种操作系统，比如在Windows 系统里面运行Linux 系统。应用程序对此毫无感知，因为虚拟机看上去跟真实系统一模一样，而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。这类虚拟机完美的运行了另一套系统，能够使应用程序，操作系统和硬件三者之间的逻辑不变。比如VMware Workstation Pro，但是这样是资源占用多、冗余步骤多、启动慢 Linux Containers（LXC）Linux 容器不是模拟一个完整的操作系统，而是对进程进行隔离。有了容器，就可以将软件运行所需的所有资源打包到一个隔离的容器中。容器与虚拟机不同，不需要捆绑一整套操作系统，只需要软件工作所需的库资源和设置。系统因此而变得高效轻量并保证部署在任何环境中的软件都能始终如一地运行。 所以可以把Docker理解为缩小版、精细版的Linux系统！ 开发/运维更快速的应用交付和部署传统的应用开发完成后，需要提供一堆安装程序和配置说明文档，安装部署后需根据配置文档进行繁杂的配置才能正常运行。Docker化之后只需要交付少量容器镜像文件，在正式生产环境加载镜像并运行即可，应用安装配置在镜像里已经内置好，大大节省部署配置和测试验证时间。 更便捷的升级和扩缩容随着微服务架构和Docker的发展，大量的应用会通过微服务方式架构，应用的开发构建将变成搭乐高积木一样，每个Docker容器将变成一块“积木”，应用的升级将变得非常容易。当现有的容器不足以支撑业务处理时，可通过镜像运行新的容器进行快速扩容，使应用系统的扩容从原先的天级变成分钟级甚至秒级。 更简单的系统运维应用容器化运行后，生产环境运行的应用可与开发、测试环境的应用高度一致，容器会将应用程序相关的环境和状态完全封装起来，不会因为底层基础架构和操作系统的不一致性给应用带来影响，产生新的BUG。当出现程序异常时，也可以通过测试环境的相同容器进行快速定位和修复。 更高效的计算资源利用Docker是内核级虚拟化，其不像传统的虚拟化技术一样需要额外的Hypervisor支持，所以在一台物理机上可以运行很多个容器实例，可大大提升物理服务器的CPU和内存的利用率。 6、Docker的安装Docker支持以下的CentOS版本： CentOS 7 (64-bit) CentOS 6.5 (64-bit) 或更高的版本 目前，CentOS 仅发行版本中的内核支持 Docker。 Docker 运行在 CentOS 7 上，要求系统为64位、系统内核版本为 3.10 以上。 Docker 运行在 CentOS-6.5 或更高的版本的 CentOS 上，要求系统为64位、系统内核版本为 2.6.32-431 或者更高版本。 uname命令用于打印当前系统相关信息（内核版本号、硬件架构、主机名称和操作系统类型等） 7、Docker的基本组成镜像 imageDocker 镜像（Image）就是一个只读的模板。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。 镜像与容器的关系就好比面向对象编程中的对象与类 Docker 面向对象 容器 对象 镜像 类 容器 containerDocker 利用容器（Container）独立运行的一个或一组应用。容器是用镜像创建的运行实例。 它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。 可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。 容器的定义和镜像几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的。 仓库 respository仓库（Repository）是集中存放镜像文件的场所。 仓库(Repository)和仓库注册服务器（Registry）是有区别的。仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。 仓库分为公开仓库（Public）和私有仓库（Private）两种形式。 最大的公开仓库是 Docker Hub(https://hub.docker.com/)， 存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云 、网易云等 需要正确的理解仓储/镜像/容器这几个概念: Docker 本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一个可交付的运行环境，这个打包好的运行环境就似乎 image镜像文件。只有通过这个镜像文件才能生成 Docker 容器。 image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。 image 文件生成的容器实例，本身也是一个文件，称为镜像文件。 一个容器运行一种服务，当我们需要的时候，就可以通过docker客户端创建一个对应的运行实例，也就是我们的容器 至于仓储，就是放了一堆镜像的地方，我们可以把镜像发布到仓储中，需要的时候从仓储中拉下来就可以了。 8、Docker的安装https://docs.docker.com/install/linux/docker-ce/centos/ 参考官方文档，分为企业版和社区版，下面说说我的操作过程（我的系统是CentOS 7_x64） 1、更新yum yum update 2、安装依赖 yum install -y yum-utils device-mapper-persistent-data lvm2 3、设置存储仓库 yum-config-manager –add-repo https://download.docker.com/linux/centos/docker-ce.repo 4、安装DockerCE（CE是社区版） yum install docker-ce docker-ce-cli containerd.io 如果提示接受GPG密钥，请验证指纹是否匹配060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35，如果匹配，则接受该指纹 :Importing GPG key 0x621E9F35:Userid : “Docker Release (CE rpm) &#x64;&#111;&#99;&#x6b;&#101;&#x72;&#64;&#x64;&#x6f;&#x63;&#x6b;&#x65;&#x72;&#46;&#99;&#111;&#109;“Fingerprint: 060a 61c5 1b55 8a7f 742b 77aa c52f eb6b 621e 9f35From : https://download.docker.com/linux/centos/gpg 5、启动Docker systemctl start docker 6、验证安装Docker docker versiondocker run hello-world 7、阿里云镜像加速 国外的仓库很慢的，就和Maven一样，所以需要配置阿里云的仓库(CentOS 7) 打开这个网站：https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors 通过修改daemon配置文件/etc/docker/daemon.json来使用加速器 1234567891011sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF' &#123; \"registry-mirrors\": [\"https://g7g0wjjq.mirror.aliyuncs.com\"] &#125; EOF sudo systemctl daemon-reload sudo systemctl restart docker 9、看看Docker的HelloWorld[root@localhost ~]# docker run hello-worldUnable to find image ‘hello-world:latest’ locallylatest: Pulling from library/hello-world1b930d010525: Pull completeDigest: sha256:6540fc08ee6e6b7b63468dc3317e3303aae178cb8a45ed3123180328bcc1d20fStatus: Downloaded newer image for hello-world:latest​Hello from Docker!This message shows that your installation appears to be working correctly.​To generate this message, Docker took the following steps: The Docker client contacted the Docker daemon. The Docker daemon pulled the “hello-world” image from the Docker Hub.(amd64) The Docker daemon created a new container from that image which runs theexecutable that produces the output you are currently reading. The Docker daemon streamed that output to the Docker client, which sent itto your terminal.​To try something more ambitious, you can run an Ubuntu container with:$ docker run -it ubuntu bash​Share images, automate workflows, and more with a free Docker ID:https://hub.docker.com/​For more examples and ideas, visit:https://docs.docker.com/get-started/ 通过上面的HelloWorld我的可以得到这个Docker的运行流程：","updated":"2020-03-13T03:06:29.650Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"}]},{"title":"彻底解决ADB连接问题","date":"2019-07-31T03:55:47.000Z","path":"2019/07/31/彻底解决ADB连接问题/","text":"Android 调试桥 (adb) 是一种功能多样的命令行工具，可让您与设备进行通信。adb 命令便于执行各种设备操作（例如安装和调试应用），并提供对 Unix shell（可用来在设备上运行各种命令）的访问权限。它是一种客户端-服务器程序，包括以下三个组件： 客户端：用于发送命令。客户端在开发计算机上运行。您可以通过发出 adb 命令从命令行终端调用客户端。 守护进程 (adbd)：在设备上运行命令。守护进程在每个设备上作为后台进程运行。 服务器：管理客户端和守护进程之间的通信。服务器在开发计算机上作为后台进程运行。 但是经常出现的问题就是ADB无法连接，或者其他的ADB驱动问题等等.. 其实在Mac下和Linux下是不存在这样的问题的，但是WIN下总是容易ADB无法连接，这篇文章主要是针对这种问题，明明已经安装了驱动设备，ADB却找不到设备：打开AndroidStudio里面就是ADB找不到设备，首先需要安装Google USB Driver接下来安装驱动程序：在计算机管理—-》设备管理器—-》其他设备 里面找到ADB Interface，因为不只要安装Google USB Driver，而且ADB的驱动也要有，ADB驱动下载地址下载了之后解压，然后点击exe安装安装好了之后大功告成 最后呢，由于ADB驱动下载很慢，我也是翻墙下载的，附加下载连接（这是七牛云的对象存储仓库）：adbdriver.zip","updated":"2020-03-13T03:06:29.739Z","categories":[{"name":"移动开发","slug":"移动开发","permalink":"https://zouchanglin.cn/categories/%E7%A7%BB%E5%8A%A8%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://zouchanglin.cn/tags/Android/"}]},{"title":"Android快速集成OpenCV","date":"2019-07-31T01:27:16.000Z","path":"2019/07/31/Android快速集成OpenCV/","text":"之前集成OpenCV是通过NDK的方式，需要自己编译出so库，这是一种很低效的集成方式，很多时候需要自己写JNI的代码，不是一个很好的选择，而通过open CV官方的jar就可以引入很多种常用的API，下面来说说这种方式 首先需要先下载一个OpenCV的Android开发SDK： 新建一个空项目，然后Android Studio菜单–&gt;File–&gt;New–&gt;Import Module 1、选择刚刚的sdk文件夹下的java文件夹；2、Module name: Android studio会自动填充这项为openCVLibrary340 ，不用管它；3、点击下一步，会出现三个复选框，都选上，点击Finish。 如果在集成过程中出现 12345678ERROR: Failed to install the following Android SDK packages as some licences have not been accepted. build-tools;28.0.3 Android SDK Build-Tools 28.0.3 platforms;android-14 Android SDK Platform 14To build this project, accept the SDK license agreements and install the missing components using the Android Studio SDK Manager.Alternatively, to transfer the license agreements from one workstation to another, see http:&#x2F;&#x2F;d.android.com&#x2F;r&#x2F;studio-ui&#x2F;export-licenses.htmlUsing Android SDK: D:\\develop\\AndroidSDKInstall missing SDK package(s) 直接点击Install missing SDK package(s)下载即可 接着，需要把Module的几个编译参数改成和工程一致即可： 此时需要在app/src/main下新建文件夹jniLibs，把下载的OpenCV-android-sdk的sdk/native/libs下的所有文件拷贝到jniLibs下","updated":"2020-03-13T03:06:29.629Z","categories":[{"name":"移动开发","slug":"移动开发","permalink":"https://zouchanglin.cn/categories/%E7%A7%BB%E5%8A%A8%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://zouchanglin.cn/tags/Android/"}]},{"title":"深入理解HashMap","date":"2019-07-27T16:26:59.000Z","path":"2019/07/28/深入理解HashMap/","text":"一、什么是HashMapHashMap肯定是一个Java开发者经常用到的一个容器，不妨吧HashMap看成是Hash + Map Hash：散列讲一个任意的长度通过某种算法（哈希算法）转换为一个固定值Map：这个Map被翻译过来是地图，地图是一种很明显的K-V模型，地图上的每一个点对应真实存在的某一个地方，所以我认为是一种K-V模型 好了两个概念合二为一即是：把一个东西通过哈希算法找到对应的位置，并把它存储到这个位置！ 二、HashMap源码分析下面的部分是转载自IT小猛男 在JDK1.6，JDK1.7中，HashMap采用位桶+链表实现，即使用链表处理冲突，同一hash值的链表都存储在一个链表里。但是当位于一个桶中的元素较多，即hash值相等的元素较多时，通过key值依次查找的效率较低。 而JDK1.8中，HashMap采用位桶+链表+红黑树实现，当链表长度超过阈值 8 时，将链表转换为红黑树，这样大大减少了查找时间。 HashMap的实现原理首先有一个每个元素都是链表（可能表述不准确）的数组，当添加一个元素（key-value）时，就首先计算元素key的hash值，以此确定插入数组中的位置，但是可能存在同一hash值的元素已经被放在数组同一位置了，这时就添加到同一hash值的元素的后面，他们在数组的同一位置，但是形成了链表，同一各链表上的Hash值是相同的，所以说数组存放的是链表。而当链表长度太长时，链表就转换为红黑树，这样大大提高了查找的效率 HashMap的原理图是： 1、JDK1.8中的涉及到的数据结构①、位桶数组 1transient Node&lt;k,v&gt;[] table;//存储（位桶）的数组&lt;/k,v&gt; ②、数组元素Node&lt;K,V&gt;实现了Entry接口 123456789101112131415161718192021222324252627282930313233343536373839//Node是单向链表，它实现了Map.Entry接口 static class Node&lt;k,v&gt; implements Map.Entry&lt;k,v&gt; &#123; final int hash; final K key; V value; Node&lt;k,v&gt; next; //构造函数Hash值 键 值 下一个节点 Node(int hash, K key, V value, Node&lt;k,v&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + = + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; //判断两个node是否相等,若key和value都相等，返回true。可以与自身比较为true public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;!--?,?--&gt; e = (Map.Entry&lt;!--?,?--&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125; ③、红黑树 12345678910111213141516171819//红黑树 static final class TreeNode&lt;k,v&gt; extends LinkedHashMap.Entry&lt;k,v&gt; &#123; TreeNode&lt;k,v&gt; parent; // 父节点 TreeNode&lt;k,v&gt; left; //左子树 TreeNode&lt;k,v&gt; right;//右子树 TreeNode&lt;k,v&gt; prev;//needed to unlink next upon deletion boolean red; //颜色属性 TreeNode(int hash, K key, V val, Node&lt;k,v&gt; next) &#123; super(hash, key, val, next); &#125; //返回当前节点的根节点 final TreeNode&lt;k,v&gt; root() &#123; for (TreeNode&lt;k,v&gt; r = this, p;;) &#123; if ((p = r.parent) == null) return r; r = p; &#125; &#125; 2、源码中的数据域加载因子（默认0.75）：为什么需要使用加载因子，为什么需要扩容呢？因为如果填充比很大，说明利用的空间很多，如果一直不进行扩容的话，链表就会越来越长，这样查找的效率很低，因为链表的长度很大（当然最新版本使用了红黑树后会改进很多），扩容之后，将原来链表数组的每一个链表分成奇偶两个子链表分别挂在新链表数组的散列位置，这样就减少了每个链表的长度，增加查找效率 HashMap本来是以空间换时间，所以填充比没必要太大。但是填充比太小又会导致空间浪费。如果关注内存，填充比可以稍大，如果主要关注查找性能，填充比可以稍小。 12345678910111213141516public class HashMap&lt;k,v&gt; extends AbstractMap&lt;k,v&gt; implements Map&lt;k,v&gt;, Cloneable, Serializable &#123; private static final long serialVersionUID = 362498820763181265L; static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//最大容量 static final float DEFAULT_LOAD_FACTOR = 0.75f;//填充比 //当add一个元素到某个位桶，其链表长度达到8时将链表转换为红黑树 static final int TREEIFY_THRESHOLD = 8; static final int UNTREEIFY_THRESHOLD = 6; static final int MIN_TREEIFY_CAPACITY = 64; transient Node&lt;k,v&gt;[] table;//存储元素的数组 transient Set&lt;map.entry&lt;k,v&gt;&gt; entrySet; transient int size;//存放元素的个数 transient int modCount;//被修改的次数fast-fail机制 int threshold;//临界值 当实际大小(容量*填充比)超过临界值时，会进行扩容 final float loadFactor;//填充比（......后面略） &#125; 3、HashMap的构造函数HashMap的构造方法有4种，主要涉及到的参数有，指定初始容量，指定填充比和用来初始化的Map 1234567891011121314151617181920212223242526272829303132//构造函数1 public HashMap(int initialCapacity, float loadFactor) &#123; //指定的初始容量非负 if (initialCapacity &lt; 0) throw new IllegalArgumentException(Illegal initial capacity: + initialCapacity); //如果指定的初始容量大于最大容量,置为最大容量 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //填充比为正 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(Illegal load factor: + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);//新的扩容临界值 &#125; //构造函数2 public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; //构造函数3 public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted &#125; //构造函数4用m的元素初始化散列映射 public HashMap(Map&lt;!--? extends K, ? extends V--&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); &#125; 4、HashMap的存取机制①、HashMap如何getValue值，看源码 123456789101112131415161718192021222324252627282930313233public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab;//Entry对象数组 Node&lt;K,V&gt; first,e; //在tab数组中经过散列的第一个位置 int n; K k; //找到插入的第一个Node，方法是hash值和n-1相与，tab[(n - 1) &amp; hash] //也就是说在一条链上的hash值相同的 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp;(first = tab[(n - 1) &amp; hash]) != null) &#123; //检查第一个Node是不是要找的Node if (first.hash == hash &amp;&amp; ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) //判断条件是hash值要相同，key值要相同 return first; //检查first后面的node if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //遍历后面的链表，找到key值和hash值都相同的Node do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; get(key)方法时获取key的hash值，计算hash&amp;(n-1)得到在链表数组中的位置first=tab[hash&amp;(n-1)],先判断first的key是否与参数key相等，不等就遍历后面的链表找到相同的key值返回对应的Value值即可 ②、HashMap如何put(key，value);看源码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; //很显然 tab是节点数组 p是节点临时变量 n是数组长度 HashMap.Node&lt;K,V&gt;[] tab; HashMap.Node&lt;K,V&gt; p; int n, i; //初始化表 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //(n - 1) &amp; hash 确保最大值为15，从而不会越界， //同样数组大小设定为2的n次幂就是为了这个n-1 //2的n次幂-1，二进制都是1，这样进行与操作的时候可以保证最大散列度， //否则二进制中会含有0会降低散列程度 if ((p = tab[i = (n - 1) &amp; hash]) == null) //如果table的在（n-1）&amp;hash的值是空，就新建一个节点插入在该位置 tab[i] = newNode(hash, key, value, null); //表示有冲突,开始处理冲突 else &#123; Node&lt;K,V&gt; e; K k; //检查第一个Node，p是不是要找的值 if (p.hash == hash &amp;&amp;((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; //指针为空就挂在后面 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); //如果冲突的节点数已经达到8个，看是否需要改变冲突节点的存储结构， //treeifyBin首先判断当前hashMap的长度，如果不足64，只进行 //resize，扩容table，如果达到64，那么将冲突的存储结构为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //如果有相同的key值就结束遍历 if (e.hash == hash &amp;&amp;((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //就是链表上有相同的key值 if (e != null) &#123; // existing mapping for key，就是key的Value存在 V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue;//返回存在的Value值 &#125; &#125; ++modCount; //如果当前大小大于门限，门限原本是初始容量*0.75 if (++size &gt; threshold) resize();//扩容两倍 afterNodeInsertion(evict); return null;&#125; 下面简单说下添加键值对put(key,value)的过程：1、判断键值对数组tab[]是否为空或为null，否则以默认大小resize()；2、根据键值key计算hash值得到插入的数组索引i，如果tab[i]==null，直接新建节点添加，否则转入33、判断当前数组中处理hash冲突的方式为链表还是红黑树(check第一个节点类型即可),分别处理 5、HasMap的扩容机制resize()构造hash表时，如果不指明初始大小，默认大小为16（即Node数组大小16），如果Node[]数组中的元素达到（填充比*Node.length）重新调整HashMap大小 变为原来2倍大小,扩容很耗时。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //如果旧表的长度不是空 if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; //把新表的长度设置为旧表长度的两倍，newCap=2*oldCap else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) //把新表的门限设置为旧表门限的两倍，newThr=oldThr*2 newThr = oldThr &lt;&lt; 1; // double threshold &#125; //如果旧表的长度的是0，就是说第一次初始化表 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; //新表长度乘以加载因子 float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; //下面开始构造新表，初始化表中的数据 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; //把新表赋值给table table = newTab; //原表不是空要把原表中数据移动到新表中 if (oldTab != null) &#123; //遍历原来的旧表 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) //说明这个node没有链表直接放在新表的e.hash &amp; (newCap - 1)位置 newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //如果e后边有链表,到这里表示e后面带着个单链表，需要遍历单链表，将每个结点重 else &#123; // preserve order保证顺序 //新计算在新表的位置，并进行搬运 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next;//记录下一个结点 //新表是旧表的两倍容量，实例上就把单链表拆分为两队， //e.hash&amp;oldCap为偶数一队，e.hash&amp;oldCap为奇数一对 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); //lo队不为null，放在新表原位置 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; //hi队不为null，放在新表j+oldCap位置 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 6、JDK1.8使用红黑树的改进在Java jdk8中对HashMap的源码进行了优化，在jdk7中，HashMap处理“碰撞”的时候，都是采用链表来存储，当碰撞的结点很多时，查询时间是O（n）。在jdk8中，HashMap处理“碰撞”增加了红黑树这种数据结构，当碰撞结点较少时，采用链表存储，当较大时（&gt;8个），采用红黑树（特点是查询时间是O（logn））存储（有一个阀值控制，大于阀值(8个)，将链表存储转换成红黑树存储） 7、问题分析你可能还知道哈希碰撞会对hashMap的性能带来灾难性的影响。如果多个hashCode()的值落到同一个桶内的时候，这些值是存储到一个链表中的。最坏的情况下，所有的key都映射到同一个桶中，这样hashmap就退化成了一个链表——查找时间从O(1)到O(n)。 随着HashMap的大小的增长，get()方法的开销也越来越大。由于所有的记录都在同一个桶里的超长链表内，平均查询一条记录就需要遍历一半的列表。 JDK1.8HashMap的红黑树是这样解决的： 如果某个桶中的记录过大的话（当前是TREEIFY_THRESHOLD = 8），HashMap会动态的使用一个专门的treemap实现来替换掉它。这样做的结果会更好，是O(logn)，而不是糟糕的O(n)。 它是如何工作的？前面产生冲突的那些KEY对应的记录只是简单的追加到一个链表后面，这些记录只能通过遍历来进行查找。但是超过这个阈值后HashMap开始将列表升级成一个二叉树，使用哈希值作为树的分支变量，如果两个哈希值不等，但指向同一个桶的话，较大的那个会插入到右子树里。如果哈希值相等，HashMap希望key值最好是实现了Comparable接口的，这样它可以按照顺序来进行插入。这对HashMap的key来说并不是必须的，不过如果实现了当然最好。如果没有实现这个接口，在出现严重的哈希碰撞的时候，你就并别指望能获得性能提升了。 哈希函数高16位与低16位做异或运算 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;","updated":"2020-03-13T03:06:29.752Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"}]},{"title":"KMP其实也没那么难","date":"2019-07-26T16:07:53.000Z","path":"2019/07/27/KMP其实也没那么难/","text":"Knuth-Morris-Pratt 字符串查找算法，简称为 KMP算法，KMP是我们经常听到的一种字符串匹配算法。KMP算法听起来很难，但是如果真正明白它的匹配过程其实不难，接下来看看KMP究竟是如何匹配字符串的？ 假设现在有如图所示两个字符串， 图表所列的是匹配串的所有子串，这个不难理解 两个概念：前缀和后缀前缀 指除了最后一个字符以外，一个字符串的全部头部组合；后缀 指除了第一个字符以外，一个字符串的全部尾部组合。 比如对于第三行和第五行 求出每一个子串中前缀和后缀相等的部分的最大长度，比如第5行求出来最大长度为2求得原匹配串的所有子串对应的各个前缀后缀的公共元素的最大长度表： 接下来需要一个next数组相当于最大长度值数组整体向右移动一位，然后把0号位置赋为-1，多出来的一位已经用不着了，直接划掉。 接下来开始匹配 如果字母匹配直接都朝着右边走，如果不匹配就需要把子串索引为0的元素移动到不匹配的位置，其他的元素移动同样的距离，如下图所示： 现在不匹配的位置是next=-1，所以需要把匹配串中索引为-1的元素移动至不匹配的位置，其他元素移动同样的长度现在匹配到5号位置如图所示发生了对应字母不一致，此时next数组对应值为2，所以匹配串中的索引为2的应该移动至发生对比不一致的地方： 这样便可以一直对比下去，由此可见KMP的高效匹配原来是这样完成的！ 所以对KMP算法总结如下几点：1、先求出匹配串所有子串2、求每一个子串中前缀和后缀相等的部分的最大长度3、这样便求出了next数组，以-1开头的next数组是更容易理解的方式4、有了next数组剩下的就是匹配了，匹配时注意遇到不一样的字母时先看next数组对应值，此值就是匹配串索引处应该向不匹配位置挪动的字母索引，简单说就是：匹配串中的那个字母应该挪动到不匹配处由next数组决定","updated":"2020-03-13T03:06:29.670Z","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"字符串","slug":"字符串","permalink":"https://zouchanglin.cn/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"}]},{"title":"关于本站","date":"2019-07-24T16:33:39.000Z","path":"2019/07/25/关于本站/","text":"作为一个写博客的程序员，会经历这几个阶段：1、在博客服务提供商上写，主要是因为免费、零运维并且能够通过平台引流。2、通过开源程序自建自运维，主要是因为自由、能自定制二开。3、自己开发博客程序，主要是因为..技痒吧… 大多数人止步于 1，甚至是 0.5（写几个月就放弃的）；一部分人止步于 2，他们的重点是写作，工具选择顺手的就行，他们很容易在 1 和 2 之间来回切换。骨骼精奇的人会走到第三阶段，他们一般都患有选择困难症，自命不凡，如果运气好的话他们可以改变世界。 说实话，自己亲身体会过这几个阶段。开始我的写作平台是CSDN，后台由于CSDN的种种行为….(大家懂的)，我感hexo那些对应的主题界面整洁，代码样式也好看，而且运维费用几乎没有（除非你要用CDN加速之类的），所以这种写作方式我还是一直在用！ 我在看来hexo+next的缺点：1、插件很多，如果喜欢折腾系统十分不建议使用此组合，因为配置一次很麻烦的，所以即使折腾也要做好备份。2、使用GitHubPage有时会很慢，甚至加载不出来，所以不得不使用CDN加速。3、当然在国内可以使用七牛云等作为载体，但是同样也是需要七牛云上传的插件等等，配置之繁琐实在是做好一次巴不得把系统打成快照，比如：在虚拟机里面写就好了。 但是这些缺点都是可以避免的： Dokcer镜像封装可以解决一切问题！ 所以目前的部署方式是hexo+docker+华为云服务器的方式，真香… 我自己写过一个博客系统，使用技术框架是Thymeleaf + SpringBoot，虽然已经实现了大部分功能，部分功能也改改BUG，但是由于我的审美的原因，用BootStrap也无济于事….好吧，而且markdown编辑器采用的是Editor.md，字体和代码高亮有点丑（应该是特别丑），其实我感觉前端还真的是要Vue.js的一写开源库完成，写成那种完全前后端分离的形式，但是我还得去学一下vue.js，哈哈等有时间在学吧，现在其他的事情也多的很，写好高并发、高可用的后端代码应该是更重要的事情！","updated":"2020-03-13T03:06:29.721Z","categories":[{"name":"设计工具","slug":"设计工具","permalink":"https://zouchanglin.cn/categories/%E8%AE%BE%E8%AE%A1%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"博客","slug":"博客","permalink":"https://zouchanglin.cn/tags/%E5%8D%9A%E5%AE%A2/"}]},{"title":"Ubuntu安装旧版本Docker","date":"2019-07-24T05:10:26.000Z","path":"2019/07/24/Ubuntu安装旧版本Docker/","text":"目前此操作方法不支持Ubuntu18.04 第一步：添加源 12345678910111213# 下面这个Docker的源deb https://apt.dockerproject.org/repo ubuntu-xenial maindeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse 版本对应关系如下： 所以在写的时候这样写就好了 Precise 12.04 (LTS) deb https://apt.dockerproject.org/repo ubuntu-precise main Trusty 14.04 (LTS) deb https://apt.dockerproject.org/repo ubuntu-trusty main Wily 15.10 deb https://apt.dockerproject.org/repo ubuntu-wily main Xenial 16.04 (LTS) deb https://apt.dockerproject.org/repo ubuntu-xenial main 第二步：添加秘钥 1apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D 第三步：apt更新 1apt-get update 第四步：查看可选的Docker版本 1apt-cache policy docker-engine 第五步：安装想要的版本的Docker 1apt install docker-engine=1.11.2-0~xenial 在这个网站查看支持的版本https://apt.dockerproject.org/repo/dists/","updated":"2020-03-13T03:06:29.710Z","categories":[{"name":"虚拟化技术","slug":"虚拟化技术","permalink":"https://zouchanglin.cn/categories/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://zouchanglin.cn/tags/Docker/"},{"name":"文件下载","slug":"文件下载","permalink":"https://zouchanglin.cn/tags/%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD/"}]},{"title":"Mac下强有力的生产工具","date":"2019-06-16T10:09:30.000Z","path":"2019/06/16/Mac下强有力的生产工具/","text":"下面记载一下我常用的工具，希望对大家有所帮助！ Typorahttps://typora.io/ markdown编辑器，3大平台都支持，界面美观，是我这种markdown重度使用症的必备软件！ itext截图即可文字识别+翻译，十分强大，现在收费了 不过已经有更好的替代品了，需要的话联系我 Math Snipping Toolhttps://mathpix.com/ 还在敲公式、练语法！？！没必要，直接图像识别即可 ShadowsocksX-NGhttps://github.com/shadowsocks/ShadowsocksX-NG 科学上网，大家都懂，同时也有Android版本的 Termiushttps://termius.com/ 和win下的XShell差不多，很好用的Shell工具，而且这是三大平台都有的 FileZillahttps://filezilla-project.org/ 和win下的Xftp差不多，向Linux传文件挺好用的，scp命令也行 SourceTreehttps://www.sourcetreeapp.com/ Git图形化管理，简单的用命令行，复杂的操作我还是就用图形界面吧 PostManhttps://www.getpostman.com/ 接口测试的利器，用了很久了 IINAhttps://iina.io/ 一个视频播放器（支持多种视频格式），VLC淘汰了，暴风音影之类的也可以淘汰了 Paint-X和win和画图板基本一样，甚至更好用 Navicat for MySQLhttps://www.navicat.com/en/products/navicat-for-mysql MySQL可视化操作工具，虽然IDEA里面也自带了插件，但是我个人还是更喜欢用Navicat for MySQL Dashhttps://kapeli.com/dash 可能你平时要查Java的文档，又要查查HTML的文档，还有jQuery….太麻烦，而且很多是CHM，Dash集成了常用的开发说明文档，想要哪个就下载哪个即可！ Alfredhttps://www.alfredapp.com/ 在windows下与之相似的有Everything吧 Sniphttps://snip.qq.com/ 截图的，不过平时QQ登录了之后QQ截图也还能用 The Unarchiverhttps://theunarchiver.com/ 解压工具，比如平时没法解压rar文件，就可以使用这款工具 VisualStudioCodehttps://code.visualstudio.com/ 这个不用多介绍，我平时把它当做记事本来用，显然是杀鸡用牛刀了，丰富的插件堪比IDE 坚果云https://www.jianguoyun.com/ 备份文件超级好用，自动备份，跨各大平台，一个字：骚 Foxmailhttps://www.foxmail.com/mac 不过谷歌浏览器的Gmail也还行，挺好用的 TeamViewerhttps://www.teamviewer.com/en-us/ 远程控制工具，平时帮小伙伴处理电脑问题，用起来蛮好用 Jetbrains家族https://www.jetbrains.com/ 它们家族就不用多说了吧，如果说宇宙非要有个最强IDE的话，非Jetbrains和VisualStudio莫属 VMware Fusionhttps://www.vmware.com/products/fusion.html 功能完善的虚拟机，这个也是常用的！","updated":"2020-03-13T03:06:29.685Z","categories":[{"name":"开发工具","slug":"开发工具","permalink":"https://zouchanglin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"https://zouchanglin.cn/tags/Mac/"}]},{"title":"Jenkins自动部署","date":"2019-06-03T10:09:30.000Z","path":"2019/06/03/Jenkins自动部署/","text":"事先确保服务器上有Java环境与Maven环境和 Git 安装Jenkins这里是Jenkins的文档 https://jenkins.io/zh/doc/ 这里是Jenkins的下载地址https://jenkins.io/zh/download/ 最好的方式应该是自己去下载Jenkins的Jar包，直接像运行jar包即可 1nohup java -jar jenkins.war --httpPort=80 这样直接访问172.16.45.112:8080 即可！ 成功开启服务后应该是下面这个样子： 登录密码： 1cat /var/lib/jenkins/secrets/initialAdminPassword 我直接选择推荐插件了： Jenkins自动构建原文参考 《Jenkins自动构建》 当我们提交代码到GitHub后，可以在Jenkins上执行构建，但是每次都要动手去执行略显麻烦，今天我们就来实战Jenkins的自动构建功能，每次提交代码到GitHub后，Jenkins会进行自动构建！ 注意点GitHub收到提交的代码后要主动通知Jenkins，所以Jenkins所在服务器一定要有外网IP，否则GitHub无法访问，我的Jenkins服务器是部署在腾讯云的云主机上，带有外网IP； 整个流程 GitHub上准备一个spring boot的web工程； GitHub上配置Jenkins的webhook地址； 在GitHub上创建一个access token，Jenkins做一些需要权限的操作的时候就用这个access token去鉴权； Jenkins安装GitHub Plugin插件； Jenkins配置GitHub访问权限； Jenkins上创建一个构建项目，对应的源码是步骤1中的web工程； 修改web工程的源码，并提交到GitHub上； 检查Jenkins的构建项目是否被触发自动构建，构建成功后，下载工程运行，看是不是基于最新的代码构建的","updated":"2020-03-13T03:06:29.667Z","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://zouchanglin.cn/categories/DevOps/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://zouchanglin.cn/tags/Jenkins/"}]},{"title":"网站接入支付宝","date":"2019-05-31T10:09:30.000Z","path":"2019/05/31/网站接入支付宝/","text":"沙箱环境沙箱环境也就是特意为开发者准备的环境，从而避免去注册公司、审核之类的问题！这个是沙箱环境的链接 先要下载这两个工具 环境一定要配置正确，尤其是秘钥之类的，至于回调地址先不考虑，如果要测试的话首先要保证你本机的服务可以映射到公网，内网穿透就是一种解决方案，而且域名还便宜! 开始支付包第三方Jar引入： 12345&lt;dependency&gt; &lt;groupId&gt;com.alipay.sdk&lt;/groupId&gt; &lt;artifactId&gt;alipay-sdk-java&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt;&lt;/dependency&gt; 在resources目录下新建一个支付宝的配置文件：alipay.properties 12345678910111213141516# 应用ID,您的APPID，收款账号既是您的APPID对应支付宝账号app_id = 2016092900623298# 商户私钥，您的PKCS8格式RSA2私钥merchant_private_key = 你生成的商户私钥# 支付宝公钥 对应APPID下的支付宝公钥。alipay_public_key = 你的支付宝公钥# 服务器异步通知页面路径 需http://格式的完整路径，不能加?id=123这类自定义参数notify_url = http://106.12.202.93:8080/pay/payFinish# 页面跳转同步通知页面路径 需http://格式的完整路径，不能加?id=123这类自定义参数return_url = http://106.12.202.93:8080/pay/paySyncFinish# 签名方式sign_type = RSA2# 字符编码格式charset = utf-8# 支付宝网关gatewayUrl = https://openapi.alipaydev.com/gateway.do Controller层： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package edu.xpu.game.controller.shopping;@Slf4j@Controller@RequestMapping(\"/pay\")public class PayController &#123; private final OrderServiceImpl orderService; private final PayServiceImpl payService; @Autowired public PayController(OrderServiceImpl orderService, PayServiceImpl payService) &#123; this.orderService = orderService; this.payService = payService; &#125; @ResponseBody @RequestMapping(\"/affirmPay\") public String payOrder(@RequestParam(\"masterOrderId\") String masterOrderId)&#123; Optional&lt;OrderMaster&gt; byId = orderService.findOrderById(masterOrderId); AlipayBean alipayBean = new AlipayBean(); alipayBean.setOut_trade_no(masterOrderId); //设置订单Id alipayBean.setSubject(\"农家商城购物\"); if(byId.isPresent())&#123; alipayBean.setTotal_amount(new StringBuffer() .append(byId.get().getOrderAmount())); try &#123; String aliPay = payService.aliPay(alipayBean); log.info(\"[pay.affirmPay] aliPay = &#123;&#125;\", aliPay); return aliPay; &#125; catch (AlipayApiException e) &#123; return JsonUtil.toJson(ResultVOUtil.error(2, \"支付过程出错\")); &#125; &#125;else&#123; return JsonUtil.toJson(ResultVOUtil.error(1, \"不存在此订单\")); &#125; &#125; /** * 支付成功的回调 * @param trade_status 交易状态 * @param out_trade_no 商户订单号 */ @RequestMapping(\"/payFinish\") public void payFinishCallback(String trade_status, String out_trade_no)&#123; log.info(\"支付回调：trade_status = \"+trade_status); if(\"TRADE_SUCCESS\".equals(trade_status))&#123; //支付成功 Optional&lt;OrderMaster&gt; orderById = orderService.findOrderById(out_trade_no); if(orderById.isPresent())&#123; OrderMaster orderMaster = orderById.get(); //修改支付状态 orderMaster.setPayStatus(PayStatusEnum.SUCCESS.getCode()); OrderMaster changeRet = orderService.change(orderMaster); log.info(\"changeRet\", changeRet); &#125; &#125;else&#123; //支付失败 Optional&lt;OrderMaster&gt; orderById = orderService.findOrderById(out_trade_no); if(orderById.isPresent())&#123; OrderMaster orderMaster = orderById.get(); //修改支付状态 orderMaster.setPayStatus(PayStatusEnum.WAIT.getCode()); orderService.change(orderMaster); &#125; &#125; &#125; @RequestMapping(\"/paySyncFinish\") public String paySyncFinishCallback()&#123; return \"redirect:http://ahojcn.natapp1.cc/#/pay\"; &#125;&#125; 支付宝支付信息实体类对象 12345678910111213141516@Data@Accessors(chain = true)public class AlipayBean &#123; /*商户订单号，必填*/ private String out_trade_no; /*订单名称，必填*/ private String subject; /*付款金额，必填*/ private StringBuffer total_amount; /*商品描述，可空*/ private String body; /*超时时间参数*/ private String timeout_express=\"10m\"; private String product_code=\"FAST_INSTANT_TRADE_PAY\";&#125; Service层 1234567@Servicepublic class PayServiceImpl implements PayService &#123; @Override public String aliPay(AlipayBean alipayBean) throws AlipayApiException &#123; return AlipayUtil.connect(alipayBean); &#125;&#125; 支付工具类 123456789101112131415161718public class AlipayUtil &#123; public static String connect(AlipayBean alipayBean) throws AlipayApiException &#123; AlipayClient alipayClient = new DefaultAlipayClient( PropertiesConfig.getKey(\"gatewayUrl\"),//支付宝网关 PropertiesConfig.getKey(\"app_id\"),//appid PropertiesConfig.getKey(\"merchant_private_key\"),//商户私钥 \"json\", PropertiesConfig.getKey(\"charset\"),//字符编码格式 PropertiesConfig.getKey(\"alipay_public_key\"),//支付宝公钥 PropertiesConfig.getKey(\"sign_type\")//签名方式 ); AlipayTradePagePayRequest alipayRequest = new AlipayTradePagePayRequest(); alipayRequest.setReturnUrl(PropertiesConfig.getKey(\"return_url\")); alipayRequest.setNotifyUrl(PropertiesConfig.getKey(\"notify_url\")); alipayRequest.setBizContent(JSON.toJSONString(alipayBean)); return alipayClient.pageExecute(alipayRequest).getBody(); &#125;&#125; 可以使用Android版本的沙箱支付宝支付，也可以登录账户支付，沙箱环境不是很稳定最好多测试几次，支付成功和支付失败的情景！","updated":"2020-03-13T03:06:29.761Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[]},{"title":"解决ssh_exchange_identificationread connection reset by peer","date":"2019-05-19T10:09:30.000Z","path":"2019/05/19/解决ssh_exchange_identificationread connection reset by peer/","text":"问题最近好久没连接过自己的云服务器了，但是的时候使用ssh访问服务器时出现：ssh_exchange_identification: read: Connection reset by peer这样的连接错误 原因 服务器防火墙限定 是否达到ssh的最大连接数，超过之后会服务器端会拒绝新的连接，直到有新的连接释放出来 /etc/hosts.allow和/etc/hosts.deny配置文件限定ip登录 解决先关闭防火墙，尤其是云服务厂商为你设定的防火墙，具体解决方式要参考云服务器厂商。看看网络状态，看看Linux是否运行着shhd服务，如果没有那么有可能是连ssh服务程序都没有安装，应该先安装ssh服务器才可以。接着如果还是不行的话： 1vim /etc/hosts.allow 追加上： 1sshd: ALL 接着重启shh服务 1service sshd restart 如果这还不行留言讨论，OK反正我的是可以正常连接了!","updated":"2020-03-13T03:06:29.764Z","categories":[{"name":"开发工具","slug":"开发工具","permalink":"https://zouchanglin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"SSH","slug":"SSH","permalink":"https://zouchanglin.cn/tags/SSH/"}]},{"title":"Nginx应用和选择","date":"2019-05-18T10:09:30.000Z","path":"2019/05/18/Nginx应用和选择/","text":"Nginx的应用场景Nginx常见的应用场景如下图所示，下面这张图可以说是很详细了： 静态资源服务曾经的Web开发并不是完全的前后端分离，最明显的例子就是JSP，基本是前后端不分离，直到后来Spring生态体系和前端框架的发展，Web应用程序成了单纯的提供应用接口服务的程序，因此静态资源就没必要交给应用服务器去处理，而是交给Nginx 反响代理服务Nginx的缓存加速和负载均衡，极大提高了整体系统的并发能力，Nginx的并发相对于应用服务器要强大太多 API服务Nginx里面提供了数据库服务、缓存、应用服务相关API，模块化设计。比如数据库服务器的并发性能本来就远高于应用服务器，如果直接让应用服务器去操作数据库的话无疑是降低了系统的性能，于是Nginx提供了数据库访问的API，提升系统整体性能 现在越来越多的网络设备接入，尤其是在我们国家这样的人口大国，每年双十一这样的并发量不是单纯的应用服务器能够撑得住的。虽然计算机硬件在不断升级，但是这种问题就目前来看从硬件的角度发生的改变是微乎其微的，早期的Apache服务器维护一个连接就需要一个进程来维护，这样做的代价就是从硬件的角度就已经限制了服务器的并发。即使从8核处理器换到16核心处理器并发性能也不会得到成倍的提升，因为操作系统和软件没做好服务和多核架构的准备！ 下面这张图是最近几年服务器市场份额变化图： 虽然现在排名第二，但是在一些追求长期稳定的服务中通常不会更换服务器，就比如学校的教务网站，ASP.NET实现的，虽然每次网上选课系统都会崩盘，但是学校为了稳定性不会轻易更换服务架构的。所以表面看起来Apache的使用高于Nginx的，但是往往新增的服务器都会使用Nginx Nginx的优点 高并发需要条件就是每个链接需要更小的内存，吃更少的CPU资源。Nginx做到了，从上图中可以看到，在并发量很低的时候Nginx的性能其实还不如其他的服务器，但是Nginx在高并发量的时候，性能仍然很稳定，而其他的服务器的性能会严重下降，所以Nginx的高并发性能成为了人们选择Nginx的理由 Nginx采取了分阶段资源分配技术，使得它的CPU与内存占用率非常低。Nginx官方表示保持10000个没有活动的连接，它只占2.5M内存 Nginx与其他服务器的对比 Nginx的组成 Nginx版本发布 从这个图上呢也可以看出来Nginx的版本迭代情况，绿色的线表示Nginx的Bug修复情况，Bug越来越少也同样是说明了Nginx越来越稳定了，2009年Nginx开始支持Windows操作系统，2011年才成立Nginx的商业公司…… Nginx版本选择 免费开源版本 nginx.org 商业版 nginx.com 阿里巴巴的Tengine 免费版OpenResty 商业版OpenResty","updated":"2020-03-13T03:06:29.697Z","categories":[{"name":"高性能服务器","slug":"高性能服务器","permalink":"https://zouchanglin.cn/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://zouchanglin.cn/tags/Nginx/"}]},{"title":"AsyncHttpClient如何进行Session的保存","date":"2019-05-11T10:09:30.000Z","path":"2019/05/11/AsyncHttpClient如何进行Session的保存/","text":"在之前我从来没有通过AsyncHttp这个请求框架去保存Session，但是今天不得不用呀！接口设计有限，我也只好用专门给浏览器用的接口了，这次我把AsyncHttpClient封装成了一个单例，通过这样的方式来保存Session，暂时先这么用吧，等到找到其他解决方案了再更新此文！ 一、封装先封装一个专门处理Cookie的工具类： 123456789101112131415161718192021222324252627282930313233public class CookieUtils &#123; private static List&lt;Cookie&gt; cookies; /* 返回cookies列表 */ public static List&lt;Cookie&gt; getCookies() &#123; return cookies != null ? cookies : new ArrayList&lt;&gt;(); &#125; /* 设置cookies列表 */ public static void setCookies(List&lt;Cookie&gt; cookies) &#123; CookieUtils.cookies = cookies; &#125; /* 存储cookie */ public static void saveCookie(AsyncHttpClient client, Context context) &#123; PersistentCookieStore cookieStore = new PersistentCookieStore(context); client.setCookieStore(cookieStore); &#125; /* 得到cookie */ public static List&lt;Cookie&gt; getCookie(Context context) &#123; PersistentCookieStore cookieStore = new PersistentCookieStore(context); List&lt;Cookie&gt; cookies = cookieStore.getCookies(); return cookies; &#125; /* 清除cookie */ public static void clearCookie(Context context) &#123; PersistentCookieStore cookieStore = new PersistentCookieStore(context); cookieStore.clear(); &#125;&#125; 再封装一下AsyncHttpClient，这样所有的AsyncHttpClient用的都是同一个对象了！ 1234567891011121314151617181920public class FinalAsyncHttpClient &#123; private AsyncHttpClient client; /* 构造方法 */ public FinalAsyncHttpClient() &#123; client = new AsyncHttpClient();//实例化client client.setTimeout(5);//设置5秒超时 // 获取cookie列表 if (CookieUtils.getCookies() != null) &#123; BasicCookieStore bcs = new BasicCookieStore(); bcs.addCookies(CookieUtils.getCookies().toArray( new Cookie[CookieUtils.getCookies().size()]));//得到cookie列表 client.setCookieStore(bcs);//给client加载cookie &#125; &#125; /* 得到client对象方法 */ public AsyncHttpClient getAsyncHttpClient() &#123; return this.client; &#125;&#125; 二、使用获取对象并且设置保存Cookie 12AsyncHttpClient asyncHttpClient = new FinalAsyncHttpClient().getAsyncHttpClient();CookieUtils.saveCookie(asyncHttpClient,LoginAty.this); 登录成功 12345678910asyncHttpClient.post(AppConfig.loginAddress, params, new AsyncHttpResponseHandler() &#123; @Override public void onSuccess(int statusCode, Header[] headers, byte[] responseBody) &#123; //.... XToast.success(getContext(), \"登录成功\").show(); CookieUtils.setCookies(CookieUtils.getCookie(LoginAty.this)); //.... &#125; 三、拓展但是这样也有问题，每次开启App的时候cookie就消失了，我们把cookie通过序列化的方式存起来就好了，如果发现cookie过期，现在有更好的解决方式，直接在App开启的时候就更新一次cookie就好了！","updated":"2020-03-13T03:06:29.631Z","categories":[{"name":"移动开发","slug":"移动开发","permalink":"https://zouchanglin.cn/categories/%E7%A7%BB%E5%8A%A8%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://zouchanglin.cn/tags/Android/"}]},{"title":"网易云的MV下载","date":"2019-05-03T10:09:30.000Z","path":"2019/05/03/网易云的MV下载/","text":"最近想把自己喜欢的MV放在个人站点上面，但是Mac客户端和网页版的网易云连个下载按钮都没有！好吧， 是时候自己动手…丰衣足食了，网页上能播放出来的肯定是能拿到的，最神奇最稳定的办法当然是开启录屏软件啦。但是本次为了播放效果，能不用最低端的方式就尽量不用啦！ 很久没写过IO方面的东西了，现在拿这个例子来练练手： 我们要拿到的无非就是HTTP的请求链接和请求头信息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public static void main(String[] args) throws Exception&#123; //请输入视频地址 String url = \"https://vodkgeyttp8.vod.126.net/cloudmusic/MCQ4IjAxICAwICEhICAgIQ==/294001/672888e4902fac1298f8e984182690c7.mp4?wsSecret=809e59d2b289d82917dedef9c05c64b2&amp;wsTime=1556853453\"; File file = new File(\"/Users/tim/Desktop/a\"+\".mp4\"); //创建文件 if(!file.createNewFile()) throw new RuntimeException(\"文件创建失败\"); HttpURLConnection con; FileOutputStream fs = null; InputStream is; BufferedInputStream bs = null; try &#123; con = (HttpURLConnection) new URL(url).openConnection(); con.setRequestProperty(\"User-Agent\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36\"); //输入流 is = con.getInputStream(); bs = new BufferedInputStream(is); fs = new FileOutputStream(file); byte[] bytes = new byte[1024 * 10]; int line ; while((line = bs.read(bytes))!= -1)&#123; fs.write(bytes, 0, line); fs.flush(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally&#123; if(fs!= null)&#123; try &#123; fs.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if(bs!=null)&#123; try &#123; bs.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; 好了，首页的MV就是这样完成了下载，所以想要下载MV就是这么简单！","updated":"2020-03-13T03:06:29.760Z","categories":[{"name":"设计工具","slug":"设计工具","permalink":"https://zouchanglin.cn/categories/%E8%AE%BE%E8%AE%A1%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"上传下载","slug":"上传下载","permalink":"https://zouchanglin.cn/tags/%E4%B8%8A%E4%BC%A0%E4%B8%8B%E8%BD%BD/"}]},{"title":"Spring入门笔记(三)","date":"2019-02-05T11:00:00.000Z","path":"2019/02/05/Spring入门笔记(三)/","text":"SpringAOPAOP前奏SpringAOP是面向切面编程，那么我们先来看看那什么是面向切面编程，首先看一个例子，假设我们有一个计算类的接口AtithmeticCalculator，有一个实现类AtithmeticCalculatorImpl： 需求1-日志：在程序执行期间追踪正在发生的活动 需求2-验证：希望计算器只能处理正数的运算 出现这样的需求该怎么办呢？我们先实现第一个需求（加入计算的日志），代码如下： 1234567891011121314151617181920212223242526272829303132333435package com.xpu.hello;public class AtithmeticCalculatorImpl_Logging implements AtithmeticCalculator &#123; @Override public int add(int i, int j) &#123; System.out.println(\"The method add begin with[\"+i+\",\"+j+\"]\"); int ret = i+j; System.out.println(\"The method add end with[\"+i+\",\"+j+\"]\"); return ret; &#125; @Override public int sub(int i, int j) &#123; System.out.println(\"The method sub begin with[\"+i+\",\"+j+\"]\"); int ret = i-j; System.out.println(\"The method sub end with[\"+i+\",\"+j+\"]\"); return ret; &#125; @Override public int mul(int i, int j) &#123; System.out.println(\"The method mul begin with[\"+i+\",\"+j+\"]\"); int ret = i*j; System.out.println(\"The method mul end with[\"+i+\",\"+j+\"]\"); return ret; &#125; @Override public int div(int i, int j) &#123; System.out.println(\"The method div begin with[\"+i+\",\"+j+\"]\"); int ret = i/j; System.out.println(\"The method div end with[\"+i+\",\"+j+\"]\"); return ret; &#125;&#125; 代码混乱：越来越多的非业务需求(日志和验证等)加入后，原有的业务方法急剧膨胀， 每个方法在处理核心逻辑的同时还必须兼顾其他多个关注点.。 代码分散: 以日志需求为例，只是为了满足这个单一需求, 就不得不在多个模块（方法）里多次重复相同的日志代码。 如果日志需求发生变化， 必须修改所有模块。 那么怎么样才能把打印日志的方法从其中抽离出来呢？当然是可以的，我们可以采用如下的方式实现： 动态代理对于上面的问题，我们完全可以使用动态代理来完成： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.xpu.hello;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;/** * 动态代理此类 */class AtithmeticCalculatorLoggingproxy &#123; //动态代理对象 private AtithmeticCalculator target; //通过构造方法传入代理的对象 AtithmeticCalculatorLoggingproxy(AtithmeticCalculator atithmeticCalculator) &#123; this.target = atithmeticCalculator; &#125; //获取代理后的执行对象 AtithmeticCalculator getLoggingProxy() &#123; AtithmeticCalculator proxy = null; //获取类加载器（代理对象由那个类加载器负责加载） ClassLoader classLoader = target.getClass().getClassLoader(); //代理对象的类型，即其中有哪些方法 Class[] instance = new Class[] &#123;AtithmeticCalculator.class&#125;; //当调用代理对象其中的方法时，该执行的代码 InvocationHandler invok = new InvocationHandler() &#123; /** * proxy:正在返回的那个代理对象，一般情况下，在invoke方法中都不使用 * method：正在被调用的方法 * args：调用方法时传入的参数 */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; //获取到正在执行的方法名 String method_name = method.getName(); //日志start System.out.println(\"XPU--&gt;The method \"+method_name+\" begin with[\"+args[0]+\",\"+args[1]+\"]\"); //执行运算方法 Object ret = method.invoke(target, args); //日志end System.out.println(\"XPU--&gt;The method \"+method_name+\" end with[\"+args[0]+\",\"+args[1]+\"]\"); return ret; &#125; &#125;; //得到执行实例 proxy = (AtithmeticCalculator) Proxy.newProxyInstance(classLoader, instance, invok); //返回执行实例 return proxy; &#125;&#125; 这样就是动态代理的做法，让AtithmeticCalculator对象成为动态代理类的一个成员，再通过反射的方式去执行被代理对象的方法，从而可以实现在方法执行的前后打印日志：代理设计模式的原理：使用一个代理将对象包装起来，然后用该代理对象取代原始对象，任何对原始对象的调用都要通过代理，代理对象决定是否以及何时将方法调用转到原始对象上。 什么是AOPAOP(Aspect-Oriented Programming， 面向切面编程): 是一种新的方法论， 是对传统 OOP(Object-Oriented Programming，面向对象编程) 的补充。 AOP 的主要编程对象是切面(aspect)， 而切面模块化横切关注点。在应用 AOP 编程时， 仍然需要定义公共功能， 但可以明确的定义这个功能在哪里， 以什么方式应用，并且不必修改受影响的类。这样一来横切关注点就被模块化到特殊的对象(切面)里。 AOP 的好处： 每个事物逻辑位于一个位置，代码不分散，便于维护和升级 业务模块更简洁， 只包含核心业务代码 AOP相关术语 切面(Aspect): 横切关注点(跨越应用程序多个模块的功能)被模块化的特殊对象 通知(Advice): 切面必须要完成的工作 目标(Target): 被通知的对象 代理(Proxy): 向目标对象应用通知之后创建的对象 连接点（Joinpoint）：程序执行的某个特定位置：如类某个方法调用前、调用后、方法抛出异常后等。连接点由两个信息确定：方法表示的程序执行点；相对点表示的方位。例如 ArithmethicCalculator#add() 方法执行前的连接点，执行点为 ArithmethicCalculator的add()； 方位为该方法执行前的位置 切点（pointcut）：每个类都拥有多个连接点：例如 ArithmethicCalculator 的所有方法实际上都是连接点，即连接点是程序类中客观存在的事务。AOP 通过切点定位到特定的连接点 。类比：连接点相当于数据库中的记录，切点相当于查询条件。切点和连接点不是一对一的关系，一个切点匹配多个连接点，切点通过 org.springframework.aop.Pointcut 接口进行描述，它使用类和方法作为连接点的查询条件。 Spring的AOPAspectJ：Java 社区里最完整最流行的 AOP 框架。在 Spring2.0 以上版本中， 可以使用基于 AspectJ 注解或基于 XML 配置的 AOP 要在 Spring 应用中使用 AspectJ 注解，必须在 classpath 下包含 AspectJ 类库: aopalliance.jar、aspectj.weaver.jar 和 spring-aspects.jar 将 aop Schema 添加到 &lt;beans&gt;根元素中。 要在 Spring IOC 容器中启用 AspectJ 注解支持， 只要在 Bean 配置文件中定义一个空的 XML 元素 &lt;aop:aspectj-autoproxy&gt; 当 Spring IOC 容器侦测到 Bean 配置文件中的 &lt;aop:aspectj-autoproxy&gt; 元素时， 会自动为与 AspectJ 切面匹配的 Bean 创建代理。 123456789101112131415161718&lt;!-- 需要依赖的jar包--&gt;&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjrt&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;aopalliance&lt;/groupId&gt; &lt;artifactId&gt;aopalliance&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt; 用 AspectJ 注解声明切面要在 Spring 中声明 AspectJ 切面， 只需要在 IOC 容器中将切面声明为 Bean 实例。 当在 Spring IOC 容器中初始化 AspectJ 切面之后， Spring IOC 容器就会为那些与 AspectJ 切面相匹配的 Bean 创建代理。 在 AspectJ 注解中, 切面只是一个带有 @Aspect 注解的 Java 类。通知是标注有某种注解的简单的 Java 方法。 AspectJ 支持 5 种类型的通知注解: @Before: 前置通知, 在方法执行之前执行 @After: 后置通知, 在方法执行之后执行 @AfterRunning: 返回通知, 在方法返回结果之后执行 @AfterThrowing: 异常通知, 在方法抛出异常之后 @Around: 环绕通知, 围绕着方法执行 下面通过AOP面向切面编程的思想完成这个日志功能;AtithmeticCalculatorImpl.java和AtithmeticCalculator.java都不变，需要把AtithmeticCalculatorImpl这个实现类通过@Component注解交给IOC容器管理，只是完成了计算的功能，现在新添加一个LoggingAspect类： 123456789101112131415161718192021222324252627282930package com.xpu.aop;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.annotation.After;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.springframework.stereotype.Component;//把这个类声明为切面：1、把该类放入IOC容器中 2、再声明为一个切面@Aspect@Componentpublic class LoggingAspect &#123; //声明该方法是一个前置通知：在目标方法开始之前执行 //@Before(\"execution(public int com.xpu.aop.AtithmeticCalculator.add(int,int))\") @Before(\"execution(public int com.xpu.aop.AtithmeticCalculator.*(int,int))\") public void beforeMethod(JoinPoint joinPoint)&#123; String methodName = joinPoint.getSignature().getName(); Object[] args = joinPoint.getArgs(); System.out.println(\"The method \"+ methodName + \" begins [\"+args[0]+\", \"+args[1]+\"]\"); &#125; //声明该方法是一个后置通知：在目标方法开始之后执行 @After(\"execution(public int com.xpu.aop.AtithmeticCalculator.*(int,int))\") public void endMethod(JoinPoint joinPoint)&#123; String methodName = joinPoint.getSignature().getName(); Object[] args = joinPoint.getArgs(); System.out.println(\"The method \"+ methodName + \" endwith [\"+args[0]+\", \"+args[1]+\"]\"); &#125;&#125; 123456&lt;!-- 配置自动扫描的scan --&gt;&lt;context:component-scan base-package=\"com.xpu.aop\"&gt;&lt;/context:component-scan&gt;&lt;!-- 使ApactJ注解起作用，自动为匹配的类生成代理对象 --&gt;&lt;aop:aspectj-autoproxy&gt;&lt;/aop:aspectj-autoproxy&gt; 这是测试方法： 123456789101112131415@org.junit.Test public void fun()&#123; //1、创建Spring的IOC容器 ApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); //2、从IOC容器中获取Bean的实例 AtithmeticCalculator bean = context.getBean(AtithmeticCalculator.class); //3.1 使用Bean的add int add = bean.add(10, 20); System.out.println(add); //3.2 使用Bean的sub int sub = bean.sub(30, 10); System.out.println(sub);&#125; 分析这个AOP示例前置通知就是在方法执行之前执行的通知，可以直接了当地理解为一个方法！ 前置通知使用 @Before 注解，并将切入点表达式的值作为注解值。其中@Aspect标识这是一个切面，同样的@Component也是要把自己交给IOC容器去管理，@After标识这个方法是个前置通知， 切点表达式表示执行ArithmeticCalculator 接口的 add() 方法， *代表匹配任意修饰符及任意返回值, 参数列表中的..匹配任意数量的参数，execution里面是方法签名，利用方法签名编写 AspectJ 切入点表达式： 最典型的切入点表达式时根据方法的签名来匹配各种方法:execution * com.atguigu.spring.ArithmeticCalculator.*(..) 匹配 ArithmeticCalculator 中声明的所有方法,第一个 * 代表任意修饰符及任意返回值。第二个* 代表任意方法。 .. 匹配任意数量的参数。若目标类与接口与该切面在同一个包中, 可以省略包名 execution public * ArithmeticCalculator.*(..) 匹配 ArithmeticCalculator 接口的所有公有方法。 execution public double ArithmeticCalculator.*(..) 匹配 ArithmeticCalculator 中返回 double 类型数值的方法。 execution public double ArithmeticCalculator.*(double, ..) 匹配第一个参数为 double 类型的方法, .. 匹配任意数量任意类型的参数。 execution public double ArithmeticCalculator.*(double, double) 匹配参数类型为 double, double 类型的方法。 可以在通知方法中声明一个类型为 JoinPoint 的参数，然后就能访问链接细节， 如方法名称和参数值！ 合并切入点表达式在 AspectJ 中, 切入点表达式可以通过操作符 &amp;&amp;、||、 ! 结合起来 后置通知后置通知是在连接点完成之后执行的，即连接点返回结果或者抛出异常的时候， 下面的后置通知记录了方法的终止。在后置通知中还不能访问目标方法的执行结果，执行结果需要在返回通知里面访问，无论方法是否出现异常，后置通知都会执行，比如演示一个除零异常; 返回通知、异常通知和环绕通知一个切面可以包括一个或者多个通知。我们可以看出，后置通知访问不到方法的返回值，而返回通知就是专门用来接收返回值，异常通知就是在方法执行中出现异常才会执行的代码，而且可以指定异常的种类来决定是否触发代码！ 无论连接点是正常返回还是抛出异常，后置通知都会执行。如果只想在连接点返回的时候记录日志，应使用返回通知代替后置通知。在返回通知中，只要将 returning 属性添加到 @AfterReturning 注解中，就可以访问连接点的返回值。 该属性的值即为用来传入返回值的参数名称。必须在通知方法的签名中添加一个同名参数， 在运行时， Spring AOP 会通过这个参数传递返回值。原始的切点表达式需要出现在 pointcut 属性中： 12345678/** *返回通知:在方法正常结束后执行的代码 */@AfterReturning(pointcut=\"execution(public int com.xpu.aop.AtithmeticCalculator.*(int,int))\", returning = \"result\")public void afterReturning(JoinPoint joinPoint, Object result)&#123; String methodName = joinPoint.getSignature().getName(); System.out.println(\"The method \"+methodName+\" result=\" +result);&#125; 异常通知只在连接点抛出异常时才执行异常通知，将 throwing 属性添加到 @AfterThrowing 注解中， 也可以访问连接点抛出的异常。 Throwable 是所有错误和异常类的超类。 所以在异常通知方法可以捕获到任何错误和异常。如果只对某种特殊的异常类型感兴趣， 可以将参数声明为其他异常的参数类型。 然后通知就只在抛出这个类型及其子类的异常时才被执行。 12345678/** * 异常通知，只有执行方法出现异常的时候才会用到 */@AfterThrowing(value=\"execution(public int com.xpu.aop.AtithmeticCalculator.*(int,int))\", throwing = \"e\")public void afterThrowing(JoinPoint joinPoint, Exception e)&#123; String methodName = joinPoint.getSignature().getName(); System.out.println(\"The method \"+methodName+\" throwsException = \" + e);&#125; 环绕通知是所有通知类型中功能最为强大的， 能够全面地控制连接点。 甚至可以控制是否执行连接点，对于环绕通知来说， 连接点的参数类型必须是 ProceedingJoinPoint 。 它是 JoinPoint 的子接口，允许控制何时执行， 是否执行连接点。 在环绕通知中需要明确调用 ProceedingJoinPoint 的 proceed() 方法来执行被代理的方法。 如果忘记这样做就会导致通知被执行了， 但目标方法没有被执行。 注意: 环绕通知的方法需要返回目标方法执行之后的结果， 即调用 joinPoint.proceed(); 的返回值，否则会出现空指针异常 12345678910/** * 环绕通知需要携带proceedingJoinPoint类型参数，类似于动态代理的全过程 * @param proceedingJoinPoint 这个参数可以决定是否执行目标方法 * 而且环绕通知必须有返回值，返回值即为目标方法的返回值 */@Around(\"execution(public int com.xpu.aop.AtithmeticCalculator.*(int,int))\")public Object aroundMethod(ProceedingJoinPoint proceedingJoinPoint)&#123; System.out.println(\"aroundMethod\"); return 100;&#125; 切面的优先级在同一个连接点上应用不止一个切面时，除非明确指定，否则它们的优先级是不确定的。切面的优先级可以通过实现 Ordered 接口或利用 @Order 注解指定。实现 Ordered 接口， getOrder() 方法的返回值越小，优先级越高。若使用 @Order 注解， 序号出现在注解中！ 12345678910@Order(1)@Aspect@Componentpublic class ValidationAspectJ &#123; @Before(value = \"execution(public int com.xpu.aop.AtithmeticCalculator.*(int,int))\") public void validateArgs(JoinPoint joinPoint)&#123; System.out.println(\"validate:\"+ Arrays.asList(joinPoint.getArgs())); &#125;&#125; 重用切入点在编写 AspectJ 切面时， 可以直接在通知注解中书写切入点表达式。 但同一个切点表达式可能会在多个通知中重复出现。 在 AspectJ 切面中，可以通过 @Pointcut 注解将一个切入点声明成简单的方法。 切入点的方法体通常是空的，因为将切入点定义与应用程序逻辑混在一起是不合理的。 切入点方法的访问控制符同时也控制着这个切入点的可见性。 如果切入点要在多个切面中共用，最好将它们集中在一个公共的类中。 在这种情况下， 它们必须被声明为 public。 在引入这个切入点时，必须将类名也包括在内。 如果类没有与这个切面放在同一个包中， 还必须包含包名。其他通知可以通过方法名称引入该切入点，使用示例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package com.xpu.aop;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.*;import org.springframework.core.annotation.Order;import org.springframework.stereotype.Component;//把这个类声明为切面：1、把该类放入IOC容器中 2、再声明为一个切面@Order(1)@Aspect@Componentpublic class LoggingAspect &#123; //定义一个方法，用于声明切入点表达式，一般该方法中不需要在写入其他的代码 @Pointcut(value = \"execution(public int com.xpu.aop.AtithmeticCalculator.*(..))\") public void declareJointPointExpression()&#123;&#125; //声明该方法是一个前置通知：在目标方法开始之前执行 //@Before(\"execution(public int com.xpu.aop.AtithmeticCalculator.add(int,int))\") //@Before(\"execution(public int com.xpu.aop.AtithmeticCalculator.*(..)\") //@Before(\"execution(public int com.xpu.aop.AtithmeticCalculator.*(int,int))\") @Before(value = \"declareJointPointExpression()\") public void beforeMethod(JoinPoint joinPoint)&#123; String methodName = joinPoint.getSignature().getName(); Object[] args = joinPoint.getArgs(); System.out.println(\"The method \"+ methodName + \" begins [\"+args[0]+\", \"+args[1]+\"]\"); &#125; //@After(\"execution(public int com.xpu.aop.AtithmeticCalculator.*(int,int))\") @After(value = \"declareJointPointExpression()\") public void endMethod(JoinPoint joinPoint)&#123; String methodName = joinPoint.getSignature().getName(); Object[] args = joinPoint.getArgs(); System.out.println(\"The method \"+ methodName + \" endwith [\"+args[0]+\", \"+args[1]+\"]\"); &#125; /** *返回通知:在方法正常结束后执行的代码 */ //@AfterReturning(pointcut=\"execution(public int com.xpu.aop.AtithmeticCalculator.*(int,int))\", returning = \"result\") @AfterReturning(pointcut = \"declareJointPointExpression()\", returning = \"result\") public void afterReturning(JoinPoint joinPoint, Object result)&#123; String methodName = joinPoint.getSignature().getName(); System.out.println(\"The method \"+methodName+\" result=\" +result); &#125; /** * 异常通知，只有执行方法出现异常的时候才会用到 */ //@AfterThrowing(value=\"execution(public int com.xpu.aop.AtithmeticCalculator.*(int,int))\", throwing = \"e\") @AfterThrowing(pointcut=\"declareJointPointExpression()\", throwing = \"e\") public void afterThrowing(JoinPoint joinPoint, Exception e)&#123; String methodName = joinPoint.getSignature().getName(); System.out.println(\"The method \"+methodName+\" throwsException = \" + e); &#125; /** * 环绕通知需要携带proceedingJoinPoint类型参数，类似于动态代理的全过程 * @param proceedingJoinPoint 这个参数可以决定是否执行目标方法 * 而且环绕通知必须有返回值，返回值即为目标方法的返回值 */ //@Around(\"execution(public int com.xpu.aop.AtithmeticCalculator.*(int,int))\") @Around(value = \"declareJointPointExpression()\") public Object aroundMethod(ProceedingJoinPoint proceedingJoinPoint)&#123; System.out.println(\"aroundMethod\"); return 100; &#125;&#125; 基于配置文件配置AOP切入点使用 &lt;aop:pointcut&gt; 元素声明切入点必须定义在 &lt;aop:aspect&gt; 元素下, 或者直接定义在 &lt;aop:config&gt; 元素下 定义在 &lt;aop:aspect&gt; 元素下: 只对当前切面有效 定义在 &lt;aop:config&gt; 元素下: 对所有切面都有效 基于 XML 的 AOP 配置不允许在切入点表达式中用名称引用其他切入点.现在假设如下类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import org.aspectj.lang.JoinPoint;public class ValidationAspect &#123; public void validateArgs(JoinPoint joinPoint)&#123; Object[] args = joinPoint.getArgs(); System.out.println(\"校验参数validate:[\"+args[0]+\",\"+args[1]+\"]\"); &#125; public void getReturn(JoinPoint joinPoint, Object ret)&#123; System.out.println(\"返回通知：result=\"+ret); &#125; public void throwException(JoinPoint joinPoint, Exception e)&#123; System.out.println(\"抛出异常:\"+e); &#125;&#125;public class LoggingAspect &#123; public void beforeMethod(JoinPoint joinPoint)&#123; String methodName = joinPoint.getSignature().getName(); Object[] args = joinPoint.getArgs(); System.out.println(\"The method \"+ methodName + \" begins [\"+args[0]+\", \"+args[1]+\"]\"); &#125; public void afterMethod(JoinPoint joinPoint)&#123; String methodName = joinPoint.getSignature().getName(); Object[] args = joinPoint.getArgs(); System.out.println(\"The method \"+ methodName + \" endwith [\"+args[0]+\", \"+args[1]+\"]\"); &#125;&#125;//测试代码@org.junit.Testpublic void fun()&#123; ApplicationContext context = new ClassPathXmlApplicationContext (\"applicationContext-xml.xml\"); AtithmeticCalculator calc = (AtithmeticCalculator)context.getBean(\"atithmeticCalculator\"); calc.add(10,20); calc.div(10,0);&#125; 在XML文件中做如下配置： 123456789101112131415161718192021222324&lt;!-- 配置bean --&gt;&lt;bean id=\"atithmeticCalculator\" class=\"com.xpu.xml.AtithmeticCalculatorImpl\"/&gt;&lt;!-- 配置切面的Bean --&gt;&lt;bean id=\"LoggingAspect\" class=\"com.xpu.xml.LoggingAspect\"/&gt;&lt;bean id=\"ValidationAspect\" class=\"com.xpu.xml.ValidationAspect\"/&gt;&lt;!-- 配置AOP --&gt;&lt;aop:config&gt; &lt;!-- 配置切点表达式 --&gt; &lt;aop:pointcut id=\"point\" expression=\"execution(* com.xpu.xml.AtithmeticCalculator.*(..))\"/&gt; &lt;!-- 配置切面及通知 --&gt; &lt;aop:aspect ref=\"LoggingAspect\" order=\"0\"&gt; &lt;aop:after method=\"afterMethod\" pointcut-ref=\"point\"/&gt; &lt;aop:before method=\"beforeMethod\" pointcut-ref=\"point\"/&gt; &lt;/aop:aspect&gt; &lt;aop:aspect ref=\"ValidationAspect\" order=\"1\"&gt; &lt;aop:after method=\"validateArgs\" pointcut-ref=\"point\"/&gt; &lt;aop:after-returning method=\"getReturn\" returning=\"ret\" pointcut-ref=\"point\"/&gt; &lt;aop:after-throwing method=\"throwException\" throwing=\"e\" pointcut-ref=\"point\"/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 很显然，使用xml配置AOP也是完全可以的！ Spring对JDBC的支持为了使 JDBC 更加易于使用， Spring 在 JDBC API 上定义了一个抽象层， 以此建立一个 JDBC 存取框架。作为 Spring JDBC 框架的核心，JDBC 模板的设计目的是为不同类型的 JDBC 操作提供模板方法。 每个模板方法都能控制整个过程， 并允许覆盖过程中的特定任务。 通过这种方式， 可以在尽可能保留灵活性的情况下， 将数据库存取的工作量降到最低。 JdbcTemplate先写一个配置文件db.properties 1234567jdbc.user&#x3D;rootjdbc.password&#x3D;1234jdbc.driverClass&#x3D;com.mysql.jdbc.Driverjdbc.jdbcUrl&#x3D;jdbc:mysql:&#x2F;&#x2F;127.0.0.1&#x2F;spring_demojdbc.initPoolSize&#x3D;10jdbc.maxPoolSize&#x3D;50 123456789101112131415161718&lt;!-- 导入资源文件 --&gt;&lt;context:property-placeholder location=\"classpath:db.properties\"/&gt;&lt;!-- 配置C3P0数据源 --&gt;&lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"user\" value=\"$&#123;jdbc.user&#125;\"/&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\"/&gt; &lt;property name=\"jdbcUrl\" value=\"$&#123;jdbc.jdbcUrl&#125;\"/&gt; &lt;property name=\"driverClass\" value=\"$&#123;jdbc.driverClass&#125;\"/&gt; &lt;property name=\"maxPoolSize\" value=\"$&#123;jdbc.maxPoolSize&#125;\"/&gt; &lt;property name=\"initialPoolSize\" value=\"$&#123;jdbc.initPoolSize&#125;\"/&gt;&lt;/bean&gt;&lt;!-- 配置Spring的JDBCTemplate --&gt;&lt;bean id=\"jdbcTemplate\" class=\"org.springframework.jdbc.core.JdbcTemplate\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt;&lt;/bean&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748private ApplicationContext context = null;private JdbcTemplate jdbcTemplate = null;&#123; context = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); jdbcTemplate = (JdbcTemplate) context.getBean(\"jdbcTemplate\");&#125;@Test//测试数据源public void fun() &#123; DataSource dataSource = context.getBean(DataSource.class); try &#123; Connection conn = dataSource.getConnection(); System.out.println(conn); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;&#125;//测试修改@Testpublic void testUpdate() &#123; String sql = \"UPDATE employee SET LAST_NAME=? WHERE ID=?\"; int update = jdbcTemplate.update(sql, \"JACKs\", 5); System.out.println(update);&#125;//批量更新@Testpublic void testBatch() &#123; String sql = \"INSERT INTO employee(LAST_NAME, EMAIL, DEPT_ID) values(?,?,?)\"; List&lt;Object[]&gt; list = new ArrayList&lt;&gt;(); list.add(new Object[] &#123;\"ZhangSan\", \"137@qq.com\", 2&#125;); list.add(new Object[] &#123;\"LiSi\", \"127@163.com\", 2&#125;); list.add(new Object[] &#123;\"WangWu\", \"111@gmail.com\", 3&#125;); int[] update = jdbcTemplate.batchUpdate(sql, list); System.out.println(Arrays.toString(update));&#125;@Testpublic void testQuery() &#123; String sql = \"SELECT ID id,LAST_NAME last_name, EMAIL email, DEPT_ID as \\\"department.id\\\" FROM employee WHERE ID=?\"; RowMapper&lt;Employee&gt; rowMapper = new BeanPropertyRowMapper&lt;&gt;(Employee.class); Employee employee = jdbcTemplate.queryForObject(sql,rowMapper , 1); System.out.println(employee);&#125; 太鸡肋，用到的时候再说吧…… Spring对事物的支持事务管理是企业级应用程序开发中必不可少的技术， 用来确保数据的完整性和一致性。事务就是一系列的动作， 它们被当做一个单独的工作单元。 这些动作要么全部完成，要么全部不起作用。 事务的四个关键属性(ACID) 原子性(atomicity): 事务是一个原子操作， 由一系列动作组成。 事务的原子性确保动作要么全部完成要么完全不起作用。 一致性(consistency): 一旦所有事务动作完成， 事务就被提交。 数据和资源就处于一种满足业务规则的一致性状态中。 隔离性(isolation): 可能有许多事务会同时处理相同的数据， 因此每个事物都应该与其他事务隔离开来， 防止数据损坏。 持久性(durability): 一旦事务完成， 无论发生什么系统错误， 它的结果都不应该受到影响。 通常情况下， 事务的结果被写到持久化存储器中。 123456789101112131415161718192021222324252627282930public void purchase() &#123; Connection conn = null; try &#123; conn = dataSource.getConnetction(); conn.setAutoCommit(false); //.... conn.commit(); &#125;catch(SQLException e) &#123; e.printStackTrace(); if(conn != null) &#123; try &#123; conn.rollback(); &#125;catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; throw new RuntimeException(e); &#125;finally &#123; if(conn != null) &#123; try &#123; conn.close(); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; 来看一看上面这段代码，其实获取连接在AOP中就是前置通知，提交事务就是返回通知呀，catch那一段就是异常通知，finally就是后置通知呀，现在再看AOP应该是比较清晰明了！ Spring中的事务管理：作为企业级应用程序框架， Spring 在不同的事务管理 API 之上定义了一个抽象层。 而应用程序开发人员不必了解底层的事务管理 API， 就可以使用 Spring 的事务管理机制。 Spring 既支持编程式事务管理，也支持声明式的事务管理。 编程式事务管理：将事务管理代码嵌入到业务方法中来控制事务的提交和回滚。 在编程式管理事务时， 必须在每个事务操作中包含额外的事务管理代码。 声明式事务管理：大多数情况下比编程式事务管理更好用， 它将事务管理代码从业务方法中分离出来，以声明的方式来实现事务管理。 事务管理作为一种横切关注点， 可以通过 AOP 方法模块化。Spring 通过 Spring AOP 框架支持声明式事务管理。 Spring 从不同的事务管理 API 中抽象了一整套的事务机制。开发人员不必了解底层的事务 API，就可以利用这些事务机制。有了这些事务机制， 事务管理代码就能独立于特定的事务技术了。Spring 的核心事务管理抽象是它为事务管理封装了一组独立于技术的方法。 无论使用 Spring 的哪种事务管理策略(编程式或声明式)，事务管理器都是必须的。这就意味着处理Hibernate，处理MyBatis也是一样的！ Spring 的声明式事物除了在带有切入点，通知和增强器的 Bean 配置文件中声明事务外， Spring 还允许简单地用 @Transactional 注解来标注事务方法。 为了将方法定义为支持事务处理的， 可以为方法添加 @Transactional 注解。 根据 Spring AOP 基于代理机制, 只能标注公有方法。 可以在方法或者类级别上添加 @Transactional 注解。当把这个注解应用到类上时， 这个类中的所有公共方法都会被定义成支持事务处理的。在 Bean 配置文件中只需要启用&lt;tx:annotation-driven&gt; 元素， 并为之指定事务管理器就可以了。 如果事务处理器的名称是 transactionManager，就可以在&lt;tx:annotation-driven&gt; 元素中省略 transaction-manager 属性。这个元素会自动检测该名称的事务处理器。数据库如下，一个账户表，一个书的信息表，一个书的库存表： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647SET NAMES utf8mb4;SET FOREIGN_KEY_CHECKS = 0;-- ------------------------------ Table structure for account-- ----------------------------DROP TABLE IF EXISTS `account`;CREATE TABLE `account` ( `username` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL, `balance` int(11) NOT NULL, PRIMARY KEY (`username`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;-- ------------------------------ Records of account-- ----------------------------INSERT INTO `account` VALUES ('Tom', 20);-- ------------------------------ Table structure for book-- ----------------------------DROP TABLE IF EXISTS `book`;CREATE TABLE `book` ( `isbn` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL, `book_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL, `price` int(11) DEFAULT NULL, PRIMARY KEY (`isbn`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;-- ------------------------------ Records of book-- ----------------------------INSERT INTO `book` VALUES ('1', 'Java', 50);-- ------------------------------ Table structure for book_stock-- ----------------------------DROP TABLE IF EXISTS `book_stock`;CREATE TABLE `book_stock` ( `isbn` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL, `stock` int(255) DEFAULT NULL) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;-- ------------------------------ Records of book_stock-- ----------------------------INSERT INTO `book_stock` VALUES ('1', 7); applicationContext.xml的配置： 1234567891011121314151617181920212223242526&lt;!-- 导入资源文件 --&gt;&lt;context:property-placeholder location=\"classpath:db.properties\"/&gt;&lt;!-- 配置C3P0数据源 --&gt;&lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"user\" value=\"$&#123;jdbc.user&#125;\"/&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\"/&gt; &lt;property name=\"jdbcUrl\" value=\"$&#123;jdbc.jdbcUrl&#125;\"/&gt; &lt;property name=\"driverClass\" value=\"$&#123;jdbc.driverClass&#125;\"/&gt; &lt;property name=\"maxPoolSize\" value=\"$&#123;jdbc.maxPoolSize&#125;\"/&gt; &lt;property name=\"initialPoolSize\" value=\"$&#123;jdbc.initPoolSize&#125;\"/&gt;&lt;/bean&gt;&lt;!-- 配置Spring的JDBCTemplate --&gt;&lt;bean id=\"jdbcTemplate\" class=\"org.springframework.jdbc.core.JdbcTemplate\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 配置事务管理器 --&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 启用事物注解 --&gt;&lt;tx:annotation-driven transaction-manager=\"transactionManager\"/&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@Repository(\"bookShopDao\")public class BookShopDaoImpl implements BookShopDao&#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public int findBookPriceByIsbn(String isbn) &#123; String sql = \"select price from book where isbn=?\"; return jdbcTemplate.queryForObject(sql, Integer.class, isbn); &#125; @Override public void updateBookStock(String isbn) &#123; //检查书的库存是否足够，若不够则抛出异常 String sql2 = \"select stock from book_stock where isbn=?\"; int stock = jdbcTemplate.queryForObject(sql2, Integer.class, isbn); if(stock == 0) &#123; throw new BookStockException(\"库存不足\"); &#125; String sql = \"update book_stock set stock = stock - 1 where isbn = ?\"; jdbcTemplate.update(sql, isbn); &#125; @Override public void updateUserAccount(String name, int price) &#123; //检查用户余额是否足够，若不够则抛出异常 String sql2 = \"select balance from account where username=?\"; int balance = jdbcTemplate.queryForObject(sql2, Integer.class, name); if(balance &lt; price) &#123; throw new UserAccountException(\"余额不足\"); &#125; String sql = \"update account set balance=balance-? where username=?\"; jdbcTemplate.update(sql, price, name); &#125;&#125;@Servicepublic class BookShopServiceImpl implements BookShopService&#123; @Autowired private BookShopDao bookShopDao; //添加事物注解 @Transactional @Override public void purchase(String name, String isbn) &#123; int price = bookShopDao.findBookPriceByIsbn(isbn); bookShopDao.updateBookStock(isbn); bookShopDao.updateUserAccount(name, price); &#125;&#125; 事务传播属性当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。 例如: 方法可能继续在现有事务中运行， 也可能开启一个新事务，并在自己的事务中运行。事务的传播行为可以由传播属性指定。Spring 定义了 7 种类传播行为：PROPAGATION_REQUIRED 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。REQUIRED:业务方法需要在一个容器里运行。如果方法运行时，已经处在一个事务中，那么加入到这个事务，否则自己新建一个新的事务。 NOT_SUPPORTED:声明方法不需要事务。如果方法没有关联到一个事务，容器不会为他开启事务，如果方法在一个事务中被调用，该事务会被挂起，调用结束后，原先的事务会恢复执行。 REQUIRESNEW:不管是否存在事务，该方法总汇为自己发起一个新的事务。如果方法已经运行在一个事务中，则原有事务挂起，新的事务被创建。 MANDATORY：该方法只能在一个已经存在的事务中执行，业务方法不能发起自己的事务。如果在没有事务的环境下被调用，容器抛出例外。 SUPPORTS:该方法在某个事务范围内被调用，则方法成为该事务的一部分。如果方法在该事务范围外被调用，该方法就在没有事务的环境下执行。 NEVER：该方法绝对不能在事务范围内执行。如果在就抛例外。只有该方法没有关联到任何事务，才正常执行。 NESTED:如果一个活动的事务存在，则运行在一个嵌套的事务中。如果没有活动事务，则按REQUIRED属性执行。它使用了一个单独的事务，这个事务 拥有多个可以回滚的保存点。内部事务的回滚不会对外部事务造成影响。它只对DataSourceTransactionManager事务管理器起效。 REQUIRED 传播行为当 bookService 的 purchase() 方法被另一个事务方法 checkout() 调用时，它默认会在现有的事务内运行。这个默认的传播行为就是 REQUIRED。 因此在 checkout() 方法的开始和终止边界内只有一个事务， 这个事务只在 checkout() 方法结束的时候被提交，结果用户一本书都买不了，事务传播属性可以在 @Transactional 注解的 propagation 属性中定义。其实就是checkout里面定义了事物，purchase也定义了事物，但是只要在其中一个purchase中出现了异常，那么check中的所有事物都会回滚！ REQUIRES_NEW 传播行为另一种常见的传播行为是 REQUIRES_NEW，它表示该方法必须启动一个新事务，并在自己的事务内运行，如果有事务在运行，就应该先挂起它。 12345678910//添加事物注解//使用propagation指定事物的传播行为，即当前的事物方法被另外一个事物方法调用时//REQUIRES_NEW使用自己的事物@Transactional(propagation = Propagation.REQUIRES_NEW)@Overridepublic void purchase(String name, String isbn) &#123; int price = bookShopDao.findBookPriceByIsbn(isbn); bookShopDao.updateBookStock(isbn); bookShopDao.updateUserAccount(name, price);&#125; 并发事务所导致的问题当同一个应用程序或者不同应用程序中的多个事务在同一个数据集上并发执行时，可能会出现许多意外的问题：并发事务所导致的问题可以分为下面三种类型: 脏读: 对于两个事物 T1， T2， T1 读取了已经被 T2 更新但还没有被提交的字段。之后，若 T2 回滚，T1读取的内容就是临时且无效的。 不可重复读:对于两个事物 T1，T2，T1 读取了一个字段， 然后 T2 更新了该字段。之后， T1再次读取同一个字段， 值就不同了。 幻读:对于两个事物 T1， T2， T1 从一个表中读取了一个字段，然后 T2 在该表中插入了一些新的行。之后， 如果 T1 再次读取同一个表，就会多出几行。 事务的隔离级别从理论上来说，事务应该彼此完全隔离， 以避免并发事务所导致的问题。然而，那样会对性能产生极大的影响，因为事务必须按顺序运行。在实际开发中，为了提升性能，事务会以较低的隔离级别运行。事务的隔离级别可以通过隔离事务属性指定。事务的隔离级别要得到底层数据库引擎的支持，而不是应用程序或者框架的支持。Oracle 支持的 2 种事务隔离级别：READ_COMMITED , SERIALIZABLEMysql 支持 4 中事务隔离级别。用 @Transactional 注解声明式地管理事务时可以在 @Transactional 的 isolation 属性中设置隔离级别： 12@Transactional(propagation = Propagation.REQUIRES_NEW , isolation=Isolation.READ_COMMITTED) 默认情况下Spring的声明式事物对所有运行时异常回滚，也可以对相应的属性进行设置，默认情况下只有未检查异常(RuntimeException和Error类型的异常)会导致事务回滚。而受检查异常不会； 事务的回滚规则可以通过 @Transactional 注解的 rollbackFor 和 noRollbackFor 属性来定义，这两个属性被声明为 Class[] 类型的，因此可以为这两个属性指定多个异常类，rollbackFor: 遇到时必须进行回滚，noRollbackFor: 一组异常类，遇到时必须不回滚。 12@Transactional(propagation = Propagation.REQUIRES_NEW , isolation=Isolation.READ_COMMITTED ,noRollbackFor= &#123;UserAccountException.class&#125;) 这样的话，假设UserAccountException这种运行时异常发生的时候就不会进行事物回滚，通过noRollbackFor属性就可以设置对哪些异常不进行回滚！使用readOnly属性可以指定为事物是否只读，如果这个事物真的只是一个读取数据库值的方法，那么应该设置为readOnly为true，这样可以帮助数据库引擎优化事物： 123@Transactional(propagation = Propagation.REQUIRES_NEW , isolation=Isolation.READ_COMMITTED ,noRollbackFor= &#123;UserAccountException.class&#125;, readOnly=false) 由于事务可以在行和表上获得锁， 因此长事务会占用资源， 并对整体性能产生影响。如果一个事物只读取数据但不做修改，数据库引擎可以对这个事务进行优化。 超时事务属性:：事务在强制回滚之前可以保持多久，这样可以防止长期运行的事务占用资源。 只读事务属性： 表示这个事务只读取数据但不更新数据， 这样可以帮助数据库引擎优化事务。使用timeout可以指定强制回滚事物之前可以占用的时间，可以防止链接占用时间过长，超时和只读属性可以在 @Transactional 注解中定义。超时属性以秒为单位来计算： 123@Transactional(propagation = Propagation.REQUIRES_NEW , isolation=Isolation.READ_COMMITTED ,noRollbackFor= &#123;UserAccountException.class&#125;, readOnly=false, timeout=1) Spring通过xml配置事物12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;context:component-scan base-package=\"com.xpu.xml\"&gt;&lt;/context:component-scan&gt; &lt;!-- 导入资源文件 --&gt;&lt;context:property-placeholder location=\"classpath:db.properties\"/&gt;&lt;!-- 配置C3P0数据源 --&gt; &lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"user\" value=\"$&#123;jdbc.user&#125;\"/&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\"/&gt; &lt;property name=\"jdbcUrl\" value=\"$&#123;jdbc.jdbcUrl&#125;\"/&gt; &lt;property name=\"driverClass\" value=\"$&#123;jdbc.driverClass&#125;\"/&gt; &lt;property name=\"maxPoolSize\" value=\"$&#123;jdbc.maxPoolSize&#125;\"/&gt; &lt;property name=\"initialPoolSize\" value=\"$&#123;jdbc.initPoolSize&#125;\"/&gt; &lt;/bean&gt; &lt;!-- 配置Bean --&gt; &lt;bean id=\"jdbcTemplate\" class=\"org.springframework.jdbc.core.JdbcTemplate\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"bookShopDao\" class=\"com.xpu.xml.BookShopDaoImpl\"&gt; &lt;property name=\"jdbcTemplate\" ref=\"jdbcTemplate\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"bookShopService\" class=\"com.xpu.xml.BookShopServiceImpl\"&gt; &lt;property name=\"bookShopDao\" ref=\"bookShopDao\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"cashier\" class=\"com.xpu.xml.CashierImpl\"&gt; &lt;property name=\"bookShopService\" ref=\"bookShopService\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 1、 配置事务管理器 --&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 2、 配置事物属性 --&gt; &lt;tx:advice id=\"txAdvice\" transaction-manager=\"transactionManager\"&gt; &lt;tx:attributes&gt; &lt;tx:method name=\"checkout\" propagation=\"REQUIRED\"/&gt; &lt;!-- 对只读事物配置只读属性 --&gt; &lt;tx:method name=\"find*\" read-only=\"true\"/&gt; &lt;tx:method name=\"get*\" read-only=\"true\"/&gt; &lt;tx:method name=\"*\" propagation=\"REQUIRED\"/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 3、 配置事物切点 --&gt; &lt;aop:config&gt; &lt;aop:pointcut expression=\"execution(* com.xpu.xml.BookShopService.*(..))\" id=\"txPointCut\"/&gt; &lt;aop:advisor advice-ref=\"txAdvice\" pointcut-ref=\"txPointCut\"/&gt; &lt;/aop:config&gt;","updated":"2020-04-14T03:04:44.801Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaEE","slug":"JavaEE","permalink":"https://zouchanglin.cn/tags/JavaEE/"},{"name":"SpringCore","slug":"SpringCore","permalink":"https://zouchanglin.cn/tags/SpringCore/"}]},{"title":"Spring入门笔记(二)","date":"2019-02-04T23:00:00.000Z","path":"2019/02/05/Spring入门笔记(二)/","text":"SpringBean之间的关系1、SpringBean之间的继承假设现有一个Address类表示地点： 1234567package com.xpu.bean;public class Address &#123; private String city; private String street; setter()/getter()....&#125; 现在如何需要两个对象，则可以这写： 123&lt;bean id=\"address1\" class=\"com.xpu.bean.Address\" p:city=\"Xian\" p:street=\"Wulukou\"&gt;&lt;/bean&gt;&lt;bean id=\"address2\" class=\"com.xpu.bean.Address\" p:city=\"Xian\" p:street=\"Beidajie\"&gt;&lt;/bean&gt; 但是这样写如果在类属性过多的时候就不是很好用了，所以，出现了配置之间的继承关系，但是这个和类之间的继承不是一回事： 1234&lt;bean id=\"address1\" class=\"com.xpu.bean.Address\" p:city=\"Xian\" p:street=\"Wulukou\"&gt;&lt;/bean&gt;&lt;!-- 配置之间的继承关系 ：使用bean的parent属性指定--&gt;&lt;bean id=\"address2\" p:street=\"Beidajie\" parent=\"address1\"&gt;&lt;/bean&gt; Spring 允许继承 bean 的配置, 被继承的 bean 称为父 bean. 继承这个父 Bean 的 Bean 称为子 Bean，子 Bean 从父 Bean 中继承配置, 包括 Bean 的属性配置，子 Bean 也可以覆盖从父 Bean 继承过来的配置，父 Bean 可以作为配置模板, 也可以作为 Bean 实例。 若只想把父 Bean 作为模板, 可以设置 的abstract 属性为 true, 这样 Spring 将不会实例化这个 Bean： 12&lt;-- 抽象bean不能被IOC容器实例化，只能用继承配置 --&gt;&lt;bean id=\"address1\" class=\"com.xpu.bean.Address\" abstract=\"true\"&gt;&lt;/bean&gt; 一旦这样配置之后这个配置是不能实例化出对象的，而需要配置出他的子配置，否则就无法创建对象，若某一个bean的class未指定，那么该bean必须是一个抽象bean，可以忽略父 Bean 的 class 属性，让子 Bean 指定自己的类，而共享相同的属性配置。但此时 abstract 必须设为 true，并不是 元素里的所有属性都会被继承. 比如: autowire, abstract 2、SpringBean之间的依赖Spring 允许用户通过 depends-on 属性设定 Bean 前置依赖的Bean，前置依赖的 Bean 会在本 Bean 实例化之前创建好如果前置依赖于多个 Bean，则可以通过逗号，空格或的方式配置 Bean 的名称 1234567public class Students &#123; private int stuId; private String stuName; private int stuAge; private Address stuAdd; setter()/getter()...&#125; 这样需要在配置文件中的bean标签中使用depends-on属性来指定： 123&lt;bean id=\"address\" class=\"com.xpu.bean.Address\" p:city=\"BeiJing\" p:street=\"Hello\"&gt;&lt;/bean&gt;&lt;bean id=\"student\" class=\"com.xpu.bean.Students\" p:stuId=\"1\" p:stuName=\"Tim\" p:stuAge=\"10\" depends-on=\"address\"&gt;&lt;/bean&gt; SpringBean的作用域如何使用配置文件配置一个单例呢？ 1234&lt;bean id=\"address\" class=\"com.xpu.bean.Address\"&gt; &lt;property name=\"city\" value=\"BeiJing\"&gt;&lt;/property&gt; &lt;property name=\"street\" value=\"WuLuKou\"&gt;&lt;/property&gt;&lt;/bean&gt; 1234ApplicationContext context = new ClassPathXmlApplicationContext(\"beans-relation.xml\");Address bean1 = (Address) context.getBean(\"address\");Address bean2 = (Address) context.getBean(\"address\");System.out.println(bean1 == bean2);//true 由此可见，IOC容器对Bean的默认配置就是单例的：在 Spring 中, 可以在 &lt;bean&gt; 元素的 scope 属性里设置 Bean 的作用域.默认情况下, Spring 只为每个在 IOC 容器里声明的 Bean 创建唯一一个实例, 整个 IOC 容器范围内都能共享该实例：所有后续的 getBean() 调用和 Bean 引用都将返回这个唯一的 Bean 实例.该作用域被称为 singleton, 它是所有 Bean 的默认作用域，下面是scope 属性的说明： 类别 说明 singleton 在SpringIOC容器中仅存在一个Bean实例，Bean以单例的方式存在 prototype 每次调用getBean()时都会返回一个新的实例 request 每次HTTP请求都会创建一个新的Bean，该Bean作用域仅适用于WebApplicationContext环境 session 同一个HTTP Session共享一个Bean，不同的Http Session使用不同的Bean，仅适用于WebApplicationContext环境 在单例模式下，一旦执行 1ApplicationContext context = new ClassPathXmlApplicationContext(\"beans-relation.xml\"); 就会构造出相应的对象，而无需等到context.getBean(&quot;address&quot;);，每次获取对象的时候都会返回已经创建好的Bean，你可以理解为这是饿汉式单例，而prototype每次等到context.getBean(&quot;address&quot;);才创建对象，你可以理解为这是一种很懒的方式，但是却不是单例模式 SpringBean使用外部属性文件现在假设我们需要配置一个数据库连接，我们可以在配置文件中这样写： 123456&lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"user\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"1234\"&gt;&lt;/property&gt; &lt;property name=\"driverClass\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/property&gt; &lt;property name=\"jdbcUrl\" value=\"jdbc:mysql://localhost:3306/spring_demo\"&gt;&lt;/property&gt;&lt;/bean&gt; 接下来只需要这三个步骤便可以拿到数据库的连接对象： 123ApplicationContext context = new ClassPathXmlApplicationContext(\"beans-properties.xml\");DataSource dataSources = (DataSource) context.getBean(\"dataSource\");Connection connection = dataSources.getConnection(); 可是这样的做法在配置项目特别多的时候就特别复杂，我们还是希望在配置文件层面上能解耦合，把数据库的连接信息独立到一个配置文件中，于是可以使用外部属性文件来完成： 在配置文件里配置 Bean 时, 有时需要在 Bean 的配置里混入系统部署的细节信息(例如: 文件路径, 数据源配置信息等). 而这些部署细节实际上需要和 Bean 配置相分离 Spring 提供了一个 PropertyPlaceholderConfigurer 的 BeanFactory 后置处理器, 这个处理器允许用户将 Bean 配置的部分内容外移到属性文件中. 可以在 Bean 配置文件里使用形式为 ${var} 的变量, PropertyPlaceholderConfigurer 从属性文件里加载属性, 并使用这些属性来替换变量。Spring 还允许在属性文件中使用 ${propName}，以实现属性之间的相互引用。这是我新建的db.properties配置文件： 1234user&#x3D;rootpassword&#x3D;1234driverClass&#x3D;com.mysql.jdbc.DriverjdbcUrl&#x3D;jdbc:mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;spring_demo 123456789&lt;!-- 导入属性配置文件 --&gt;&lt;context:property-placeholder location=\"db.properties\"/&gt;&lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"user\" value=\"$&#123;user&#125;\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"$&#123;password&#125;\"&gt;&lt;/property&gt; &lt;property name=\"driverClass\" value=\"$&#123;driverClass&#125;\"&gt;&lt;/property&gt; &lt;property name=\"jdbcUrl\" value=\"$&#123;jdbcUrl&#125;\"&gt;&lt;/property&gt;&lt;/bean&gt; Spring表达式语言：SpELSpring 表达式语言（简称SpEL）：是一个支持运行时查询和操作对象图的强大的表达式语言。 语法类似于 EL：SpEL 使用 #{…} 作为定界符，所有在大框号中的字符都将被认为是 SpEL SpEL 为 bean 的属性进行动态赋值提供了便利 通过 SpEL 可以实现： 通过 bean 的 id 对 bean 进行引用 调用方法以及引用对象中的属性 计算表达式的值 正则表达式的匹配 1、字面量 2、引用 Bean、属性和方法首先呢，还是拿Students和Address类作为演示： 12345678910111213public class Address &#123; private String city; private String street; getter()/setter()...&#125;public class Students &#123; private int stuId; private String stuName; private int stuAge; private Address stuAdd; getter()/setter()...&#125; 引用其他对象，通过ID引用 12345678&lt;bean id=\"address\" class=\"com.xpu.bean.Address\" p:city=\"Xian\" p:street=\"Wulukou\"&gt;&lt;/bean&gt; &lt;bean id=\"students\" class=\"com.xpu.bean.Students\"&gt; &lt;property name=\"stuId\" value=\"1\"&gt;&lt;/property&gt; &lt;property name=\"stuName\" value=\"Tom\"&gt;&lt;/property&gt; &lt;property name=\"stuAge\" value=\"20\"&gt;&lt;/property&gt; &lt;property name=\"stuAdd\" value=\"#&#123;address&#125;\"&gt;&lt;/property&gt;&lt;/bean&gt; 引用其他对象的属性 12345678&lt;bean id=\"address\" class=\"com.xpu.bean.Address\" p:city=\"Xian\" p:street=\"Wulukou\"&gt;&lt;/bean&gt;&lt;bean id=\"students\" class=\"com.xpu.bean.Students\"&gt; &lt;property name=\"stuId\" value=\"1\"&gt;&lt;/property&gt; &lt;property name=\"stuName\" value=\"#&#123;address.city&#125;\"&gt;&lt;/property&gt; &lt;property name=\"stuAge\" value=\"20\"&gt;&lt;/property&gt; &lt;property name=\"stuAdd\" value=\"#&#123;address&#125;\"&gt;&lt;/property&gt;&lt;/bean&gt; 调用其他对象的方法，并支持链式调用 12345678&lt;bean id=\"address\" class=\"com.xpu.bean.Address\" p:city=\"Xian\" p:street=\"Wulukou\"&gt;&lt;/bean&gt;&lt;bean id=\"students\" class=\"com.xpu.bean.Students\"&gt; &lt;property name=\"stuId\" value=\"1\"&gt;&lt;/property&gt; &lt;property name=\"stuName\" value=\"#&#123;address.toString().trim()&#125;\"&gt;&lt;/property&gt; &lt;property name=\"stuAge\" value=\"20\"&gt;&lt;/property&gt; &lt;property name=\"stuAdd\" value=\"#&#123;address&#125;\"&gt;&lt;/property&gt;&lt;/bean&gt; 3、SpEL支持的运算符 算数运算符：+，-， *， /， %，^ 加号还可以用作字符串连接 比较运算符： &lt;、 &gt;、==、&lt;=、&gt;=、lt、gt、eq 、le、 ge 逻辑运算符号： and、 or、not、 | if-else 运算符：?: (ternary)、?: (Elvis) 正则表达式：matches 调用静态方法或静态属性：通过 T() 调用一个类的静态方法，它将返回一个 Class Object，然后再调用相应的方法或属性，下面是一些使用示例： 123456789101112131415&lt;bean&gt; &lt;property name=\"add\" value=\"#&#123;students.id + 10&#125;\"&gt;&lt;/property&gt; &lt;property name=\"sub\" value=\"#&#123;students.id - 10&#125;\"&gt;&lt;/property&gt; &lt;property name=\"mul\" value=\"#&#123;students.id * 10&#125;\"&gt;&lt;/property&gt; &lt;property name=\"div\" value=\"#&#123;students.id / 10&#125;\"&gt;&lt;/property&gt; &lt;property name=\"rem\" value=\"#&#123;students.id % 10&#125;\"&gt;&lt;/property&gt; &lt;property name=\"pow\" value=\"#&#123;students.id ^ 10&#125;\"&gt;&lt;/property&gt; &lt;property name=\"stradd\" value=\"#&#123;students.name + '' +students.name&#125;\"&gt;&lt;/property&gt; &lt;property name=\"equal\" value=\"&#123;students.id == 1&#125;\"&gt;&lt;/property&gt; &lt;property name=\"logic_a\" value=\"&#123;students.id == 1 and students.age == 20&#125;\"&gt;&lt;/property&gt; &lt;property name=\"logic_o\" value=\"&#123;students.id == 1 or students.age == 20&#125;\"&gt;&lt;/property&gt; &lt;property name=\"logic_n\" value=\"&#123;not students.id == 1&#125;\"&gt;&lt;/property&gt; &lt;property name=\"if-else\" value=\"&#123;students.id == 1 ? 'Tom':'Tim'&#125;\"&gt;&lt;/property&gt; &lt;property name=\"PI\" value=\"#&#123;T(java.long.Math).PI&#125;\"&gt;&lt;/property&gt;&lt;/bean&gt; IOC容器中Bean的生命周期方法Spring IOC 容器可以管理 Bean 的生命周期, Spring 允许在 Bean 生命周期的特定点执行定制的任务.。Spring IOC 容器对 Bean 的生命周期进行管理的过程: 1、通过构造器或工厂方法创建 Bean 实例 2、为 Bean 的属性设置值和对其他 Bean 的引用 3、调用 Bean 的初始化方法 4、Bean 可以使用了 5、当容器关闭时, 调用 Bean 的销毁方法 在 Bean 的声明里设置 init-method 和 destroy-method 属性, 为 Bean 指定初始化和销毁方法。 12345678910111213141516171819202122232425262728293031package com.xpu.bean_cycle;public class Car &#123; public Car() &#123; System.out.println(\"Car's Constructor...\"); &#125; private String brand; public void setBrand(String brand) &#123; System.out.println(\"setBrand...\"); this.brand = brand; &#125; public void init() &#123; System.out.println(\"init...\"); &#125; public void destroy() &#123; System.out.println(\"distroy...\"); &#125;&#125; //测试代码：public void test() &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"beans-cycle.xml\"); Car car = (Car) context.getBean(\"car\"); System.out.println(car); //关闭容器 context.close();&#125; bean的配置： 123&lt;bean id=\"car\" class=\"com.xpu.bean_cycle.Car\" init-method=\"init\" destroy-method=\"destroy\"&gt; &lt;property name=\"brand\" value=\"Audi\"&gt;&lt;/property&gt;&lt;/bean&gt; Bean 后置处理器Bean 后置处理器允许在调用初始化方法前后对 Bean 进行额外的处理。Bean 后置处理器对 IOC 容器里的所有 Bean 实例逐一处理， 而非单一实例。 其典型应用是: 检查 Bean 属性的正确性或根据特定的标准更改 Bean 的属性。对Bean 后置处理器而言， 需要实现 BeanPostProcessor 接口。 在初始化方法被调用前后， Spring 将把每个 Bean 实例分别传递给上述接口的以下两个方法: 12public Object postProcessAfterInitialization(Object beanObject, String id)public Object postProcessBeforeInitialization(Object beanObject, String id) 如何设置Bean的后置处理器呢？使用一个类去实现BeanPostProcessor里面的上述两个方法，postProcessBeforeInitialization是在init方法之前被调用，postProcessAfterInitialization在init方法之后被调用，bean就是实例本身，beanName就是IOC容器中的bean配置的ID，返回值是实际上返回给用户的Bean，可以在以上两个方法中修改返回的Bean，甚至返回一个新的Bean，我们自己去实现BeanPostProcessor的类，只需要在IOC容器中配置一个Bean即可，无需指定ID和其他的东西，IOC容器会自动识别后置处理器： 12&lt;!-- 配置bean的后置处理器 --&gt;&lt;bean class=\"com.xpu.bean_cycle.MyBeanPostProcessor\"&gt;&lt;/bean&gt; 添加Bean后置处理器后 Bean 的生命周期，Spring IOC 容器对 Bean 的生命周期进行管理的过程: 通过构造器或工厂方法创建 Bean 实例 为 Bean 的属性设置值和对其他 Bean 的引用 将 Bean 实例传递给 Bean 后置处理器的 postProcessBeforeInitialization 方法 调用 Bean 的初始化方法 将 Bean 实例传递给 Bean 后置处理器的 postProcessAfterInitialization方法Bean 可以使用了 当容器关闭时, 调用 Bean 的销毁方法1234567891011121314151617181920212223public class MyBeanPostProcessor implements BeanPostProcessor&#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(\"postProcessBeforeInitialization\"+bean+\",\"+beanName); if(\"Car\".equals(beanName)) &#123; //... &#125;else if(\"XXX\".equals(beanName)) &#123; //... &#125; return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(\"postProcessAfterInitialization\"+bean+\",\"+beanName); if(\"Car\".equals(beanName)) &#123; //... &#125;else if(\"XXX\".equals(beanName)) &#123; //... &#125; return car; &#125;&#125; Bean 的配置方式通过全类名（反射）、通过工厂方法（静态工厂方法 &amp; 实例工厂方法）、FactoryBean，接下来我记录一下如何通过工厂方法来配置Bean：1、静态工厂方法调用静态工厂方法创建 Bean是将对象创建的过程封装到静态方法中. 当客户端需要对象时, 只需要简单地调用静态方法, 而不同关心创建对象的细节.要声明通过静态方法创建的 Bean, 需要在 Bean 的 class 属性里指定拥有该工厂的方法的类, 同时在 factory-method 属性里指定工厂方法的名称. 最后, 使用 &lt;constrctor-arg&gt;元素为该方法传递方法参数.123456789101112131415161718192021package com.xpu.bean_factory;import java.util.HashMap;import java.util.Map;/** * 静态工厂方法：直接调用某一个类的静态方法就可以返回一个bean的实例 */public class StaticCarFactory &#123; private static Map&lt;String, Car&gt; cars = new HashMap&lt;&gt;(); static &#123; cars.put(\"audi\", new Car(\"audi\",300000)); cars.put(\"ford\", new Car(\"ford\",400000)); &#125; //静态工厂方法 public static Car getCar(String name) &#123; return cars.get(name); &#125;&#125; 12345678&lt;!-- 通过静态方法工厂来配置Bean，注意不是配置静态方法工厂实例，而是Bean实例 --&gt;&lt;!-- class属性指向静态方法工厂的Class factory-method属性指向静态工厂的名字 constructor-arg如果工厂方法需要传入参数，则使用constructor-arg来配置参数 --&gt;&lt;bean id=\"car1\" class=\"com.xpu.bean_factory.StaticCarFactory\" factory-method=\"getCar\"&gt; &lt;constructor-arg value=\"audi\"&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 2、实例工厂方法实例工厂方法: 将对象的创建过程封装到另外一个对象实例的方法里. 当客户端需要请求对象时, 只需要简单的调用该实例方法而不需要关心对象的创建细节.要声明通过实例工厂方法创建的 Bean 在 bean 的 factory-bean 属性里指定拥有该工厂方法的 Bean 在 factory-method 属性里指定该工厂方法的名称 使用 construtor-arg 元素为工厂方法传递方法参数1234567891011121314151617181920package com.xpu.bean_factory;import java.util.HashMap;import java.util.Map;/** * 实例工厂的方法，即先需要创建工厂本身，再调用工厂的实例方法，再返回Bean实例 */public class InstanceCarFactory &#123; private static Map&lt;String, Car&gt; cars = null; public InstanceCarFactory() &#123; cars = new HashMap&lt;&gt;(); cars.put(\"audi\", new Car(\"audi\",300000)); cars.put(\"ford\", new Car(\"ford\",400000)); &#125; public Car getCar(String brand) &#123; return cars.get(brand); &#125;&#125; 1234567891011&lt;!-- 配置工厂的实例 --&gt;&lt;bean id=\"carFactory\" class=\"com.xpu.bean_factory.InstanceCarFactory\"&gt;&lt;/bean&gt;&lt;!-- 通过实例工厂方法来配置Bean --&gt;&lt;!-- factory-bean属性指向实例方法工厂的Class factory-method属性指向实例工厂的名字 constructor-arg如果工厂方法需要传入参数，则使用constructor-arg来配置参数 --&gt;&lt;bean id=\"car2\" factory-bean=\"carFactory\" factory-method=\"getCar\"&gt; &lt;constructor-arg value=\"ford\"&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 3、FactoryBean实现 FactoryBean 接口在 Spring IOC 容器中配置 Bean，Spring 中有两种类型的 Bean, 一种是普通Bean， 另一种是工厂Bean，即FactoryBean.。 工厂 Bean 跟普通Bean不同，其返回的对象不是指定类的一个实例，其返回的是该工厂 Bean 的 getObject 方法所返回的对象 ，示例如下： 123456789101112131415161718192021222324252627282930package com.xpu.factory_spring;import org.springframework.beans.factory.FactoryBean;//自定义的FactoryBean需要实现Spring提供的FactoryBean接口public class CarFactoryBean implements FactoryBean&lt;Car&gt; &#123; private String brand; public void setBrand(String brand) &#123; this.brand = brand; &#125; //返回Bean的对象 @Override public Car getObject() throws Exception &#123; return new Car(brand, 500000); &#125; //返回Bean的类型 @Override public Class&lt;?&gt; getObjectType() &#123; return Car.class; &#125; //是否是单例的 @Override public boolean isSingleton() &#123; return true; &#125;&#125; 1234567&lt;!-- 通过FactoryBean来配置Bean的实例 class执行FactoryBean的全类名 property配置的是FactoryBean的属性，但是返回的FactoryBean的getObject()方法返回的实例 --&gt;&lt;bean id=\"car\" class=\"com.xpu.factory_spring.CarFactoryBean\"&gt; &lt;property name=\"brand\" value=\"BMW\"&gt;&lt;/property&gt;&lt;/bean&gt; 4、基于注解的方式接下来说说基于注解的方式配置Bean，这也是常见的方式：组件扫描(component scanning): Spring 能够从 classpath 下自动扫描, 侦测和实例化具有特定注解的组件，特定组件包括: @Component: 基本注解，标识了一个受 Spring 管理的组件 @Respository: 标识持久层组件 @Service: 标识服务层(业务层)组件 @Controller: 标识表现层组件 对于扫描到的组件, Spring 有默认的命名策略： 使用非限定类名， 第一个字母小写， 也可以在注解中通过 value 属性值标识组件的名称 当在组件类上使用了特定的注解之后, 还需要在 Spring 的配置文件中声明需要扫描的包，而且不要忘记声明命名空间&lt;context:component-scan&gt; ： 1&lt;context:component-scan base-package=\"com.xpu.annnotations\"&gt;&lt;/context:component-scan&gt; base-package 属性指定一个需要扫描的基类包，Spring 容器将会扫描这个基类包里及其子包中的所有类。当需要扫描多个包时, 可以使用逗号分隔，如果仅希望扫描特定的类而非基包下的所有类，可使用 resource-pattern 属性过滤特定的类，示例： 12&lt;!-- 可以通过resource-pattern可以指定扫描的资源 --&gt;&lt;context:component-scan base-package=\"com.xpu.annnotations\" resource-pattern=\"service/*.class\" &gt;&lt;/context:component-scan&gt; &lt;context:include-filter&gt; 子节点表示要包含的目标类 &lt;context:exclude-filter&gt; 子节点表示要排除在外的目标类 &lt;context:component-scan&gt; 下可以拥有若干个 &lt;context:include-filter&gt; 和&lt;context:exclude-filter&gt; 子节点 12345678&lt;!-- context:exclude-filter就是排除哪些类 --&gt;&lt;context:component-scan base-package=\"com.xpu.annnotations\" use-default-filters=\"false\"&gt; &lt;context:exclude-filter type=\"annotation\" expression=\"com.xpu.annnotations.controller.UserController\"/&gt;&lt;/context:component-scan&gt;&lt;context:component-scan base-package=\"com.xpu.annnotations\" use-default-filters=\"false\"&gt; &lt;context:include-filter type=\"annotation\" expression=\"com.xpu.annnotations.controller.UserController\"/&gt;&lt;/context:component-scan&gt; 注意对于那些已经配置了注解的类，想要使自己的过滤器生效，就需要配置一个use-default-filters=&quot;false&quot; 要求不要使用默认的过滤器！ &lt;context:include-filter&gt; 和 &lt;context:exclude-filter&gt; 子节点支持多种类型的过滤表达式：| 类别 | 示例 | 说明 || :——— | :——————– | :———————————————————– || annotation | com.xpu.XxxAnnotation | 所有标注了XxxAnnotation的类，该类型采用目标类型是否标注了某个注解进行过滤 || assinable | com.xpu.XxxService | 继承或者扩展 XxxService的类，该类型采用目标类型是否继承或者拓展某个特定的类进行过滤 || aspectj | com.xpu.Service+ | 所有类名以Service结束及继承或扩展它们的类，该类型采用aspectJ表达式进行过滤 || regex | com.xpu.anno. | 所有com.xpu.anno包下的类，该类型采用正则表达式根据类名进行过滤 || custom | com.xpu.XxxTypeFilter | 采用XxxTypeFilter通过代码的方式定义过滤规则。该类必须实现org.springframework.core.type.TypeFilter接口 | 组件装配&lt;context:component-scan&gt; 元素还会自动注册AutowiredAnnotationBeanPostProcessor 实例， 该实例可以自动装配具有 @Autowired 和 @Resource 、@Inject注解的属性： 1、使用 @Autowired 自动装配Bean@Autowired注解自动装配具有兼容类型的单个 Bean属性 构造器, 普通字段(即使是非 public)，一切具有参数的方法都可以应用@Authwired注解 默认情况下, 所有使用 @Authwired 注解的属性都需要被设置，当 Spring 找不到匹配的 Bean 装配属性时，会抛出异常，若某一属性允许不被设置，可以设置 @Authwired 注解的 required 属性为 false 默认情况下， 当 IOC 容器里存在多个类型兼容的 Bean 时， 通过类型的自动装配将无法工作。 此时可以在 @Qualifier 注解里提供 Bean 的名称。 Spring 允许对方法的入参标注 @Qualifiter 已指定注入 Bean 的名称 @Authwired 注解也可以应用在数组类型的属性上， 此时 Spring 将会把所有匹配的 Bean 进行自动装配。 @Authwired 注解也可以应用在集合属性上，此时 Spring 读取该集合的类型信息, 然后自动装配所有与之兼容的 Bean. @Authwired 注解用在 java.util.Map 上时，若该 Map 的键值为 String，那么 Spring 将自动装配与之 Map 值类型兼容的 Bean， 此时 Bean 的名称作为键值 123456789101112131415@Controller(\"userController\")public class UserController &#123; @Autowired(required=false) private UserService userService; @Autowired public void setUserService(UserService userService) &#123; this.userService = userService; &#125; public void execute() &#123; System.out.println(\"UserController execute()...\"); userService.add(); &#125;&#125; 现在假设IOC容器中没有userService这个Bean，那么Spring会抛出异常，不过给配置一个@Autowired(required=false)即可，如果像这样的UserResponseImpl这样的类没有指定Repository中的名称，于是便可以使用@Qualifier(&quot;userResponseImpl&quot;)注解类中依赖的属性名称，这就好像如果你不自己给自己取个名字，那么就只能用系统给你的默认的名称，下面是示例： 12345678910111213141516@Servicepublic class UserService &#123; @Autowired @Qualifier(\"userResponseImpl\") private UserResponse userResponse; public void add() &#123; System.out.println(\"UserService add()...\"); userResponse.save(); &#125; public UserResponse getUserResponse() &#123; return userResponse; &#125; public void setUserResponse(@Qualifier(\"userResponseImpl\") UserResponse userResponse) &#123; this.userResponse = userResponse; &#125;&#125; Spring 还支持 @Resource 和 @Inject 注解，这两个注解和 @Autowired 注解的功用类似：1、@Resource 注解要求提供一个 Bean 名称的属性，若该属性为空，则自动采用标注处的变量或方法名作为 Bean 的名称2、@Inject 和 @Autowired 注解一样也是按类型匹配注入的 Bean， 但没有 reqired 属性，建议使用 @Autowired 注解 Spring泛型依赖注入Spring 4.x 中可以为子类注入子类对应的泛型类型的成员变量的引用：现在假设有如下类，从UML中也可以看出这几个类的关系： 1&lt;context:component-scan base-package=\"com.xpu.generic\"&gt;&lt;/context:component-scan&gt; 1234567891011121314151617181920public class BaseService&lt;T&gt; &#123; @Autowired protected BasetRepository&lt;T&gt; repository; public void add() &#123; System.out.println(\"add\"); System.out.println(repository); &#125;&#125;@Servicepublic class UserService extends BaseService&lt;User&gt;&#123; &#125;public class Demo&#123; public void test()&#123; ApplicationContext context = new ClassPathXmlApplicationContext(\"beans-generic-di.cml.xml\"); UserService service = (UserService) context.getBean(\"userService\"); service.add(); &#125;&#125; 泛型依赖注入就是允许我们在使用spring进行依赖注入的同时，利用泛型的优点对代码进行精简，将可重复使用的代码全部放到一个类之中，方便以后的维护和修改。同时在不增加代码的情况下增加代码的复用性。Spring 4.0开始支持的泛型依赖注入对于我们使用泛型非常重要，以后还会经常遇到的！","updated":"2020-04-14T03:04:51.728Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaEE","slug":"JavaEE","permalink":"https://zouchanglin.cn/tags/JavaEE/"},{"name":"SpringCore","slug":"SpringCore","permalink":"https://zouchanglin.cn/tags/SpringCore/"}]},{"title":"Hibernate","date":"2019-02-04T11:20:00.000Z","path":"2019/02/04/Hibernate/","text":"持久化即把数据（如内存中的对象）保存到可永久保存的存储设备中（如磁盘）。持久化的主要应用是将内存中的数据存储在关系型的数据库中，当然也可以存储在磁盘文件中、XML数据文件中等等，那么什么是持久层（Persistence Layer），即专注于实现数据持久化应用领域的某个特定系统的一个逻辑层面，将数据使用者和数据实体相关联。 持久化是将程序数据在持久状态和瞬时状态间转换的机制。比如JDBC就是一种持久化机制。文件IO也是一种持久化机制。 广义的理解，”持久化”包括和数据库相关的各种操作：保存：把对象永久保存到数据库中。更新：更新数据库中对象(记录)的状态。删除：从数据库中删除一个对象。查询：根据特定的查询条件，把符合查询条件的一个或多个对象从数据库加载到内存中。加载：根据特定的OID，把一个对象从数据库加载到内存中 ORM对象关系映射 Object Relational Mapping，一种程序技术，用于实现面向对象编程语言里不同类型系统的数据之间的转换。从效果上说，它其实是创建了一个可在编程语言里使用的——“虚拟对象数据库”。| 面向对象概念 | 面向关系概念 || :———– | :————- || 类 | 表 || 对象 | 表的行（记录） || 属性 | 表的列（字段） | ORM的思想：将关系数据库中表中的记录映射成为对象，以对象的形式展现，程序员可以把对数据库的操作转化为对对象的操作。 ORM 采用元数据来描述对象 —— 关系映射细节， 元数据通常采用 XML 格式，并且存放在专门的对象 —— 关系映射文件中. 那么什么是元数据呢？元数据是一种描述数据的数据，任何文件系统中的数据分为数据和元数据。数据是指普通文件中的实际数据，而元数据指用来描述一个文件的特征的系统数据，诸如访问权限、文件拥有者以及文件数据块的分布信息等等，其实在Linux文件系统中元数据就是INode！ hibernatehibernate为应用程序提供了高效的O/R关系映射和查询服务，为面向对象的领域模型到传统的关系型数据库的映射，提供了一个使用方便的框架。 hibernate的Helloworldhibernate这个框架呢我还是从helloworld开始复习，第一步纯粹当做是预习了，首先说一下普通的Java工程中如何使用hibernate框架，Java版本的HelloWorld正式开始！ 首先需要一些jar包（点击这里即可下载），我要说明的是这些jar包的hibernate版本是4.2，如果你需要新的去网上下载就好了！接着我使用的eclipse演示这个额HelloWorld程序，需要安装一个hibernate的插件：在workwith里面输入：http://download.jboss.org/jbosstools/updates/stable/kepler/ 我的项目目录结构就是这样了：hibernate.cfg.xml是通过刚才安装的eclipse的hibernate插件生成的，内容如下(指定关联的 .hbm.xml 文件是后面生成的)： 123456789101112131415161718192021222324252627282930&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE hibernate-configuration PUBLIC \"-//Hibernate/Hibernate Configuration DTD 3.0//EN\" \"http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd\"&gt;&lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;!-- 指定数据库的驱动、用户名、密码 --&gt; &lt;property name=\"hibernate.connection.driver_class\"&gt;com.mysql.jdbc.Driver&lt;/property&gt; &lt;property name=\"hibernate.connection.password\"&gt;1234&lt;/property&gt; &lt;property name=\"hibernate.connection.url\"&gt;jdbc:mysql://127.0.0.1/hibernate4&lt;/property&gt; &lt;property name=\"hibernate.connection.username\"&gt;root&lt;/property&gt; &lt;!-- 重点，这里容易出错，最常使用MySQL5InnoDBDialect 或者 MySQLInnoDBDialect --&gt; &lt;property name=\"hibernate.dialect\"&gt;org.hibernate.dialect.MySQL5InnoDBDialect&lt;/property&gt; &lt;!-- 在执行过程中打印sql语句 --&gt; &lt;property name=\"show_sql\"&gt;true&lt;/property&gt; &lt;!-- 将打印的sql语句格式化--&gt; &lt;property name=\"format_sql\"&gt;true&lt;/property&gt; &lt;!-- 指定自动生成数据表的策略 --&gt; &lt;property name=\"hbm2ddl.auto\"&gt;update&lt;/property&gt; &lt;!-- 设置Hibernate的事务隔离级别 --&gt; &lt;property name=\"connection.isolation\"&gt;2&lt;/property&gt; &lt;!-- 指定关联的 .hbm.xml 文件 --&gt; &lt;mapping resource=\"com/xpu/demo_01/News.hbm.xml\"/&gt; &lt;/session-factory&gt;&lt;/hibernate-configuration&gt; 从上面也可以看出来，hibernate.cfg.xml是hibernate的基本配置信息，在JDBC阶段就已经接触过，我就不在此一一细说了，接下来我们需要一个实体类来对应表，类的对象来对应表的记录，于是此设计如下： 123456789101112131415161718192021222324252627282930package com.xpu.demo_01;import java.util.Date;public class News &#123; private Integer id; private String title; private String author; private Date date; public News() &#123; super(); &#125; //ID我们在数据库中设定的是自增，所以不在构造方法的参数中 public News(String title, String author, Date date) &#123; super(); this.title = title; this.author = author; this.date = date; &#125; public Integer getId() &#123;...&#125; public void setId(Integer id) &#123;...&#125; public String getTitle() &#123;...&#125; public void setTitle(String title) &#123;...&#125; public String getAuthor() &#123;...&#125; public void setAuthor(String author) &#123;...&#125; public Date getDate() &#123;...&#125; public void setDate(Date date) &#123;...&#125; @Override public String toString() &#123;...&#125;&#125; 根据此类生成一个对象关系映射文件，就是News.hbm.xml： 12345678910111213141516171819202122&lt;?xml version=\"1.0\"?&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC \"-//Hibernate/Hibernate Mapping DTD 3.0//EN\"\"http://hibernate.sourceforge.net/hibernate-mapping-3.0.dtd\"&gt;&lt;!-- Generated 2019-1-30 13:27:35 by Hibernate Tools 3.5.0.Final --&gt;&lt;hibernate-mapping&gt; &lt;class name=\"com.xpu.demo_01.News\" table=\"NEWS\"&gt; &lt;id name=\"id\" type=\"java.lang.Integer\"&gt; &lt;!-- 主键自增让数据库原生的去操作 --&gt; &lt;column name=\"ID\" /&gt; &lt;generator class=\"native\" /&gt; &lt;/id&gt; &lt;property name=\"title\" type=\"java.lang.String\"&gt; &lt;column name=\"TITLE\" /&gt; &lt;/property&gt; &lt;property name=\"author\" type=\"java.lang.String\"&gt; &lt;column name=\"AUTHOR\" /&gt; &lt;/property&gt; &lt;property name=\"date\" type=\"java.sql.Date\"&gt; &lt;column name=\"DATE\" /&gt; &lt;/property&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt; 相信这个还是非常容易明白的，ID主键自增让数据库原生的去操作，所以generaor配置的是native，其他的内容标题、作者等都是String类型，时间就是Java的java.util.Date类型，这样就建立了类的属性和表的字段的关系，这很关键！接下来就该使用代码去操作了： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.xpu.demo_01;import java.util.Date;import org.hibernate.Session;import org.hibernate.SessionFactory;import org.hibernate.Transaction;import org.hibernate.cfg.Configuration;import org.hibernate.service.ServiceRegistry;import org.hibernate.service.ServiceRegistryBuilder;public class HibnateTest1 &#123; public void main(String[] args) &#123; // 1、创建配置对象 负责加载配置文件 Configuration configuration = new Configuration().configure(); // 2、创建一个 ServiceRegistry对象,hibernate的任何配置和服务都需要在该对象中注册后才能有效. ServiceRegistry serviceRegistry = new ServiceRegistryBuilder().applySettings(configuration.getProperties()) .buildServiceRegistry(); // 3、创建一个 SessionFactory对象 SessionFactory sessionFactory = configuration.buildSessionFactory(serviceRegistry); // 4、获得Session对象 Session session = sessionFactory.openSession(); // 5、开启事务 Transaction transaction = session.beginTransaction(); // 6、创建对象，通过Session保存相应的数据至数据库 News tabledataEntity = new News(\"Java\", \"Tim\", new Date()); session.save(tabledataEntity); // 6.1获取对象（如果你之前保存过一条数据，那么此时也可以取出） // News news2 = (News)session.get(News.class, 15); // System.out.println(news2); // 7、提交事务 transaction.commit(); // 8、关闭Session session.close(); // 9、关闭SessionFactory sessionFactory.close(); &#125;&#125; 执行完毕后，数据库的表中就会多一条数据，如果失败，可能是由于编码问题导致的，我的eclipse工程配置的都是UTF-8的编码，你可以在项目属性中尝试如下编码配置方式：普通Java工程的hibernate的Helloworld就说完了，API以后再慢慢解释，下面说一下Maven工程下如何搭建这个Helloworld，Maven的依赖如下： 1234567891011121314151617181920&lt;!-- https://mvnrepository.com/artifact/org.hibernate/hibernate-core --&gt;&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-core&lt;/artifactId&gt; &lt;version&gt;4.2.4.Final&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.hibernate/hibernate-c3p0 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-c3p0&lt;/artifactId&gt; &lt;version&gt;4.2.4.Final&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.6&lt;/version&gt;&lt;/dependency&gt; 与普通Java工程不同之处就在于News.hbm.xml文件和hibernate.cfg.xml文件的放置位置，很显然在Maven下，这两个文件都可以放在resource目录下：只不过稍微有点不同的是：在hibernate.cfg.xml中配置News.hbm.xml的位置的时候，直接写名字就好了！","updated":"2020-04-14T03:04:03.847Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaEE","slug":"JavaEE","permalink":"https://zouchanglin.cn/tags/JavaEE/"},{"name":"ORM框架","slug":"ORM框架","permalink":"https://zouchanglin.cn/tags/ORM%E6%A1%86%E6%9E%B6/"}]},{"title":"Spring入门笔记(一)","date":"2019-02-03T23:00:00.000Z","path":"2019/02/04/Spring入门笔记(一)/","text":"Spring框架是由于软件开发的复杂性而创建的。Spring使用的是基本的JavaBean来完成以前只可能由EJB完成的事情。然而，Spring的用途不仅仅限于服务器端的开发。从简单性、可测试性和松耦合性角度而言，绝大部分Java应用都可以从Spring中受益！ Spring简述Spring的一个最大的目的就是使JAVA EE开发更加容易。同时，Spring之所以与Struts、Hibernate等单层框架不同，是因为Spring致力于提供一个以统一的、高效的方式构造整个应用，并且可以将单层框架以最佳的组合揉和在一起建立一个连贯的体系。可以说Spring是一个提供了更完善开发环境的一个框架，可以为POJO(Plain Old Java Object)对象提供企业级的服务 在Spring刚出生的时候，主要的负责两件事情：IOC、AOPIOC（Inversion of Control），即“控制反转”，为什么需要控制反转？谁控制谁？为啥叫做反转？IOC究竟是做什么的？AOP 以后再说，先从IOC容器开始，下图是Spring的几大模块：Spring DAO：Spring提供了对JDBC的操作支持：JdbcTemplate模板工具类 。 Spring ORM：Spring可以与ORM框架整合。例如Spring整合Hibernate框架，其中Spring还提供HibernateDaoSupport工具类，简化了Hibernate的操作 。 Spring WEB：Spring提供了对Struts、Springmvc的支持，支持WEB开发。与此同时Spring自身也提供了基于MVC的解决方案 。 Spring AOP：Spring提供面向切面的编程，可以给某一层提供事务管理，例如在Service层添加事物控制 。 Spring JEE：J2EE开发规范的支持，例如EJB 。 Spring Core：提供IOC容器对象的创建和处理依赖对象关系 。 Spring的helloworldSpring的环境搭建首先需要说明的是：Spring不仅仅是JavaEE才能用的框架，而是SE同样适用的，所以为了方便起见，我直接新建Java工程来完成Spring的HelloWorld，但是Spring需要的jar包，所以我直接使用Maven工程来演示，假设你不用maven或者想直接使用普通的Java工程来完成Spring的HelloWorld，那么点击这里 在这里可以下载到Spring核心包的源码，Jar包以及说明文档。 eclipse如何集成Spring相关的工具包在此不再赘述，我直接使用的是Spring官方集成的工具，也是eclipse，但是你可以叫做STS，下载地址在这里 123456789101112131415161718192021222324252627282930&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.xpu.maven&lt;/groupId&gt; &lt;artifactId&gt;SpringDemo_01&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;5.0.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-context --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.0.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/commons-logging/commons-logging --&gt; &lt;dependency&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 以上是Spring的HelloWorld所需的Jar包，接下来解释一下这些Jar的作用：spring-core是Spring的核心工具包spring-cji是核心包所需的依赖包，spring-aop：Spring的面向切面编程，提供AOP（面向切面编程）的实现spring-beans：Spring IOC的基础实现，包含访问配置文件、创建和管理bean等spring-context：在基础IOC功能上提供扩展服务，此外还提供许多企业级服务的支持，有邮件服务、任务调度、JNDI定位，EJB集成、远程访问、缓存以及多种视图层框架的支持spring-expression：spring表达式语言，就像EL表达式一样的东西commons-logging：这个是只是一个日志包，不用理会 Spring的HelloWorld现有一个JavaBean是Students类： 1234567891011121314151617181920212223package com.xpu.bean;public class Students &#123; private int stuNo; private String stuName; private int stuAge; public Students() &#123; super(); &#125; public Students(int stuNo, String stuName, int stuAge) &#123; super(); this.stuNo = stuNo; this.stuName = stuName; this.stuAge = stuAge; &#125; public int getStuNo() &#123;...&#125; public void setStuNo(int stuNo) &#123;...&#125; public String getStuName() &#123;...&#125; public void setStuName(String stuName) &#123;...&#125; public int getStuAge() &#123;...&#125; public void setStuAge(int stuAge) &#123;...&#125; @Override public String toString() &#123;...&#125;&#125; 如果我们需要拿到这个Students类的对象的话其实很简单： 1Students stu = new Students(1, \"Tim\", 20); 接下来我们使用IOC的方式来获取到Students的对象，在src（如果是maven工程的话直接在resource下新建即可）下新建一个applicationContext.xml文件，如果你已经安装了插件或者直接使用STS的话，会自动生成一些内容： 1234567&lt;!-- 该文件中产生的所有对象，被Spring放入了一个称之为springIOC容器的地方 --&gt;&lt;!-- id：唯一标识符 class：JavaBean的class对象 --&gt;&lt;bean class=\"com.xpu.bean.Students\" id=\"students\"&gt; &lt;property name=\"stuNo\" value=\"1\"&gt;&lt;/property&gt; &lt;property name=\"stuName\" value=\"Tim\"&gt;&lt;/property&gt; &lt;property name=\"stuAge\" value=\"20\"&gt;&lt;/property&gt;&lt;/bean&gt; 在测试类中写如下代码： 12ApplicationContext ac = new ClassPathXmlApplicationContext(\"applicationContext.xml\");Students stu = (Students) ac.getBean(\"students\"); 这样得到的Students对象与自己直接new的对象是一样的，都是Students的对象，但是这样可能看不出Spring框架的作用到底在哪里，怎么反倒变得复杂了？ 从HelloWorld解释IOCIOC 发展史假设有一个课程接口Cource，分别有两个实现类： 1234567891011121314151617public interface ICource &#123; void learn();&#125;class JavaCource implements ICource&#123; @Override public void learn() &#123; System.out.println(\"学习Java\"); &#125;&#125;public class HtmlCource implements ICource&#123; @Override public void learn() &#123; System.out.println(\"学习HTML\"); &#125;&#125; 接下来有一个学生类和简单工厂类： 123456789101112131415public class CourceFactory &#123; public static ICource getCource(String type) &#123; if(\"HtmlCource\".equals(type)) &#123; return new HtmlCource(); &#125;else&#123; return new JavaCource(); &#125; &#125;&#125;public void learn(String str) &#123; //通过工厂的方式获取对象 ICource cource = CourceFactory.getCource(str); cource.learn();&#125; 这样就实现了new操作解耦合，但是本质上还是没变：因为对象还是由客户端创造的，而且这个简单工厂的局限性非常强，只能生产Cource类的对象，如何让这个工厂更强悍呢？或者说如何从根本上解决对象的管理（生产）呢？这个时候Spring诞生了，我们只需要在applicationContext.xml中配置两个具体实例就行了： 12&lt;bean id=\"HtmlCource\" class=\"com.xpu.history.HtmlCource\"&gt;&lt;/bean&gt;&lt;bean id=\"JavaCource\" class=\"com.xpu.history.JavaCource\"&gt;&lt;/bean&gt; 这样的话学生类的learn可以这样写： 12345public void learn(String str) &#123; //通过Spring的IOC容器来获取课程对象 ApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); ((ICource)context.getBean(str)).learn();&#125; 很多人看到这个HelloWorld可能觉得并没有什么深刻的理解，接下来听我详细到来，首先我们要获得Students对象，最开始是我们自己new的Students对象，但是后来我们拿到的对象是是直接从配置文件里定义的对象，我们暂且把这个配置文件理解成IOC容器，IOC为什么叫做”控制反转”，IOC不是什么技术，而是一种设计思想。在Java开发中，Ioc意味着将你设计好的对象交给容器控制，而不是传统的在你的对象内部直接控制。如何理解好Ioc呢？理解好Ioc的关键是要明确“谁控制谁，控制什么，为何是反转（有反转就应该有正转了），哪些方面反转了”，那我们来深入分析一下： 谁控制谁，控制什么：传统Java SE程序设计，我们直接在对象内部通过new进行创建对象，是程序主动去创建依赖对象；而IoC是有专门一个容器来创建这些对象，即由Ioc容器来控制对 象的创建；谁控制谁？当然是IoC 容器控制了对象；控制什么？那就是主要控制了外部资源获取（不只是对象包括比如文件等）。 为何是反转，哪些方面反转了：有反转就有正转，传统应用程序是由我们自己在对象中主动控制去直接获取依赖对象，也就是正转；而反转则是由容器来帮忙创建及注入依赖对象；为何是反转？因为由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。 用图例说明一下，传统程序设计和IOC设计，都是主动去创建相关对象然后再组合起来：IoC对编程带来的最大改变不是从代码上，而是从思想上，发生了“主从换位”的变化。应用程序原本是老大，要获取什么资源都是主动出击，但是在IoC/DI思想中，应用程序就变成被动的了，被动的等待IoC容器来创建并注入它所需要的资源了。 IoC很好的体现了面向对象设计法则之一就是好莱坞法则：“别找我们，我们找你”；即由IoC容器帮对象找相应的依赖对象并注入，而不是由对象主动去找。这一段摘自：《谈谈对Spring IOC的理解》 IOC与DIIOC在被改名之前叫做控制反转，但是改名之后叫做依赖注入，改名之后似乎更加好理解了一点，不过有一句话还是经典：控制反转是目的、依赖注入是手段，它提供一种机制，将需要依赖（低层模块）对象的引用传递给被依赖（高层模块）对象，下面我们用一个示例来体现依赖注入，注入就不再解释，在上述例子中你可以理解为把对应的元素值注入到基础属性中，再把基础属性注入到对象中，于是你获得了xml文件中配置的对象，那么什么是依赖呢？ 依赖注入底层是根据反射来设计的！ 假设现在有一个Teacher类： 123456789101112public class Teacher &#123; private String teaName; private int teaAge; public String getTeaName() &#123;...&#125; public void setTeaName(String teaName) &#123;...&#125; public int getTeaAge() &#123;...&#125; public void setTeaAge(int teaAge) &#123;...&#125; public Teacher() &#123;...&#125; public Teacher(String teaName, int teaAge) &#123;...&#125; @Override public String toString() &#123;...&#125;&#125; 又有一个课程类，很显然课程是依赖于老师的，所以设计如下： 123456789101112131415public class Cource &#123; private String courseName; private int courseHour; private Teacher teacher;//授课老师 public Cource() &#123; &#125; public Cource(String courseName, int courseHour, Teacher teacher) &#123;...&#125; public String getCourseName() &#123;...&#125; public void setCourseName(String courseName) &#123;...&#125; public int getCourseHour() &#123;...&#125; public void setCourseHour(int courseHour) &#123;...&#125; public Teacher getTeacher() &#123;...&#125; public void setTeacher(Teacher teacher) &#123;...&#125; @Override public String toString() &#123;...&#125;&#125; 在配置文件中应该这样写： 12345678910&lt;bean id=\"Teacher\" class=\"com.xpu.ioc.Teacher\"&gt; &lt;property name=\"teaName\" value=\"Tim\"&gt;&lt;/property&gt; &lt;property name=\"teaAge\" value=\"35\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id=\"Cource\" class=\"com.xpu.ioc.Cource\"&gt; &lt;property name=\"courseName\" value=\"Java\"&gt;&lt;/property&gt; &lt;property name=\"courseHour\" value=\"2\"&gt;&lt;/property&gt; &lt;property name=\"teacher\" ref=\"Teacher\"&gt;&lt;/property&gt;&lt;/bean&gt; 这样便完成了依赖，Cource类依赖于Teacher类，所以我们在配置文件中配置ref属性为Teacher，在通过IOC容器拿到Cource类的对象的时候便会自动生成Teacher类的对象，完成Cource类对象的依赖对象！Spring的依赖注入主要有四中方式：set注入方式、静态工厂注入方式、构造方法注入方式、基于注解的方式： 12345678910&lt;bean id=\"Cource\" class=\"com.xpu.ioc.Cource&gt; &lt;!-- 使用set方法赋值 --&gt; &lt;property name=\"courseName\" value=\"Java\"&gt;&lt;/property&gt; &lt;property name=\"courseHour\" value=\"2\"&gt;&lt;/property&gt; &lt;property name=\"teacher\" ref=\"Teacher\"&gt;&lt;/property&gt; &lt;!-- 通过构造方法赋值(name、type、index等是可选项) --&gt; &lt;constructor-arg value=\"JavaScript\" name=\"courseName\"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg value=\"3\" type=\"int\"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg ref=\"Teacher\"&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 通过构造方法赋值，如果与构造方法的参数顺序不一致，则要通过type或者index或name指定，通过命名空间注入，引入P命名空间： 1xmlns:p=\"http://www.springframework.org/schema/p\" 12&lt;bean id=\"Teacher\" class=\"com.xpu.ioc.Teacher\" p:teaName=\"Tom\" p:teaAge=\"23\"&gt;&lt;bean id=\"Cource\" class=\"com.xpu.ioc.Cource\" p:courseName=\"C++\" p:courseHour=\"5\" p:teacher-ref=\"Teacher\"&gt; 示例：注入各种集合类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.xpu.collection;import java.util.Arrays;import java.util.List;import java.util.Map;import java.util.Properties;import java.util.Set;public class AllCollectionType &#123; private List&lt;String&gt; listElement; private String[] arrayElement; private Set&lt;String&gt; setElement; private Map&lt;String, String&gt; mapElement; private Properties propsElement; public List&lt;String&gt; getListElement() &#123; return listElement; &#125; public void setListElement(List&lt;String&gt; listElement) &#123; this.listElement = listElement; &#125; public String[] getArrayElement() &#123; return arrayElement; &#125; public void setArrayElement(String[] arrayElement) &#123; this.arrayElement = arrayElement; &#125; public Set&lt;String&gt; getSetElement() &#123; return setElement; &#125; public void setSetElement(Set&lt;String&gt; setElement) &#123; this.setElement = setElement; &#125; public Map&lt;String, String&gt; getMapElement() &#123; return mapElement; &#125; public void setMapElement(Map&lt;String, String&gt; mapElement) &#123; this.mapElement = mapElement; &#125; public Properties getPropsElement() &#123; return propsElement; &#125; public void setPropsElement(Properties propsElement) &#123; this.propsElement = propsElement; &#125; @Override public String toString() &#123; return \"AllCollectionType [listElement=\" + listElement + \"\\narrayElement=\" + Arrays.toString(arrayElement) + \"\\nsetElement=\" + setElement + \"\\nmapElement=\" + mapElement + \"\\npropsElement=\" + propsElement + \"]\"; &#125;&#125; set、list、数组、都有自己的标签，但是list可以混用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;!-- 复杂类型的赋值的写法 --&gt;&lt;bean id=\"collection\" class=\"com.xpu.collection.AllCollectionType\"&gt; &lt;property name=\"listElement\"&gt; &lt;list&gt; &lt;value&gt;足球&lt;/value&gt; &lt;value&gt;排球&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=\"arrayElement\"&gt; &lt;array&gt; &lt;value&gt;111&lt;/value&gt; &lt;value&gt;222&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;property name=\"setElement\"&gt; &lt;set&gt; &lt;value&gt;足球&lt;/value&gt; &lt;value&gt;排球&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;property name=\"mapElement\"&gt; &lt;map&gt; &lt;entry&gt; &lt;key&gt; &lt;value&gt;foot&lt;/value&gt; &lt;/key&gt; &lt;value&gt;足球&lt;/value&gt; &lt;/entry&gt; &lt;entry&gt; &lt;key&gt; &lt;value&gt;basket&lt;/value&gt; &lt;/key&gt; &lt;value&gt;篮球&lt;/value&gt; &lt;/entry&gt; &lt;entry&gt; &lt;key&gt; &lt;value&gt;hand&lt;/value&gt; &lt;/key&gt; &lt;value&gt;排球&lt;/value&gt; &lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;property name=\"propsElement\"&gt; &lt;props&gt; &lt;prop key=\"foot\"&gt;足球&lt;/prop&gt; &lt;prop key=\"hand\"&gt;排球&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt; value=&quot;&quot;方式与&lt;value&gt;&lt;/value&gt;方式的区别： 使用子元素&lt;value&gt;注入 使用value属性注入 参数值位置 写在首尾标签&lt;value&gt;&lt;/value&gt;之间，不加双引号 写在value属性中的至必须加上双引号 type属性 有（可选），可以通过type指定数据类型 无，但是在构造注入的时候可以使用type 参数值包含的特殊字符 两种处理方式：1、使用&lt;![CDATA{ }]&gt;标记；2、使用XML预定义的实体引用 一种处理方式：使用XML预定义的实体引用 给属性赋空值需要如下写法： 1234&lt;!-- 赋值为null --&gt;&lt;property name=\"name\"&gt; &lt;/null&gt; &lt;/property&gt;&lt;!-- 赋值为\"\" --&gt;&lt;property name=\"name\"&gt; &lt;value&gt;&lt;/value&gt; &lt;/property&gt; 在IOC容器定义bean，必须定义无参构造 IOC自动装配（只适用于引用类型，基本类型无法装配），这种方式必须满足：在当前的IOC容器中只能有一个Bean是满足要求的： 12345&lt;!-- Course类中有一个ref属性，属性名为teacher，并且该IOC容器中恰好有一个id也是teacher --&gt;&lt;bean id=\"Cource\" class=\"com.xpu.ioc.Cource\" autowire=\"byName\"&gt; &lt;property name=\"courseName\" value=\"Java\"&gt; &lt;/property&gt; &lt;property name=\"courseHour\" value=\"5\"&gt;&lt;/property&gt;&lt;/bean&gt; byName：byName是自动寻找其他JavaBean的id值与该类的ref属性名一致的 byType：上面autowire的属性可以是byType,byType会自动寻找其他JavaBean的类型与该类的ref属性类型一致的 constructor：constructor会看其他的JavaBean是否与该类的构造方法参数的类型一致 在根标签设置所有类的自动装配方式： 1default-autowire=\"byName\" 但是字标签的优先级更高，所以灵活度很大，接下来说说自动装配的缺点： 可读性差，容易出错，不建议经常使用 使用注解定义Bean：通过注解的方式将Bean以及相应的属性值放入IOC容器 123//&lt;bean id=\"StudentDaoImpl\" class=\"com.xpu.dao.StudentDaoImpl\"&gt;&lt;/bean&gt;@Component(\"StudentDaoImpl\")public class StudentDaoImpl &#123; ... &#125; 在配置文件中配置： 1234&lt;!-- 先加入context --&gt;xmlns:context=\"http://www.springframework.org/schema/context\"&lt;!-- 配置扫描器，多个包使用\"，\"隔开 --&gt;&lt;context:component-scan base-package=\"com.xpu.dao\"&gt;&lt;/context:component-scan&gt; 在启动的时候，扫描器会扫描在配置中的包，而且含有@Component注解的类，如果有则将Bean加入到IOC容器中 @Component的细化，在不知道使用那一层的情况下就用@Component注解： 使用场景 注解 dao层 @Repository service层 @Service 控制器层(MVC) @Controller 结语如果文中有错误，希望大家能够及时指出，我的邮箱是：zouchanglin@stu.xpu.cn，对了，今天是除夕，晒晒家里的年夜饭吧：","updated":"2020-04-25T01:10:08.199Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaEE","slug":"JavaEE","permalink":"https://zouchanglin.cn/tags/JavaEE/"},{"name":"SpringCore","slug":"SpringCore","permalink":"https://zouchanglin.cn/tags/SpringCore/"}]},{"title":"MyBatis（二）","date":"2019-02-03T11:20:00.000Z","path":"2019/02/03/MyBatis（二）/","text":"作用域（Scope）和生命周期SqlSessionFactoryBuilder这个类可以被实例化、使用和丢弃，一旦创建了 SqlSessionFactory，就不再需要它了。因此 SqlSessionFactoryBuilder 实例的最佳作用域是方法作用域（也就是局部方法变量）。你可以重用 SqlSessionFactoryBuilder 来创建多个 SqlSessionFactory 实例，但是最好还是不要让其一直存在以保证所有的 XML 解析资源开放给更重要的事情。 SqlSessionFactory 一旦被创建就应该在应用的运行期间一直存在，没有任何理由对它进行清除或重建。使用 SqlSessionFactory 的最佳实践是在应用运行期间不要重复创建多次，多次重建 SqlSessionFactory 被视为一种代码“坏味道（bad smell）”。因此 SqlSessionFactory 的最佳作用域是应用作用域。有很多方法可以做到，最简单的就是使用单例模式或者静态单例模式。 每个线程都应该有它自己的 SqlSession 实例。SqlSession 的实例不是线程安全的，因此是不能被共享的，所以它的最佳的作用域是请求或方法作用域。绝对不能将 SqlSession 实例的引用放在一个类的静态域，甚至一个类的实例变量也不行。也绝不能将 SqlSession 实例的引用放在任何类型的管理作用域中，比如 Servlet 架构中的 HttpSession。如果你现在正在使用一种 Web 框架，要考虑 SqlSession 放在一个和 HTTP 请求对象相似的作用域中。换句话说，每次收到的 HTTP 请求，就可以打开一个 SqlSession，返回一个响应，就关闭它。这个关闭操作是很重要的，你应该把这个关闭操作放到 finally 块中以确保每次都能执行关闭。下面的示例就是一个确保 SqlSession 关闭的标准模式： 123456SqlSession session = sqlSessionFactory.openSession();try &#123; // do work&#125; finally &#123; session.close();&#125; 所以归纳一下：| 类名称 | SCOPE || :———————– | :———————————- || SqlSessionFactoryBuilder | method || SqlSessionFactory | application || SqlSession | request/method （可以认为是线程级） | Result MapsresultMap 元素是 MyBatis 中最重要最强大的元素。它可以让你从 90% 的 JDBC ResultSets 数据提取代码中解放出来, 并在一些情形下允许你做一些 JDBC 不支持的事情。 实际上，在对复杂语句进行联合映射的时候，它很可能可以代替数千行的同等功能的代码。 ResultMap 的设计思想是，简单的语句不需要明确的结果映射，而复杂一点的语句只需要描述它们的关系就行了。 下面的示例基于《MyBatis（一）》，但是在数据库层面稍有修改，新增department表代表部门和部门id，employee表新增d_id，意思是员工的部门编号：现在假设我们根据employee表设计出来的JavaBean是这样的： 1234567891011121314151617181920212223242526272829303132333435package com.xpu.bean;import java.io.Serializable;public class Employee&#123; private Integer e_id; private String e_name; private char e_gender; private String e_address; private Department dept; public Department getDept() &#123;.&#125; public void setDept(Department dept) &#123;.&#125; public Employee() &#123; super(); &#125; public Employee(Integer e_id, String e_name, char e_gender, String e_address) &#123; super(); this.e_id = e_id; this.e_name = e_name; this.e_gender = e_gender; this.e_address = e_address; &#125; public Integer getE_id() &#123;...&#125; public void setE_id(Integer e_id) &#123;...&#125; public String getE_name() &#123;...&#125; public void setE_name(String e_name) &#123;...&#125; public char getE_gender() &#123;...&#125; public void setE_gender(char e_gender) &#123;...&#125; public String getE_address() &#123;...&#125; public void setE_address(String e_address) &#123;...&#125; @Override public String toString() &#123;...&#125;&#125; 很明显，我们设计的JavaBean的属性名称和employee表的列并不是对应的，此时我们可以通过在SQL语句给列名起别名的方式来保证正确性，但是一般不着这样做，而是从配置文件下手，应该如何写配置文件呢？ 同样这个示例也是一个多表查询的例子： 123456789101112131415161718&lt;resultMap type=\"com.xpu.bean.Employee\" id=\"resultMap_01\"&gt; &lt;id column=\"id\" property=\"e_id\" /&gt; &lt;result column=\"name\" property=\"e_name\" /&gt; &lt;result column=\"gender\" property=\"e_gender\" /&gt; &lt;result column=\"address\" property=\"e_address\" /&gt; &lt;result column=\"d_id\" property=\"dept.d_id\"/&gt; &lt;result column=\"d_name\" property=\"dept.d_name\"/&gt;&lt;/resultMap&gt; &lt;!-- 多表查询 --&gt;&lt;select id=\"selectAll\" resultMap=\"resultMap_01\"&gt; select e.id id, e.name name, e.gender gender, e.address address, d.d_id d_id, d.d_name d_name from employee e, department d where e.d_id = d.d_id &lt;/select&gt; 在上述配置文件中，resultMap的id必须和上面配置的id一致，这样就解决了即使JavaBean和表的字段不匹配也可以查询到数据了！还有需要注意的是：resultMap 中的id是主键，需要使用id标签，其他属性或字段都是result标签，在IEmployee.java中写法和以前一致，看看调用测试： 12345678910111213public void fun()&#123; Reader reader = Resources.getResourceAsReader(\"resource.xml\"); SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(reader); SqlSession session = factory.openSession(true); IEmployeeDao mapper = session.getMapper(IEmployeeDao.class); List&lt;Employee&gt; list = mapper.selectAll2(); for (Employee employee : list) &#123; System.out.println(employee); &#125;&#125; 这样就完成了多表查询，而且是一次性完成，下面看看如何分步查询？ 分步查询与延迟加载1234567891011121314151617&lt;!-- association 可以使用分步骤查询 --&gt;&lt;resultMap type=\"com.xpu.bean.Employee\" id=\"resultMap_02\"&gt; &lt;id column=\"id\" property=\"e_id\" /&gt; &lt;result column=\"name\" property=\"e_name\" /&gt; &lt;result column=\"gender\" property=\"e_gender\" /&gt; &lt;result column=\"address\" property=\"e_address\" /&gt; &lt;association property=\"dept\" javaType=\"com.xpu.bean.Department\"&gt; &lt;id column=\"d_id\" property=\"d_id\"/&gt; &lt;result column=\"d_name\" property=\"d_name\"/&gt; &lt;/association&gt;&lt;/resultMap&gt;&lt;select id=\"selectAll2\" resultMap=\"resultMap_02\"&gt; select e.id id, e.name name, e.gender gender, e.address address, d.d_id d_id, d.d_name d_name from employee e, department d where e.d_id = d.d_id &lt;/select&gt; 通过association 标签制定一个分步查询的下一个步骤，其实总体上来说就是先根据Id找到员工employee，再根据employee的d_id字段查找到对应的部门，所以这个是分步骤的多表查询：Department.xml 和IDpartmentDao.java： 12345&lt;mapper namespace=\"com.xpu.dao.IDepartmentDao\"&gt; &lt;select id=\"getDepartmentById\" resultType=\"com.xpu.bean.Department\"&gt; select * from department where d_id = #&#123;d_id&#125; &lt;/select&gt;&lt;/mapper&gt; 123public interface IDepartmentDao &#123; public Department getDepartmentById(String id);&#125; 为什么要分步骤查询呢？假设电商平台，查询订单信息时需要查询部分的用户信息；相对于关联查询来说，分步查询将查询SQL拆分，这里引申出一个问题是：分步查询与关联表查询的不同 关联表查询能够有效的简化代码编写逻辑，减小代码编写难度，同时尽量避免出错，而分步查询则能够增强代码的可用性，关联表查询毕竟只需要查询一次数据库，对于业务量较小的系统来说，效率更高，数据库压 力相对较小；分步查询虽然需要多次查询数据，但是这也意味着能够更好地使用数据缓存服务，且缓存的数据耦合度低，利用率高，而且单次查询效率很高，数据库压力较小（对于业务量较大的系统来说）。还有一点则是数据库锁的问题，毕竟关联查询是多表同时使用，分步查询每次只操作一个表！ 分步骤查询很多时候是配合延迟加载使用，下面是一个示例： 123456789101112&lt;resultMap type=\"com.xpu.bean.Employee\" id=\"selectByStep\"&gt; &lt;id column=\"id\" property=\"e_id\" /&gt; &lt;result column=\"name\" property=\"e_name\" /&gt; &lt;result column=\"gender\" property=\"e_gender\" /&gt; &lt;result column=\"address\" property=\"e_address\" /&gt; &lt;association property=\"dept\" select=\"com.xpu.dao.IDepartmentDao.getDepartmentById\" column=\"d_id\"&gt; &lt;/association&gt;&lt;/resultMap&gt;&lt;select id=\"selectStep\" resultMap=\"selectByStep\"&gt; select * from employee where id=#&#123;id&#125;&lt;/select&gt; 当在某项业务里需要同时获取两份数据，但是其中份数据又不需要立即使用，当程序需要加载另一份数据时，再去请求数据库来获取那一份数据，而不是一次性将数据全部取出来或者重新发送一份请求，这就是延迟加载。为了节省内存的消耗！ 想要使用懒加载，有些配置是默认的，但是由于版本在更替，有些默认设置还是自己指明比较好，如果想单个开启或禁用延迟加载，可以使用fetchType属性来实现，设置延迟加载需要在总体配置文件中配置如下： 123&lt;!-- 防止版本更新发生改变 --&gt;&lt;setting name=\"lazyLoadingEnabled\" value=\"true\"/&gt;&lt;setting name=\"aggressiveLazyLoading\" value=\"false\"/&gt; 这个是测试懒加载的代码，需要用的时候再加载： 123456Employee step = mapper.selectStep(\"1\");// 这个时候还没有用到d_id、等到用到的时候再加载，节省内存开销System.out.println(step.getE_name());//执行下面这句代码的时候才会去查询数据库获得相应的数据System.out.println(step.getDept().getD_name()); MyBatis的缓存Mybatis中有一级缓存和二级缓存，默认情况下一级缓存是开启的，而且是不能关闭的。一级缓存是指SqlSession级别的缓存，当在同一个SqlSession中进行相同的SQL语句查询时，第二次以后的查询不会从数据库查询，而是直接从缓存中获取，一级缓存最多缓存1024条SQL。二级缓存是指可以跨SqlSession的缓存，Mybatis中进行SQL查询是通过org.apache.ibatis.executor.Executor接口进行的，总体来讲，它一共有两类实现，一类是BaseExecutor，一类是CachingExecutor。前者是非启用二级缓存时使用的，而后者是采用的装饰器模式，在启用了二级缓存时使用，当二级缓存没有命中时，底层还是通过BaseExecutor来实现的： 一、一级缓存的有效期是多久呢？ 1、当不是同一个SqlSession对象的时候，s1对象的缓存只能s1用2、当Sqlsession对象调用clearCache()的时候，缓存清除3、当SqlSession对象close的时候4、当在两次查询中穿插了add、delete、update的时候，缓存就无效了 二、一级缓存如何实现呢？ 1、一级缓存通过简单地Map来实现的，没有对Map集合大小的限制2、一级缓存是一个粗粒度的缓存，没有办法去精确的控制数据是否过长，以及更新数据3、多个SqlSession对象缓存的数据无法共享，为了解决这个问题就需要二级缓存二级缓存属于nameSpace级别（一个xml文件对应一个二级缓存），在一个Web应用中，你可以理解为只有一个二级缓存！二级缓存并没有默认开启，那么如何开启二级缓存呢？通样在全局配置文件中配置如下： 123&lt;settings&gt; &lt;setting name=\"cacheEnabled\" value=\"true\"/&gt;&lt;/settings&gt; 注意配置的顺序： 在前面的一定要将标签配置在前面，否则是无法解析导致出错的！我们要想使用二级缓存，是需要在对应的xml文件中定义其中的查询语句需要使用哪个cache来缓存数据的。这有两种方式可以定义，一种是通过cache元素定义，一种是通过cache-ref元素来定义。但是需要注意的是对于同一个Mapper来讲，它只能使用一个Cache，当同时使用了和时使用定义的优先级更高。Mapper使用的Cache是与我们的Mapper对应的namespace绑定的，一个namespace最多只会有一个Cache与其绑定，所以我配置了一个 useCache=&quot;true&quot; 12345&lt;!-- 多表查询 --&gt;&lt;select id=\"selectAll\" resultMap=\"resultMap_01\" useCache=\"true\"&gt; select e.id id, e.name name, e.gender gender, e.address address, d.d_id d_id, d.d_name d_name from employee e, department d where e.d_id = d.d_id &lt;/select&gt; 还有一个问题，在使用缓存的时候需要引入一些Jar包，除了cglib别忘记其他的jar包，并且JavaBean要实现序列化接口：好了，MyBatis学习至此就算是入了个门，重点还是以后在实际开发中的应用，以后遇到了问题再慢慢总结（下面的图纯粹是写完娱乐一下，哈哈），如果文中有错误还望及时指出我不胜感激！","updated":"2020-04-14T03:04:18.622Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaEE","slug":"JavaEE","permalink":"https://zouchanglin.cn/tags/JavaEE/"},{"name":"ORM框架","slug":"ORM框架","permalink":"https://zouchanglin.cn/tags/ORM%E6%A1%86%E6%9E%B6/"}]},{"title":"MyBatis（一）","date":"2019-02-02T23:20:00.000Z","path":"2019/02/03/MyBatis（一）/","text":"MyBatis简介首先说一下MyBatis是什么？MyBatis就是下图中的鸟，哈哈！MyBatis是一款优秀的持久层框架，它支持定制化SQL、存储过程以及高级映射。MyBatis避免了几乎所有的JDBC代码和手动设首参数以及获取结果集。MyBatis可以使用简单的XML或注解来配罝和映射原生信息，将接口和Java的POJOs(Plain Old Java Objects,普通的Java对象)映射成数据库中的记录 MyBatis和Hibernate一样，是一个ROM框架（Object Relational Mapping，是对象到关系的映射，是一种解决实体对象与关系型数据库相互匹配的技术，它的实现思想就是将数据库中数据表映射成为对象，对关系型数据以对象的形式进行操作。在软件开发中，对象和关系数据是业务实体的两种表现形式，ORM通过使用描述对象和数据库之间映射的元数据，将对象自动持久化到关系数据库中。 MyBatis相对于Hibernate来说更加轻量级，所以MyBatis其实不具备像Hibernate那样自动建表的功能，但是MyBatis现在对我来说足够用了，现在开始记录一下学习MyBatis过程中遇到的问题或者MyBatis的知识点！ 从HelloWorld开始下面从MyBatis的helloworld说起，如何开始学习MyBatis：首先是MyBatis的官方网站，这里具有最齐全的MyBatis文档，关键是居然有中文版的！？！？http://www.mybatis.org/mybatis-3/zh/index.html另外，这个文档有PDF版本的，可以在网上下载到，如果你希望使用离线版的文档去下载这个PDF即可！ 这个是MyBatis的maven依赖，如果没学过maven也不要紧，可以参考我的一片博客《一文读懂Maven》，题目有点夸张，大家将就着看吧！如果你不希望使用maven的话直接导入jar包就好了，MyBatis只有一个jar包（但是不要忘记mysql驱动程序的jar包）！ 12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;x.x.x&lt;/version&gt;&lt;/dependency&gt; 我就演示一下非maven版本的HelloWorld，首先准备一个数据库和表，如图：这是我整个工程的目录结构：resource.xml 12345678910111213141516171819202122&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE configurationPUBLIC \"-//mybatis.org//DTD Config 3.0//EN\"\"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;transactionManager type=\"JDBC\" /&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"com.mysql.jdbc.Driver\" /&gt; &lt;property name=\"url\" value=\"jdbc:mysql://127.0.0.1:3306/mybatis\" /&gt; &lt;property name=\"username\" value=\"root\" /&gt; &lt;property name=\"password\" value=\"1234\" /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource=\"com/xpu/bean/Employee.xml\" /&gt; &lt;/mappers&gt; &lt;/configuration&gt; dataSource 标签配置的数据库连接的基本信息，mappers标签配置的是对象映射关系文件，说一下关于引入dtd约束文档的问题，如果未配置dtd约束文档，那么eclipse是没有提示的，你可以在Window—&gt;Preference–&gt;XML—&gt;XMLCatalog里面配置dtd，点击Add，选择URL，填入dtd的URL，选择FileSystem点击打开下载好的dtd文件即可！接着创建一个JavaBean： 12345678910111213141516171819202122232425262728package com.xpu.bean;public class Employee &#123; private Integer id; private String name; private char gender; private String address; public Employee() &#123; super(); &#125; public Employee(Integer id, String name, char gender, String address) &#123; super(); this.id = id; this.name = name; this.gender = gender; this.address = address; &#125; public Integer getId() &#123;...&#125; public void setId(Integer id) &#123;...&#125; public String getName() &#123;...&#125; public void setName(String name) &#123;...&#125; public char getGender() &#123;...&#125; public void setGender(char gender) &#123;...&#125; public String getAddress() &#123;...&#125; public void setAddress(String address) &#123;...&#125; @Override public String toString() &#123;...&#125;&#125; 创建关系映射文件employee.xml，注意namespace必须和JavaBean一一对应，sql查询语句不要写错，id是要传入的一个参数： 123456789&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapperPUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\"\"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.xpu.bean.Employee\"&gt; &lt;select id=\"selOne\" parameterType=\"java.lang.String\" resultType=\"com.xpu.bean.Employee\"&gt; select * from employee where id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; Test.java 123456789101112131415161718192021222324252627package com.xpu.bean;import java.io.IOException;import java.io.Reader;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;public class Test &#123; public static void main(String[] args) throws IOException &#123; //通过resource.xml中的配置信息获取一个SqlSessionFactory对象 Reader reader = Resources.getResourceAsReader(\"resource.xml\"); SqlSessionFactory sqlSessionFty = new SqlSessionFactoryBuilder().build(reader); //开始一个会话 SqlSession sqlSession = sqlSessionFty.openSession(); //执行查询方法 Employee one = sqlSession.selectOne(\"com.xpu.bean.Employee.selOne\", \"1\"); System.out.println(one); //关闭会话 sqlSession.close(); &#125;&#125; 这便查询到了ID为1的用户，HelloWorld的演示到此结束！ MyBatis的开发模式1、配置文件resource.xml 123&lt;mappers&gt; &lt;mapper resource=\"com/xpu/bean/Employee.xml\" /&gt;&lt;/mappers&gt; Employee.xml 12345&lt;mapper namespace=\"com.xpu.bean.Employee\"&gt; &lt;select id=\"selOne\" parameterType=\"java.lang.String\" resultType=\"com.xpu.bean.Employee\"&gt; select * from employee where id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 这样的方式就是我的HelloWorld中所用到的方式，Employee.xml的namespace是我的JavaBean，执行查询的时候需要使用如下方式，selOne是select标签的id，parameterType代表参数的类型，resultType代表返回值的类型，中间是sql查询语句！ 12//执行查询方法Employee one = sqlSession.selectOne(\"com.xpu.bean.Employee.selOne\", \"1\"); 2、接口IEmployeeDao.java 12345public interface IEmployeeDao &#123; //查询所有记录 @Select(\"select * from employee\") public List&lt;Employee&gt; getAll();&#125; resource.xml 123&lt;mappers&gt; &lt;mapper class=\"com.xpu.dao.IEmployeeDao\"/&gt;&lt;/mappers&gt; 使用该模式测试： 12345678910111213141516171819202122232425262728package com.xpu.dao;import java.io.IOException;import java.io.Reader;import java.util.List;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import com.xpu.bean.Employee;public class Test &#123; public static void main(String[] args) throws IOException &#123; Reader reader = Resources.getResourceAsReader(\"resource.xml\"); SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(reader); //true表示是否自动提交，默认值是false SqlSession session = factory.openSession(true); IEmployeeDao mapper = session.getMapper(IEmployeeDao.class); List&lt;Employee&gt; all = mapper.getAll(); for (Employee employee : all) &#123; System.out.println(employee); &#125; &#125;&#125; 3、接口+配置文件这种模式是最常用的模式：Employee.xml: 123456789&lt;mapper namespace=\"com.xpu.dao.IEmployeeDao\"&gt; &lt;select id=\"getAll\" resultType=\"com.xpu.bean.Employee\"&gt; select * from employee &lt;/select&gt; &lt;select id=\"getOne\" resultType=\"com.xpu.bean.Employee\"&gt; select * from employee where id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; resource.xml 123&lt;mappers&gt; &lt;mapper resource=\"com/xpu/bean/Employee.xml\" /&gt;&lt;/mappers&gt; IEmployeeDao.java 1234567package com.xpu.dao;public interface IEmployeeDao &#123; public List&lt;Employee&gt; getAll(); public Employee getOne(String id);&#125; 以上就是MyBatis的三种开发模式，第三种用的比较多，第一种局限性较大，第二种耦合度太强，开发本来的原则就是高内聚、低耦合，使用第二种方式反倒是的耦合度增强不宜使用，所以常用的方式就是第三种开发模式！ 下面对需要注意的地方总结一下，参数类型的配置要写全路径名称，避免出错，使用第二种开发模式的时候，需要将接口配置成class，定义返回值类型的时候注意，虽然getAll返回的是一个List集合，但是集合中装的还是对象，所以我们在配置返回值的时候还是需要配置成Employee类: 1&lt;mapper class=\"com.xpu.dao.IEmployeeDao\"/&gt; 最后说一个比较重要的问题，为什么我们通过IEmployeeDao.class对象得到的mapper对象是一个实例呢？我们并没有实现接口呀？没有实现接口的话为什么可以拿到他的实现类对象呢？ 这是其实是个代理对象，是通过动态代理来实现的，具体参考Proxy类，看看如何通过动态代理生成代理对象，具体可以参考《Mybatis是如何通过mapper接口生成代理对象的》 MyBatis的CURDMyBatis最核心的作用还是增删改查嘛，接下来我通过第三种开发模式总结一下MyBatis的CURD，看看Employee.xml中的SQL语句: 1234567891011121314151617181920212223242526&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapperPUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\"\"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.xpu.dao.EmployeeDao\"&gt; &lt;select id=\"selAll\" resultType=\"com.xpu.bean.Employee\"&gt; select * from employee &lt;/select&gt; &lt;select id=\"selOne\" parameterType=\"java.lang.String\" resultType=\"com.xpu.bean.Employee\"&gt; select * from employee where id = #&#123;id&#125; &lt;/select&gt; &lt;insert id=\"insertEmp\" parameterType=\"com.xpu.bean.Employee\"&gt; insert into employee values(#&#123;id&#125;, #&#123;name&#125;, #&#123;gender&#125;, #&#123;address&#125;) &lt;/insert&gt; &lt;update id=\"updateEmp\" parameterType=\"com.xpu.bean.Employee\"&gt; update employee set name = #&#123;name&#125;, address = #&#123;address&#125; where id = #&#123;id&#125; &lt;/update&gt; &lt;delete id=\"daleteEmp\"&gt; delete from employee where id = #&#123;id&#125; &lt;/delete&gt;&lt;/mapper&gt; IEmployeeDao.java 12345678910111213141516public interface EmployeeDao &#123; //查询所有 public List&lt;Employee&gt; selAll(); //查询单一 public Employee selOne(String id); //新增 public void insertEmp(Employee emp); //修改 public void updateEmp(Employee emp); //删除 public void daleteEmp(String id);&#125; 测试代码： 123456789101112131415public class Demo &#123; public static void main(String[] args) throws IOException&#123; Reader reader = Resources.getResourceAsReader(\"resource.xml\"); SqlSessionFactory sessionFactory = new SqlSessionFactoryBuilder().build(reader); SqlSession session = sessionFactory.openSession(); IEmployeeDao mapper = session.getMapper(IEmployeeDao.class); mapper.insertEmp(new Employee(5, \"Lucy\", '1', \"Lucy@163.com\")); mapper.daleteEmp(\"4\"); mapper.updateEmp(new Employee(5, \"Lucy\", '1', \"Lucy@qq.com\")); System.out.println(mapper.selOne(\"2\")); session.commit(); session.close(); &#125;&#125; MyBatis的传参1、出错演示好了，增删查改的基本功能已经演示完毕，但是假设我们需要做一个多条件查询的方法，则需要这样写(一下代码均为省略代码，只贴上了重要的代码，其他的代码均在CURD的演示中)：Employee.xml 123&lt;select id=\"queryList\" resultType=\"com.xpu.bean.Employee\"&gt; select * from employee where name like '#&#123;name&#125;%' and gender = #&#123;gender&#125;&lt;/select&gt; IEmployeeDao.java 12//多条件查询public List&lt;Employee&gt; queryList(String name, char gender); 多条件查询测试： 1234567public void test() &#123; IEmployeeDao mapper = session.getMapper(IEmployeeDao.class); List&lt;Employee&gt; list = mapper.queryList(\"t\", '1'); for (Employee employee : list) &#123; System.out.println(employee); &#125;&#125; 但是却报错了，注意红线标记的地方，我们需要修改参数的传递方式： 2、顺序传参法其实对于这种多参数的情况，应该这样写Employee.xml，这叫做顺序传参法： 1234&lt;select id=\"queryList\" resultType=\"com.xpu.bean.Employee\"&gt; select * from employee where name like '$&#123;param1&#125;%' and gender = #&#123;param2&#125; select * from employee where name = #&#123;0&#125; and gender = #&#123;1&#125;&lt;/select&gt; #{ }里面的数字代表你传入参数的顺序，SQL语句的语义表达不直观，且一旦顺序调整容易出错，所以不建议使用！ 3、注解传参法但是这样写的可读性太差，可以使用注解的方式： 123&lt;select id=\"queryList\" resultType=\"com.xpu.bean.Employee\"&gt; select * from employee where name like '$&#123;name&#125;%' and gender = #&#123;gender&#125;&lt;/select&gt; 12//多条件查询public List&lt;Employee&gt; queryList(@Param(\"name\") String name, @Param(\"gender\") char gender); 这样可读性强，而且不会因为参数的顺序问题导致程序异常！ 4、Map传参法Employee.xml 123&lt;select id=\"queryList\" resultType=\"com.xpu.bean.Employee\"&gt; select * from employee where name like '$&#123;name&#125;%' and gender = #&#123;gender&#125;&lt;select&gt; IEmployeeDao.java 1public List&lt;Employee&gt; queryList(Map map); 测试代码： 1234567public void test() &#123; IEmployeeDao mapper = session.getMapper(IEmployeeDao.class); Map map = new HashMap(); map.put(\"name\", \"t\"); map.put(\"gender\", '1'); List&lt;Employee&gt; list = mapper.queryList(map);&#125; 5、JavaBean传参法这个在MyBatis的CURD中已经演示过了，resultType=&quot;com.xpu.bean.Employee&quot; ，通过#{ 对象属性 } 便可以传参，在此不再赘述！ MyBatis动态SQLMyBatis 的强大特性之一便是它的动态 SQL。你可以在这个网站学习动态SQL《动态 SQL》 ，如果你有使用 JDBC 或其他类似框架的经验，你就能体会到根据不同条件拼接 SQL 语句有多么痛苦。拼接的时候要确保不能忘了必要的空格，还要注意省掉列名列表最后的逗号。利用动态 SQL 这一特性可以彻底摆脱这种痛苦。 if就是简单的条件判断，利用if语句我们可以实现某些简单的条件选择 1234567&lt;!-- 动态SQL --&gt; &lt;select id=\"queryByIf\" resultType=\"com.xpu.bean.Employee\"&gt; select * from employee where 1=1 &lt;if test=\"name != null\"&gt; and name like '$&#123;name&#125;%' &lt;/if&gt; &lt;/select&gt; 但是在上面的例子中，如果name为空的话，where后面就什么都没有，所以加上1=1完全是为了保证语法的正确性，这个问题将在下面解决！ where+ifwhere标签知道如果它包含的标签中有返回值的话，它就插入一个where。此外，如果标签返回的内容是以AND 或 OR开头的，则它会剔除掉。 1234567891011&lt;select id=\"queryByWhere\" resultType=\"com.xpu.bean.Employee\"&gt; select * from employee &lt;where&gt; &lt;if test=\"name != null\"&gt; name like '$&#123;name&#125;%' &lt;/if&gt; &lt;if test=\"address != null\"&gt; and address = #&#123;address&#125; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; 上述例子中，如果name不为空，address不为空，则SQL语句是select * from employee where name like &#39;${name}%&#39; and address = #{address} ,即使name为空，address前面的and也会被去掉，同样是合法的SQL语句，如果都为空，那就变成了查询所有记录，同样也是合法的SQL语句！ trimtrim标签是可以让我们自己去实现一个where标签，或者实现更多的功能，下面用trim标签去实现一个where标签： 1234567891011&lt;select id=\"queryByTrim\" resultType=\"com.xpu.bean.Employee\"&gt; select * from employee &lt;trim prefix=\"where\" prefixOverrides=\"AND | OR\"&gt; &lt;if test=\"name != null\"&gt; name like '$&#123;name&#125;%' &lt;/if&gt; &lt;if test=\"address != null\"&gt; and address = #&#123;address&#125; &lt;/if&gt; &lt;/trim&gt; &lt;/select&gt; 这个trim标签其实就是一个可以通过自定义属性去完成一些功能的标签，prefix=&quot;where&quot; 就是代表如果标签中有内容则加上where，如果where后面直接遇到AND或者OR这样的标签就会将其剔除！ set12345678910111213&lt;update id=\"updateEmployee\" parameterType=\"com.xpu.bean.Employee\"&gt; update user &lt;set&gt; &lt;if test=\"name!=null\"&gt; name=#&#123;name&#125; &lt;/if&gt; &lt;/set&gt; &lt;where&gt; &lt;if test=\"address!=null\"&gt; address=#&#123;address&#125; &lt;/if&gt; &lt;/where&gt;&lt;/update&gt; set标签代替了sql中set关键字，set标签可以自动去除sql中的多余的&#39;,&#39;，这个和where标签是一样的道理，同样的我们可以使用trim来实现同样的功能： 12345678910111213&lt;update id=\"updateEmployee\" parameterType=\"com.xpu.bean.Employee\"&gt; update user &lt;trim prefix=\"set\" prefixOverrides=\",\"&gt; &lt;if test=\"name!=null\"&gt; name=#&#123;name&#125; &lt;/if&gt; &lt;/trim&gt; &lt;where&gt; &lt;if test=\"address!=null\"&gt; address=#&#123;address&#125; &lt;/if&gt; &lt;/where&gt;&lt;/update&gt; choosechose标签类似于流程控制中switch case default语句，配合when、otherwise标签使用： 1234567891011121314&lt;select id=\"queryByChoose\" resultType=\"com.xpu.bean.Employee\"&gt; select * from employee where 1=1 &lt;choose&gt; &lt;when test=\"name != null\"&gt; and name like '$&#123;name&#125;%' &lt;/when&gt; &lt;when test=\"address != null\"&gt; and address = $&#123;address&#125; &lt;/when&gt; &lt;otherwise&gt; order by name &lt;/otherwise&gt; &lt;/choose&gt; &lt;/select&gt; foreachforeach标签可迭代任何对象（如列表、集合等）和任何的字典或者数组对象传递给foreach作为集合参数，下面是一个根据ID批量删除的示例： 123456789101112131415&lt;delete id=\"deleteByList\"&gt; delete from employee where id in &lt;foreach collection=\"list\" open=\"(\" separator=\",\" close=\")\" item=\"myitem\"&gt; #&#123;myitem&#125; &lt;/foreach&gt;&lt;/delete&gt;&lt;delete id=\"deleteByArray\"&gt; delete from employee where id in &lt;foreach collection=\"array\" open=\"(\" separator=\",\" close=\")\" item=\"myitem\"&gt; #&#123;myitem&#125; &lt;/foreach&gt;&lt;/delete&gt; 12345//批量删除public Integer deleteByList(List&lt;String&gt; list);//批量删除public Integer deleteByArray(String[] strs); 12345678910111213public void func() throws IOException &#123; Integer array = mapper.deleteByArray(new String[] &#123;\"1\", \"2\"&#125;); System.out.println(\"影响行数：\" + array); List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(\"3\"); list.add(\"4\"); Integer deleteByList = mapper.deleteByList(list); System.out.println(\"影响行数：\" + deleteByList); session.commit(); session.close(); &#125; 动态 SQL 语句的编写往往就是一个拼接的问题，为了保证拼接准确，我们最好首先要写原生的 SQL 语句出来，然后在通过 MyBatis 动态SQL 对照着改，防止出错！","updated":"2020-04-14T03:04:27.829Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaEE","slug":"JavaEE","permalink":"https://zouchanglin.cn/tags/JavaEE/"},{"name":"ORM框架","slug":"ORM框架","permalink":"https://zouchanglin.cn/tags/ORM%E6%A1%86%E6%9E%B6/"}]},{"title":"理解Java内存模型","date":"2019-01-31T10:09:30.000Z","path":"2019/01/31/理解Java内存模型/","text":"网上有很多关于Java内存模型的文章，在《深入理解Java虚拟机》和《Java并发编程的艺术》等书中也都有关于这个知识点的介绍。但是，很多人读完之后还是搞不清楚，甚至有的人说自己更懵了。本文，就来整体的介绍一下Java内存模型，目的很简单，让你读完本文以后，就知道到底Java内存模型是什么，为什么要有Java内存模型，Java内存模型解决了什么问题等。 本文中，有很多定义和说法，都是笔者自己理解后定义出来的。希望能够让读者可以对Java内存模型有更加清晰的认识。当然，如有偏颇，欢迎指正。 为什么要有内存模型在介绍Java内存模型之前，先来看一下到底什么是计算机内存模型，然后再来看Java内存模型在计算机内存模型的基础上做了哪些事情。要说计算机的内存模型，就要说一下一段古老的历史，看一下为什么要有内存模型。 内存模型，英文名Memory Model，他是一个很老的老古董了。他是与计算机硬件有关的一个概念。那么我先给你介绍下他和硬件到底有啥关系。 CPU和缓存一致性我们应该都知道，计算机在执行程序的时候，每条指令都是在CPU中执行的，而执行的时候，又免不了要和数据打交道。而计算机上面的数据，是存放在主存当中的，也就是计算机的物理内存啦。 刚开始，还相安无事的，但是随着CPU技术的发展，CPU的执行速度越来越快。而由于内存的技术并没有太大的变化，所以从内存中读取和写入数据的过程和CPU的执行速度比起来差距就会越来越大,这就导致CPU每次操作内存都要耗费很多等待时间。 这就像一家创业公司，刚开始，创始人和员工之间工作关系其乐融融，但是随着创始人的能力和野心越来越大，逐渐和员工之间出现了差距，普通员工原来越跟不上CEO的脚步。老板的每一个命令，传到到基层员工之后，由于基层员工的理解能力、执行能力的欠缺，就会耗费很多时间。这也就无形中拖慢了整家公司的工作效率。 可是，不能因为内存的读写速度慢，就不发展CPU技术了吧，总不能让内存成为计算机处理的瓶颈吧。 所以，人们想出来了一个好的办法，就是在CPU和内存之间增加高速缓存。缓存的概念大家都知道，就是保存一份数据拷贝。他的特点是速度快，内存小，并且昂贵。 那么，程序的执行过程就变成了： 当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中。 之后，这家公司开始设立中层管理人员，管理人员直接归CEO领导，领导有什么指示，直接告诉管理人员，然后就可以去做自己的事情了。管理人员负责去协调底层员工的工作。因为管理人员是了解手下的人员以及自己负责的事情的。所以，大多数时候，公司的各种决策，通知等，CEO只要和管理人员之间沟通就够了。 而随着CPU能力的不断提升，一层缓存就慢慢的无法满足要求了，就逐渐的衍生出多级缓存。 按照数据读取顺序和与CPU结合的紧密程度，CPU缓存可以分为一级缓存（L1），二级缓存（L3），部分高端CPU还具有三级缓存（L3），每一级缓存中所储存的全部数据都是下一级缓存的一部分。 这三种缓存的技术难度和制造成本是相对递减的，所以其容量也是相对递增的。 那么，在有了多级缓存之后，程序的执行就变成了： 当CPU要读取一个数据时，首先从一级缓存中查找，如果没有找到再从二级缓存中查找，如果还是没有就从三级缓存或内存中查找。 随着公司越来越大，老板要管的事情越来越多，公司的管理部门开始改革，开始出现高层，中层，底层等管理者。一级一级之间逐层管理。 单核CPU只含有一套L1，L2，L3缓存；如果CPU含有多个核心，即多核CPU，则每个核心都含有一套L1（甚至和L2）缓存，而共享L3（或者和L2）缓存。 公司也分很多种，有些公司只有一个大Boss，他一个人说了算。但是有些公司有比如联席总经理、合伙人等机制。 单核CPU就像一家公司只有一个老板，所有命令都来自于他，那么就只需要一套管理班底就够了。 多核CPU就像一家公司是由多个合伙人共同创办的，那么，就需要给每个合伙人都设立一套供自己直接领导的高层管理人员，多个合伙人共享使用的是公司的底层员工。 还有的公司，不断壮大，开始差分出各个子公司。各个子公司就是多个CPU了，互相之前没有共用的资源。互不影响。 下图为一个单CPU双核的缓存结构。 随着计算机能力不断提升，开始支持多线程。那么问题就来了。我们分别来分析下单线程、多线程在单核CPU、多核CPU中的影响。 单线程。 cpu核心的缓存只被一个线程访问。缓存独占，不会出现访问冲突等问题。 单核CPU，多线程。 进程中的多个线程会同时访问进程中的共享数据，CPU将某块内存加载到缓存后，不同线程在访问相同的物理地址的时候，都会映射到相同的缓存位置，这样即使发生线程的切换，缓存仍然不会失效。但由于任何时刻只能有一个线程在执行，因此不会出现缓存访问冲突。 多核CPU，多线程。 每个核都至少有一个L1 缓存。多个线程访问进程中的某个共享内存，且这多个线程分别在不同的核心上执行，则每个核心都会在各自的caehe中保留一份共享内存的缓冲。由于多核是可以并行的，可能会出现多个线程同时写各自的缓存的情况，而各自的cache之间的数据就有可能不同。 在CPU和主存之间增加缓存，在多线程场景下就可能存在缓存一致性问题，也就是说，在多核CPU中，每个核的自己的缓存中，关于同一个数据的缓存内容可能不一致。 如果这家公司的命令都是串行下发的话，那么就没有任何问题。 如果这家公司的命令都是并行下发的话，并且这些命令都是由同一个CEO下发的，这种机制是也没有什么问题。因为他的命令执行者只有一套管理体系。 如果这家公司的命令都是并行下发的话，并且这些命令是由多个合伙人下发的，这就有问题了。因为每个合伙人只会把命令下达给自己直属的管理人员，而多个管理人员管理的底层员工可能是公用的。 比如，合伙人1要辞退员工a，合伙人2要给员工a升职，升职后的话他再被辞退需要多个合伙人开会决议。两个合伙人分别把命令下发给了自己的管理人员。合伙人1命令下达后，管理人员a在辞退了员工后，他就知道这个员工被开除了。而合伙人2的管理人员2这时候在没得到消息之前，还认为员工a是在职的，他就欣然的接收了合伙人给他的升职a的命令。 处理器优化和指令重排上面提到在在CPU和主存之间增加缓存，在多线程场景下会存在缓存一致性问题。除了这种情况，还有一种硬件问题也比较重要。那就是为了使处理器内部的运算单元能够尽量的被充分利用，处理器可能会对输入代码进行乱序执行处理。这就是处理器优化。 除了现在很多流行的处理器会对代码进行优化乱序处理，很多编程语言的编译器也会有类似的优化，比如Java虚拟机的即时编译器（JIT）也会做指令重排。 可想而知，如果任由处理器优化和编译器对指令重排的话，就可能导致各种各样的问题。 关于员工组织调整的情况，如果允许人事部在接到多个命令后进行随意拆分乱序执行或者重排的话，那么对于这个员工以及这家公司的影响是非常大的。 并发编程的问题前面说的和硬件有关的概念你可能听得有点蒙，还不知道他到底和软件有啥关系。但是关于并发编程的问题你应该有所了解，比如原子性问题，可见性问题和有序性问题。 其实，原子性问题，可见性问题和有序性问题。是人们抽象定义出来的。而这个抽象的底层问题就是前面提到的缓存一致性问题、处理器优化问题和指令重排问题等。 这里简单回顾下这三个问题，并不准备深入展开，感兴趣的读者可以自行学习。我们说，并发编程，为了保证数据的安全，需要满足以下三个特性： 原子性是指在一个操作中就是cpu不可以在中途暂停然后再调度，既不被中断操作，要不执行完成，要不就不执行。 可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 有序性即程序执行的顺序按照代码的先后顺序执行。 有没有发现，缓存一致性问题其实就是可见性问题。而处理器优化是可以导致原子性问题的。指令重排即会导致有序性问题。所以，后文将不再提起硬件层面的那些概念，而是直接使用大家熟悉的原子性、可见性和有序性。 什么是内存模型前面提到的，缓存一致性问题、处理器器优化的指令重排问题是硬件的不断升级导致的。那么，有没有什么机制可以很好的解决上面的这些问题呢？ 最简单直接的做法就是废除处理器和处理器的优化技术、废除CPU缓存，让CPU直接和主存交互。但是，这么做虽然可以保证多线程下的并发问题。但是，这就有点因噎废食了。 所以，为了保证并发编程中可以满足原子性、可见性及有序性。有一个重要的概念，那就是——内存模型。 为了保证共享内存的正确性（可见性、有序性、原子性），内存模型定义了共享内存系统中多线程程序读写操作行为的规范。通过这些规则来规范对内存的读写操作，从而保证指令执行的正确性。它与处理器有关、与缓存有关、与并发有关、与编译器也有关。他解决了CPU多级缓存、处理器优化、指令重排等导致的内存访问问题，保证了并发场景下的一致性、原子性和有序性。 内存模型解决并发问题主要采用两种方式：限制处理器优化和使用内存屏障。本文就不深入底层原理来展开介绍了，感兴趣的朋友可以自行学习。 什么是Java内存模型前面介绍过了计算机内存模型，这是解决多线程场景下并发问题的一个重要规范。那么具体的实现是如何的呢，不同的编程语言，在实现上可能有所不同。 我们知道，Java程序是需要运行在Java虚拟机上面的，Java内存模型（Java Memory Model ,JMM）就是一种符合内存模型规范的，屏蔽了各种硬件和操作系统的访问差异的，保证了Java程序在各种平台下对内存的访问都能保证效果一致的机制及规范。 提到Java内存模型，一般指的是JDK 5 开始使用的新的内存模型，主要由JSR-133: JavaTM Memory Model and Thread Specification 描述。感兴趣的可以参看下这份PDF文档 Java内存模型规定了所有的变量都存储在主内存中，每条线程还有自己的工作内存，线程的工作内存中保存了该线程中是用到的变量的主内存副本拷贝，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量的传递均需要自己的工作内存和主存之间进行数据同步进行。 而JMM就作用于工作内存和主存之间数据同步过程。他规定了如何做数据同步以及什么时候做数据同步。 这里面提到的主内存和工作内存，读者可以简单的类比成计算机内存模型中的主存和缓存的概念。特别需要注意的是，主内存和工作内存与JVM内存结构中的Java堆、栈、方法区等并不是同一个层次的内存划分，无法直接类比。《深入理解Java虚拟机》中认为，如果一定要勉强对应起来的话，从变量、主内存、工作内存的定义来看，主内存主要对应于Java堆中的对象实例数据部分。工作内存则对应于虚拟机栈中的部分区域。 所以，再来总结下，JMM是一种规范，目的是解决由于多线程通过共享内存进行通信时，存在的本地内存数据不一致、编译器会对代码指令重排序、处理器会对代码乱序执行等带来的问题。 Java内存模型的实现了解Java多线程的朋友都知道，在Java中提供了一系列和并发处理相关的关键字，比如volatile、synchronized、final、concurren包等。其实这些就是Java内存模型封装了底层的实现后提供给程序员使用的一些关键字。 在开发多线程的代码的时候，我们可以直接使用synchronized等关键字来控制并发，从来就不需要关心底层的编译器优化、缓存一致性等问题。所以，Java内存模型，除了定义了一套规范，还提供了一系列原语，封装了底层实现后，供开发者直接使用。 本文并不准备把所有的关键字逐一介绍其用法，因为关于各个关键字的用法，网上有很多资料。读者可以自行学习。本文还有一个重点要介绍的就是，我们前面提到，并发编程要解决原子性、有序性和一致性的问题，我们就再来看下，在Java中，分别使用什么方式来保证。 原子性在Java中，为了保证原子性，提供了两个高级的字节码指令monitorenter和monitorexit。在synchronized的实现原理文章中，介绍过，这两个字节码，在Java中对应的关键字就是synchronized。 因此，在Java中可以使用synchronized来保证方法和代码块内的操作是原子性的。 可见性Java内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值的这种依赖主内存作为传递媒介的方式来实现的。 Java中的volatile关键字提供了一个功能，那就是被其修饰的变量在被修改后可以立即同步到主内存，被其修饰的变量在每次是用之前都从主内存刷新。因此，可以使用volatile来保证多线程操作时变量的可见性。 除了volatile，Java中的synchronized和final两个关键字也可以实现可见性。只不过实现方式不同，这里不再展开了。 有序性在Java中，可以使用synchronized和volatile来保证多线程之间操作的有序性。实现方式有所区别： volatile关键字会禁止指令重排。synchronized关键字保证同一时刻只允许一条线程操作。 好了，这里简单的介绍完了Java并发编程中解决原子性、可见性以及有序性可以使用的关键字。读者可能发现了，好像synchronized关键字是万能的，他可以同时满足以上三种特性，这其实也是很多人滥用synchronized的原因。 但是synchronized是比较影响性能的，虽然编译器提供了很多锁优化技术，但是也不建议过度使用。 总结在读完本文之后，相信你应该了解了什么是Java内存模型、Java内存模型的作用以及Java中内存模型做了什么事情等。关于Java中这些和内存模型有关的关键字，希望读者还可以继续深入学习，并且自己写几个例子亲自体会一下。 可以参考《深入理解Java虚拟机》和《Java并发编程的艺术》两本书。 原文地址：《再有人问你Java内存模型是什么，就把这篇文章发给他》","updated":"2020-03-13T03:06:29.754Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JVM","slug":"JVM","permalink":"https://zouchanglin.cn/tags/JVM/"}]},{"title":"了解并使用Maven","date":"2019-01-28T10:09:30.000Z","path":"2019/01/28/了解并使用Maven/","text":"前言如果你经常使用第三方jar包的话（前提是你现在不会Maven），那么那些jar包可能会让我们有点难受，下面是一个示例： 假设你的工程需要这么多的Jar包（当然可能远远不止这些），那么Jar是非常难以维护的，有些Jar包又依赖于另一些Jar包，版本也不一样，更难受的是如果你的电脑是固态硬盘、固态硬盘、固态硬盘、，那么容量就不像机械硬盘那么阔气，于是本人想了一个好办法，就是把需要要用到的Jar包放在一起，要用的时候就去复制粘贴到项目的lib中，右键add build path 简直完美！！！ 不过后来又出现了很多问题，比如突然搞不清楚Jar包的版本了，Jar包之间的依赖记混了，不小心删除了Jar包又得去下载(某某网站下载的需要积分不说，而且乱七八糟的下载站你懂得……动不动就是全家桶)，所以这是我最开始选择Maven的第一个理由，当然Maven的其他好处后面再说，如果你遇到和我一样的问题，那么Maven是一种你非常值得选择的方案 什么是Maven？其实，我在没接触Maven这种东西之前都是一个项目就是一个工程，就像这样： 生产环境下开发不再是一个项目一个工程，而是每一个模块创建一个工程，而多个模块整合在一起就需要使用到像Maven这样的构建工具。Maven 是干什么用的？这是我刚开始接触 Maven 时最大的问题。之所以会提出这个问题，是因为即使不使用 Maven 我们仍然可以进行 B / S 结构项目的开发。从表述层、业务逻辑层到持久化层再到数据库都有成熟的解决方案，这也就意味着：不使用 Maven 我们一样可以开发项目 Maven的作用看到上面的框架是不是感觉Maven不在其中，是的，因为Maven不是用来辅助编码的，Maven是为了工程模块化开发，这个概念有点笼统，所以下面介绍的比较详细，Maven的出现是为了解决如下问题（这些都是具体问题）： 1、添加第三方Jar包在今天的JavaEE 开发领域，有大量的第三方框架和工具可以供我们使用。要使用这些 Jar 包最简单的方法就是复制粘贴到WEB-INF/lib目录下。但是这会导致每次创建一个新的工程就需要将jar包重复复制到该目录下，从而造成工作区中存在大量重复的文件，让我们的工程显得很臃肿。而使用 Maven 后每个Jar包本身只在本地仓库中保存一份，需要 Jar 包的工程只需要以坐标的方式简单的引用一下就可以了。不仅极大的节约了存储空间，让项目更轻巧，更避免了重复文件太多而造成的混乱。 2、解决Jar包之间的依赖关系jar包往往不是孤立存在的，很多 jar 包都需要在其他 jar 包的支持下才能够正常工作，我们称之为jar包的依赖关系。你不可能知道所有 jar 包的依赖关系吗？当你拿到一个新的从未使用过的 jar 包，你如何得知他需要哪些 jar 包的支持呢？如果不了解这个情况，份入的 jar 包不够，那么现有的程序将不能正常工作。如果你的工程里面有1000个jar包，这简直是不可想象的。而引入Maven后，Maven就可以替我们自动的将当前 jar 包所依赖的其他所有jar包全部导入进来，无而人工参与，节约了我们大量的时间和精力。 3、获取第三方jar包JavaEE开发中需要使用到的jar包种类繁多，几乎每个jar包在其本身的官网上获取方式都不尽相同。为了查找一个jar包找遍互联网，以不规范的方式获取到的jar包也是不规范的。使用 Maven我们可以享受到一个完全统一规范的jar包管理体系 。你只需要 在你的项目中以坐标的方式依赖一个jar包， Maven就会自动从就会自动从中央仓库进行 下载，并同时下载这个jar包所依赖的其他jar包！ 4、项目模块化JavaEE 项目的规模越来越庞大，开发团队的规模也与日俱增。几百上千的人开发的项目是同一个工程。那么架构师、项目经理该如何划分项目的模块、如何分工呢？这么大的项目必须划分模块，必须将项目拆分成多个工程协同开发。多个模块工程中有的是Java工程，有的是Web工程，此时就需要Maven 什么是构建？构建就是以我们编写的Java代码、框架配置文件、国际化等其他资源文件、JSP页面和图片等静态资源作为原材料，去生产出一个可以运行的项目的过程。通俗地讲就是：我们用一堆原材料去制作一个可运行起来的工程！工程运行起来后还需要把源代码一起放在服务器上面吗？当然不能！！！ 关于构建的概念①清理：删除以前的编译结果，为重新编译做好准备。②编译：将Java源程序编译为字节码文件。③测试：针对项目中的关键点进行测试，确保项目在迭代开发过程中关键点的正确性。④报告：在每一次测试后以标准的格式记录和展示测试结果。⑤打包：将一个包含诸多文件的工程封装为一个压缩文件用于安装或部署。Java工程对应jar包，Web工程对应 war包。⑥安装：在 Maven环境下特指将打包的结果——jar包或war包安装到本地仓库中。⑦部署：将打包的结果部署到远程仓库或将 war包部署到服务器上运行。 自动化构建其实上述环节我们在 Eclipse中都可以找到对应的操作，只是不太标准。那么既然IDE已经可以进行构建了我们为什么还要使用 Maven这样的构建工具呢？ 现在假设你就职于某公司，早上收邮件说你的代码有BUG，于是你开始解决这个BUG： 但是实际上你把时间花在了编译、打包、部署、测试等固定流程上面，解决问题的核心在于分析问题和编码，这是非常浪费时间的，于是出现了Maven这款自动化构建工具帮你完成编译、打包、部署、测试等固定流程，这就是自动化构建： Maven核心概念1、约定的目录结构下图是一个典型的Maven项目的结构： 约定&gt;配置&gt;编码。意思就是能用配置解决的问题就不编码，能基于约定的就不进行配置。而Maven正是因为指定了特定文件保存的目录才能够对我们的Java工程进行自动化构建。 2、POMPOM全称是 Project Object Model：项目对象模型。将Java工程的相关信息封装为对象作为便于操作和管理的模型。Maven工程的核心配置，可以说学习Maven就是学习 pom.xml文件的配置。 3、坐标在一个平面中使用 x、y两个向量可以唯一的确定平面中的一个点。在空间中使用 x、y、z三个向量可以唯一的确定空间中的一个点。Maven的坐标就是要确定唯一的一个Maven工程！使用如下三个向量在 Maven的仓库中唯一的确定一个Maven工程： groupid：公司或组织的域名倒序+当前项目名称 artifactId：当前项目的模块名称 version：当前模块的版本 如何通过坐标到仓库中查找jar包？ 我们自己的 Maven工程必须执行安装操作才会进入仓库。安装的命令是：mvn install 4、依赖管理Maven中最关键的部分，我们使用Maven最主要的就是使用它的依赖管理功能。要理解和掌握Maven的依赖管理，我们只需要解决一下几个问题： ① 依赖的目的是什么当Ajar包用到了Bjar包中的某些类时，A就对B产生了依赖，这是概念上的描述。那么如何在项目中以依赖的方式引入一个我们需要的jar包呢？ 1234567&lt;!-- 引入一个servlet-api的jar包 --&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; ② 依赖的范围是什么大家注意到上面的依赖信息中除了目标jar包的坐标还有一个scope设置，这是依赖的范围。依赖的范围有几个可选值，我们用得到的是：compile、test、provided三个。 举个例子，如上图所示，很显然假设我们需要引入junit包，但是junit包确实测试需要用的包，但是junit包在测试只是在测试的时候需要，编译的时候既需要编译主程序，也要编译测试的程序，所以compile掌控的范围也就宽了，Test范围的东西只需要在测试方面接入就可以了！ compile和provided的区别的区别是什么呢？ 如上图所示，开发Web工程的时候我们需要开发环境中就要有servlet-api的jar包，也就是给工程添加的server-runtime环境： 但是项目部署的时候是Tomcat提供的server-runtime环境，所以provided的东西虽然在编译和测试的时候是必不可少的，但是部署的时候却由Servlet容器提供！ ③ 有效性总结 ④ 依赖的传递性A依赖B，B依赖C，A能否使用C呢？那要看B依赖C的范围是不是compile，如果是则可用，否则不可用。 ⑤ 依赖的排除如果我们在当前工程中引入了一个依赖是A，而A又依赖了B，那么Maven会自动将A依赖的B引入当前工程，但是个别情况下B有可能是一个不稳定版，或对当前工程有不良影响。这时我们可以在引入A的时候将B排除： 很明显，我们在使用标签的时候发现，其实标签就是排除不稳定的依赖，通过groupId和artifactId就可以定位到这个依赖，version标签不在其中，当然就是排除它的所有版本咯！！！ ⑥ 统一管理所依赖jar包的版本对同一个框架的一组jar包最好使用相同的版本。为了方便升级框架，可以将jar包的版本信息统一提取出来 ⑦ 依赖的原则：解决 jar包冲突 注意：这里“声明”的先后顺序指的是 dependency标签配置的先后顺序 5、仓库管理仓库的分类本地仓库：为当前本机电脑上的所有Maven工程服务远程仓库： 私服：架设在当前局域网环境下，为当前局域网范围内的所有 Maven工程服务 中央仓库：架设在 Internet上，为全世界所有Maven工程服务 中央仓库的镜像：架设在各个大洲，为中央仓库分担流量。减轻中央仓库的压力，同时更快的响应用户请求 库中的文件 Maven的插件 我们自己开发的项目的模块 第三方框架或工具的 jar包 不管是什么样的 jar包，在仓库中都是按照坐标生成目录结构，所以可以通过统一的方式查询或依赖 6、生命周期Maven生命周期定义了各个构建环节的执行顺序，有了这个清单，Maven就可以自动化的执行构建命令了。 Maven有三套相互独立的生命周期，分别是：①Clean Lifecycle在进行真正的构建之前进行一些清理工作。②Default Lifecycle构建的核心部分，编译，测试，打包，安装，部署等等。③Site Lifecycle生成项目报告，站点，发布站点 它们是相互独立的，你可以仅仅调用 clean来清理工作目录，仅仅调用 site来生成站点。当然你也可以直接运行 mvn clean install site运行所有这三套生命周期每套生命周期都由一组阶段(Phase)组成，我们平时在命令行输入的命令总会对应于一个特定的阶段。比如，运行mvn clean，这个 clean是 Clean生命周期的一个阶段。有 Clean生命周期，也有clean阶段: Clean生命周期Clean生命周期一共包含了三个阶段： ① pre-clean执行一些需要在clean之前完成的工作② clean移除所有上一次构建生成的文件③ post-clean执行一些需要在clean之后立刻完成的工作 Site生命周期① pre-site执行一些需要在生成站点文档之前完成的工作② site生成项目的站点文档③ post-site执行一些需要在生成站点文档之后完成的工作，并且为部署做准备④ site-deploy将生成的站点文档部署到特定的服务器上 这里经常用到的是site阶段和 site-deploy阶段，用以生成和发布 Maven站点，这可是 Maven相当强大的功能，Manager比较喜欢，文档及统计数据自动生成，很好看 Default生命周期Default生命周期是 Maven生命周期中最重要的一个，绝大部分工作都发生在这个生命周期中。这里，只解释一些比较重要和常用的阶段： process-resources 复制并处理资源文件，至目标目录，准备打包。compile 编译项目的源代码process-test-resources 复制并处理资源文件，至目标测试目录test-compile 编译测试源代码test 使用合适的单元测试框架运行测试。这些测试代码不会被打包或部署package 接受编译好的代码，打包成可发布的格式，如JAR。install 将包安装至本地仓库，以让其它项目依赖。deploy 将最终的包复制到远程的仓库，以让其它开发人员与项目共享或部署到服务器上运行 生命周期与自动化构建运行任何一个阶段的时候，它前面的所有阶段都会被运行，例如我们运行 mvn install的时候，代码会被编译，测试，打包。这就是 Maven为什么能够自动执行构建过程的各个环节的原因。此外，Maven的插件机制是完全依赖Maven的生命周期的，因此理解生命周期至关重要！ 7、插件和目标 Maven的核心仅仅定义了抽象的生命周期，具体的任务都是交由插件完成的。 每个插件都能实现多个功能，每个功能就是一个插件目标。 Maven的生命周期与插件目标相互绑定，以完成某个具体的构建任务 compile就是插件 maven-compiler-plugin的一个目标；pre-clean是插件 maven-clean-plugin的一个目标。 8、继承由于非 compile范围的依赖信息是不能在”依赖链”中传递的，所以有需要的工程只能单独配置，假设三个过程都要依赖junit4.0.0.jar这个jar包，此时如果项目需要将各个模块的 junit版本统一更新为 4.9，那么到各个工程中手动修改无疑是非常不可取的。使用继承机制就可以将这样的依赖信息统一提取到父工程模块中进行统一管理： 【1】创建父工程创建父工程和创建一般的 Java工程操作一致，唯一需要注意的是：打包方式处要设置为 pom 【2】在子工程中引用父工程 1234567&lt;parent&gt;&lt;!-- 父工程坐标 --&gt; &lt;groupId&gt;...&lt;/groupId&gt; &lt;artifactId&gt;...&lt;/artifactId&gt; &lt;version&gt;...&lt;/version&gt; &lt;relativePath&gt;从当前目录到父项目的 pom.xml文件的相对路径&lt;/relativePath&gt;&lt;/parent&gt; 注意：relativePath是从当前目录到父项目的 pom.xml文件的相对路径【3】在父工程中管理依赖 12345678910&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.9&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 在子项目中重新指定需要的依赖，删除范围和版本号: 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 9、聚合什么是聚合？将多个工程拆分为模块后，需要手动逐个安装到仓库后依赖才能够生效。修改源码后也需要逐个手动进行clean操作。而使用了聚合之后就可以批量进行Maven工程的安装、清理工作，这就是聚合！ 如何配置聚合？在总的聚合工程中使用 modules/module标签组合，指定模块工程的相对路径即可： 123456&lt;modules&gt; &lt;module&gt;../Project_01&lt;/module&gt; &lt;module&gt;../Project_02&lt;/module&gt; &lt;module&gt;../Project_03&lt;/module&gt; &lt;module&gt;...&lt;/module&gt;&lt;/modules&gt; 10、如何找到我们需要的Jar包的依赖配置？点击这里进行搜索即可☞mvnrepository Maven的下载和eclipse/IDEA配置【1】首先下载Maven，去阿帕奇官网下载就可以了【2】将压缩包解压，注意解压路径不要有中文或者空格【3】配置环境变量，你可以配置MAVEN_HOME，也可以M2_HOME【3】新建一个文件夹作为本地仓库，注意路径不要有中文或者空格【4】修改Maven的conf目录下的setting.xml，将本地仓库配置到指定的目录,下面是我主要的配置 1234567891011121314151617181920212223242526&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!-- 设置本地仓库 --&gt;&lt;localRepository&gt;D:\\software\\RepMaven&lt;/localRepository&gt;&lt;!-- 配置阿里云仓库 --&gt;&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;&lt;/mirror&gt;&lt;!-- 默认web工程为jdk1.8 --&gt;&lt;profile&gt; &lt;id&gt;jdk-1.8&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt; &lt;/properties&gt;&lt;/profile&gt; 【5】进入eclipse直接添加即可，如果不够详细，请参考：《Maven安装与配置》","updated":"2020-03-13T03:06:29.716Z","categories":[{"name":"工程构建","slug":"工程构建","permalink":"https://zouchanglin.cn/categories/%E5%B7%A5%E7%A8%8B%E6%9E%84%E5%BB%BA/"}],"tags":[{"name":"Maven","slug":"Maven","permalink":"https://zouchanglin.cn/tags/Maven/"}]},{"title":"浏览器User agent里的历史故事","date":"2019-01-22T10:09:30.000Z","path":"2019/01/22/浏览器User agent里的历史故事/","text":"故事还得从头说起，最初的主角叫NCSA Mosaic，简称Mosaic（马赛克），是1992年末位于伊利诺伊大学厄巴纳-香槟分校的国家超级计算机应用中心（National Center for Supercomputing Applications，简称NCSA）开发，并于1993年发布的一款浏览器。它自称“NCSA_Mosaic/2.0（Windows 3.1）”，Mosaic可以同时展示文字和图片，从此浏览器变得有趣多了。 然而很快就出现了另一个浏览器，这就是著名的Mozilla，中文名称摩斯拉。一说 Mozilla = Mosaic + Killer，意为Mosaic杀手，也有说法是 Mozilla = Mosaic &amp; Godzilla，意为马赛克和哥斯拉，而Mozilla最初的吉祥物是只绿色大蜥蜴，后来更改为红色暴龙，跟哥斯拉长得一样。 但Mosaic对此非常不高兴，于是后来Mozilla更名为Netscape，也就是网景。Netscape自称“Mozilla/1.0(Win3.1)”，事情开始变得更加有趣。网景支持框架（frame），由于大家的喜欢框架变得流行起来，但是Mosaic不支持框架，于是网站管理员探测user agent，对Mozilla浏览器发送含有框架的页面，对非Mozilla浏览器发送没有框架的页面。 后来网景拿微软寻开心，称微软的Windows是“没有调试过的硬件驱动程序”。微软很生气，后果很严重。此后微软开发了自己的浏览器，这就是Internet Explorer，并希望它可以成为Netscape Killer。IE同样支持框架，但它不是Mozilla，所以它总是收不到含有框架的页面。微软很郁闷很快就沉不住气了，它不想等到所有的网站管理员都了解IE并且给IE发送含有框架的页面，它选择宣布IE是兼容Mozilla，并且模仿Netscape称IE为“Mozilla/1.22(compatible; MSIE 2.0; Windows 95)”，于是IE可以收到含有框架的页面了，所有微软的人都嗨皮了，但是网站管理员开始晕了。 因为微软将IE和Windows捆绑销售，并且把IE做得比Netscape更好，于是第一次浏览器血腥大战爆发了，结果是Netscape以失败退出历史舞台，微软更加嗨皮。但没想到Netscape居然以Mozilla的名义重生了，并且开发了Gecko，这次它自称为“Mozilla/5.0(Windows; U; Windows NT 5.0; en-US; rv:1.1) Gecko/20020826”。 Gecko是一款渲染引擎并且很出色。Mozilla后来变成了Firefox，并自称“Mozilla/5.0 (Windows; U; Windows NT 5.1; sv-SE; rv:1.7.5) Gecko/20041108 Firefox/1.0”。Firefox性能很出色，Gecko也开始攻城略地，其他新的浏览器使用了它的代码，并且将它们自己称为“Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.7.2) Gecko/20040825 Camino/0.8.1”，以及“Mozilla/5.0 (Windows; U; Windows NT 5.1; de; rv:1.8.1.8) Gecko/20071008 SeaMonkey/1.0”，每一个都将自己装作Mozilla，而它们全都使用Gecko。 Gecko很出色，而IE完全跟不上它，因此user agent探测规则变了，使用Gecko的浏览器被发送了更好的代码，而其他浏览器则没有这种待遇。Linux的追随者对此很难过，因为他们编写了Konqueror，它的引擎是KHTML，他们认为KHTML和Gecko一样出色，但却因为不是Gecko而得不到好的页面，于是Konqueror为得到更好的页面开始将自己伪装成“like Gecko”，并自称为“Mozilla/5.0 (compatible; Konqueror/3.2; FreeBSD) (KHTML, like Gecko)”。自此user agent变得更加混乱。 这时更有Opera跳出来说“毫无疑问，我们应该让用户来决定他们想让我们伪装成哪个浏览器。”于是Opera干脆创建了菜单项让用户自主选择让Opera浏览器变成“Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.51”，或者“Mozilla/5.0 (Windows NT 6.0; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.51”， 或者“Opera/9.51 (Windows NT 5.1; U; en)”。 后来苹果开发了Safari浏览器，并使用KHTML作为渲染引擎，但苹果加入了许多新的特性，于是苹果从KHTML另辟分支称之为WebKit，但它又不想抛弃那些为KHTML编写的页面，于是Safari自称为“Mozilla/5.0 (Macintosh; U; PPC Mac OS X; de-de) AppleWebKit/85.7 (KHTML, like Gecko) Safari/85.5”，这进一步加剧了user agent的混乱局面。 因为微软十分忌惮Firefox，于是IE重装上阵，这次它自称为“Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0) ”，并且渲染效果同样出色，但是需要网站管理员的指令它这么做才行。 再后来，谷歌开发了Chrome浏览器，Chrome使用Webkit作为渲染引擎，和Safari之前一样，它想要那些为Safari编写的页面，于是它伪装成了Safari。于是Chrome使用WebKit，并将自己伪装成Safari，WebKit伪装成KHTML，KHTML伪装成Gecko，最后所有的浏览器都伪装成了Mozilla，这就是为什么所有的浏览器User-Agent里都有Mozilla。Chrome自称为“Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/525.13 (KHTML, like Gecko) Chrome/0.2.149.27 Safari/525.13”。 因为以上这段历史，现在的User-Agent字符串变得一团糟，几乎根本无法彰显它最初的意义。追根溯源，微软可以说是这一切的始作俑者，但后来每一个人都在试图假扮别人，最终把User-Agent搞得混乱不堪。 一句话结论：因为网站开发者可能会因为你是某浏览器（这里是 Mozilla），所以输出一些特殊功能的程序代码（这里指好的特殊功能），所以当其它浏览器也支持这种好功能时，就试图去模仿 Mozilla 浏览器让网站输出跟 Mozilla 一样的内容，而不是输出被阉割功能的程序代码。大家都为了让网站输出最好的内容，都试图假装自己是 Mozilla 一个已经不存在的浏览器…… 附各大浏览器诞生年表： 1993年1月23日：Mosaic 1994年12月：Netscape 1994年：Opera 1995年8月16日：Internet Explorer 1996年10月14日：Kongqueror 2003年1月7日：Safari 2008年9月2日：Chrome 原文地址：《浏览器User-agent String里的历史故事》","updated":"2020-03-13T03:06:29.751Z","categories":[{"name":"生活","slug":"生活","permalink":"https://zouchanglin.cn/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"生活","slug":"生活","permalink":"https://zouchanglin.cn/tags/%E7%94%9F%E6%B4%BB/"}]},{"title":"泛型编程与C++模板","date":"2019-01-12T10:09:30.000Z","path":"2019/01/12/泛型编程与C++模板/","text":"模板初阶模板就是让编译器去推到类型，从而使我们的代码更加简洁，复用性更好！ 泛型编程：其实在高级语言中大多数都是支持泛型编程的，所谓泛型编程就是编写与类型无关的代码，是一种代码的复用，对于C++来说，模板就是实现泛型编程的基础，没有模板就没有STL，对于Java来说就没有集合框架，由此可见泛型编程的重要性！ 函数模板函数模板格式注意：typename 是定义模板的关键字，也可以用class代替，不可以用struct代替 12template&lt;typename t1,typename t2,...typename t3&gt;返回值类型 函数名(参数列表)&#123; &#125; 案例：（交换两个数字的值） 模板是一个蓝图，它本身并不是函数，是编译器产生特定具体类型函数的模具。所以其实模板就是将本来应该我们做的重复的事情交给了编译器。在编译阶段，对于模板函数的使用，编译器需要根据传入的实参类型来推演生成对应类型的函数以供调用。例如：当用double类型使用函数模板时，编译器通过对实参类型的推演，将T确定为double类型，然后产生一份专门处理double类型的代码，所以模板只是把程序员应该写的代码交给了编译器去做，并没有减轻计算机的工作！ 显/隐式实例化如果类型不匹配，编译器会尝试进行隐式类型转换，如果无法转换成功编译器将会报错 12345678910111213141516171819template&lt;class T&gt;T Func(const T&amp; x1, const T&amp; x2)&#123; return x1 - x2;&#125;int main()&#123; Func(10, 20); Func(10.0, 20.0); //Func(a1, d1);Error，两个类型编译器不知道要用那个类型生成新的代码 //解决方法:1.将d1强制类型转换为int或者把a1强制类型转换为double Func(10, (int)20.0); //解决方法:2.采用显式实例化 Func&lt;int&gt;(10, 20.0); return 0;&#125; 模板参数的匹配原则一、一个非模板函数可以和一个同名的函数模板同时存在，而且该函数模板还可以被实例化为这个非函数模板函数 1234567891011121314151617181920212223242526#include &lt;iostream&gt;using namespace std;//专门处理int的函数int Func(const int&amp; x1, const int&amp; x2)&#123; cout &lt;&lt; \"Sub(int,int)\" &lt;&lt; endl; return x1 - x2;&#125;//调用的函数模板template&lt;class T&gt;T Func(const T&amp; x1, const T&amp; x2)&#123; cout &lt;&lt; \"Sub(T,T)\" &lt;&lt; endl; return x1 - x2;&#125;int main()&#123; int a1 = 2, a2 = 3; Func(a1, a2);//与非模板函数匹配，编译器不需要特化 Func&lt;int&gt;(a1, a2);//调用编译器特化的函数 return 0;&#125; 二、对于非模板函数和同名函数模板，如果其他条件都相同，在调动时会优先调用非模板函数而不会从该模板产生出一个实例。如果模板可以产生一个具有更好匹配的函数， 那么将选择模板 12345678910111213141516171819202122#include &lt;iostream&gt;// 专门处理int的函数int Func(int left, int right)&#123; std::cout &lt;&lt; \"Func(int,int)\" &lt;&lt; std::endl; return left + right;&#125;// 通用模板函数template&lt;class T1, class T2&gt;T1 Func(T1 left, T2 right)&#123; std::cout &lt;&lt; \"Func(T1,T2)\" &lt;&lt; std::endl; return left + right;&#125;void main()&#123; Func(1, 2); // 与非函数模板类型完全匹配，不需要函数模板实例化 Func(1, 2.0); // 模板函数可以生成更加匹配的版本，编译器根据实参生成更加匹配的Add函数&#125; 三、显式指定一个空的模板实参列表，该语法告诉编译器只有模板才能来匹配这个调用， 而且所有的模板参数都应该根据实参推演出来 1234567891011121314151617181920212223#include &lt;iostream&gt;//专门处理int的函数int Func(const int&amp; x1, const int&amp; x2)&#123; std::cout &lt;&lt; \"Func(int, int)\" &lt;&lt; std::endl; return x1 - x2;&#125;//调用的函数模板template&lt;class T&gt;T Func(const T&amp; x1, const T&amp; x2)&#123; std::cout &lt;&lt; \"Func(T, T)\" &lt;&lt; std::endl; return x1 - x2;&#125;int main()&#123; Func(1, 2);//与非函数模板类型完全匹配，不需要函数模板实例化 Func&lt;&gt;(1, 2);//调用模板生成的Add函数 return 0;&#125; 四、模板函数不允许自动类型转换，但普通函数可以进行自动类型转换 类模板1template&lt;class T1, class T2, ..., class Tn&gt;class 类模板名&#123; &#125;; 类模板实例化与函数模板实例化不同，类模板实例化需要在类模板名字后跟&lt;&gt;，然后将实例化的类型放在&lt;&gt; 中即可，类模板名字不是真正的类，而实例化的结果才是真正的类。 模板进阶非类型模板参数模板参数分类类型形参与非类型形参： 类型形参：出现在模板参数列表中，跟在class或者typename之类的参数类型名称 非类型形参：就是用一个常量作为类(函数)模板的一个参数，在类(函数)模板中可将该参数当成常量来使用 123456789101112#include &lt;iostream&gt;using namespace std;// 定义一个模板类型的静态数组template&lt;class T, size_t N = 10&gt;class Array &#123;private: T _arr[N]; size_t _size;&#125;; 浮点数、类对象以及字符串是不允许作为非类型模板参数的！非类型的模板参数必须在编译期就能确认结果，比如10+20、rand()…都是可以的! 模板的特化通常情况下，使用模板可以实现一些与类型无关的代码，但对于一些特殊类型的可能会得到一些错误的结果，如下所示： 1234567891011121314151617template&lt;class T&gt; bool IsEqual(T&amp; x1, T&amp; x2)&#123; return x1 == x2;&#125;void Test()&#123; char* s1 = \"hello\"; char* s2 = \"world\"; if (IsEqual(s1, s2)) cout &lt;&lt; \"Equal\" &lt;&lt; endl; else cout &lt;&lt; \"No Equal\" &lt;&lt; endl;&#125; 此时，就需要对模板进行特化。即：在原模板类的基础上，针对特殊类型所进行特殊化的实现方式。模板特化中分为函数模板特化与类模板特化！ 函数模板特化函数模板的特化步骤： 必须要先有一个基础的函数模板 关键字template后面接一对空的尖括号&lt;&gt; 函数名后跟一对尖括号，尖括号中指定需要特化的类型 函数形参表: 必须要和模板函数的基础参数类型完全相同，如果不同编译器可能会报一些奇怪的错误 1234567template&lt;&gt;bool IsEqual&lt;char*&gt;(char*&amp; left, char*&amp; right)&#123; if (strcmp(left, right) == 0) return true; return false;&#125; 一般情况下如果函数模板遇到不能处理或者处理有误的类型，为了实现简单通常都是将该函数直接给出而不是进行特化，模板匹配时自动会匹配类型严格的函数，匹配的规则在上面已经说到！ 类模板特化类模板特化又分为全特化与偏特化： 全特化全特化指的是在类模板的基础上，再重新定义一个类，该类与类模板的内容完全一致，唯一的区别是指定了类模板的所有类型 123456789101112131415161718192021222324252627#include &lt;iostream&gt;template&lt;class T1, class T2&gt;class Data&#123;public: Data() &#123; std::cout &lt;&lt; \"Data&lt;T1, T2&gt;\" &lt;&lt; std::endl; &#125;private: T1 _d1; T2 _d2;&#125;;template&lt;&gt;class Data&lt;int, char&gt;&#123;public: Data() &#123; std::cout &lt;&lt; \"Data&lt;int, char&gt;\" &lt;&lt; std::endl; &#125;private: int _d1; char _d2;&#125;;int main()&#123; Data&lt;int, int&gt; d1; Data&lt;int, char&gt; d2;&#125; 偏特化任何针对模版参数进一步进行条件限制设计的特化版本。比如对于以下模板类： 123456789template&lt;class T1, class T2&gt;class Data&#123;public: Data() &#123;std::cout &lt;&lt; \"Data&lt;T1, T2&gt;\" &lt;&lt; std::endl;&#125;private: T1 _d1; T2 _d2;&#125; 偏特化有以下两种表现形式：① 部分特化：将参数模板类表中的一部分参数特化 123456789template&lt;class T1&gt;class Data&lt;T1, int&gt;&#123;public: Data() &#123; std::cout &lt;&lt; \"Data&lt;T1, T2&gt;\" &lt;&lt; std::endl; &#125;private: T1 _d1; T2 _d2;&#125;; ② 对参数更进一步的限制：偏特化并不仅仅是指特化部分参数，而是针对模板参数更进一步的条件限制所设计出来的一个特化版本 12345678910111213141516171819202122232425262728//两个参数偏特化为指针类型template&lt;class T1, class T2&gt;class Data&lt;T1*, T2*&gt;&#123;public: Data() &#123; std::cout &lt;&lt; \"Data&lt;T1*, T2*&gt;\" &lt;&lt; std::endl; &#125;private: T1* _d1; T2* _d2;&#125;;//两个参数偏特化为引用类型template&lt;class T1, class T2&gt;class Data&lt;T1&amp;, T2&amp;&gt;&#123;public: Data(const T1&amp; d1, const T2&amp; d2) :_d1(d1), _d2(d2) &#123; std::cout &lt;&lt; \"Data&lt;T1&amp;, T2&amp;&gt;\" &lt;&lt; std::endl; &#125;private: T1&amp; _d1; T2&amp; _d2;&#125;;Data&lt;int , double&gt; d2; // 调用基础的模板Data&lt;double , int&gt; d1; // 调用特化的int版本Data&lt;int *, int*&gt; d3; // 调用特化的指针版本Data&lt;int&amp;, int&amp;&gt; d4(1, 2); // 调用特化的指针版本 类模板特化应用之类型萃取现在假设我们要实现一个通用的拷贝函数： 使用memcpy\\循环复制1234567891011121314template&lt;class T&gt;void Copy(T* dst, const T* src, size_t size)&#123; memcpy(dst, src, sizeof(T)*size);&#125;template&lt;class T&gt;void Copy(T* dst, const T* src, size_t size)&#123; for (size_t i = 0; i &lt; size; ++i) &#123; dst[i] = src[i]; &#125;&#125; 拷贝自定义类型对象就可能会出错，因为自定义类型对象有可能会涉及到深拷贝(比如string)，而memcpy属于浅拷贝。如果对象中涉及到资源管理，就只能用赋值，用循环赋值的方式虽然可以，但是代码的效率比较低，而C/C++程序最大的优势就是效率高。那能否将另种方式的优势结合起来呢？遇到内置类型就用memcpy来拷贝，遇到自定义类型就用循环赋值方式来做呢？ 答案是肯定的，但是由用户来判断是自定义类型还是内置类型有时难免传参会出错，所以优先使用函数自动推导来帮助我们完成这个问题： 1234567891011121314151617181920212223#include &lt;string&gt;bool IsPODType(const char* strType)&#123; const char* arrType[] = &#123; \"char\", \"short\", \"int\", \"long\", \"long long\", \"float\", \"double\", \"long double\" &#125;; for (size_t i = 0; i &lt; sizeof(arrType) / sizeof(arrType[0]); ++i) &#123; if (0 == strcmp(strType, arrType[i])) return true; &#125; return false;&#125;template&lt;class T&gt;void Copy(T* dst, const T* src, size_t size)&#123; if (IsPODType(typeid(T).name())) memcpy(dst, src, sizeof(T)*size); else &#123; for (size_t i = 0; i &lt; size; ++i) dst[i] = src[i]; &#125;&#125; 运行时类型识别（Run-Time Type Identification）RTTI，RTTI允许应用程序在执行期间标识一个对象的类型，在非多态语言(如C语言)中找不到这个概念的。非多态语言不需要运行时的类型信息，因为每个对象的类型在编译时就已经确定了。但是在支持多态的语言中(如C++)，可能存在这种情况：在编译时你并不知道某个对象的类型信息，而只有在程序运行时才能获得对象的准确信息。C++是通过类的层次结构、虚函数以及基类指针来实现多态的。基类指针可以用来指向基类的对象或者其派生类的对象，也就是说，我们并不总是能够在任何时刻都预先知道基类指针所指向对象的实际类型。因此，必须在程序中使用”运行时类型识别”来识别对象的实际类型。typeid返回指针或引用所指对象的实际类型! 类型萃取为了将内置类型与自定义类型区分开，给出以下两个类分别代表内置类型与自定义类型。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// 代表内置类型struct TrueType&#123; static bool Get()&#123; return true; &#125;&#125;;// 代表自定义类型struct FalseType&#123; static bool Get()&#123; return false; &#125;&#125;;template&lt;class T&gt;struct TypeTraits&#123; typedef FalseType IsPODType;&#125;;template&lt;&gt;struct TypeTraits&lt;char&gt;&#123; typedef TrueType IsPODType;&#125;;template&lt;&gt;struct TypeTraits&lt;short&gt;&#123; typedef TrueType IsPODType;&#125;;template&lt;&gt;struct TypeTraits&lt;int&gt;&#123; typedef TrueType IsPODType;&#125;;template&lt;&gt;struct TypeTraits&lt;long&gt;&#123; typedef TrueType IsPODType;&#125;;// ... 所有内置类型都特化一下template&lt;class T&gt;void Copy(T* dst, const T* src, size_t size)&#123; if (TypeTraits&lt;T&gt;::IsPODType::Get()) memcpy(dst, src, sizeof(T)*size); else &#123; for (size_t i = 0; i &lt; size; ++i) dst[i] = src[i]; &#125;&#125; T为int的时候：TypeTraits已经特化过，程序运行时就会使用已经特化过的TypeTraits, 该类中的IsPODType刚好为类TrueType，而TrueType中Get函数返回true，内置类型使用memcpy方式拷贝，T为string：TypeTraits没有特化过，程序运行时使用TypeTraits类模板, 该类模板中的IsPODType刚好为类FalseType，而FalseType中Get函数返回true，自定义类型使用赋值方式拷贝 在STL中也使用了类型萃取，可以参考【STL】类型萃取（TypeTraits） 模板分离编译一个程序（项目）由若干个源文件共同实现，而每个源文件单独编译生成目标文件，最后将所有目标文件链接起来形成单一的可执行文件的过程称为分离编译模式 C/C++程序要运行，一般经历以下步骤：预处理—&gt;编译—&gt;汇编—&gt;链接 预处理：头文件展开、宏替换、条件编译、去注释、编译：语法检查、生成汇编代码汇编：生成机器码，生成目标文件链接：把目标文件组合起来，生成可执行程序或者动（静）态库 现假设有a.hpp、a.cpp、main.cpp等文件 模板不支持分离编译的解决方案：1.将声明和定义放到一个文件 “xxx.hpp” 里面或者xxx.h其实也是可以的。推荐使用这种2.模板定义的位置显式实例化。这种方法不实用，不推荐使用 模板总结优点：模板复用了代码，节省资源，更快的迭代开发，C++的标准模板库(STL)因此而产生，同时模板也增强了代码的灵活性 缺点：模板会导致代码膨胀问题，也会导致编译时间变长，出现模板编译错误时，错误信息非常凌乱，不易定位错误","updated":"2020-03-13T03:06:29.747Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://zouchanglin.cn/tags/C-C/"}]},{"title":"你真的了解C语言吗","date":"2019-01-08T10:09:30.000Z","path":"2019/01/08/你真的了解C语言吗/","text":"Dennis Ritchie 过世了，他发明了C语言，一个影响深远并彻底改变世界的计算机语言。一门经历40多年的到今天还长盛不衰的语言，今天很多语言都受到C的影响，C++，Java，C#，Perl， PHP， Javascript， 等等。但是，你对C了解吗？顺便注明一下，下面的一些例子来源于这个slides。 首先，我们先来看下面这个经典的代码： 12345int main()&#123; int a = 42; printf(\"%d\\n\", a);&#125; 从这段代码里你看到了什么问题？我们都知道，这段程序里少了一个#include还少了一个return 0;的返回语句。不过，让我们来深入的学习一下: 这段代码在C++下无法编译，因为C++需要明确声明函数 这段代码在C的编译器下会编译通过，因为在编译期，编译器会生成一个printf的函数定义，并生成.o文件，链接时，会找到标准的链接库，所以能编译通过。 但是，你知道这段程序的退出码吗？在ANSI-C下，退出码是一些未定义的垃圾数。但在C89下，退出码是3，因为其取了printf的返回值。为什么printf函数返回3呢？因为其输出了4，2，\\n 三个字符。而在C99下，其会返回0，也就是成功地运行了这段程序。你可以使用gcc的 -std=c89或是-std=c99来编译上面的程序看结果。 另外，我们还要注意main()，在C标准下，如果一个函数不要参数，应该声明成main(void)，而main()其实相当于main(…)，也就是说其可以有任意多的参数，这与C++的异常捕获是一致的 我们再来看一段代码： 1234567891011121314151617void f(void)&#123; static int a = 3; static int b; int c; ++a; ++b; ++c; printf(\"a=%d\\n\", a);//4 5 6 printf(\"b=%d\\n\", b);//1 2 3 printf(\"c=%d\\n\", c);//随机 随机 随机&#125;int main(void)&#123; f(); f(); f();&#125; 这个程序会输出什么？ 我相信你对a的输出相当有把握，就分别是4，5，6，因为那个静态变量。 对于c呢，你应该也比较肯定，那是一堆乱数。 但是你可能不知道b的输出会是什么？答案是1，2，3。为什么和c不一样呢？因为，如果要初始化，每次调用函数里，编译器都要初始化函数栈空间，这太费性能了。但是c的编译器会初始化静态变量为0，因为这只是在启动程序时的动作。 全局变量同样会被初始化。 说到全局变量，你知道静态全局变量和一般全局变量的差别吗？是的，对于static 的全局变量，其对链接器不可以见，也就是说，这个变量只能在当前文件中使用 我们再来看一个例子： 12345678910111213141516void foo(void)&#123; int a; printf(\"%d\\n\", a);&#125;void bar(void)&#123; int a = 42;&#125;int main(void)&#123; bar(); foo();&#125; 你知道这段代码会输出什么吗？是42还是一个随机值？ 都对，如果你使用一般的编译，会输出42，因为我们的编译器优化了函数的调用栈（重用了之前的栈），为的是更快，这没有什么副作用。反正你不初始化，他就是随机值，既然是随机值，什么都无所谓。但是，如果你的编译打开了代码优化的开关，-O，这意味着，foo()函数的代码会被优化成main()里的一个inline函数，也就是说没有函数调用，就像宏定义一样。于是你会看到一个随机的垃圾数。 下面，我们再来看一个示例 : 123456789101112131415int b(void)&#123; printf(\"3\"); return 3;&#125;int c(void) &#123; printf(\"4\"); return 4; &#125;int main(void)&#123; int a = b() + c(); printf(\"%d\\n\", a);&#125; 这段程序会输出什么？，你会说是，3，4，7。但是我想告诉你，这也有可能输出，4，3，7。为什么呢？ 这是因为，在C/C++中，表达的评估次序是没有标准定义的。编译器可以正着来，也可以反着来，所以，不同的编译器会有不同的输出。你知道这个特性以后，你就知道这样的程序是没有可移植性的。 我们再来看看下面的这堆代码，他们分别输出什么呢？示例一 1int a=41; a++; printf(\"%d\\n\", a); 示例二 1int a=41; a++ &amp; printf(\"%d\\n\", a); 示例三 1int a=41; a++ &amp;&amp; printf(\"%d\\n\", a); 示例四 1int a=41; if (a++ &lt; 42) printf(\"%d\\n\", a); 示例五 1int a=41; a = a++; printf(\"%d\\n\", a); 只有示例一，示例三，示例四输出42，而示例二和五的行为则是未定义的。关于这种未定义的东西是因为Sequence Points的影响（Sequence Points是一种规则，也就是程序执行的序列点，在两点之间的表达式只能对变量有一次修改），因为这会让编译器不知道在一个表达式顺列上如何存取变量的值。比如a = a++，a + a++，不过，在C中，这样的情况很少。 下面，再看一段代码：（假设int为4字节，char为1字节） 12345678int main()&#123; struct X &#123; int a; char b; int c; &#125;; printf(\"%d,\", sizeof(struct X)); struct Y &#123; int a; char b; int c; char d; &#125;; printf(\"%d\\n\", sizeof(struct Y));&#125; 答案是12、16，这是结构体的内存对齐： 但是，你知道为什么要字节对齐吗？还是因为性能。因为这些东西都在内存里，如果不对齐的话，我们的编译器就要向内存一个字节一个字节的取，这样一来，struct X，就需要取9次，太浪费性能了，而如果我一次取4个字节，那么我三次就搞定了。所以，这是为了性能的原因。 但是，为什么struct Y不向12 对齐，却要向16对齐，因为char d; 被加在了最后，当编译器计算一个结构体的尺寸时，是边计算，边对齐的。也就是说，编译器先看到了int，很好，4字节，然后是 char，一个字节，而后面的int又不能填上还剩的3个字节，不爽，把char b对齐成4，于是计算到d时，就是13 个字节，于是就是16啦。但是如果换一下d和c的声明位置，就是12了。 另外，再提一下，上述程序的printf中的%d并不好，因为，在64位下，sizeof的size_t是unsigned long，而32位下是 unsigned int，所以，C99引入了一个专门给size_t用的%zu。这点需要注意。在64位平台下，C/C++ 的编译需要注意很多事。你可以参看《64位平台C/C++开发注意事项》 下面，我们再说说编译器的Warning，请看代码: 123456#include &lt;stdio.h&gt;int main(void)&#123; int a; printf(\"%d\\n\", a);&#125; cc -Wall a.c cc -Wall -O a.c 前一种是不会编译出a未初化的警告信息的，而只有在-O的情况下，才会有未初始化的警告信息。这点就是为什么我们在makefile里的CFLAGS上总是需要-Wall和 -O。 最后，我们再来看一个指针问题，你看下面的代码: 123456789#include &lt;stdio.h&gt;int main(void)&#123; int a[5]; printf(\"%d\\n\", a); printf(\"%d\\n\", a + 1); printf(\"%d\\n\", &amp;a); printf(\"%d\\n\", &amp;a + 1);&#125; 假如我们的a的地址是：1527552（为了方便我使用了%d输出），而且是32位机，那么这个程序会输出什么？ 第一条printf语句应该没有问题，就是 1527552 第二条printf语句你可能会以为是1527553。那就错了，a+1，编译器会编译成 a+ 1*sizeof(int)，int在32位下是4字节，所以是加4，也就是1527556 第三条printf语句可能是你最头疼的，我们怎么知道a的地址？我不知道吗？可不就是bfe2e100。那岂不成了a==&amp;a啦？这怎么可能？自己存自己的？也许很多人会觉得指针和数组是一回事，那么你就错了。如果是 int *a那么没有问题，因为a是指针，所以&amp;a 是指针的地址，a 和&amp;a不一样。但是这是数组啊a[ ]，所以&amp;a其实是被编译成了&amp;a[0]。 第四条printf语句就很自然了，就是bfe2e104。还是不对，因为是&amp;a是数组，被看成int(*)[5]，所以sizeof(a)是5，也就是5*sizeof(int)，也就是152727572。 看下面这一段代码： 12345678int x = 5;int fun() &#123; int x = 3; &#123; extern int x; return x; &#125;&#125; fun函数的返回值是多少？当然是5 看过这么多，你可能会觉得C语言设计得真扯淡啊。不过我要告诉下面几点Dennis当初设计C语言的初衷： 相信程序员，不阻止程序员做他们想做的事 保持语言的简洁，以及概念上的简单 保证性能，就算牺牲移植性 今天很多语言进化得很高级了，语法也越来越复杂和强大，但是C语言依然光芒四射，Dennis离世了，但是C语言的这些设计思路将永远不朽。 转载自酷壳；","updated":"2020-03-13T03:06:29.717Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://zouchanglin.cn/tags/C-C/"}]},{"title":"UML建模总结","date":"2019-01-02T10:09:30.000Z","path":"2019/01/02/UML建模总结/","text":"UML简介UML（United Modeling Language）统一建模语言，一种基于面向对象的可视化建模语言 UML采用一组形象化的图像（如类图）符号作为建模语言，是这些符号可以形象地描述系统的各个方面UML通过建立图形之间的各种关系来描述模型！ UML工具我在学习中使用的是StarUML，这是一款开源免费的UML图绘制工具，点击这里即可下载，当然在有一些其他的UML工具，比如RationalRose 、PowerDesigner 等等不在此一一赘述！ 软件工程生命周期软件工程可以分为三个大阶段：需求、设计、测试与维护 一、需求：开发目标、可行性分析、需求分析 二、设计：概要设计、详细设计、编码与单元测试 三、测试与维护：综合测试、维护 一般来说，软件开发生命周期可以使用PDCA来概述： P（Plan）——软件规格说明，规定软件的功能及其运行时的限制。 D（DO）——软件开发，开发出满足规格说明的软件。 C（Check）——软件确认，确认开发的软件能够满足用户的需求。 A（Action）——软件演进，软件在运行过程中不断改进以满足客户新的需求。 从软件开发的观点看，它就是使用适当的资源（包括人员，软硬件资源，时间等），为开发软件进行的一组开发活动，在活动结束时输入（即用户的需求）转化为输出（最终符合用户需求的软件产品） UML中的10种图静态模型图描述系统的静态结构 类图 对象图 包图 组件图 部署图 动态模型图描述系统行为的各个方面 用例图 时序图 协作图 状态图 活动图 UML中的关系UML中的关系主要是包括四种：关联关系（association）依赖关系（dependency）泛化关系（generalization）实现关系（realization） 用例图 UseCase Diagram用例图（Use Case Diagram）：也称为用户模型图，是从软件需求分析到最终实现的第一步，它是从客户的角度来描述系统功能 用例图包括3个基本组件：参与者（Actor）、用例（Use Case）、关系 参与者：与系统打交道的人或者其他系统即使使用该系统的人或者事物，在UML中参与者用人形图标表示 用例：代表系统的某项完整的功能，在UML中使用一个椭圆来表示 关系：定义用例之间的关系……泛化关系、扩展关系、包含关系 相关操作：右键添加各种图像，通过文件导出为图像 用例关系 —— 泛化关系 Generalization泛化关系：表示同一业务目的（父用例）的不同技术实现（各个子用例），在UML中，用例泛化用一个三角箭头从子用例指向父用例 用例关系 —— 包含关系一个用例可以包含其他用例的行为，并把它包含的用例行为作为自身行为的一部分，在UML中包含关系用虚线箭头+《include》，箭头指向被包含的用例 用例关系 —— 拓展关系如果在完成某个功能的时候偶尔会执行另外一个功能，则用拓展关系表示，在UML中拓展关系用虚线箭头+《extend》，箭头指向被拓展的用例 用例图的小练习下面是关于一个公司的人事管理系统的需求的简单描述，建立其相应的用例模型：该人事管理系统的用户是公司的人事管理干部.该系统具有人事档案库， 保存员工的人亊信息，包括姓名，性别，出生年月，健 康状况，文化程度，学位，职称，岗位，聘任时间，任期 ，工资，津贴，奖罚记录，业绩，论著和家庭情况等，系统提供的基本眼务有人事信息的管理，包括人事规定的权调动与聘任，职称评定,奖罚等，并且可以按照限查询人事信息，生成与输出统计报表等.该人事系统每月向公司的财务系统提供员工的工资，津贴等数据. 类图 Class diagram 类图是面向对象建模中常用的图，是定义其他图的基础 类图主要是用来显示系统中的类，接口以及它们之间的关系 类图包含的主要元素有类，接口和关系，其中关系包括：泛化关系、关联关系、依赖关系和实现关系，在类图中也可以包含注释和约束 类图的表示法类是类图的重要组件，由三部分组成：类名、属性和方法 在UML中类用矩形表示，顶端存放类名，中间存放类的属性(属性的类型及值)，底部存放类的方法(方法的参数和返回值类型) 在UML中可以根据实际情况有选择的隐藏属性部分和方法部分或者两者都隐藏 在UML中公有(public)的东西使用+表示，私有(private)的东西使用-表示，保护(protected)的东西使用#表示，UML工具的开发者也可以自定义符号来表示 属性的完整表示方式：可见性 名称 ：类型 [ = 缺省值] 中括号中的内容表示是可选的，例如下图所示的类图： 接口的表示法接口中包含方法，但是不包含属性，在UML中接口用一个带有名称的圆圈表示，并且通过一条实线与它的模型元素相连，但是有时候接口也用普通类的矩形符号表示： 类与类关系——泛化关系 Generalization在UML中泛化关系用来表示类与类、接口与接口之间的继承关系，泛化关系有时也称为：is a kind of关系 在UML中泛化关系用一条实线空心箭头由子类指向父类 类与类关系——依赖关系 Dependency以下面的代码作为例子，假设Person类一个对象上班需要乘客车，或者这个对象是卖客车的，那么Person类就与Car类产生联系，这种联系就叫做依赖关系： 123456789class Car&#123; &#125;;class Person&#123;public: //上班乘车 void GoToWork(Car&amp; car)&#123; &#125; //卖车 Car&amp; SellCar()&#123; &#125;&#125;; 类与类关系——关联关系 Association关联关系很好理解，借助上面的例子，原来Person对象是做客车去上班，关联关系就可以理解为这次Person对象是开私家车去上班！ 两个相对独立的系统，当一个系统的实例与另一个系统的一些特定实例存在固定的对应关系时，这两个系统之间应该是关联关系，意思就是：一个类是另一个类的成员变量，例如：订单与客户之间的关系，每个订单对应着特定的客户，每个客户对应着特定的订单！ 123456789101112class Car&#123;public: void run()&#123; &#125;;&#125;;class Person&#123;public: void GoWork()&#123; myCar.run(); &#125;;private: Car myCar;&#125;; 关联关系的多重性关联关系的多重性是指有多少个对象可以参与该关联，多重性可以用来表达一个取值范围，特定值，无限定值的范围： 关联关系——聚合 Aggregation这个比较简单，这好比一辆汽车可以选择很多型号和品牌的发动机 聚合关系是关联关系的一种是更强的关联关系 聚合是整体和部分之间的关系，例如：电脑由CPU、内存、输出输出设备组成 聚合关系也是通过成员变量实现的，但是关联关系所涉及的两个类处于同一个层次上，而聚合关系中，两个类处于不同的层次上，一个代表整体，一个代表部分 聚合使用 空心菱形+实线表示 关联关系——组合 Composition如果是聚合关系是一辆汽车可以选择很多型号和品牌的发动机，那么组合关系联系就更加紧密了，这就好比人和自己的五脏六腑，生命周期一致的缺一不可！ 在UML中组合关系是一种关联关系，是一种比聚合还要强的关系 代表整体的对象负责代表部分对象的生命周期 组合更加强调生命周期的一致性 组合使用 实心菱形+实线表示 类图的小练习汽车和自行车都是交通工具（vehicle），一辆自行车（bicycle）只归一个人（person）所有，但是一辆汽车（auto）可以归一个人或者两个人所有，一个人可能没有自行车或者汽车，也可能有多辆自行车或者汽车，人分为男人（male）和女人（female），每个人都有年龄（age）和名字（name），每辆交通工具都有自己的颜色（color）和商标（brand），每辆汽车都有两个前灯（headlight）和一台发动机（motor） 对象图 Object Diagram 对象图是类图的一个实例，用于显示系统执行时的一个可能的快照，即在某一时间上系统可能出现的样子，对象图用带下划线的对象名称来表示对象 表现对象的特征 对象图展现了多个对象的特征以及对象之间的关系 下面是一个Person对象使用MacBook Pro对象和ASUS_Windows的示例 时序图 Sequence Diagram 时序图用于描述对象之间的消息传递的时间顺序，即用例中的行为顺序 当执行一个用例时，时序图的每条消息对应了一个类操作或者引起转换的触发时间 在UML中，时序图表示为一个二维的关系图，其中纵轴是时间轴，时间沿竖线向下延伸。横轴代表在协作中各个独立的对象，当对象存在时，生命线用一条虚线表示消息从一个对象的生命线到另一个对象生命线的箭头表示，箭头以时间的顺序在图中上下排列 时序图基本概念对象时序图中对象使用矩形表示，并且对象名称下有下划线，将对象至于时序图的顶部说明在交互开始时对象就已经存在了，如果对象的位置不在顶部表示对象实在交互的过程中被创建的 生命线生命线是一条垂直的虚线，表示时序图中对象在一段生命周期内的存在，每个对象底部中心位置都带有生命线 消息两个对象之间的单路通信，从发送方指向接收方，在时序图中很少使用返回消息 时序图练习关于登录的时序图的练习： 活动图 Activity Diagram在UML中，活动图本质上就是流程图，它用于描述系统的活动，判定点，分支等等 活动图基本概念动作状态原子的，不可中断的动作，并在此动作完成之后向另一动作转变，在UML中动作状态用圆角矩形表示，动作状态所表示的动作卸载圆角矩形的内部 分支与合并分支在软件系统中很常见，一般用于表示对象所具有的条件行为。用一个布尔表达式的真假来判断动作的流向，条件行为用分支和合并表达。在活动图中，分支用空心小菱形表示，分支包括一个入转换和两个带条件的出转换，出转换的条件应该是互斥的，须保证只有一条出转换能够被触发，合并两个带条件的入转换和一个出转换 分叉与汇合分叉用来描述并发线程，每个分叉可以有一个输入转换和两个或多个输出转换，每个转换都可以是独立的控制流，汇合代表两个或者多个并发控制流同步发生，当所有的控制流都达到汇合点后，控制才能继续往下进行，每个汇合可以有两个或多个输入转换和一个输出转换，在UML中分叉和汇合用一条粗直线表示 泳道泳道将活动图的活动划分为若干组，并将每一组指定给负责这组活动的业务组织，泳道区分负责活动的对象，明确的表示是哪些活动是由哪些对象进行的。每个活动制定明确的属于一个泳道，在活动图中，泳道用垂直的实线绘出，垂直线分割的区域即为泳道 活动图练习 某公司销售人员接到订单后，将订单传给财务人员和 仓库人员.财务人员开具发票，并收款。仓库人员准备货物，并查看是否货物加急,葙是加急采用EMS方式发货，否则采用普通包裹方式发货.完成之后由销售人员关闭该订单.根据上面描述画出该公司销售过程的活动图： 状态图 Statechart Diagram状态图通过建立对象的生命周期模型来描述对象随时间变化的动态行为！ 状态图基本概念状态用圆角矩形表示，状态名称表示状态的名字， 通常用字符串表示，一个状态的名称在状态图所在的上下文中应该是唯一的. 转换用带箭头的直线表示，一端连着源状态，一端连着目标状态. 初始状态每个状态图都有一个初始状态，此状态代表状态图的起始位置，初始状态只能作为转换的源，不能作为转换的目标，并且在状态图中只能有一个。初始状态用一个实心圆表示. 终止状态模型元素的最后状态，是一个状态图的终止点，终止状态在一个状态图中可以有多个 状态图的练习下面是Linux进程状态图的练习，当然不是很详细，像僵尸状态没有画出来： 协作图 Collaboration Diagram协作图（也叫作合作图、通信图）是一种交互图 时序图主要侧重于对象间消息传递在时间上的先后关系，而协作图表达对象之间的交互的过程以及对象之间的关联关系，时序图跟协作图可以相互转化，不难理解，协作图的构成有角色，对象，连接，消息。具体含义同时序图。 协作图表现的是对象在空间上的联系，所以不存在时序图中的生命线和激活器 协作图是动态图的另一种表现形式 强调参加交互的各对象结构的信息 是一种类图，包含各类元角色和关联角色，而不仅仅是类元和关联 强调参加交互的各对象的组织 协作图可以被视为对象图的扩展 协作图的练习 包图 Package Diagram包图由包和包之间的关系组成，包的图标就如同一个带标签的文件夹 维护和控制系统总体结构的重要建模工具 方便理解和处理整个模型 设计良好的包是高内聚、低耦合的，并对其内容的访问具有严密的控制 包提供了一种用于组织各种元素的分组机制，在UML中，包用来对元素进行分组，并为这些元素提供命名空间，包所拥有的或者引用的所有元素称为包的内容，包没有实例 组件图 Component Diagram组件图用来建立系统中各组件之间的关系，各组件通过功能组织在一起 构件图 = 构件（Component）+接口（Interface）+关系（Relationship）+端口（Port）+连接器（Connector） JavaBean、ejb、jsp都是组件，在UML中，组件使用左侧有个两个小矩形的的大矩形来表示 组件图可以用来设计系统的整体框架 部署图 Deployment Diagram部署图用来帮助开发者了解软件中的各个组件驻留在什么硬件位置，以及这些硬件之间的交互关系。使用部署图可以显示运行时系统的结构，同时还传达构成应用程序的硬件和软件元素的配置和部署方式。 节点：用来表示一种硬件，可以是打印机，计算机等。节点的标记符号是一个三维框，在框的左上方包含 了节点的名称。包括节点的表示，节点的分类，节点中的构件，节点属性，节点与构件。 节点分类 处理器（Processor），处理器是能够执行软件、具有计算能力的节点。 设备（Device） :设备是没有计算能力的节点，通常情况下都是通过其接口为外部提供某种服务，例如打印机、IC读写器，如果我们的系统不考虑它们内部的芯片，就可以把它们看作设备 通信关联 节点通过通信关联建立彼此的关系，采用从节点到节点绘制实线来表示关联，对于企业的计算机系统硬件设备间的关系，但是通常关心的是节点之间是如何连接的，因此描述节点间的关系一般不使用名称，而是使用构造型描述","updated":"2020-03-13T03:06:29.711Z","categories":[{"name":"设计工具","slug":"设计工具","permalink":"https://zouchanglin.cn/categories/%E8%AE%BE%E8%AE%A1%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"UML","slug":"UML","permalink":"https://zouchanglin.cn/tags/UML/"}]},{"title":"2018年度总结","date":"2019-01-01T10:09:30.000Z","path":"2019/01/01/2018年度总结/","text":"不知不觉2018已经过去了，又是一年不知不觉消失在指尖，不过2018真的是我自从进入大学以来最有收获的一年，现在马上就是2019了，刚刚躺下的我居然又坐在电脑面前，做事要有始有终，既然在2018开通了博客就在最后写个总结吧，始终还是觉得键盘的感觉还是比写字的感觉的爽，现在呢不扯那些没用的，先回顾回顾这个2018到底干了啥？ 首先，2018是让我重新认识C语言的一年，因为在去年自己只是粗略的看了一下C语言，说实话，学校教的void main 至今让我感觉是那么强的违和感，从基础的C语言到进阶C语言，再到看完《C陷阱和缺陷》（其实这本书让我认识到C语言本质上就是一套标准）、《C与指针》，从书中获得的知识远远比想象中多太多了，宋真宗说道：书中自有千钟粟、书中自有黄金屋、书中自有颜如玉，虽是古人之言但到如今也有几分道理，多看书，多思考，多实践才是硬道理，这一年写了很多都是C代码，明白了C语言的很多好处或者说是C语言的设计思想，设计理念。在学习C语言的过程中细节很重要，像浮点数存储、函数栈帧、结构体内存对齐这种东西在去年重来都没有想过，以前非常惧怕C语言的指针，然而现在想起来也都不过如此，而且指针真的是C语言的精华所在，果然C生万物，此话不假！ 接下来是数据结构，数据结构与算法对我来说一直是弱项，在2018这一年算是对基础的数据结构与算法狂补了一下，其中顺序表、链表、哈希表、栈和队列、二叉树、堆、排序算法等等都是基础的数据结构或算法，我想接下来我要做的是好好巩固这些数据结构，也要把高阶数据结构学好！以前那些连看看不敢看的算法题现在虽然做出来也要花费一下时间，但是能做出来，只是要多花一点时间罢了，这都不是重点，重点是我以前对待算法题那种恐惧感没有了，这也许就是自信吧，希望从我踏出大学校门的那一刻，数据结构和算法这样的内功是非常深厚的！ 关于操作系统很要说的就更多了，不过我主要是学习的Linux操作系统，《鸟哥的Linux私房菜》和《现代操作系统》确实给了我很大的启发（不过都只是选章节看），在2017年刚开始接触Linux操作系统，甚至直接把自己的电脑装成Ubuntu18.04LTS（在2017年用的Ubuntu16.04LTS），不过还是装回了Windows，有时候Linux桌面系统还是很不稳定的，从当初的苦苦折腾桌面系统，再到直接只命令行从中还是让我学到了不少的东西，其实在2017年有直接在命令行下编程的想法，但是那个时候我的vim和gdb用的一塌糊涂，最终在2017年结尾也没有时间去掌握Vim以及GDB（其实也不是没时间，主要是对自己不够狠，或者理解为VisualStudio的诱惑很大也行），现在呢再也没有遇到命令行那种恐惧感，也再也不会对vim那种神之编辑器产生恐惧感，确实让我见证了vim的强大，很多时候使用命令行操作确实比桌面系统更便捷，用久了便会深有体会！从冯诺依曼结构的计算机基础结构到进程、信号、线程等等，接下来的还差的就是网络了编程了，Linux系统确实是一款优秀的系统，通过Linux的源码确实可以让我感受到了Linux内核开发者的设计思想，但是我对这方面的体会远远不够，一切皆文件定要好好体会，希望在2019年我对Linux的认识可以上一个新台阶！ 至于今年学的C++呢，我感觉2017年学的C++还是有些肤浅，毕竟这一年没用到太多C++的地方，C代码到时写了不少，C++总体感觉不是很多，但是同C语言一样，要注意细节问题，原理性问题，而且C++的更新迭代也比较快以前从来没用过C++11那些新特性，今年看了看也还挺好，C++之旅还未结束。在之前STL那些苦只是会用那些接口，自己从来没有尝试过去实现那些接口，最多也就试了试String的模拟实现，但是在2018还是尝试着把STL中基本数据结构都实现了一下，迭代器设计模式已经深深映入脑海，反正以前自己用这些接口的时候是从来都没想过怎么实现的，想在想想也不是很难，需要在那还中有个大致框架，毕竟是语法和代码要实际操练的才会记忆深刻！ 已经到了2019，希望继续保持学习的热情，骐骥一跃不能十步，驽马十驾功在不舍！","updated":"2020-03-13T03:06:29.615Z","categories":[{"name":"生活","slug":"生活","permalink":"https://zouchanglin.cn/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"生活","slug":"生活","permalink":"https://zouchanglin.cn/tags/%E7%94%9F%E6%B4%BB/"}]},{"title":"Linux信号机制","date":"2018-12-25T11:20:00.000Z","path":"2018/12/25/Linux信号机制/","text":"信号的概念信号的基本概念很简单，谍战剧里面的信号的概念就体现的非常形象，每次情报人员之间沟通的时候就用电台，就比如电台和密码本，每个对应的电台信号都有一个对应的意义，Key-Value形式的，比如A信号表示进攻、B信号表示撤退，非常容易理解的概念。再比如街上的红绿灯，红灯停、绿灯行…. 我们既然知道了什么是信号，那么如何处理信号呢？ 收到信号执行默认动作，比如看到红灯就停下来 忽略信号，比如看到红灯就当没看到，继续往前走 收到信号执行自定义动作，比如看到红灯就躺在街上睡觉，然后被车碾压… 那么Linux下的进程能够处理信号的前提是认识信号，这就和我们要处理红绿灯的信号的前提是必须认识红绿灯信号，进程收到信号有可能并不会立即处理，而是在合适的时候！ 查看Linux下所有的信号（编号34以上的是实时信号，实时信号必须立即处理）：信号事件的产生对进程而言是异步的，这个不难理解，因为你也不知道别人什么时候给你发信号，所以信号的产生跟进程不是同步的，这是两个没有因果关系的东西！ 进程即使收到信号可能也无法立即处理，信号如果无法立即处理就应该把信号保存起来，保存在PCB的一个位图里面，因为只需要用31个比特位来存储是否收到信号即可，一个int32字节，所以使用一个int就可以保存31个信号 所以：发送信号的本质就是让操作系统去修改目标进程的信号位图 在此我猜想一下，Java等高级语言捕获异常的原理只不过是程序出错后屏蔽了导致进程退出的信号而已！！！ 信号的产生键盘通过键盘产生，比如Ctrl C产生终止进程的SIGIN信号注意：键盘上的组合键形成的信号只能用于前台进程！前台进程随时随地都可以收到一个信号，因为你在这个进程运行的任何时刻，你都可以出入Ctrl-C终止该进程。这也就说明了：信号对于进程来说是异步的。 程序运行时异常程序运行时异常，比如除0产生SIGFPE信号 12345678910int main()&#123; int i = 0; while(1)&#123; if(i++ &gt;= 5)&#123; i /= 0; &#125; cout &lt;&lt; \"hello,world pid = \"&lt;&lt; getpid() &lt;&lt; endl; &#125; return 0;&#125; 接下来说一个调试程序BUG的技巧：事后调试 就是在程序已经出错的情况下通过产生的Core Dump来查看程序异常信息，当一个进程要异常终止时，可以选择把进程的用户空间内存数据全部保存到磁盘上，文件名通常是core，这叫做Core Dump。进程异常终止通常是因为有Bug，比如非法内存访问导致段错误，事后可以用调试器检查core文件以查清错误原因，这叫做Post-mortem Debug（事后调试）。 一个进程允许产生多大的core文件取决于进程的Resource Limit(这个信息保存 在PCB中)。默认是不允许产生core文件的，因为core文件中可能包含用户密码等敏感信息，不安全。在开发调试阶段可以用ulimit命令改变这个限制,允许产生core文件。首先用ulimit命令改变Shell进程的Resource Limit,允许core文件最大为1024K:ulimit -c 1024 123456789101112131415161718192021222324252627282930313233343536[root@xpu code]# ulimit -acore file size (blocks, -c) 0data seg size (kbytes, -d) unlimitedscheduling priority (-e) 0file size (blocks, -f) unlimitedpending signals (-i) 7424max locked memory (kbytes, -l) 64max memory size (kbytes, -m) unlimitedopen files (-n) 65535pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200real-time priority (-r) 0stack size (kbytes, -s) 10240cpu time (seconds, -t) unlimitedmax user processes (-u) 7424virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited[root@xpu code]# ulimit -c 1024[root@xpu code]# ulimit -acore file size (blocks, -c) 1024data seg size (kbytes, -d) unlimitedscheduling priority (-e) 0file size (blocks, -f) unlimitedpending signals (-i) 7424max locked memory (kbytes, -l) 64max memory size (kbytes, -m) unlimitedopen files (-n) 65535pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200real-time priority (-r) 0stack size (kbytes, -s) 10240cpu time (seconds, -t) unlimitedmax user processes (-u) 7424virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited 其中限定了一些用户使用资源的上限，比如core文件大小，最多打开的文件数目，最多多少消息队列等等，只要把core文件大小设定一下，就可以把系统产生的core文件保存下来，因为core文件通常较大和安全性的问题，所以默认是设置core文件大小为0 当然，gcc和g++编译器默认生成的是Release版本的程序，无调试功能，在编译的时候需要加上-g选项才能进行事后调试： 12345678910111213141516171819202122[root@xpu code]# gdb testGNU gdb (GDB) Red Hat Enterprise Linux (7.2-92.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type \"show copying\"and \"show warranty\" for details.This GDB was configured as \"x86_64-redhat-linux-gnu\".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...Reading symbols from /root/code/test...done.(gdb) core-file core.30712[New Thread 30712]Reading symbols from /lib64/libm.so.6...(no debugging symbols found)...donLoaded symbols for /lib64/libm.so.6Loaded symbols for /lib64/ld-linux-x86-64.so.2Core was generated by `./test'.Program terminated with signal 8, Arithmetic exception.#0 0x00000000004009c4 in main () at test.cpp:724 i /= 0; Missing separate debuginfos, use: debuginfo-install 很明显的除0错误，连行号都可以显示出来！ 很显然，进程收到信号的时候有很多种选择，执行默认动作，忽略，执行自定义动作，那么信号如何捕捉？ 1234#include &lt;signal.h&gt;typedef void (*sighandler_t)(int);sighandler_t signal(int signum, sighandler_t handler); 只需要使用signal函数即可捕捉信号，参数里面的函数指针锁指向的函数将决定收到信号后究竟会做什么，下面这个例子展示了如何捕捉信号： 12345678910111213141516171819202122232425#include &lt;iostream&gt;#include &lt;unistd.h&gt;#include &lt;signal.h&gt;#include &lt;sys/types.h&gt;using namespace std;void handler(int signo)&#123; sleep(1); cout &lt;&lt; \"catch a sig, sigo:\" &lt;&lt; signo &lt;&lt; \" pid:\" &lt;&lt; getpid() &lt;&lt; endl;&#125;int main()&#123; int i = 0; //捕获一个除0之后异常的信号 signal(SIGFPE, handler); while(1)&#123; if(i++ &gt;= 5)&#123; i /= 0; &#125; cout &lt;&lt; \"hello,world pid = \"&lt;&lt; getpid() &lt;&lt; endl; sleep(1); &#125; return 0;&#125; 为什么虽然捕捉到了信号，但是一直不停的捕捉信号呢？原因是因为捕捉到异常信号后没有终止进程，导致PCB上下文中保存着CPU的寄存器中的信息，该进程被切换出去之后当再次获得CPU执行权的时候，等到寄存器中的错误信息一恢复又会出现硬件错误，操作系统又会给进程发送SIGFPE信号，所以在捕捉到异常信号的时候别忘记终止进程，本例中也就是在handler函数中添加一句exit(1)； Kill命令kill命令产生信号，这个不难理解，比如我们杀死进程用的kill -9 系统调用123#include &lt;signal.h&gt;int kill(pid_t pid, int signo);int raise(int signo); 这两个函数都是成功返回0，错误返回-1。kill一般用于向别的进程发送信号，而raise用于进程自己向自己发送信号 12#include &lt;stdlib.h&gt;void abort(void); abort函数使当前进程接收到信号而异常终止，完全可以理解为调用abort函数的进程打算使用6号信号自杀，就像exit函数一样，abort函数总是会成功的，所以没有返回值。即使SIGABORT被进程设置为阻塞信号，调用abort()后，SIGABORT仍然能被进程接收！ Kill命令其实就是调用了kill系统接口实现的命令，简单的实现一个kill命令： 123456789//mykill 1234 9int main(int argc, char* argv[])&#123; if(argc != 3)&#123; cout &lt;&lt; \"参数异常\" &lt;&lt; endl; return -1; &#125; kill(atoi(argv[1]), atoi(argv[2])); return 0;&#125; 软件条件产生信号软件条件产生信号比较特殊，因为像野指针访问内存错误、除零这种错误其实都属于硬件错误，由于硬件发送错误引起的异常(例如：你使用了野指针，那么直接导致报错的硬件就是MMU)，但是接下来要说的这种情况是软件产生的异常：在学习管道的时候我们就发现，如果读端都把文件描述符关闭了，那么写端也不会再写了，操作系统向写端发送9号信号终止写端进程，避免资源浪费，所以由此可见：不但硬件错误会产生信号，软件条件或者错误同样会产生信号！ 12#include &lt;unistd.h&gt;unsigned int alarm(unsigned int seconds); 调用alarm函数可以设定一个闹钟,也就是告诉内核在seconds秒之后给当前进程发送SIGALRM信号, 该信号的默认处理动作是终止当前进程。接下来的演示就是捕获一下alarm函数产生的信号，应该是捕获14 SIGALRM信号： 12345678910111213141516#include &lt;iostream&gt;#include &lt;signal.h&gt;#include &lt;unistd.h&gt;using namespace std;void handler(int singo)&#123; cout &lt;&lt; \"singo = \" &lt;&lt; singo &lt;&lt; endl;&#125;int main()&#123; signal(SIGALRM, handler); alarm(1); sleep(5); return 0;&#125; 阻塞信号信号相关常见概念 实际执行信号的处理动作称为信号递达(Delivery) 信号从产生到递达之间的状态,称为信号未决(Pending) 进程可以选择阻塞 (Block )某个信号 被阻塞的信号产生时将保持在未决状态,直到进程解除对此信号的阻塞,才执行递达的动作 注意：阻塞和忽略是不同的,只要信号被阻塞就不会递达,而忽略是在递达之后可选的一种处理动作。 信号在内核中的示意图每个信号都有两个标志位分别表示阻塞(block)和未决(pending)，还有一个函数指针表示处理动作。信号产生时，内核在进程控制块中设置该信号的未决标志，直到信号递达才清除该标志。在上图中，SIGHUP信号未阻塞也未产生过，当它递达时执行默认处理动作。SIGINT信号产生过，但正在被阻塞,所以暂时不能递达。虽然它的处理动作是忽略，但在没 有解除阻塞之前不能忽略这个信号,因为进程仍有机会改变处理动作之后再解除阻塞。 SIGQUIT信号未产生过，一旦产生SIGQUIT信号将被阻塞，它的处理动作是用户自定义函数sighandler。如果在进程解除对某信号的阻塞之前这种信号产生过多次，将如何处理？POSIX.1允许系统递送该信号一次或多次。Linux是这样实现的：常规信号在递达之前产生多次只计一次，而实时信号在递达之前产生多次可以依次放在一个队列里，暂时不讨论实时信号。 sigset_tsigset_t是一种结构体，sigset_t类型对于每种信号用个bit表示”有效”或”无效”状态，至于怎么实现，我们作为使用者无须在意，其定义在/usr/include/bits/sigeset.h从上图来看,每个信号只有一个bit的未决标志，非0即1，不记录该信号产生了多少次，阻塞标志也是这样表示的。 因此，未决和阻塞标志可以用相同的数据类型sigset_t来存储，sigset_t称为信号集，这个类型可以表示每个信号的”有效”或”无效”状态，在阻塞信号集中”有效”和”无效”的含义是该信号是否被阻塞。而在未决信号集中”有效”和”无效”的含义是该信号是否处于未决状态。 阻塞信号集也叫做当前进程的信号屏蔽字(Signal Mask)，注意：这里的”屏蔽”应该理解为阻塞而不是忽略。 信号集操作函数为什么提供了一组信号集操作函数呢？很明显Linux系统的设计者认为让其他人自行操作信号机是非常危险的一件事情，或者说设计者们根本不信任我们对比特位的操作能力，于是乎为我们提供了一种API来操作信号集，当然你可以理解为这是为了让我们这些使用者更方便！ sigset_t类型对于每种信号用一个bit表示”有效”或”无效”状态，至于这个类型内部如何存储这些bit则依赖于系统实现，从使用者的角度是不必关心的，使用者只能调用以下函数来操作sigset_t变量，而不应该对它的内部数据做任何解释，比如用printf直接打印sigset_t变量是没有意义的 123456#include &lt;signal.h&gt;int sigemptyset(sigset_t *set);int sigfillset(sigset_t *set);int sigaddset (sigset_t *set, int signo);int sigdelset(sigset_t *set, int signo);int sigismember（const sigset_t *set, int signo); sigemptyset 此函数用于清空信号集，使得目标信号集中不包含任何有效信号 sigfillse 此函数用于初始化目标信号集，把所有的信号加入到此信号集里即将所有的信号标志位置为1，可以理解为把所有信号都加入集合, 如果你不想阻塞哪些信号再sigdel单独删去它们即可 sigaddset 和sigdelset 在初始信号集之后就可以调用sigaddset和sigdelset在该信号集中添加或删除某种有效信号，两个函数都是成功返回0，出错返回-1 sigismember 是一个布尔函数，用于判断一个信号集的有效信号中是否包含某种信号，若包含则返回1，不包含则返回0，出错返回-1sigprocmask此函数用于读取或更改进程的信号屏蔽字(阻塞信号集)123#include &lt;signal.h&gt;/* Prototype for the glibc wrapper function */int sigprocmask(int how, const sigset_t *set, sigset_t *oldset); 如果oldset是非空指针，则读取进程的当前信号屏蔽字通过oldset参数传出。如果set是非空指针，则更改进程的信号屏蔽字，参数how指示如何更改。如果oldset和set都是非空指针,则先将原来的信号屏蔽字备份到oldset里，然后根据set和how参数更改信号屏蔽字。 how : 如何更改进程的信号屏蔽字假设当前的信号屏蔽字为mask,下表说明了how参数的可选值：| 选项 | 描述 || :———: | :———————————————————-: || SIG_BLOCK | set包含了我们希望添加到当前信号屏蔽字的信号，相当于mask=mask|set || SIG_UNBLOCK | set包含了我们希望从当前信号屏蔽字中解除的信号，相当于mask=mask&amp;~set || SIG_SETMASK | 设置当前信号屏蔽字为set所指向的值，相当于mask=set |sigset_t *set：要更改的信号屏蔽字的结构体指针sigset_t *oldset：将原来的信号屏蔽字备份到oldset中，不需要备份传入NULL即可return：若成功则为0，若出错则为-1 注意：如果调用sigprocmask解除了对当前若干个未决信号的阻塞，则在sigprocmask返回之前，至少将其中一个信号递达！ sigpending读取当前进程的未决信号集，通过set参数传出，调用成功返回0，失败返回-1 12#include &lt;signal.h&gt;int sigpending(sigset_t *set); 下面的一个示例程序演示了上述函数的作用 12345678910111213141516171819202122232425262728293031323334353637#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;signal.h&gt;void printsigset( sigset_t *set )&#123; int i = 0; for (; i &lt; 32; i++ ) &#123; if ( sigismember( set, i ) ) /* 判断指定信号是否在信号集中 */ &#123; putchar( '1' ); &#125;else&#123; putchar( '0' ); &#125; &#125; puts( \"\" );&#125;int main()&#123; /* 定义信号集对象，并清空初始化 */ sigset_t s, p; sigemptyset( &amp;s ); sigaddset( &amp;s, SIGINT ); /* 设置阻塞信号集，阻塞SIGINT信号 */ sigprocmask( SIG_BLOCK, &amp;s, NULL ); while ( 1 ) &#123; /* 获取未决信号集 */ sigpending( &amp;p ); printsigset( &amp;p ); sleep( 1 ); &#125; return(0);&#125; 由于我们阻塞了SIGINT信号，所以Ctrl C也终止不了程序，SIGINT信号处于未决状态，但是可以按Ctrl \\ 来终止程序 捕捉信号 内核如何捕捉信号如果信号的处理动作是用户的自定义函数，在信号递达的时候就会调用这个函数，这就是信号捕捉！ 由于信号处理函数的代码是在用户空间，处理过程比较复杂，举例如下: 用户程序注册了SIGQUIT信号的处理函数sighandler。 当前正在执行main函数，这时发生中断或异常切换到内核态。 在中断处理完毕后要返回用户态的main函数之前检查到有信号SIGQUIT递达。 内核决定返回用户态后不是恢复main函数的上下文继续执行，而是执行sighandler函数，sighandler和main函数使用不同的堆栈空间，它们之间不存在调用和被调用的关系，是两个独立的控制流程。 sighandler函数返回后自动执行特殊的系统调用sigreturn再次进入内核态。 如果没有新的信号要递达，这次再返回用户态就是恢复main函数的上下文继续执行了。 sigactionsigaction函数的功能是检查或修改与指定信号相关联的处理动作（或同时执行这两种操作） 123#include &lt;signal.h&gt;int sigaction(int signum, const struct sigaction *act, struct sigaction *oldact); signum：指定信号的编号或者类型act：指定新的信号处理方式sigaction：原来的信号处理方式sigaction结构体： 1234567struct sigaction &#123; void (*sa_handler)(int); void (*sa_sigaction)(int, siginfo_t *, void *); sigset_t sa_mask; int sa_flags; void (*sa_restorer)(void);&#125; 若act指针非空，则根据act修改该信号的处理动作。若oldact指针非空,则通过oact传出该信号原来的处理动作。act和oldact指向sigaction结构体：将sahandler赋值为常数SIGIGN传给sigaction表示忽略信号，赋值为常数SIG_DFL表示执行系统默认动作，赋值为一个函数指针表示用自定义函数捕捉信号，或者说向内核注册了一个信号处理函数，该函数返回值为void，可以带一个int参数，通过参数可以得知当前信号的编号，这样就可以用同一个函数处理多种信号。显然，这也是一个回调函数，不是被main函数调用，而是被系统所调用 当某个信号的处理函数被调用时，内核自动将当前信号加入进程的信号屏蔽字，当信号处理函数返回时自动恢复原来的信号屏蔽字，这样就保证了在处理某个信号时，如果这种信号再次产生,那么 它会被阻塞到当前处理结束为止。 如果在调用信号处理函数时，除了当前信号被自动屏蔽之外,还希望自动屏蔽另外一些信号,则用sa_mask字段说明这些需要额外屏蔽的信号，当信号处理函数返回时自动恢复原来的信号屏蔽字。 sa_flags字段包含一些选项,本章的代码都把sa_flags设为0，sa_sigaction是实时信号的处理函数，暂时不用关心 pausepause函数使调用进程挂起直到有信号递达 123#include &lt;unistd.h&gt;int pause(void); 如果信号的处理动作是终止进程，则进程终止，pause函数没有机会返回；如果信号的处理动作是忽略，则进程继续处于挂起状态，pause不返回；如果信号的处理动作是捕捉，则调用了信号处理函数之后pause返回-1，errno设置为EINTR，所以pause只有出错的返回值，这和程序替换那几个函数是一样的，出错才返回，错误码EINTR表示”被信号中断”。 接下来演示一个用闹钟+信号的方式实现的sleep函数mysleep() 123456789101112131415161718192021222324252627282930313233343536373839#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;signal.h&gt;void sig_alrm( int signo )&#123; /* DO NOTHING */&#125;unsigned int mysleep( unsigned int nescs )&#123; struct sigaction new, old; unsigned int unslept = 0; new.sa_handler = sig_alrm; sigemptyset( &amp;new.sa_mask ); new.sa_flags = 0; //注册信号处理函数 sigaction( SIGALRM, &amp;new, &amp;old ); //设置闹钟 alarm( nescs ); pause(); //清空闹钟 unslept = alarm( 0 ); //恢复默认信号处理动作 sigaction( SIGALRM, &amp;old, NULL ); return(unslept);&#125;int main()&#123; while ( 1 ) &#123; mysleep( 5 ); printf( \"5 seconds passed\\n\" ); &#125; return(0);&#125; 执行流程分析：1.main函数调用mysleep函数，后者调用sigaction注册了SIGALRM信号的处理函数sig_alrm2. 调用alarm(nsecs)设定闹钟3. 调用pause等待，内核切换到别的进程运行4. nsecs秒之后，闹钟超时，内核发SIGALRM给这个进程5. 从内核态返回这个进程的用户态之前处理未决信号，发现有SIGALRM信号，其处理函数是sig_alrm6. 切换到用户态执行sig_ alrm函数，进入sig_ alrm函数时SIGALRM信号被自动屏蔽，从sig_alrm函数返回时SIGALRM信号自动解除屏蔽。然后自动执行系统调用sigreturn再次进入内核，再返回用户态继续执行进程的主控制流程(main函数调用的mysleep函数)7. pause函数返回-1，然后调用alarm(0)取消闹钟，调用sigaction恢复SIGALRM信号以前的处理动作 接下来说明关于mysleep函数的几个问题：问：信号处理函数sig_alrm什么都没干，为什么还要注册它作为SIGALRM的处理函数?不注册信号处理函数可以吗?答：很显然，注册sig_alrm函数是很有必要的，因为绑定了自定义的处理函数则会从内核态切换到用户态运行sig_alrm函数，这样才不至于回到主控制流程，pause函数使调用进程挂起直到有信号递达，如果不注册SIGALRM处理函数，当有信号SIGALRM信号产生时会执行默认动作，终止进程问：为什么在mysleep函数返回前要恢复SIGALRM信号原来的sigaction? 不恢复会怎样？答：必须要恢复信号处理方式，因为sleep函数是不会修改SIGALRM信号的，将SIGALRM 不恢复会使alarm()失效问：mysleep函数的返回值表示什么含义？什么情况下返回非0值？答：mysleep的返回值是在信号SIGALRM信号传来时闹钟还剩余的秒数；当闹钟结束前有其他信号发送给该进程，并该进程对其进行了相关的处理时，alarm(0)取消闹钟会使返回值非零 可重入函数这个概念不难理解，现在假设一个进程正陷入内核态，现在正好要返回用户态执行到一个函数function的时候，现在呢进程收到了一个信号，进程当然要处理这个信号，于是执行自定义动作，在用户自定义处理该信号的函数中，恰好又调用了function函数，那么这就叫做该函数被重入了！ 下面这个例子很详细，非常能说明可重入函数的概念： main函数调用insert函数向一个链表head中插入节点node1，插入操作分为两步，刚做完第一步的时候，因为硬件中断使进程切换到内核,再次回用户态之前检查到有信号待处理，于是切换到sighandler函数，sighandler也调用insert函数向同一个链表head中插入节点node2，插入操作的两步都做完之后从sighandler返回内核态，再次回到用户态就从main函数调用的insert函数中继续 往下执行，先前做第一步之后被打断，现在继续做完第二步。结果是，main函数和sighandler先后向链表中插入两个节点，最后只有一个节点真正插入链表中了。 像上例这样，insert函数被不同的控制流程调用,有可能在第一次调用还没返回时就再次进入该函数，这称为重入，insert函数访问一个全局链表,有可能因为重入而造成错乱，像这样的函数称为不可重入函数，反之，如果一个函数只访问自己的局部变量或参数，则称为可重入(Reentrant) 函数。想一下,为什么两个不同的控制流程调用同一个函数，访问它的同一个局部变量或参数就不会造成错乱? 不可重入的函数的条件 其中有static、全局变量等的函数也是不可重入函数 调用了malloc或free，因为malloc也是用全局链表来管理堆的 调用了标准I/O库函数，标准I/O库的很多实现都以不可重入的方式使用全局数据结构 volatilevolatile关键字的作用是保证内存的可见性，确保本条指令不会因编译器的优化而省略，且要求每次直接读值，volatile的变量是说这变量可能会被意想不到地改变，这样，编译器就不会去假设这个变量的值了！ 1234567891011121314151617#include &lt;stdio.h&gt;#include &lt;signal.h&gt;//int flag = 0;volatile int flag = 0;void handler(int singo)(&#123; flag = 1; printf(\"chage flag to 1\\n\");&#125;int main()&#123; signal(2, handler); while(!flag); printf(\"proc done ...\\n\"); return 0;&#125; 在本例中：假设我们不加volatile修饰变量flag，那么在gcc编译器优化级别为2的时候，main执行流是不会从内存中拿数据的，即使已经进程收到SIGINT信号之后改了flag的值，main执行流也是直接从寄存器上面拿数据，所以Ctrl C也不会结束进程！使用volatile修饰之后无论编译器的优化级别是怎么样的，CPU都可以从内存中拿flag的值，所以只需要将flag用volatile修饰便可以的到我们预期的结果！ 竞态条件与sigsuspend函数最难处理的问题很多都是时序问题！！！ 设想上述mysleep这样的时序: 1.注册SIGALRM信号的处理函数2.调用alarm(nsecs)设定闹钟3.内核调度优先级更高的进程取代当前进程执行，并且优先级更高的进程有很多个，每个都要执行很长时间4.nsecs秒钟之后闹钟超时了，内核发送SIGALRM信号给这个进程，处于未决状态5.优先级更高的进程执行完了，内核要调度回这个进程执行。SIGALRM信号递达，执行处理函数sig_alrm之后再次进入内核6.返回这个进程的主控制流程，alarm(nsecs)返回，调用pause()挂起等待7.可是SIGALRM信号已经处理完了，还等待什么呢? 出现这个问题的根本原因是系统运行的时序(Timing)并不像我们写程序时所设想的那样。 虽然alarm(nsecs)紧接着的下一行就是pause()，但是无法保证pause()一定会在调用 alarm(nsecs)之 后的nsecs秒之内被调用。由于异步事件在任何时候都有可能发生(这里的异步事件指出现更高优先级的进程)，如果我们写程序时考虑不周密，就可能由于时序问题 而导致错误，这叫做竞态条件 (Race Condition) 很显然，我们需要解决的问题就是：从解除信号屏蔽到调用pause之间存在间隙，SIGALRM仍有可能在这个间隙递达。 要消除这个间隙， 我们把解除屏蔽移到pause后面可以吗？很显然不行，还没有解除屏蔽信号就调用pause将会导致根本等不到SIGALRM信号，我们需要的是将”解除信号屏蔽”和”挂起等待信号”这两步能合并成一个原子操作，这就是sigsuspend函数的功能。sigsuspend包含了pause的挂起等待功能，同时解决了竞态条件的问题，在对时序要求严格的场合下都应该调用sigsuspend而不是pause 12#include &lt;signal.h&gt;int sigsuspend(const sigset_t *mask); 和pause一样，sigsuspend没有成功返回值，只有执行了一个信号处理函数之后 sigsuspend才返回，返回值为-1，errno设置为EINTR调用sigsuspend时，进程的信号屏蔽字由sigmask参数指定，可以通过指定sigmask来临时 解除对某个信号的屏蔽，然后挂起等待，当sigsuspend返回时，进程的信号屏蔽字恢复为原来的值，如果原来对该信号是屏蔽的，从sigsuspend返回后仍然是屏蔽的 下面是使用示例： 123456789101112131415161718192021222324252627282930313233343536373839#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;signal.h&gt;void sig_alrm(int signo)&#123; printf(\"%d, sig_alrm\\n\", signo);&#125;unsigned int mysleep(unsigned int nsecs)&#123; struct sigaction newact, oldact; sigset_t newmask, oldmask, suspmask; unsigned int unslept; newact.sa_handler = sig_alrm; sigisemptyset(&amp;newact.sa_mask); newact.sa_flags = 0; sigaction(SIGALRM, &amp;newact, &amp;oldact); sigemptyset(&amp;newmask); sigaddset(&amp;newmask, SIGALRM); sigprocmask(SIG_BLOCK, &amp;newmask, &amp;oldmask); alarm(nsecs); suspmask = oldmask; sigdelset(&amp;suspmask, SIGALRM); sigsuspend(&amp;suspmask); unslept = alarm(0); sigaction(SIGALRM, &amp;oldact, NULL); sigprocmask(SIG_SETMASK, &amp;oldmask, NULL); return unslept;&#125;int main()&#123; mysleep(5); printf(\"5s seconds pass...\\n\"); return 0; SIGCHLD信号在之前学过的进程中，父进程通过wait和waitpid函数清理僵尸进程，父进程可以阻塞等待子进程结束，也可以非阻塞地查询是否有子进程结束等待清理(也就是轮询的方式)。采用第一种方式，父进程阻塞了就不能处理自己的工作了；采用第二种方式，父进程在处理自己的工作的同时还要记得时不时地轮询一下，程序实现复杂。其实，子进程在终止时会给父进程发SIGCHLD信号，该信号的默认处理动作是忽略，父进程可以自定义函数捕获SIGCHLD信号：父进程在信号处理函数中调用wait清理子进程即可 由于UNIX 的历史原因，要想不产生僵尸进程还有另外一种办法:父进程调用sigaction将SIGCHLD的处理动作置为SIG_IGN，这样fork出来的子进程在终止时会自动清理掉，不会产生僵尸进程，也不会通知父进程。系统默认的忽略动作和用户用sigaction函数自定义的忽略通常是没有区别的，但这是一个特例。此方法对于Linux可用，但不保证在其它UNIX系统上都可用： 12345678910111213141516171819202122232425262728293031323334#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;signal.h&gt;#include &lt;stdlib.h&gt;#include &lt;wait.h&gt;void handler(int sig)&#123; pid_t id; printf(\"sig = %d\\n\", sig); while((id = waitpid(-1, NULL, WNOHANG)) &gt; 0) &#123; printf(\"wait child success: %d\\n\", id); &#125; printf(\"child is quit! %d\\n\", getpid());&#125;int main()&#123; signal(SIGCHLD, handler); pid_t cid; if((cid = fork()) == 0) &#123; printf(\"child: %d\\n\", getpid()); sleep(3); exit(1); &#125; while(1) &#123; printf(\"father proc is doing some thing!\\n\"); sleep(1); &#125; return 0;&#125; 信号部分总结完毕，接下来看看常用的普通信号（慢慢遇到了再补充）：| 编号 | 信号 | 含义 | 缺省动作 || :— | :—— | :——————————- | :—————————————- || 1 | SIGHUP | 终端挂起或者控制进程终止 | 终止进程 || 2 | SIGINT | 键盘中断（如Ctrl C） | 终止进程 || 3 | SIGQUIT | 键盘的退出键被按下 | 终止进程并核心转储（dump core） || 6 | SIGABRT | 由abort()发出的退出指令 | 终止进程并核心转储（dump core） || 8 | SIGFPE | 浮点异常，比如错零错误 | 终止进程并核心转储（dump core） || 9 | SIGKILL | Kill信号 | 终止进程、信号不能被捕获 、信号不能被忽略 || 11 | SIGSEGV | 无效的内存引用，比如野指针 | 终止进程并核心转储（dump core） || 13 | SIGPIPE | 管道破裂: 写一个没有读端口的管道 | 终止进程 || 14 | SIGALRM | 由alarm函数发出的信号 | 终止进程 || 17 | SIGCHLD | 子进程结束信号 | 忽略此信号 || … | … | … | … |","updated":"2020-04-14T03:03:23.993Z","categories":[{"name":"操作系统理论","slug":"操作系统理论","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"},{"name":"异常处理","slug":"异常处理","permalink":"https://zouchanglin.cn/tags/%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"}]},{"title":"35个Java代码性能优化总结","date":"2018-12-25T10:09:30.000Z","path":"2018/12/25/35个Java代码性能优化总结/","text":"非常有力的35个代码性能优化，看完后深有感悟(ps就是自己看完后的感想)，不过其中有一些错误我也删改了，回想起自己之前的代码写的很烂，从现在开始一步一步注意这些点，每一点都是很实用而且是非常优秀的编码习惯！！！值得学习！！！ 前言代码优化，一个很重要的课题。可能有些人觉得没用，一些细小的地方有什么好修改的，改与不改对于代码的运行效率有什么影响呢？这个问题我是这么考虑的，就像大海里面的鲸鱼一样，它吃一条小虾米有用吗？没用，但是，吃的小虾米一多之后，鲸鱼就被喂饱了。代码优化也是一样，如果项目着眼于尽快无BUG上线，那么此时可以抓大放小，代码的细节可以不精打细磨；但是如果有足够的时间开发、维护代码，这时候就必须考虑每个可以优化的细节了，一个一个细小的优化点累积起来，对于代码的运行效率绝对是有提升的。 代码优化的目标是：1、减小代码的体积2、提高代码运行的效率 代码优化细节1、尽量指定类、方法的final修饰符 带有final修饰符的类是不可派生的。在Java核心API中，有许多应用final的例子，例如java.lang.String，整个类都是final的。为类指定final修饰符可以让类不可以被继承，为方法指定final修饰符可以让方法不可以被重写。如果指定了一个类为final，则该类所有的方法都是final的。Java编译器会寻找机会内联所有的final方法，内联对于提升Java运行效率作用重大，具体参见Java运行期优化。此举能够使性能平均提高50%。 2、尽量重用对象 特别是String对象的使用，出现字符串连接时应该使用StringBuilder/StringBuffer代替。由于Java虚拟机不仅要花时间生成对象，以后可能还需要花时间对这些对象进行垃圾回收和处理，因此，生成过多的对象将会给程序的性能带来很大的影响。 3、尽可能使用局部变量 调用方法时传递的参数以及在调用中创建的临时变量都保存在栈中速度较快，其他变量，如静态变量、实例变量等，都在堆中创建，速度较慢。另外，栈中创建的变量，随着方法的运行结束，这些内容就没了，不需要额外的垃圾回收。 4、及时关闭流 Java编程过程中，进行数据库连接、I/O流操作时务必小心，在使用完毕后，及时关闭以释放资源。因为对这些大对象的操作会造成系统大的开销，稍有不慎，将会导致严重的后果。 5、尽量减少对变量的重复计算 明确一个概念，对方法的调用，即使方法中只有一句语句，也是有消耗的，包括创建栈帧、调用方法时保护现场、调用方法完毕时恢复现场等。所以例如下面的操作： 1for (int i = 0; i &lt; list.size(); i++)&#123; ... &#125; 建议替换为： 1for (int i = 0, length = list.size(); i &lt; length; i++)&#123; ... &#125; 这样，在list.size()很大的时候，就减少了很多的消耗 6、尽量采用懒加载的策略，即在需要的时候才创建 例如： 1234String str = \"aaa\";if (i == 1)&#123; list.add(str);&#125; 建议替换为： 1234if (i == 1)&#123; String str = \"aaa\"; list.add(str);&#125; 7、慎用异常 异常对性能不利。抛出异常首先要创建一个新的对象，Throwable接口的构造函数调用名为fillInStackTrace()的本地同步方法，fillInStackTrace()方法检查堆栈，收集调用跟踪信息。只要有异常被抛出，Java虚拟机就必须调整调用堆栈，因为在处理过程中创建了一个新的对象。异常只能用于错误处理，不应该用来控制程序流程。 8、不要在循环中使用try…catch…，应该把其放在最外层 除非不得已。如果毫无理由地这么写了，只要你的领导资深一点、有强迫症一点，八成就要骂你为什么写出这种垃圾代码来了 9、如果能估计到待添加的内容长度，为底层以数组方式实现的集合、工具类指定初始长度 比如ArrayList、LinkedLlist、StringBuilder、StringBuffer、HashMap、HashSet等等，以StringBuilder为例： 123StringBuilder() // 默认分配16个字符的空间StringBuilder(int size) // 默认分配size个字符的空间StringBuilder(String str) // 默认分配16个字符+str.length()个字符空间 可以通过类（这里指的不仅仅是上面的StringBuilder）的来设定它的初始化容量，这样可以明显地提升性能。比如StringBuilder吧，length表示当前的StringBuilder能保持的字符数量。因为当StringBuilder达到最大容量的时候，它会将自身容量增加到当前的2倍再加2，无论何时只要StringBuilder达到它的最大容量，它就不得不创建一个新的字符数组然后将旧的字符数组内容拷贝到新字符数组中—-这是十分耗费性能的一个操作。试想，如果能预估到字符数组中大概要存放5000个字符而不指定长度，最接近5000的2次幂是4096，每次扩容加的2不管，那么： 在4096 的基础上，再申请8194个大小的字符数组，加起来相当于一次申请了12290个大小的字符数组，如果一开始能指定5000个大小的字符数组，就节省了一倍以上的空间 把原来的4096个字符拷贝到新的的字符数组中去 这样，既浪费内存空间又降低代码运行效率。所以，给底层以数组实现的集合、工具类设置一个合理的初始化容量是错不了的，这会带来立竿见影的效果。但是，注意，像HashMap这种是以数组+链表实现的集合，别把初始大小和你估计的大小设置得一样，因为一个table上只连接一个对象的可能性几乎为0。初始大小建议设置为2的N次幂，如果能估计到有2000个元素，设置成new HashMap(128)、new HashMap(256)都可以。 10、当复制大量数据时，使用System.arraycopy()命令 这个在我之前的博客中有提到：《Java中数组复制的效率比较》 11、乘法和除法使用移位操作 1234for (val = 0; val &lt; 100000; val += 5)&#123; a = val * 8; b = val / 2;&#125; 用移位操作可以极大地提高性能，因为在计算机底层，对位的操作是最方便、最快的，因此建议修改为： 1234for (val = 0; val &lt; 100000; val += 5)&#123; a = val &lt;&lt; 3; b = val &gt;&gt; 1;&#125; 移位操作虽然快，但是可能会使代码不太好理解，因此最好加上相应的注释。 12、循环内不要不断创建对象引用 123for (int i = 1; i &lt;= count; i++)&#123; Object obj = new Object();&#125; 这种做法会导致内存中有count份Object对象引用存在，count很大的话，就耗费内存了，建议为改为： 1234Object obj = null;for (int i = 0; i &lt;= count; i++) &#123; obj = new Object(); &#125; 这样的话，内存中只有一份Object对象引用，每次new Object()的时候，Object对象引用指向不同的Object罢了，但是内存中只有一份，这样就大大节省了内存空间了。 13、基于效率和类型检查的考虑，应该尽可能使用array，无法确定数组大小时才使用ArrayList 14、尽量使用HashMap、ArrayList、StringBuilder，除非线程安全需要，否则不推荐使用Hashtable、Vector、StringBuffer，后三者由于使用同步机制而导致了性能开销 15、不要将数组声明为public static final 因为这毫无意义，这样只是定义了引用为static final，数组的内容还是可以随意改变的，将数组声明为public更是一个安全漏洞，这意味着这个数组可以被外部类所改变 16、尽量在合适的场合使用单例 使用单例可以减轻加载的负担、缩短加载的时间、提高加载的效率，但并不是所有地方都适用于单例，简单来说，单例主要适用于以下三个方面： （1）控制资源的使用，通过线程同步来控制资源的并发访问 （2）控制实例的产生，以达到节约资源的目的 （3）控制数据的共享，在不建立直接关联的条件下，让多个不相关的进程或线程之间实现通信 17、尽量避免随意使用静态变量 要知道，当某个对象被定义为static的变量所引用，那么gc通常是不会回收这个对象所占有的堆内存的，如： 123public class A&#123; private static B b = new B();&#125; 此时静态变量b的生命周期与A类相同，如果A类不被卸载，那么引用B指向的B对象会常驻内存，直到程序终止! 18、及时清除不再需要的会话 为了清除不再活动的会话，许多应用服务器都有默认的会话超时时间，一般为30分钟。当应用服务器需要保存更多的会话时，如果内存不足，那么操作系统会把部分数据转移到磁盘，应用服务器也可能根据MRU（最近最频繁使用）算法把部分不活跃的会话转储到磁盘，甚至可能抛出内存不足的异常。如果会话要被转储到磁盘，那么必须要先被序列化，在大规模集群中，对对象进行序列化的代价是很昂贵的。因此，当会话不再需要时，应当及时调用HttpSession的invalidate()方法清除会话。 19、实现RandomAccess接口的集合比如ArrayList，应当使用最普通的for循环而不是foreach循环来遍历 这是JDK推荐给用户的。JDK API对于RandomAccess接口的解释是：实现RandomAccess接口用来表明其支持快速随机访问，此接口的主要目的是允许一般的算法更改其行为，从而将其应用到随机或连续访问列表时能提供良好的性能。实际经验表明，实现RandomAccess接口的类实例，假如是随机访问的，使用普通for循环效率将高于使用foreach循环；反过来，如果是顺序访问的，则使用Iterator会效率更高。可以使用类似如下的代码作判断： 12345678if (list instanceof RandomAccess)&#123; for (int i = 0; i &lt; list.size(); i++)&#123;&#125;&#125;else&#123; Iterator&lt;?&gt; iterator = list.iterable(); while (iterator.hasNext())&#123; iterator.next() &#125;&#125; foreach循环的底层实现原理就是迭代器Iterator，参见Java语法糖:可变长度参数以及foreach循环原理。所以后半句反过来，如果是顺序访问的，则使用Iterator会效率更高的意思就是顺序访问的那些类实例，使用foreach循环去遍历。 20、使用同步代码块替代同步方法 这点在多线程模块中的synchronized锁方法块一文中已经讲得很清楚了，除非能确定一整个方法都是需要进行同步的，否则尽量使用同步代码块，避免对那些不需要进行同步的代码也进行了同步，影响了代码执行效率。 21、将常量声明为static final，并以大写命名 这样在编译期间就可以把这些内容放入常量池中，避免运行期间计算生成常量的值。另外，将常量的名字以大写命名也可以方便区分出常量与变量 22、不要创建一些不使用的对象，不要导入一些不使用的类 这毫无意义，如果代码中出现”The value of the local variable i is not used”、”The import java.util is never used”，那么请删除这些无用的内容 23、程序运行过程中避免使用反射 关于，请参见反射。反射是Java提供给用户一个很强大的功能，功能强大往往意味着效率不高。不建议在程序运行过程中使用尤其是频繁使用反射机制，特别是Method的invoke方法，如果确实有必要，一种建议性的做法是将那些需要通过反射加载的类在项目启动的时候通过反射实例化出一个对象并放入内存，用户只关心和对端交互的时候获取最快的响应速度，并不关心对端的项目启动花多久时间。 24、使用数据库连接池和线程池 这两个池都是用于重用对象的，前者可以避免频繁地打开和关闭连接，后者可以避免频繁地创建和销毁线程 25、使用带缓冲的输入输出流进行IO操作 带缓冲的输入输出流，即BufferedReader、BufferedWriter、BufferedInputStream、BufferedOutputStream，这可以极大地提升IO效率，ps:一个字节一个字节的写入会让你的硬盘很恼火的…. 26、顺序插入和随机访问比较多的场景使用ArrayList，元素删除和中间插入比较多的场景使用LinkedList 这个，理解ArrayList和LinkedList的原理就知道了 27、不要让public方法中有太多的形参 public方法即对外提供的方法，如果给这些方法太多形参的话主要有两点坏处： 违反了面向对象的编程思想，Java讲求一切都是对象，太多的形参，和面向对象的编程思想并不契合 参数太多势必导致方法调用的出错概率增加 至于这个 “太多” 指的是多少个，3、4个吧。比如我们用JDBC写一个insertStudentInfo方法，有10个学生信息字段要插如Student表中，可以把这10个参数封装在一个实体类中，作为insert方法的形参 28、字符串变量和字符串常量equals的时候将字符串常量写在前面 这是一个比较常见的小技巧了，如果有以下代码： 12String str = \"123\";if (str.equals(\"123\")) &#123; ... &#125; 建议修改为： 1String str = \"123\";if (\"123\".equals(str))&#123; ... &#125; 这么做主要是可以避免空指针异常 29、请知道，在java中if (i == 1)和if (1 == i)是没有区别的，但从阅读习惯上讲，建议使用前者 平时有人问，if (i == 1)和if (1== i)有没有区别，这就要从C/C++讲起。在C/C++中，if (i == 1)判断条件成立，是以0与非0为基准的，0表示false，非0表示true，如果有这么一段代码： 123456int i = 2;if (i == 1)&#123; ...&#125;else&#123; ...&#125; C/C++判断i==1不成立，所以以0表示，即false，但是如果： 123456int i = 2;if (i = 1) &#123; ... &#125;else&#123; ... &#125; 万一程序员一个不小心，把if (i == 1)写成if (i = 1)，这样就有问题了。在if之内将i赋值为1，if判断里面的内容非0，返回的就是true了，但是明明i为2，比较的值是1，应该返回的false。这种情况在C/C++的开发中是很可能发生的并且会导致一些难以理解的错误产生，所以，为了避免开发者在if语句中不正确的赋值操作，建议将if语句写为： 123456int i = 2;if (1 == i) &#123; ... &#125;else&#123; ...&#125; 这样，即使开发者不小心写成了1 = i，C/C++编译器也可以第一时间检查出来，因为我们可以对一个变量赋值i为1，但是不能对一个常量赋值1为i！ ps:这种写法直接把运行期的错误提升到了编译期，对我们开发是非常有用的，在《C专家编程》记录的一个故事，就是因为把i == 1写成i = 1结果导致电话超过五公里之外无法接通的BUG！ 但是，在Java中，C/C++这种if (i = 1)的语法是不可能出现的，因为一旦写了这种语法，Java就会编译报错”Type mismatch: cannot convert from int to boolean”。但是，尽管Java的if (i == 1)和if (1 == i)在语义上没有任何区别，但是从阅读习惯上讲，建议使用前者会更好些。 30、不要对数组使用toString()方法 看一下对数组使用toString()打印出来的是什么： 1234public static void main(String[] args)&#123; int[] is = new int[]&#123;1, 2, 3&#125;; System.out.println(is.toString());&#125; 结果是：[I@18a992f 这样的字符串 本意是想打印出数组内容，却有可能因为数组引用is为空而导致空指针异常。不过虽然对数组toString()没有意义，但是对集合toString()是可以打印出集合里面的内容的，因为集合的父类AbstractCollections重写了Object的toString()方法。 31、不要对超出范围的基本数据类型做向下强制转型 这绝不会得到想要的结果： 12345public static void main(String[] args)&#123; long l = 12345678901234L; int i = (int)l; System.out.println(i);&#125; 我们可能期望得到其中的某几位，但是结果却是：1942892530 解释一下:Java中long是8个字节64位的，所以12345678901234在计算机中的表示应该是：0000 0000 0000 0000 0000 1011 0011 1010 0111 0011 1100 1110 0010 1111 1111 0010一个int型数据是4个字节32位的，从低位取出上面这串二进制数据的前32位是0111 0011 1100 1110 0010 1111 1111 0010这串二进制表示为十进制1942892530，所以就是我们上面的控制台上输出的内容。从这个例子上还能顺便得到两个结论： 整型默认的数据类型是int，long l = 12345678901234L，这个数字已经超出了int的范围了，所以最后有一个L，表示这是一个long型数。顺便，浮点型的默认类型是double，所以定义float的时候要写成float f = 3.5f 接下来再写一句int ii = l + i;会报错，因为long + int是一个long，不能赋值给int 32、公用的集合类中不使用的数据一定要及时remove掉 如果一个集合类是公用的（也就是说不是方法里面的属性），那么这个集合里面的元素是不会自动释放的，因为始终有引用指向它们。所以，如果公用集合里面的某些数据不使用而不去remove掉它们，那么将会造成这个公用集合不断增大，使得系统有内存泄露的隐患。 33、把一个基本数据类型转为字符串，基本数据类型.toString()是最快的方式String.valueOf(数据)次之、数据+&quot;&quot;最慢 把一个基本数据类型转为一般有三种方式，一个Integer型数据i，可以使用i.toString()、String.valueOf(i)、i+&quot;&quot;三种方式，以后遇到把一个基本数据类型转为String的时候，优先考虑使用toString()方法。至于为什么，很简单： 1、String.valueOf()方法底层调用了Integer.toString()方法，但是会在调用前做空判断 2、Integer.toString()方法就不说了，直接调用了 3、i + &quot;&quot;底层使用了StringBuilder实现，先用append方法拼接，再用toString()方法获取字符串 三者对比下来，明显是2最快、1次之、3最慢 34、使用最有效率的方式去遍历Map 遍历Map的方式有很多，通常场景下我们需要的是遍历Map中的Key和Value，那么推荐使用的、效率最高的方式是： 123456789101112public static void main(String[] args)&#123; HashMap&lt;String, String&gt; hm = new HashMap&lt;String, String&gt;(); hm.put(\"111\", \"222\"); Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = hm.entrySet(); Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iter = entrySet.iterator(); while (iter.hasNext())&#123; Map.Entry&lt;String, String&gt; entry = iter.next(); System.out.println(entry.getKey() + \"\\t\" + entry.getValue()); &#125;&#125; 如果你只是想遍历一下这个Map的key值，那用”Set keySet = hm.keySet();”会比较合适一些 35、对资源的close()建议分开操作 意思是，比如我有这么一段代码： 123456try&#123; XXX.close(); YYY.close();&#125;catch (Exception e)&#123; ...&#125; 建议修改为： 1234567891011try&#123; XXX.close(); &#125;catch (Exception e) &#123; ... &#125;try&#123; YYY.close();&#125;catch (Exception e) &#123; ... &#125; 虽然有些麻烦，却能避免资源泄露。我们想，如果没有修改过的代码，万一XXX.close()抛异常了，那么就进入了cath块中了，YYY.close()不会执行，YYY这块资源就不会回收了，一直占用着，这样的代码一多，是可能引起资源句柄泄露的。而改为下面的写法之后，就保证了无论如何XXX和YYY都会被close掉。 原文地址：《35个Java代码性能优化总结》","updated":"2020-03-13T03:06:29.618Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"}]},{"title":"通俗理解RAII","date":"2018-12-10T10:09:30.000Z","path":"2018/12/10/通俗理解RAII/","text":"最近在学习c++多线程编程的时候，偶然看到了RAII的概念，有种这么多年c++白学了的感觉，路漫漫其修远兮啊。下面是我在查找RAII资料时候看到的一篇非常好的博客，因为觉得自己实在写不出比这篇更好的对于RAII的总结的博客了，所以就把文章转摘过来了。 RAII是指C++语言中的一个惯用法(idiom)，它是“Resource Acquisition Is Initialization”的首字母缩写。中文可将其翻译为“资源获取就是初始化”。虽然从某种程度上说这个名称并没有体现出该惯性法的本质精神，但是作为标准C++资源管理的关键技术，RAII早已在C++社群中深入人心。 我记得第一次学到RAII惯用法是在Bjarne Stroustrup的《C++程序设计语言(第3版)》一书中。当讲述C++资源管理时，Bjarne这样写道：使用局部对象管理资源的技术通常称为“资源获取就是初始化”。这种通用技术依赖于构造函数和析构函数的性质以及它们与异常处理的交互作用。 Bjarne这段话是什么意思呢？ 首先让我们来明确资源的概念，在计算机系统中，资源是数量有限且对系统正常运转具有一定作用的元素。比如，内存，文件句柄，互斥锁(mutex locks)等等，它们都属于系统资源。由于资源的数量不是无限的，有的资源甚至在整个系统中仅有一份，因此我们在使用资源时必须严格遵循的步骤是： 获取资源 使用资源 释放资源 12345void UseFile(char const* fn)&#123; FILE* f = fopen(fn,\"r\"); //获取资源 //在此使用文件句柄 //使用资源 fclose(f); //释放资源&#125; 调用fopen()打开文件就是获取文件句柄资源，操作完成之后，调用fclose()关闭文件就是释放该资源。资源的释放工作至关重要，如果只获取而不释放，那么资源最终会被耗尽。 但是上面的代码是否能够保证在任何情况下都调用fclose函数吗？不一定，请考虑如下情况 1234567891011121314void UseFile(char const* fn)&#123; FILE* f = fopen(fn,\"r\"); //获取资源 //使用资源 if(!g())&#123; //如果操作g失败 return; &#125; //... if(!h())&#123; //如果操作h失败 return; &#125; //... fclose(f); //释放资源&#125; 很明显，这里忘记了一个重要的步骤：在操作g或h失败之后，UseFile函数必须首先调用fclose()关闭文件，然后才能返回其调用者，否则会造成资源泄漏。因此，需将UseFile函数修改为 12345678910111213141516void UseFile(char const* fn)&#123; FILE* f = fopen(fn,\"r\"); //获取资源 //使用资源 if(!g())&#123; //如果操作g失败 fclose(f); return; &#125; //... if(!h())&#123; //如果操作h失败 fclose(f); return; &#125; //... fclose(f); //释放资源&#125; 现在的问题是：用于释放资源的代码fclose(f)需要在不同的位置重复书写多次。如果再加入异常处理，情况会变得更加复杂。例如，在文件f的使用过程中，程序可能会抛出异常 12345678910111213141516171819void UseFile(char const* fn)&#123; FILE* f = fopen(fn,\"r\"); //获取资源 //使用资源 try&#123; if(!g())&#123; //如果操作g失败 fclose(f); return; //... if(!h())&#123; //如果操作h失败 fclose(f); return; &#125; &#125;catch(...)&#123; fclose(f); throw; &#125; fclose(f); //释放资源&#125; 我们必须依靠catch(…)来捕获所有的异常，关闭文件f，并重新抛出该异常。随着控制流程复杂度的增加，需要添加资源释放代码的位置会越来越多。如果资源的数量还不止一个，那么程序员就更加难于招架了。 可以想象这种做法的后果是：代码臃肿，效率下降，更重要的是，程序的可理解性和可维护性明显降低。是否存在一种方法可以实现资源管理的自动化呢？答案是肯定的。假设UseResources函数要用到n个资源，则进行资源管理的一般模式为： 1234567891011void UseResources()&#123; //获取资源1 //... //获取资源n //使用这些资源 //释放资源n //... //释放资源1&#125; 不难看出资源管理技术的关键在于：要保证资源的释放顺序与获取顺序严格相反。这自然使我们联想到局部对象的创建和销毁过程。 在C++中，定义在栈空间上的局部对象称为自动存储(automatic memory)对象。管理局部对象的任务非常简单，因为它们的创建和销毁工作是由系统自动完成的。我们只需在某个作用域(scope)中定义局部对象(这时系统自动调用构造函数以创建对象)，然后就可以放心大胆地使用之，而不必担心有关善后工作；当控制流程超出这个作用域的范围时，系统会自动调用析构函数，从而销毁该对象。 读者可能会说：如果系统中的资源也具有如同局部对象一样的特性，自动获取，自动释放，那该有多么美妙啊！。事实上，您的想法已经与RAII不谋而合了。既然类是C++中的主要抽象工具，那么就将资源抽象为类，用局部对象来表示资源，把管理资源的任务转化为管理局部对象的任务。这就是RAII惯用法的真谛！可以毫不夸张地说，RAII有效地实现了C++资源管理的自动化。例如，我们可以将文件句柄FILE抽象为FileHandle类： 1234567891011121314class FileHandle&#123;public: FileHandle(char const* n, char const* a)&#123; p = fopen(n, a); &#125; ~FileHandle()&#123; fclose(p); &#125;private: //禁止拷贝操作 FileHandle(FileHandle const&amp;) = delete; FileHandle&amp; operator=(FileHandle const&amp;) = delete; File *p;&#125; FileHandle类的构造函数调用fopen()获取资源；FileHandle类的析构函数调用fclose()释放资源。请注意，考虑到FileHandle对象代表一种资源，它并不具有拷贝语义，因此我们将拷贝构造函数和赋值运算符声明为私有成员。如果利用FileHandle类的局部对象表示文件句柄资源，那么前面的UseFile函数便可简化为： 12345void UseFile(char const* fn)&#123; FileHandle file(fn, &quot;r&quot;); &#x2F;&#x2F;此处使用文件句柄 &#x2F;&#x2F;超出此作用域时，系统会自动调用file的析构函数，从而释放资源&#125; 现在我们就不必担心隐藏在代码之中的return语句了；不管函数是正常结束，还是提前返回，系统都必须“乖乖地”调用f的析构函数，资源一定能被释放。Bjarne所谓“使用局部对象管理资源的技术……依赖于构造函数和析构函数的性质”，说的正是这种情形。 且慢！如若使用文件file的代码中有异常抛出，难道析构函数还会被调用吗？此时RAII还能如此奏效吗？问得好。事实上，当一个异常抛出之后，系统沿着函数调用栈，向上寻找catch子句的过程，称为栈辗转开解(stack unwinding)。C++标准规定，在辗转开解函数调用栈的过程中，系统必须确保调用所有已创建起来的局部对象的析构函数。例如： 123456void Foo()&#123; FileHandle file1(\"n1.txt\", \"r\"); FileHandle file1(\"n2.txt\", \"w\"); Bar(); //可能会抛出异常 FileHandle file1(\"n3.txt\", \"rw\");&#125; 当Foo()调用Bar()时，局部对象file1和file2已经在Foo的函数调用栈中创建完毕，而file3却尚未创建。如果Bar()抛出异常，那么file2和file1的析构函数会被先后调用(注意：析构函数的调用顺序与构造函数相反)；由于此时栈中尚不存在file3对象，因此它的析构函数不会被调用。只有当一个对象的构造函数执行完毕之后，我们才认为该对象的创建工作已经完成。栈辗转开解过程仅调用那些业已创建的对象的析构函数。 RAII惯用法同样适用于需要管理多个资源的复杂对象。例如，Widget类的构造函数要获取两个资源：文件myFile和互斥锁myLock。每个资源的获取都有可能失败并且抛出异常。 为了正常使用Widget对象，这里我们必须维护一个不变式(invariant)：当调用构造函数时，要么两个资源全都获得，对象创建成功；要么两个资源都没得到，对象创建失败。获取了文件而没有得到互斥锁的情况永远不能出现，也就是说，不允许建立Widget对象的“半成品”。如果将RAII惯用法应用于成员对象，那么我们就可以实现这个不变式： 123456789class Widget&#123;public: Widget(char const* myfile, char const* myLock) :file_(myfile) //获取文件myfile ,lock_(myLock)&#123;&#125; //获取互斥锁myLockprivate: FileHandle file_; LockHandle lock_;&#125; FileHandle和LockHandle类的对象作为Widget类的数据成员，分别表示需要获取的文件和互斥锁。资源的获取过程就是两个成员对象的初始化过程。在此系统会自动地为我们进行资源管理，程序员不必显式地添加任何异常处理代码。例如，当已经创建完file_，但尚未创建完lock_时，有一个异常被抛出，则系统会调用file_的析构函数，而不会调用lock_的析构函数。Bjarne所谓构造函数和析构函数“与异常处理的交互作用”，说的就是这种情形。 综上所述，RAII的本质内容是用对象代表资源，把管理资源的任务转化为管理对象的任务，将资源的获取和释放与对象的构造和析构对应起来，从而确保在对象的生存期内资源始终有效，对象销毁时资源必被释放。换句话说，拥有对象就等于拥有资源，对象存在则资源必定存在。由此可见，RAII惯用法是进行资源管理的有力武器。C++程序员依靠RAII写出的代码不仅简洁优雅，而且做到了异常安全。难怪微软的MSDN杂志在最近的一篇文章中承认：“若论资源管理，谁也比不过标准C++”。 最后关于C++11的RAII，请参考：《C++11的资源管理：泛化的RAII》","updated":"2020-03-13T03:06:29.773Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://zouchanglin.cn/tags/C-C/"}]},{"title":"Linux进程通信之共享内存","date":"2018-12-05T10:09:30.000Z","path":"2018/12/05/Linux进程通信之共享内存/","text":"共享内存共享内存按照页为基本单位分配的，一页是4K共享内存无同步与互斥，生命周期随内核共享内存无同步与互斥，生命周期随内核共享内存无同步与互斥，生命周期随内核共享内存区是最快的IPC形式，一旦这样的内存映射到共享它的进程的地址空间，这些进程间数据传递不再涉及到内核，换句话说是进程不再通过执行进入内核的系统调用来传递彼此的数据： 共享内存的数据结构12345678910111213struct shmid_ds &#123; struct ipc_perm shm_perm; /* operation perms */ int shm_segsz; /* size of segment (bytes) */ __kernel_time_t shm_atime; /* last attach time */ __kernel_time_t shm_dtime; /* last detach time */ __kernel_time_t shm_ctime; /* last change time */ __kernel_ipc_pid_t shm_cpid; /* pid of creator */ __kernel_ipc_pid_t shm_lpid; /* pid of last operator */ unsigned short shm_nattch; /* no. of current attaches */ unsigned short shm_unused; /* compatibility */ void *shm_unused2; /* ditto - used by DIPC */ void *shm_unused3; /* unused */&#125;; shmget得到一个共享内存标识符或创建一个共享内存对象并返回共享内存标识符 123#include &lt;sys/ipc.h&gt;#include &lt;sys/shm.h&gt;int shmget(key_t key, size_t size, int shmflg) key:此值来源于ftok返回的IPC键值size大于0的整数：新建的共享内存大小，以字节为单位shmflg 取共享内存标识符，若不存在则函数会报错 IPC_CREAT：如果内核中不存在键值与key相等的共享内存，则新建一个共享内存；如果存在这样的共享内存，返回此共享内存的标识符 IPC_CREAT|IPC_EXCL：如果内核中不存在键值与key相等的共享内存，则新建一个消息队列；如果存在这样的共享内存则报错 return:成功返回共享内存的标识符,出错返回-1，错误原因存于error中错误代码：EINVAL：参数size小于SHMMIN或大于SHMMAXEEXIST：预建立key所指的共享内存，但已经存在EIDRM：参数key所指的共享内存已经删除ENOSPC：超过了系统允许建立的共享内存的最大值(SHMALL)ENOENT：参数key所指的共享内存不存在，而参数shmflg未设IPC_CREAT位EACCES：没有权限ENOMEM：核心内存不足 shmat把共享内存区对象映射到调用进程的地址空间，其实就是将共享内存绑定到当前进程 123#include &lt;sys/types.h&gt;#include &lt;sys/shm.h&gt;void *shmat(int shmid, const void *shmaddr, int shmflg) msqid:共享内存标识符shmaddr指定共享内存出现在进程内存地址的什么位置，直接指定为NULL让内核自己决定一个合适的地址位置shmflg:SHM_RDONLY：为只读模式，其他为读写模式，默认写0即可return:成功：附加好的共享内存地址,出错：-1，错误原因存于error中 错误代码EACCES：无权限以指定方式连接共享内存EINVAL：无效的参数shmid或shmaddrENOMEM：核心内存不足注意：fork后子进程继承已连接的共享内存地址。exec后该子进程与已连接的共享内存地址自动脱离。进程结束后，已连接的共享内存地址会自动脱离 shmdt与shmat函数相反，解除当前进程与共享内存的绑定 123#include &lt;sys/types.h&gt;#include &lt;sys/shm.h&gt;int shmdt(const void *shmaddr) msqid:共享内存标识符return:成功：0 出错：-1，错误原因存于error中 错误代码EINVAL：无效的参数shmaddr函数调用并不删除所指定的共享内存区，而只是将之前绑定的共享内存解除绑定 shmctl控制共享内存 123#include &lt;sys/types.h&gt;#include &lt;sys/shm.h&gt;int shmctl(int shmid, int cmd, struct shmid_ds *buf); msqid:共享内存标识符cmd:对共享内存的控制选项 IPC_STAT：得到共享内存的状态，把共享内存的shmid_ds结构复制到buf中 IPC_SET：改变共享内存的状态，把buf所指的shmid_ds结构中的uid、gid、mode复制到共享内存的shmid_ds结构内 IPC_RMID：删除这片共享内存 buf:共享内存管理结构体，不关心则设置为NULLreturn:成功：0,出错：-1，错误原因存于error中 错误代码EACCESS：参数cmd为IPC_STAT，确无权限读取该共享内存EFAULT：参数buf指向无效的内存地址EIDRM：标识符为msqid的共享内存已被删除EINVAL：无效的参数cmd或shmidEPERM：参数cmd为IPC_SET或IPC_RMID，却无足够的权限执行 ipcs与ipcrm与消息队列一致，由于共享内存的生命周期随内核，要么使用共享内存控制函数将其释放，那么通过命令手动释放，ipcs -m就可以查看IPC资源中的共享内存，iprm -m shmid就可以释放对应shmid号的共享内存 通信示例writer.c 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/shm.h&gt;#include &lt;unistd.h&gt;#include &lt;memory.h&gt;int main() &#123; key_t k = ftok(\".\", 0x7777);//.表示当前目录 if(k &lt; 0)&#123; printf(\"ftok error\\n\"); return 1; &#125; //申请共享内存 int shmid = shmget(k, 4096,IPC_CREAT|IPC_EXCL|0666); if(shmid &lt; 0)&#123; printf(\"shmget error\\n\"); &#125; //绑定共享内存 char *buf = shmat(shmid, NULL, 0); if(buf == NULL)&#123; printf(\"shmar error\\n\"); &#125; int i =0; memset(buf, '\\0', 4096); while(i &lt; 26)&#123; sleep(1); buf[i] = 'A'+i; i++; &#125; //取消绑定共享内存 shmdt(shmid); //释放共享内存 shmctl(shmid, IPC_RMID, NULL); return 0;&#125; reder.c 12345678910111213141516171819202122232425262728293031323334353637#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/shm.h&gt;#include &lt;unistd.h&gt;int main() &#123; key_t k = ftok(\".\", 0x7777);//.表示当前目录 if(k &lt; 0)&#123; printf(\"ftok error\\n\"); return 1; &#125; //申请共享内存 int shmid = shmget(k, 4096,IPC_CREAT); if(shmid &lt; 0)&#123; printf(\"shmget error\\n\"); &#125; //绑定共享内存 char *buf = shmat(shmid, NULL, 0); if(buf == NULL)&#123; printf(\"shmar error\\n\"); &#125; while(1)&#123; sleep(1); printf(\"%s\\n\", buf); &#125; //取消绑定共享内存 shmdt(shmid); return 0;&#125;","updated":"2020-03-13T03:06:29.679Z","categories":[{"name":"操作系统实战","slug":"操作系统实战","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"}]},{"title":"Linux进程通信之信号量","date":"2018-11-27T10:09:30.000Z","path":"2018/11/27/Linux进程通信之信号量/","text":"虽然本文是记录使用信号量保证进程的同步与互斥的，但是其实也可以看做是进程之间的通信问题，为了与前面的保持一致，所以还是叫做 Linux进程间通信了！ 信号量基本概念进程间通信的方式有管道、消息队列、共享内存这些都是进程间的信息通信，而信号量可以理解为进程使用的临界资源的状态说明，信号量主要用于保证同步与互斥 临界资源：两个进程看到的一份公共资源称之为临界资源 临界区：各个进程中访问临界资源的代码叫做临界区 互斥：每个进程访问临界资源的时候必须是独占式的(排他式的)，只能自己一个人访问 同步：防止不间断的占有资源和释放资源，这样的话其他进程就会长时间得不到资源，这样会造成进程的饥饿问题 由此可见我们之前用于进程间通信的管道，消息队列，共享内存都是临界资源，管道是内核已经提供了同步与互斥，但是消息队列和共享内存都是不保证同步与互斥的 信号量PV原语信号量：本质上是一把计数器如果一个信号只有0或者1，那么这个就是二元信号量，所以二元信号量可以实现互斥锁 P操作：计数器 --V操作：计数器 ++信号量本身也是临界资源，所以P、V操作必须是原子的 信号量集结构12345678910111213141516struct ipc_perm &#123; key_t __key; // 提供给 semget（）的键 uid_t uid; // 所有者有效 UID gid_t gid; // 所有者有效 GID uid_t cuid; // 创建者有效 UID gid_t cgid; // 创建者有效 GID unsigned short mode; // 权限 unsigned short __seq; // 序列号&#125;; //信号量集的结构struct semid_ds &#123; struct ipc_perm sem_perm; // 所有者和权限 time_t sem_otime; // 上次执行semop的时间 time_t sem_ctime; // 上次更新时间 unsigned short sem_nsems; // 在信号量集合里的索引&#125; 信号量APIsemget作用：用于创建信号量集 12345#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/sem.h&gt;int semget(key_t key, int nsems, int semflg); key 信号集的名字，这个与再创建管道、消息队列、共享内存等用的key是一致的 nsems 信号集中信号量的个数，一般为1（信号集底层就是数组） semflg 同创建消息队列等一样的权限，sem_flags取两个值，IPC_CREATE 和 IPC_EXCL，需要配权限使用 IPC_CREATE 表示若信号量已存在，返回该信号量标识符 IPC_EXCL 表示若信号量已存在，返回错误 return 成功返回一个非负整数，即该信号集的标识码；失败返回-1 num_sems:信号量的数目， shmctl作用：用于控制信号量集 1234#include &lt;sys/ipc.h&gt;#include &lt;sys/shm.h&gt;int shmctl(int shmid, int semnum, int cmd, ...); shmid 这个就是要控制的信号量集 semnum 这个是具体要控制的信号量，因为shmid只能指明是哪一个信号量集(数组)，而semnum就是数组下标 cmd 将要采取的动作(有三个可取值) SETVAL (常用) 用来把信号量初始化为一个已知的值。p这个值通过union semun中的val成员设置，其作用是在信号量第一次使用的时候 GETVAL 获取信号量集中的信号量计数值 IPC_STAT 把semid_ds结构中的数据设置为信号量集的当前关联值 IPC_SET 在进程有足够权限的情况下，把信号量集的当前关联值设置为semid_ds数据结构中给出的值 IPC_RMID (常用) 删除信号量集 semop作用：修改信号量集中的值 12345#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/sem.h&gt;int semop(int semid, struct sembuf *sops, unsigned nsops); semid 这个就是要修改的信号量集 sops 如下结构体的指针，这个结构体是这样的： 12345struct sembuf&#123; short sem_num;//除非使用一组信号量，否则它为0 short sem_op;//信号量在一次操作中需要改变的数据，通常是两个数，一个是-1，即P（等待）操作，一个是+1，即V（发送信号）操作。 short sem_flg;//通常为SEM_UNDO,使操作系统跟踪信号,并在进程没有释放该信号量而终止时，操作系统释放信号量&#125;; nsops 信号量的个数 return 成功返回0，失败返回1 信号量使用示例makefile 123456test:comm.c main.c gcc -o $@ $^.PHONY:cleanclean: rm -rf $@ comm.c &amp;&amp; comm.h &amp;&amp; main.c 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140#ifndef __COMM_H__#define __COMM_H__#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/sem.h&gt;#include &lt;unistd.h&gt;#include &lt;wait.h&gt;#define PATHNAME \".\"#define PROJ_ID 0x6666union semun &#123; int val; /* Value for SETVAL */ struct semid_ds *buf; /* Buffer for IPC_STAT, IPC_SET */ unsigned short *array; /* Array for GETALL, SETALL */ struct seminfo *__buf; /* Buffer for IPC_INFO */&#125;;int createSemSet(int nums);int initSem(int semid, int nums, int initVal);int getSemSet(int nums);int P(int semid, int who);int V(int semid, int who);int destorySemSet(int semid);#endif //!__COMM_H__//---------------------comm.c----------------------------#include \"comm.h\"static int commSemSet(int nums, int flags)&#123; key_t _key = ftok(PATHNAME, PROJ_ID); if(_key &lt; 0)&#123; perror(\"ftok\"); return -1; &#125; int semid = semget(_key, nums, flags); if(semid &lt; 0)&#123; perror(\"semget\"); return -2; &#125; return semid;&#125;int createSemSet(int nums)&#123; return commSemSet(nums, IPC_CREAT|IPC_EXCL|0666); &#125;int getSemSet(int nums)&#123; return commSemSet(nums, IPC_CREAT);&#125;int initSem(int semid, int nums, int initVal)&#123; union semun _un; _un.val = initVal; if(semctl(semid, nums, SETVAL, _un) &lt; 0)&#123; perror(\"semctl\"); return -1; &#125; return 0;&#125;static int commPV(int semid, int who, int op)&#123; struct sembuf _sf; _sf.sem_num = who; _sf.sem_op = op; _sf.sem_flg = 0; if(semop(semid, &amp;_sf, 1) &lt; 0)&#123; perror(\"semop\"); return -1; &#125; return 0;&#125;int P(int semid, int who)&#123; return commPV(semid, who, -1);&#125;int V(int semid, int who)&#123; return commPV(semid, who, 1);&#125;int destorySemSet(int semid)&#123; int ret = semctl(semid, 0, IPC_RMID); if(ret &lt; 0)&#123; perror(\"semctl\"); return -1; &#125; return ret;&#125;//----------------------------main.c----------------------#include \"comm.h\"int main()&#123; int semid = createSemSet(1); initSem(semid, 0, 1); pid_t id = fork(); if(id == 0)&#123; int _semid = getSemSet(0); while(1)&#123; //P(_semid, 0); printf(\"A\"); fflush(stdout); usleep(100000); printf(\"A\"); fflush(stdout); usleep(100000); //V(_semid, 0); &#125; &#125; else&#123; while(1)&#123; //P(semid, 0); printf(\"B\"); fflush(stdout); usleep(100000); printf(\"B\"); fflush(stdout); usleep(100000); //V(semid, 0); &#125; wait(NULL); &#125; destorySemSet(semid); return 0;&#125; 打开PV操作时与未打开时的对比：同样的使用ipcs -s命令即可查看信号量，使用ipcrm -s即可释放信号量资源 进程间通信总结管道 数据只能向一个方向流动；需要双方通信时，需要建立起两个管道 匿名管道只能用于具有亲缘关系的进程，否则使用命名管道 管道内部保证同步机制，从而保证访问数据的一致性。 管道是面向字节流的 管道生命周期随进程，进程在管道在，进程消失管道对应的端口也关闭，两个进程都消失管道也消 管道读端关闭，操作系统向写端发信号终止写端进程 每写一块数据的最大长度是有上限的 System V消息队列 消息队列提供了一个从一个进程向另外一个进程发送一块数据的方法 每个数据块都被认为是有一个类型，接收者进程接收的数据块可以有不同的类型值 每写一块数据的最大长度是有上限的，这点与管道一致 每个消息队列的总的字节数是有上限的（MSGMNB），系统上消息队列的总数也有上限 消息队列生命周期随内核 不保证同步与互斥 共享内存 共享内存区是最快的IPC形式，无需内核干预，直接映射同一块物理内存 作为IPC资源存在，共享内存生命周期同样随内核 不保证同步与互斥 信号量 主要提供对进程间共享资源访问控制机制。相当于内存中的标志，进程可以根据它判定是否能够访问某些共享资源，同时，进程也可以修改该标志。除了用于访问控制外，还可用于进程同步。 二元信号量：最简单的信号量形式，信号灯的值只能取0或1，类似于互斥锁 Socket 两台计算机相互通信本质上也是两个不在同一个计算机上的进程之间的通信(事实上本机之间的进程通过Socket通信也属于这个范畴)，以后再说！","updated":"2020-03-13T03:06:29.678Z","categories":[{"name":"操作系统实战","slug":"操作系统实战","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"}]},{"title":"Nginx可以做什么","date":"2018-11-27T10:09:30.000Z","path":"2018/11/27/Nginx可以做什么/","text":"本文只针对Nginx在不加载第三方模块的情况能处理哪些事情，由于第三方模块太多所以也介绍不完，当然本文本身也可能介绍的不完整，毕竟只是我个人使用过和了解到过得，欢迎留言交流。 Nginx能做什么 反向代理 负载均衡 HTTP服务器（动静分离） 正向代理 以上就是我了解到的Nginx在不依赖第三方模块能处理的事情，下面详细说明每种功能怎么做。 反向代理反向代理应该是Nginx做的最多的一件事了，什么是反向代理呢，以下是百度百科的说法：反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。简单来说就是真实的服务器不能直接被外部网络访问，所以需要一台代理服务器，而代理服务器能被外部网络访问的同时又跟真实服务器在同一个网络环境，当然也可能是同一台服务器，端口不同而已。下面贴上一段简单的实现反向代理的代码 12345678910server &#123;listen 80; server_name localhost; client_max_body_size 1024M; location / &#123; proxy_pass http: //localhost:8080; proxy_set_header Host $host:$server_port; &#125;&#125; 保存配置文件后启动Nginx，这样当我们访问localhost的时候，就相当于访问localhost:8080了 负载均衡负载均衡也是Nginx常用的一个功能，负载均衡其意思就是分摊到多个操作单元上进行执行，例如Web服务器、FTP服务器、企业关键应用服务器和其它关键任务服务器等，从而共同完成工作任务。简单而言就是当有2台或以上服务器时，根据规则随机的将请求分发到指定的服务器上处理，负载均衡配置一般都需要同时配置反向代理，通过反向代理跳转到负载均衡。而Nginx目前支持自带3种负载均衡策略，还有2种常用的第三方策略。 1、RR（默认）每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。简单配置 12345678910111213upstream test &#123; server localhost:8080; server localhost:8081; &#125; server &#123; listen 81; server_name localhost; client_max_body_size 1024M; location / &#123; proxy_pass http://test; proxy_set_header Host $host:$server_port; &#125;&#125; 负载均衡的核心代码为 1234upstream test &#123; server localhost:8080; server localhost:8081;&#125; 这里我配置了2台服务器，当然实际上是一台，只是端口不一样而已，而8081的服务器是不存在的,也就是说访问不到，但是我们访问http://localhost 的时候,也不会有问题，会默认跳转到http://localhost:8080 具体是因为Nginx会自动判断服务器的状态，如果服务器处于不能访问（服务器挂了），就不会跳转到这台服务器，所以也避免了一台服务器挂了影响使用的情况，由于Nginx默认是RR策略，所以我们不需要其他更多的设置2、权重指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。例如 12345upstream test &#123; server localhost:8080 weight=9; server localhost:8081 weight=1;&#125; 那么10次一般只会有1次会访问到8081，而有9次会访问到80803、ip_hash上面的2种方式都有一个问题，那就是下一个请求来的时候请求可能分发到另外一个服务器，当我们的程序不是无状态的时候（采用了session保存数据），这时候就有一个很大的很问题了，比如把登录信息保存到了session中，那么跳转到另外一台服务器的时候就需要重新登录了，所以很多时候我们需要一个客户只访问一个服务器，那么就需要用iphash了，iphash的每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 123456upstream test &#123; ip_hash; server localhost:8080; server localhost:8081;&#125; 4、fair（第三方）按后端服务器的响应时间来分配请求，响应时间短的优先分配。 12345upstream backend &#123; fair; server localhost:8080; server localhost:8081;&#125; 5、url_hash（第三方）按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法 123456upstream backend &#123; hash $request_uri; hash_method crc32; server localhost:8080; server localhost:8081;&#125; 以上5种负载均衡各自适用不同情况下使用，所以可以根据实际情况选择使用哪种策略模式,不过fair和url_hash需要安装第三方模块才能使用，由于本文主要介绍Nginx能做的事情，所以Nginx安装第三方模块不会再本文介绍 HTTP服务器Nginx本身也是一个静态资源的服务器，当只有静态资源的时候，就可以使用Nginx来做服务器，同时现在也很流行动静分离，就可以通过Nginx来实现，首先看看Nginx做静态资源服务器 123456789server &#123; listen 80; server_name localhost; client_max_body_size 1024M; location / &#123; root e:wwwroot; index index.html; &#125;&#125; 这样如果访问http://localhost 就会默认访问到E盘wwwroot目录下面的index.html，如果一个网站只是静态页面的话，那么就可以通过这种方式来实现部署。 动静分离动静分离是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后，我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路 123456789101112131415161718192021222324252627282930upstream test&#123; server localhost:8080; server localhost:8081;&#125; server &#123; listen 80; server_name localhost; location / &#123; root e:wwwroot; index index.html; &#125; # 所有静态请求都由nginx处理，存放目录为html location ~ .(gif|jpg|jpeg|png|bmp|swf|css|js)$ &#123; root e:wwwroot; &#125; # 所有动态请求都转发给tomcat处理 location ~ .(jsp|do)$ &#123; proxy_pass http://test; &#125; error_page 500 502 503 504 / 50x.html; location = /50x.html &#123; root e:wwwroot; &#125;&#125; 这样我们就可以吧HTML以及图片和css以及js放到wwwroot目录下，而tomcat只负责处理jsp和请求，例如当我们后缀为gif的时候，Nginx默认会从wwwroot获取到当前请求的动态图文件返回，当然这里的静态文件跟Nginx是同一台服务器，我们也可以在另外一台服务器，然后通过反向代理和负载均衡配置过去就好了，只要搞清楚了最基本的流程，很多配置就很简单了，另外localtion后面其实是一个正则表达式，所以非常灵活 正向代理正向代理，意思是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端才能使用正向代理。当你需要把你的服务器作为代理服务器的时候，可以用Nginx来实现正向代理，但是目前Nginx有一个问题，那么就是不支持HTTPS，虽然我百度到过配置HTTPS的正向代理，但是到最后发现还是代理不了，当然可能是我配置的不对，所以也希望有知道正确方法的同志们留言说明一下。 12345678910resolver 114.114.114.114 8.8.8.8;server &#123; resolver_timeout 5s; listen 81; access_log e:wwwrootproxy.access.log; error_log e:wwwrootproxy.error.log; location / &#123; proxy_pass http://$host$request_uri; &#125;&#125; resolver是配置正向代理的DNS服务器，listen 是正向代理的端口，配置好了就可以在ie上面或者其他代理插件上面使用服务器ip+端口号进行代理了 原文地址：Nginx可以做什么?看完这篇你就懂了","updated":"2020-03-13T03:06:29.696Z","categories":[{"name":"高性能服务器","slug":"高性能服务器","permalink":"https://zouchanglin.cn/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://zouchanglin.cn/tags/Nginx/"}]},{"title":"使用git的一些规范","date":"2018-11-25T04:01:35.000Z","path":"2018/11/25/使用git的一些规范/","text":"commit message 规范代码风格统一，代码提交信息的说明准确，那么在后期协作以及Bug处理时会更加方便，先来介绍推荐采用的commit规范吧，Commit message格式如下： type(scope): subject 注意冒号后面有空格，type用于说明commit的类别，只允许使用下面7个标识： type：type用于说明commit的类别 feat：新功能（feature） fix：修补bug docs：文档（documentation） style： 格式（不影响代码运行的变动） refactor：重构（即不是新增功能，也不是修改bug的代码变动） test：增加测试 chore：构建过程或辅助工具的变动 如果type为feat和fix，则该commit将肯定出现在Change log之中。 scope：可选项，用于说明commit的影响范围 subject:：commit 目的的简短描述，不超过50个字符，且结尾不加句号 Git 分支的规范 master分支为主分支(保护分支)，不能直接在master上进行修改代码和提交； develop分支为测试分支，所以开发完成需要提交测试的功能合并到该分支； feature分支为开发分支，大家根据不同需求创建独立的功能分支，开发完成后合并到develop分支； fix分支为bug修复分支，需要根据实际情况对已发布的版本进行漏洞修复；","updated":"2020-03-13T03:06:29.719Z","categories":[{"name":"版本控制","slug":"版本控制","permalink":"https://zouchanglin.cn/categories/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://zouchanglin.cn/tags/Git/"}]},{"title":"Linux进程通信之消息队列","date":"2018-11-22T10:09:30.000Z","path":"2018/11/22/Linux进程通信之消息队列/","text":"消息队列消息队列提供了一个从一个进程向另外一个进程发送一块数据的方法每个数据块都被认为是有一个类型，接收者进程接收的数据块可以有不同的类型值消息队列也有管道一样的不足，就是每个消息的最大长度是有上限的（MSGMAX），每个消息队列的总的字节数是有上限的（MSGMNB），系统上消息队列的总数也有一个上限（MSGMNI） 消息队列不提供同步与互斥 消息队列不提供同步与互斥 消息队列不提供同步与互斥 IPC对象数据结构IPC全称是：Inter Process Communication (进程间通信)IPC的源码：/include/linux/ipc.hPS:这个链接可以下载到1.0版本的源码https://mirrors.edge.kernel.org/pub/linux/kernel/v1.0/ 12345678910struct ipc_perm&#123; key_t key; ushort uid; /* owner euid and egid */ ushort gid; ushort cuid; /* creator euid and egid */ ushort cgid; ushort mode; /* access modes see mode flags below */ ushort seq; /* sequence number */&#125;; 消息队列结构源码：/include/linux/msg.h 12345678910111213141516/* one msqid structure for each queue on the system */struct msqid_ds &#123; struct ipc_perm msg_perm; struct msg *msg_first; /* first message on queue */ struct msg *msg_last; /* last message in queue */ time_t msg_stime; /* last msgsnd time */ time_t msg_rtime; /* last msgrcv time */ time_t msg_ctime; /* last change time */ struct wait_queue *wwait; struct wait_queue *rwait; ushort msg_cbytes; /* current number of bytes on queue */ ushort msg_qnum; /* number of messages in queue */ ushort msg_qbytes; /* max number of bytes on queue */ ushort msg_lspid; /* pid of last msgsnd */ ushort msg_lrpid; /* last receive pid */&#125;; 在管道的通信方式中，两个进程需要看到同一份资源那就是管道文件，具有亲缘关系的进程之间其实很容易使用匿名管道进行通信，没有亲缘关系的进程也可以使用文件名来识别同一块资源(管道)，因为Linux下，同一个目录下不可能出现同名文件，根据这个道理，每个文件的路径+自身的文件名就会形成唯一的标识，那么对于两个不相干的进程如何看到同一份消息队列呢？其实ipc_perm结构体的key_t key值解决了这个问题，可以把这个消息队列的唯一标识存储在key_t key中，这样只要是key_t key一致，那么看到的消息对列就是一致的！ ftok根据指定的路径和一个8位的整数来生成一个唯一的识别码 123#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;key_t ftok(const char *pathname, int proj_id) pathname:指定的文件路径proj_id:自己设定的序列号return:唯一标识IDThe ftok() function uses the identity of the file named by the given pathname (which must refer to an existing, accessible file) and the least significant 8 bits of proj_id (which must be nonzero) to generate a key_t type System V IPC key, suitable for use with msgget(2),semget(2), or shmget(2).The resulting value is the same for all pathnames that name the same file, when the same value of proj_id is used. The value returned should be different when the (simultaneously existing) files or the project IDs differ.On success, the generated key_t value is returned. On failure -1 is returned, with errno indicating the error as for the stat(2) system call.ftok()函数使用由给定路径名命名的文件的标识(必须引用现有的可访问文件)和proj_id的最低8位（必须非零）来生成key_t类型System V IPC密钥 ，适用于msgget，semget或shmget。当使用相同的proj_id值时，对于命名同一文件的所有路径名，结果值是相同的。 当（同时存在的）文件或项目ID不同时，返回的值应该不同。成功时，返回生成的key_t值。 失败时返回-1，errno指示stat系统调用的错误。 共享内存，信号量，消息队列都是通过共享文件的方式进行通信，但是这些共享文件的区分方式就是需要一个识别码，你可以理解为每个人都有自己的身份证号码一样，都是唯一的，简言之就是：ftok函数可以根据指定的路径和一个8位的整数来生成一个唯一的识别码，一般在UNIX中，通常是将文件的索引节点取出，然后在前面加上子序号就得到key_t的值。注意：ftok根据文件路径生成ID和文件的权限无关 msgget用来创建和访问一个消息队列 1234#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/msg.h&gt;int msgget(key_t key, int msgflg) key: 某个消息队列的名字msgflg:由九个权限标志构成，用法和创建文件时使用的mode模式标志一致return：成功返回一个非负整数，即该消息队列的标识码；失败返回-1 关键点：IPC_CREAT：如果内核中不存在键值与key相等的消息队列，则新建一个消息队列；如果存在这样的消息队列，返回此消息队列的标识符IPC_CREAT|IPC_EXCL：如果内核中不存在键值与key相等的消息队列，则新建一个消息队列；如果存在这样的消息队列则报错 错误代码：EACCES：指定的消息队列已存在，但调用进程没有权限访问它EEXIST：key指定的消息队列已存在，而msgflg中同时指定IPC_CREAT和IPC_EXCL标志ENOENT：key指定的消息队列不存在同时msgflg中没有指定IPC_CREAT标志ENOMEM：需要建立消息队列，但内存不足ENOSPC：需要建立消息队列，但已达到系统的限制 msgctl控制消息队列 1234#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/msg.h&gt;int msgctl(int msqid, int cmd, struct msqid_ds *buf); msqid: 由msgget函数返回的消息队列标识码cmd:是将要采取的动作,（有三个可取值）return：成功返回0，失败返回-1 cmd:将要采取的动作，分别如下 命令 说明 IPC_STAT 把msqid_ds结构中的数据设置为消息队列的当前关联值，获得msgid的消息队列头数据到buf中 IPC_SET 在进程有足够权限的前提下，把消息队列的当前关联值设置为msqid_ds数据结构中给出的值，设置消息队列的属性，要设置的属性需先存储在buf中，可设置的属性包括：msg_perm.uid、msg_perm.gid、msg_perm.mode以及msg_qbytes IPC_RMID 删除消息队列 错误代码：EACCESS：参数cmd为IPC_STAT，确无权限读取该消息队列EFAULT：参数buf指向无效的内存地址EIDRM：标识符为msqid的消息队列已被删除EINVAL：无效的参数cmd或msqidEPERM：参数cmd为IPC_SET或IPC_RMID，却无足够的权限执行 msgsnd函数把一条消息添加到消息队列中 1234#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/msg.h&gt;int msgsnd(int msqid, const void *msgp, size_t msgsz, int msgflg); msgid: 由msgget函数返回的消息队列标识码msgp:是一个指向结构体的指针，指针指向准备发送的消息 msgp可以是任何类型的结构体，但第一个字段必须为long类型，即表明此发送消息的类型，msgrcv根据此接收消息 msgsz:是msgp指向的消息长度，这个长度不含保存消息类型的那个long int长整型msgflg:控制着当前消息队列满或到达系统上限时将要发生的事情， 0：当消息队列满时，msgsnd将会阻塞，直到消息能写进消息队列 IPC_NOWAIT：当消息队列已满的时候，msgsnd函数不等待立即返回 IPC_NOERROR：若发送的消息大于size字节，则把该消息截断，截断部分将被丢弃，且不通知发送进程 return：成功返回0；失败返回-1注意：消息结构必须小于系统规定的上限值；其次，必须以一个long int长整数开始，接收者函数将利用这个长整数确定消息的类型，消息结构参考形式如下： 1234struct msgbuf &#123; long mtype; char mtext[1];&#125; msgsnd()解除阻塞的三个条件：① 不满足消息队列满或个数满两个条件，即消息队列中有容纳该消息的空间。② msqid代表的消息队列被删除。③ 调用msgsnd函数的进程被信号中断。 msgrcv从标识符为msqid的消息队列读取消息并存于msgp中，读取后把此消息从消息队列中删除 1234#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/msg.h&gt;int msgsnd(int msqid, const void *msgp, size_t msgsz, int msgflg); msgid: 由msgget函数返回的消息队列标识码msgp:是一个指针，指针指向准备接收的消息，msgsz:是msgp指向的消息长度，这个长度不含保存消息类型的那个long int长整型msgtype:它可以实现接收优先级的简单形式msgflg:控制着队列中没有相应类型的消息可供接收时将要发生的事 0: 阻塞式接收消息，没有该类型的消息msgrcv函数一直阻塞等待 IPC_NOWAIT：如果没有返回条件的消息调用立即返回，此时错误码为ENOMSG IPC_EXCEPT：与msgtype配合使用返回队列中第一个类型不为msgtype的消息 IPC_NOERROR：如果队列中满足条件的消息内容大于所请求的size字节，则把该消息截断，截断部分将被丢弃 return:成功返回实际放到接收缓冲区里去的字符个数，失败返回-1 msgtype=0返回队列第一条信息msgtype&gt;0返回队列第一条类型等于msgtype的消息 msgtype&lt;0返回队列第一条类型小于等于msgtype绝对值的消息，并且是满足条件的消息类型最小的消息 ipcs、ipcrm这是两个命令，ipcs:可以显示IPC资源，ipcrm用于手动删除IPC资源管道虽然也可以用于进程间通信，但是管道的声明周期是随进程的，但是消息队列、信号量、共享内存的声明周期是随内核，需要我们自己调用函数或者命令手动清除，所以当每次使用消息队列的时候不要忘记删除IPC资源： ipcs显示 IPC 设施的信息。ipcs [资源选项...] [输出选项]ipcs -m|-q|-s -i选项：-i, –id 打印由标识的资源的详细信息-h, –help display this help-V, –version display version 资源选项：-m, –shmems 共享内存段-q, –queues 消息队列-s, –semaphores 信号量-a, –all 全部(默认)输出选项：-t, –time 显示附加、脱离和更改时间-p, –pid 显示创建者和最后操作者的 PID-c, –creator 显示创建者和拥有者-l, –limits 显示资源限制-u, –summary 显示状态摘要-h, –human 以易读格式显示大小-b, –bytes 以字节数显示大小 ipcrm移除某个 IPC 资源。ipcrm [选项]ipcrm shm|msg|sem ...选项：-m, –shmem-id 按 id 号移除共享内存段-M, –shmem-key &lt;键&gt; 按键值移除共享内存段-q, –queue-id 按 id 号移除消息队列-Q, –queue-key &lt;键&gt; 按键值移除消息队列-s, –semaphore-id 按 id 号移除信号量-S, –semaphore-key &lt;键&gt; 按键值移除信号量-a, –all[=&lt;shm|msg|sem&gt;] (将指定类别中的)全部移除-v, –verbose 解释正在进行的操作 方式一：使用ipcs -q命令查询IPC资源中消息队列的msqid，再使用ipcrm 方式二：在代码中使用消息队列控制函数msgctl设置参数为IPC_RMID下面是两个进程之间使用消息队列通信的代码：comm.c 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include \"comm.h\"static int CommGetMsgQueue(int flag)&#123; key_t k = ftok(PATHNAME, PROJID);//失败返回-1 if(k &lt; 0)&#123; perror(\"ftok error\"); return -1; &#125; int msgid = msgget(k, flag); if(msgid &lt; 0)&#123; perror(\"msgget error\"); &#125; return msgid;&#125;//创建消息队列int CreateMsgQueue()&#123; return CommGetMsgQueue(IPC_CREAT|IPC_EXCL|0666);//附加权限&#125;//打开消息队列int OpenMsgQueue()&#123; return CommGetMsgQueue(IPC_CREAT);&#125;//发送消息void SendMsg(int msgid, char msg[], int type)&#123; //准备要发出的数据 struct msgbuf s_msg; s_msg.mtype = type; strcpy(s_msg.mtext, msg); if(msgsnd(msgid, (void*)&amp;s_msg, sizeof(s_msg.mtext), 0) &lt; 0)&#123; printf(\"msgsend error\"); &#125;&#125;//接受消息void RecvMsg(int msgid, char msg[], int type)&#123; struct msgbuf _msg; //读取消息到_msg中 if(msgrcv(msgid, (void*)&amp;_msg, sizeof(_msg.mtext),type, 0) &gt; 0)&#123; //把数据传出去 strcpy(msg, _msg.mtext); &#125;&#125;//删除消息队列void DestroyMsgQueue(int msgid)&#123; msgctl(msgid, IPC_RMID, NULL);&#125; comm.h 1234567891011121314151617181920212223242526272829303132333435363738#ifndef INC_32_CODE_COMM_H#define INC_32_CODE_COMM_H#include &lt;stdio.h&gt;#include &lt;errno.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/msg.h&gt;#include &lt;memory.h&gt;#include &lt;unistd.h&gt;#define PATHNAME \"/tmp\"#define PROJID 0x6666#define SERVER_TYPE 1#define CLIENT_TYPE 2struct msgbuf&#123; long mtype; char mtext[256];&#125;;//创建消息队列int CreateMsgQueue();//打开消息队列int OpenMsgQueue();//删除消息队列void DestroyMsgQueue(int msgid);//发送消息void SendMsg(int msgid, char msg[], int type);//接受消息void RecvMsg(int msgid, char msg[], int type);#endif //INC_32_CODE_COMM_H client.c 123456789101112131415#include \"comm.h\"int main()&#123; int msgid = OpenMsgQueue(); //接收其他进程（server）发的消息 char msg[256]; while(1)&#123; RecvMsg(msgid,msg,SERVER_TYPE); printf(\"RECV = %s\\n\", msg); sleep(1); &#125; return 0;&#125; server.c 12345678910111213141516171819202122232425262728293031323334353637383940414243#include \"comm.h\"int main()&#123; int msgid = CreateMsgQueue(); printf(\"msgid = %d\\n\",msgid); while(1)&#123; //发送消息到消息队列 SendMsg(msgid, \"hello,xpu\", SERVER_TYPE); sleep(1); &#125; SendMsg(msgid, \"hello,xpu\", SERVER_TYPE); sleep(1); SendMsg(msgid, \"hello,xpu\", SERVER_TYPE); sleep(1); SendMsg(msgid, \"hello,xpu\", SERVER_TYPE); sleep(1); SendMsg(msgid, \"hello,xpu\", SERVER_TYPE); sleep(1);#if 0 //自己接收自己发的消息 char msg[256]; RecvMsg(msgid,msg,SERVER_TYPE); printf(\"RECV = %s\\n\", msg); sleep(1); RecvMsg(msgid,msg,SERVER_TYPE); printf(\"RECV = %s\\n\", msg); sleep(1); RecvMsg(msgid,msg,SERVER_TYPE); printf(\"RECV = %s\\n\", msg); sleep(1); RecvMsg(msgid,msg,SERVER_TYPE); printf(\"RECV = %s\\n\", msg); sleep(1); RecvMsg(msgid,msg,SERVER_TYPE); printf(\"RECV = %s\\n\", msg); sleep(1);#endif DestroyMsgQueue(msgid); return 0;&#125;","updated":"2020-03-13T03:06:29.680Z","categories":[{"name":"操作系统实战","slug":"操作系统实战","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"}]},{"title":"Linux进程通信之管道","date":"2018-11-21T10:09:30.000Z","path":"2018/11/21/Linux进程通信之管道/","text":"Linux进程间通信的基本思想是：让两个进程看到一份公共的资源！ Linux进程间通信的目的 数据传输：⼀个进程需要将它的数据发送给另⼀个进程 资源共享：多个进程之间共享同样的资源。 通知事件：⼀个进程需要向另⼀个或⼀组进程发送消息，通知它们发生了某种事件（如进程终止时要通知父进程）。 进程控制：有些进程希望完全控制另⼀个进程的执行（如Debug进程），此时控制进程希望能够拦截另一个进程的所有陷入和异常，并能够及时知道它的状态改变 通信方式之管道管道是Unix中最古老的进程间通信的形式。我们把从⼀个进程连接到另⼀个进程的⼀个数据流称为⼀个”管道” 管道是面向字节流的 管道的生命周期：与进程一致 管道只能用于单向通信 内核会对管道操作进行同步与互斥 接下来看看管道的使用： 匿名管道pipe函数功能：创建匿名管道 12#include &lt;unistd.h&gt;int pipe(int pipefd[2]); 返回值：On success, zero is returned. On error, -1 is returned, and errno is set appropriately.成功返回0，失败返回错误代码！参数说明：pipe() creates a pipe, a unidirectional data channel that can be used for interprocess communication. The array pipefd is used to return two file descriptors referring to the ends of the pipe. pipefd[0] refers to the read end of the pipe. pipefd[1] refers to the write end of the pipe. Data written to the write end of the pipe is buffered by the kernel until it is read from the read end of the pipe.pipe() 创建一个管道，一个可用于进程间通信的单向数据通道。该文件描述符数组用于返回引用管道末端的两个文件描述符。 pipefd [0]指的是管道的读端。 pipefd [1]指的是管道的写端。写入结束的数据管道由内核缓冲，直到从管道的读取端读取。 管道简单使用示例：从键盘读取数据，写⼊管道，读取管道，写到屏幕 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;int main(int argc, char *argv[]) &#123; int fds[2]; char buf[100] = &#123; 0 &#125;; size_t len; ssize_t r_len; if (pipe(fds) == -1) &#123; perror(\"make pipe\"); exit(1); &#125; //read from stdin while (fgets(buf, 100, stdin)) &#123; len = strlen(buf); //write to pipe if (write(fds[1], buf, len) != len) &#123; perror(\"write to pipe\"); break; &#125; memset(buf, 0, 100); //read from pipe if ((r_len = read(fds[0], buf, (size_t)100)) == -1) &#123; perror(\"read form pipe\"); break; &#125; //write to stdout if (write(1, buf, len) != len) &#123; perror(\"write to stdout\"); break; &#125; &#125; return 0;&#125; 亲缘关系进程直接的通信上面的示例演示了管道的基本使用方式，但是不包含进程之间的通信！接下来看看父子进程之间的通信： 从文件描述符理解管道： 从内核角度理解管道： 所以可以看到，管道其实也是文件，在Linux下一切皆文件！ 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;stdio.h&gt;#include &lt;errno.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;int main()&#123; int pipefd[2]; //创建一个匿名管道，失败直接退出 if(pipe(pipefd) == -1)&#123; perror(\"pipe\"); exit(EXIT_FAILURE); &#125; pid_t pid; pid = fork(); if(pid == -1)&#123; perror(\"pipe\"); exit(EXIT_FAILURE); &#125; //父进程 if(pid == 0)&#123; close(pipefd[0]); write(pipefd[1], \"hello\", 5); close(pipefd[1]); exit(EXIT_SUCCESS); &#125; close(pipefd[1]); char buf[10] = &#123;0&#125;; read(pipefd[0], buf, 10); printf(\"buf = %s\\n\", buf); return 0;&#125; 管道的读写规则 当没有数据可读时 O_NONBLOCK disable：read调用阻塞，即进程暂停执行，一直等到有数据来到为止 O_NONBLOCK enable：read调用返回-1，errno值为EAGAIN 当管道满的时候 O_NONBLOCK disable： write调用阻塞，直到有进程读走数据 O_NONBLOCK enable：调用返回-1，errno值为EAGAIN 如果所有管道写端对应的文件描述符被关闭，则read返回0如果所有管道读端对应的文件描述符被关闭，则write操作会产生信号SIGPIPE,进而可能导致write进程退出，这个不难理解，因为没有人从管道读信息时向管道写入东西是没有意义的，这是一种资源的浪费，所以管道读端对应的文件描述符被关闭时其实应该结束掉write进程； 当要写入的数据量不大于PIPE_BUF时，linux将保证写入的原子性。 当要写入的数据量大于PIPE_BUF时，linux将不再保证写入的原子性。 管道的特点 只能用于具有共同祖先的进程（具有亲缘关系的进程）之间进行通信；通常一个管道由一个进程创建，然后该进程调用fork，此后父、子进程之间就可应用该管道。 管道提供流式服务 一般而言，进程退出，管道释放，所以管道的生命周期随进程 一般而言，内核会对管道操作进行同步与互斥 管道是半双工的，数据只能向一个方向流动；需要双方通信时，需要建立起两个管道 命名管道管道应用的一个限制就是只能在具有共同祖先（具有亲缘关系）的进程间通信。如果我们想在不相关的进程之间交换数据，可以使用FIFO文件来做这项工作，它经常被称为命名管道。命名管道是一种特殊类型的文件!接下来使用一个脚本来演示用管道进行通信： 如果此时我们关闭读端，那么写端就会退出，道理很简单，和匿名管道一样，如果读端都关闭了那么此时如果写端还在写的话其实是一种资源浪费，于是操作系统直接向写端发信号终止写端进程！ 匿名管道和命名管道之间的区别 匿名管道由pipe函数创建并打开，命名管道由mkfifo函数创建，打开用open函数 FIFO(命名管道)与pipe(匿名管道)之间唯一的区别在它们创建与打开的方式不同 命名管道的打开规则如果当前打开操作是为读而打开FIFO时： O_NONBLOCK disable：阻塞直到有相应进程为写而打开该FIFO O_NONBLOCK enable：立刻返回失败，错误码为ENXIO如果当前打开操作是为写而打开FIFO时： O_NONBLOCK disable：阻塞直到有相应进程为读而打开该FIFO O_NONBLOCK enable：立刻返回失败，错误码为ENXIO 接下来是一个模拟客户端和服务器端通信的示例： client.c 12345678910111213141516171819202122232425262728293031323334#include &lt;stdio.h&gt;#include &lt;sys/stat.h&gt;#include &lt;sys/types.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#include &lt;memory.h&gt;#define FIFONAME \"mypipe\"int main() &#123; //创建一个命名管道 mkfifo(FIFONAME, 0664); //打开这个命名管道 int fd = open(FIFONAME, O_WRONLY); if(fd &lt; 0)&#123; return 1; &#125; char buf[1024]; while(1)&#123; printf(\"Please Enter Your Message To Server# \"); fflush(stdout); //从标准输入读取信息 ssize_t s = read(0,buf,sizeof(buf)); buf[s-1] = 0;//覆盖掉之前的回车换行 //向管道写 write(fd,buf,strlen(buf)); &#125; close(fd); return 0;&#125; server.c 123456789101112131415161718192021222324252627282930313233343536#include &lt;stdio.h&gt;#include &lt;sys/stat.h&gt;#include &lt;sys/types.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#define FIFONAME \"mypipe\"int main() &#123; //创建一个命名管道 mkfifo(FIFONAME, 0664); //打开这个命名管道 int fd = open(FIFONAME, O_RDONLY); if(fd &lt; 0)&#123; return 1; &#125; char buf[1024]; while(1)&#123; ssize_t s = read(fd, buf, sizeof(buf)-1); if(s &gt; 0)&#123; //读取成功 buf[s] = 0; printf(\"client# %s\\n\", buf); &#125;else if(s == 0)&#123; //写端把文件描述符关闭 printf(\"client quit！server quit too!\\n\"); break; &#125;else&#123; break; &#125; &#125; return 0;&#125; Bash中的管道是采用匿名管道的方式进行通信，由于匿名管道只能用于在具有亲缘关系之间通信，但是由Bash开启的进程之间是属于兄弟关系，自然就可以通过匿名管道进行通信，多个进程(多个管道)的情况下只要每个进程关闭或打开相应的读写端，形成链式数据结构便可以进行通信了(再次佩写服Bash的大佬)","updated":"2020-03-13T03:06:29.682Z","categories":[{"name":"操作系统实战","slug":"操作系统实战","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"}]},{"title":"Git基本使用方法","date":"2018-11-17T10:09:30.000Z","path":"2018/11/17/Git基本使用方法/","text":"Git基本概念 Workspace：工作区Index / Stage：暂存区Repository：仓库区（或本地仓库）Remote：远程仓库 Git常用命令初始化一个仓库：git init添加文件：git add 文件名添加所有文件：git add *提交到本地仓库： git commit -m &quot;提交说明&quot;查看仓库的状态：git status查看发生更改的文件的具体改变位置：git diff 文件名查看我们提交历史（以便确定要回退到哪个版本。）：git log查看我们提交历史(单行显示)：git log--pretty=oneline回退到上一个版本：git reset --hard HEAD^回退到上上一个版本：git reset --hard HEAD^^回退到100个之前的版本：git reset --hard HEAD~100回退到指定版本：git reset --hard 版本号（版本号也就是git log–pretty=oneline前面显示的id，只需要记住前几位即可）查看命令历史（用来重返未来）：git reflog用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”：git checkout删除一个文件：git rm 文件名 提交到远程仓库：1、在远程创建一个与本地仓库名一致的仓库2、git remote add origin git@github.com:用户名/仓库名.git3、git push -u origin master 初次使用配置Git第一步：设置用户名和邮箱，并且生成秘钥不要密码的话直接一直Enter就行！ 123git config --global user.name \"你的名字\"git config --global user.email \"你的邮箱\" ssh-keygen -t rsa -C \"你的邮箱\" 第二步：到git仓库，添加秘钥如果你没有改变秘钥的生成路径，那么通过家目录的.ssh下通过cat命令应该可以看到生成的秘钥，添加到GitHub（这里以Github为例）的秘钥中！ 第三步: ssh -T git@github.com 测试一下通不通，不通就是ssh-agent -s ssh-add ~/.ssh/id_rsa操作这两步 第四步：把代码clone下来，使用git clone 命令将仓库拷贝下来！ 分类操作的命令新建代码库 在当前目录新建一个Git代码库git init 新建一个目录，将其初始化为Git代码库git init [project-name] 下载一个项目和它的整个代码历史git clone [url] 配置 Git的设置文件为 .gitconfig ，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置） 显示当前的Git配置git config --list 编辑Git配置文件git config -e [--global] 设置提交代码时的用户信息 git config [--global] user.name &quot;[name]&quot; git config [--global] user.email &quot;[email address]&quot; 增加/删除文件 添加指定文件到暂存区git add [file1] [file2] ... 添加指定目录到暂存区，包括子目录git add [dir] 添加当前目录的所有文件到暂存区git add . 删除工作区文件，并且将这次删除放入暂存区git rm [file1] [file2] ... 停止追踪指定文件，但该文件会保留在工作区git rm --cached [file] 改名文件，并且将这个改名放入暂存区git mv [file-original] [file-renamed] 代码提交 提交暂存区到仓库区git commit -m [message] 提交暂存区的指定文件到仓库区git commit [file1] [file2] ... -m [message] 提交工作区自上次commit之后的变化，直接到仓库区git commit -a 提交时显示所有diff信息git commit -v*使用一次新的commit，替代上一次提交如果代码没有任何新变化，则用来改写上一次commit的提交信息git commit --amend -m [message] 重做上一次commit，并包括指定文件的新变化git commit --amend ... 分支 列出所有本地分支git branch 列出所有远程分支git branch -r 列出所有本地分支和远程分支git branch -a 新建一个分支，但依然停留在当前分支git branch [branch-name] 新建一个分支，并切换到该分支git checkout -b [branch] 新建一个分支，指向指定commitgit branch [branch] [commit] 新建一个分支，与指定的远程分支建立追踪关系git branch --track [branch] [remote-branch] 切换到指定分支，并更新工作区git checkout [branch-name] 建立追踪关系，在现有分支与指定的远程分支之间git branch --set-upstream [branch] [remote-branch] 合并指定分支到当前分支git merge [branch] 选择一个commit，合并进当前分支git cherry-pick [commit] 删除分支git branch -d [branch-name] 删除远程分支git push origin --delete、git branch -dr 标签 列出所有tag git tag 新建一个tag在当前commitgit tag [tag] 新建一个tag在指定commitgit tag [tag] [commit] 查看tag信息git show [tag] 提交指定taggit push [remote] [tag] 提交所有taggit push [remote] --tags 新建一个分支，指向某个taggit checkout -b [branch] [tag] 查看信息 显示有变更的文件git status 显示当前分支的版本历史git log 显示commit历史，以及每次commit发生变更的文件git log --stat 显示某个文件的版本历史，包括文件改名git log --follow [file]、git whatchanged [file] 显示指定文件相关的每一次diffgit log -p [file] 显示指定文件是什么人在什么时间修改过git blame [file] 显示暂存区和工作区的差异git diff 显示暂存区和上一个commit的差异git diff --cached [] 显示工作区与当前分支最新commit之间的差异git diff HEAD 显示两次提交之间的差异git diff [first-branch]...[second-branch] 显示某次提交的元数据和内容变化git show [commit] 显示某次提交发生变化的文件git show --name-only [commit] 显示某次提交时，某个文件的内容git show [commit]:[filename] 显示当前分支的最近几次提交git reflog 远程同步 下载远程仓库的所有变动git fetch [remote] 显示所有远程仓库git remote -v 显示某个远程仓库的信息git remote show [remote] 增加一个新的远程仓库，并命名git remote add [shortname] [url] 取回远程仓库的变化，并与本地分支合并git pull [remote] [branch] 上传本地指定分支到远程仓库git push [remote] [branch] 强行推送当前分支到远程仓库，即使有冲突git push [remote] --force 推送所有分支到远程仓库git push [remote] --all 撤销 恢复暂存区的指定文件到工作区git checkout [file] 恢复某个commit的指定文件到工作区git checkout [commit] [file] 恢复上一个commit的所有文件到工作区git checkout . 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变git reset [file] 重置暂存区与工作区，与上一次commit保持一致git reset --hard 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变git reset [commit] 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致git reset --hard [commit] 重置当前HEAD为指定commit，但保持暂存区和工作区不变git reset --keep [commit] 新建一个commit，用来撤销指定commit后者的所有变化都将被前者抵消，并且应用到当前分支git revert [commit] 其他 生成一个可供发布的压缩包git archive 备份当前工作区的内容git stash 从Git栈中读取最近一次保存的内容，恢复工作区的相关内容git stash pop 显示Git栈内的所有备份git stash list 清空Git栈git stash clear 常用的命令图","updated":"2020-03-13T03:06:29.652Z","categories":[{"name":"版本控制","slug":"版本控制","permalink":"https://zouchanglin.cn/categories/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://zouchanglin.cn/tags/Git/"}]},{"title":"C++内存管理","date":"2018-11-14T10:09:30.000Z","path":"2018/11/14/C++内存管理/","text":"内存管理的形式 栈： 栈又叫堆栈，非静态局部变量/函数参数/返回值等等，栈是向下增长的,当方法和语句块一结束，空间马上释放 内存映射段：是高效的I/O映射方式，用于装载一个共享的动态内存库。用户可使用系统接口创建共享共享内存，做进程间通信 堆：堆用于程序运行时动态内存分配，堆是可以上增长的，存放的是成员变量，随着对象而产生，随对象销毁而销毁 数据段：存储全局数据和静态数据 代码段：可执行的代码/只读常量 malloc/calloc/realloc函数123456#include &lt;stdlib.h&gt;void *malloc(size_t size);void free(void *ptr);void *calloc(size_t nmemb, size_t size);void *realloc(void *ptr, size_t size); The malloc() function allocates size bytes and returns a pointer to the allocated memory. The memory is not initialized.If size is 0, then malloc() returns either NULL, or a unique pointer value that can later be successfully passed to free(). The free() function frees the memory space pointed to by ptr, which must have been returned by a previous call to malloc(),calloc() or realloc(). Otherwise, or if free(ptr) has already been called before, undefined behavior occurs. If ptr is NULL, no operation is performed. The calloc() function allocates memory for an array of nmemb elements of size bytes each and returns a pointer to the allocated memory. The memory is set to zero. If nmemb or size is 0, then calloc() returns either NULL, or a unique pointer value that can later be successfully passed to free(). The realloc() function changes the size of the memory block pointed to by ptr to size bytes. The contents will be unchanged in the range from the start of the region up to the minimum of the old and new sizes. If the new size is larger than the old size, the added memory will not be initialized. If ptr is NULL, then the call is equivalent to malloc(size), for all values of size; if size is equal to zero, and ptr is not NULL, then the call is equivalent to free(ptr). Unless ptr is NULL, it must have been returned by an earlier call to malloc(), calloc() or realloc(). If the area pointed to was moved, a free(ptr) is done. malloc()函数分配指定大小字节并返回指向已分配内存的指针。内存未初始化。如果size为0，则malloc()返回NULL或一个以后可以成功传递给free()的唯一指针值。 free()函数释放ptr指向的内存空间，该内存空间必须由之前调用malloc()，calloc()或realloc()返回。否则，或者如果之前已经调用了free(ptr)，则会发生未定义的行为。如果ptr为NULL，则不执行任何操作。 calloc()函数为每个大小为nmemb字节的元素数组分配内存，并返回指向该区域的指针，存储内容设置为零。如果nmemb或size为0，则calloc()返回NULL或一个以后可以成功传递给free()的唯一指针值。 realloc()函数将ptr指向的内存块的大小更改为size字节。内容将在从区域的开始到新旧尺寸的最小范围内保持不变。如果新大小大于旧的大小，则不会初始化添加的内存。如果ptr为NULL，则对于所有size值，调用等效于malloc(size)；如果size等于零，并且ptr不为NULL，则调用等效于free(ptr)。除非ptr为NULL，否则必须由之前调用malloc()，calloc()或realloc()返回。如果指向的区域被移动，则free(ptr)。 new/delete关键字C语言内存管理方式在C++中可以继续使用，但有些地方就无能为力而且使用起来比较麻烦，因此C++又提出了自己的内存管理方式：通过new和delete操作符进行动态内存管理 new/delete操作基本数据类型1234567891011121314151617181920212223242526#include&lt;iostream&gt;using namespace std;int main()&#123; int *ptr = new int; int *ptr2 = new int(1); //cout &lt;&lt; *ptr &lt;&lt; endl; //cout &lt;&lt; *ptr2 &lt;&lt; endl; //int *arr = new int[3]; //OK 但是没有初始化 //int *arr = new int[3]&#123;&#125;; //OK 初始化为全0 //int *arr = new int[3]&#123;1, 2, 3&#125;; //OK 指定内容初始化 //int *arr = new int[]&#123;1, 2, 3&#125;; //OK 指定内容初始化 int *arr = new int[3]&#123;1, 2&#125;; //OK 制定部分内容初始化 //int *arr = new int[3]&#123;1, 2, 3, 4&#125;;//Error 指定个数与实际不符合 for (int i = 0; i &lt; 3;i++)&#123; cout &lt;&lt; arr[i] &lt;&lt; endl; &#125; delete[] arr; return 0;&#125; new/delete操作类123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;#include &lt;stdlib.h&gt;using namespace std;class Demo&#123;public: Demo()&#123; cout &lt;&lt; \"构造函数\" &lt;&lt; endl; &#125; ~Demo()&#123; cout &lt;&lt; \"析构函数\" &lt;&lt; endl; &#125;&#125;;int main()&#123; Demo *pd = (Demo*)malloc(sizeof(Demo)); free(pd); pd = NULL; cout &lt;&lt; endl; Demo *pd2 = new Demo(); delete pd2; cout &lt;&lt; endl; Demo *pd_arr = new Demo[10]; delete[] pd_arr; return 0;&#125; 注意：在申请自定义类型的空间时，new会调用构造函数，delete会调用析构函数，而malloc与free不会。 malloc/free 与 new/delete区别 malloc/free和new/delete的共同点是： 都是从堆上申请空间，并且需要用户手动释放。 malloc/free和new/delete的不同点是： malloc和free是函数，new和delete是操作符 malloc申请的空间不能初始化，new可以初始化 malloc申请空间时，需要手动计算空间大小并传递，new只需在其后跟上空间的类型即可 malloc的返回值为void*, 在使用时必须强转，new不需要，因为new后跟的是空间的类型 malloc申请空间失败时，返回的是NULL，因此使用时必须判空，new不需要，但是new需要捕获异常 malloc/free只能申请内置类型的空间，不能申请自定义类型的空间，因为其不会调用构造与析构函数， 而new可以，new在申请空间后会调用构造函数完成对象的构造，delete在释放空间前会调用析构函数 完成空间中资源的清理 malloc申请的空间一定在堆上，new不一定，因为operator new函数可以重新实现 （new的空间可能在哪呢？） new/delete比malloc和free的效率稍微低点，因为new/delete的底层封装了malloc/free operator new与operator delete函数new和delete是用户进行动态内存申请和释放的操作符，operator new 和operator delete是系统提供的全局函数，new在底层调用operator new全局函数来申请空间，delete在底层通过operator delete全局函数来释放空间。 123456789101112131415161718192021222324252627282930313233343536373839/* operator new：该函数实际通过malloc来申请空间，当malloc申请空间成功时直接返回申请空间失败，尝试执行空间不足应对措施，如果改应对措施用户设置了，则继续申请，否则抛异常。*/void *__CRTDECL operator new(size_t size) _THROW1(_STD bad_alloc)&#123; // try to allocate size bytes void *p; while ((p = malloc(size)) == 0) if (_callnewh(size) == 0) &#123; // report no memory static const std::bad_alloc nomem; _RAISE(nomem); &#125; return (p);&#125;/*operator delete: 该函数最终是通过free来释放空间的*/void operator delete(void *pUserData)&#123; _CrtMemBlockHeader * pHead; RTCCALLBACK(_RTC_Free_hook, (pUserData, 0)); if (pUserData == NULL) return; _mlock(_HEAP_LOCK); /* block other threads */ __TRY /* get a pointer to memory block header */ pHead = pHdr(pUserData); /* verify block type */ _ASSERTE(_BLOCK_TYPE_IS_VALID(pHead-&gt;nBlockUse)); _free_dbg(pUserData, pHead-&gt;nBlockUse); __FINALLY _munlock(_HEAP_LOCK); /* release other threads */ __END_TRY_FINALLY return;&#125; 通过上述两个全局函数的实现知道，operator new 实际也是通过malloc来申请空间，如果malloc申请空间成功就直接返回，否则执行用户提供的空间不足应对措施，如果用户提供该措施就继续申请，否则就抛异常。operator delete 最终是通过free来释放空间的。注意：operator new和operator delete用户也可以自己实现，用户实现时即可实现成全局函数，也可实现成类的成员函数，但是一般情况下不需要实现，除非有特殊需求。 operator new/operator delete与malloc/deletenew会调用构造方法，delete会调用析构函数operator new 与 malloc 用法是一样的，不会调用构造函数operator new 实际上就是malloc + 失败抛出异常operator delete 与 free 用法是一样的，不会调用构造与析构函数operator delete 实际上就是free + 失败抛出异常 new和delete的实现原理内置类型如果申请的是内置类型的空间，new和malloc，delete和free基本类似，不同的地方是：new/delete申请和释放的是单个元素的空间，new[]和delete[]申请的是连续空间，而且new在申请空间失败时会抛异常，malloc会返回NULL。 自定义类型new的原理 调用operator new函数申请空间 在申请的空间上执行构造函数，完成对象的构造 delete的原理 在空间上执行析构函数，完成对象中资源的清理工作 调用operator delete函数释放对象的空间 new T[N]的原理 调用operator new[] 函数，在operator new[]中实际调用operator new函数完成N个对象空间的申请 在申请的空间上执行N次构造函数 delete[]的原理 在释放的对象空间上执行N次析构函数，完成N个对象中资源的清理 调用operator delete[]释放空间，实际在operator delete[]中调用operator delete来释放空间 定位new表达式(placement-new)定位new表达式是在已分配的原始内存空间中调用构造函数初始化一个对象。使用格式： 123new (place_address) type//或者new (place_address) type(initializer-list) place_address必须是一个指针，initializer-list是类型的初始化列表使用场景：定位new表达式在实际中一般是配合内存池使用。因为内存池分配出的内存没有初始化，所以如果是自定义类型的对象，需要使用new的定义表达式进行显示调构造函数进行初始化。 1234567891011121314151617#include &lt;iostream&gt;class Demo&#123;public: Demo()&#123; std::cout &lt;&lt; \"构造函数\" &lt;&lt; std::endl; &#125;&#125;;int main()&#123; //pt现在指向的只不过是与Test对象相同大小的一段空间，还不能算是一个对象，因为构造函数没有执行 Demo *p = (Demo*)malloc(sizeof(Demo)); new(p)Demo;// 注意：如果Demo类的构造函数有参数时，此处需要传参 delete p; return 0;&#125; 如何设计只能在堆/栈上创建的类只能在堆上创建的类 将类的构造函数私有，拷贝构造声明成私有。防止别人调用拷贝在栈上生成对象。 提供一个静态的成员函数，在该静态成员函数中完成堆对象的创建 12345678910class HeapOnly&#123;public: static HeapOnly* CreatInstance()&#123; return new HeapOnly; &#125;private: HeapOnly()&#123;&#125; // 防拷贝 HeapOnly(const HeapOnly&amp;);&#125;; 只能在栈上创建的类只能在栈上创建对象，即不能在堆上创建，因此只要将new的功能屏蔽掉即可，即屏蔽掉operator new和定位new表达式，注意：屏蔽了operator new，实际也将定位new屏蔽掉 1234567class StackOnly&#123;public: StackOnly()&#123;&#125;private: void* operator new(size_t size) = delete; void operator delete(void *p) = delete;&#125;; 设计模式之单例模式一个类只能创建一个对象，即单例模式，该模式可以保证系统中该类只有一个实例，并提供一个访问它的全局访问点，该实例被所有程序模块共享！ 饿汉式单例模式在程序初始化的时候就会创造出对象！ 12345678910111213class Singleton&#123;public: Singleton* GetInstance()&#123; return &amp;instance; &#125;private: Singleton()&#123;&#125; Singleton(const Singleton&amp; other) = delete; Singleton&amp; operator=(const Singleton&amp; other) = delete; static Singleton instance;&#125;;Singleton Singleton::instance;// 在程序入口之前就完成单例对象的初始化 懒汉式单例模式有时使用饿汉模式会导致程序启动时间变长，所以还有这种懒汉式单例模式来解决这个问题！ 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;iostream&gt;#include &lt;mutex&gt;class Singleton&#123;public: static Singleton* GetInstance()&#123; // 注意这里一定要使用Double-Check的方式加锁，才能保证效率和线程安全 if (m_pInstance == nullptr)&#123; m_mutex.lock(); if (m_pInstance == nullptr)&#123; m_pInstance = new Singleton(); &#125; m_mutex.unlock(); &#125; return m_pInstance; &#125; // 实现一个内嵌垃圾回收类 class Recycle &#123; public: ~Recycle()&#123; if (Singleton::m_pInstance) delete Singleton::m_pInstance; &#125; &#125;; // 定义一个静态成员变量，程序结束时，系统会自动调用它的析构函数从而释放单例对象 static Recycle recycle;private: //构造函数私有 Singleton()&#123;&#125; //防止拷贝 Singleton(const Singleton&amp; other) = delete; Singleton&amp; operator=(const Singleton&amp; other) = delete; static Singleton* m_pInstance; // 单例对象指针 static std::mutex m_mutex;&#125;;Singleton* Singleton::m_pInstance = nullptr;std::mutex Singleton::m_mutex;Singleton::Recycle recycle;","updated":"2020-03-13T03:06:29.634Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://zouchanglin.cn/tags/C-C/"}]},{"title":"NASA的C语言编程准则","date":"2018-11-13T10:09:30.000Z","path":"2018/11/13/NASA的C语言编程准则/","text":"美国国家航空航天局（NASA）开发人员的工作是编程界最具挑战性的工作之一。 他们编写代码并开发关键任务应用程序，安全是他们主要关注的重点。在这种情况下，制定严谨的编码准则并遵循，对于他们来说十分重要。这些规则涵盖了软件开发的各个方面，如应该如何编写软件，应该使用哪些语言特性等等。 尽管很难就一个编码标准达成共识，NASA 的 JPL 首席科学家 Gerard J. Holzmann 还是制定了一套名为”发展安全关键代码的十大规则”的代码准则，由所有工作人员共同遵循。 由于 JPL 的工作内容与 C 语言相关，因此本指南主要关注用 C 编程语言编写的代码。但也可以灵活运用到其他语言上。 NASA 的十大编码准则： 1、简化控制流程：使用尽可能精简的控制流程构造编写程序 – 不要使用 setjmp 或 longjmp 构造、goto 语句，以及直接或间接的递归调用。 2、为循环使用固定次数上限：所有的循环必须有一个固定的上限。 必须可以被某个检测工具静态证实，该循环不能达到预置的迭代上限值。如果该上限值不能被静态证实，那么可以认为违背该原则。 3、不要在初始化完成后进行动态内存分配。 4、不使用冗长的函数：如果标准格式为一个语句一行、一个声明一行，那么函数的长度应在一张纸的范围内，即每个函数的代码行不能超过 60。 5、低断言密度：代码中断言的密度平均低至每个函数 2 个断言。断言被用于检测在实际执行中的异常情况。断言必须没有副作用，并应该定义为布尔测试。当一个断言失败时，应该执行一个明确的恢复操作，例如，把错误情况返回给执行该断言失败的函数调用者。对于静态工具来说，任何能被静态工具证实其永远不会失败或永远不能触发的断言违反了该规则（例如，通过增加无用的 assert(true) 语句是不可能满足这个规则的）。 6、以最小范围级别声明数据对象：该原则同时也是数据隐蔽（Data hiding）的基本原则。所有数据对象均必须以尽可能最小的范围级别进行声明。 7、检查参数和返回值：应在每次调用函数后检查非空函数的返回值，并在每个函数内部检查参数的有效性。 8、限制预处理程序的使用：预处理器的使用仅受包含头文件和简单的宏定义的限制。符号拼接、可变参数列表（省略号）和递归宏调用不被允许。所有的宏必须扩展为完整的语法单元。通常不建议使用条件编译指令，但也不总是能够避免每次在代码中这样做的时候必须有基于工具的检查器进行标记，并有充足的理由。 9、限制指针的使用：具体来说，不允许有超过一级的解除指针引用。解除指针引用操作不可隐藏在宏定义或类型声明中。不允许使用函数指针。 10、编译所有代码：从开发工作第一天开始时，在编译器开启最高级别警告选项的条件下对代码进行编译。在此设置之下，代码必须零警告编译通过。代码必须通过源代码静态分析工具，每天检查一次以上，且零警告通过。 NASA 的Gitub https://github.com/nasa/","updated":"2020-04-14T03:21:20.877Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://zouchanglin.cn/tags/C-C/"}]},{"title":"谈谈写时拷贝","date":"2018-11-10T10:09:30.000Z","path":"2018/11/10/谈谈写时拷贝/","text":"COW技术初窥Linux写时拷贝技术(copy-on-write):COW在Linux程序中，fork()会产生一个和父进程完全相同的子进程，但子进程在此后多会exec系统调用，出于效率考虑，linux中引入了“写时复制“技术，也就是只有进程空间的各段的内容要发生变化时，才会将父进程的内容复制一份给子进程。 那么子进程的物理空间没有代码，怎么去取指令执行exec系统调用呢？ 在fork之后exec之前两个进程用的是相同的物理空间（内存区），子进程的代码段、数据段、堆栈都是指向父进程的物理空间，也就是说，两者的虚拟空间不同，但其对应的物理空间是同一个。 当父子进程中有更改相应段的行为发生时，再为子进程相应的段分配物理空间，如果不是因为exec，内核会给子进程的数据段、堆栈段分配相应的物理空间（至此两者有各自的进程空间，互不影响），而代码段继续共享父进程的物理空间（两者的代码完全相同）。而如果是因为exec，由于两者执行的代码不同，子进程的代码段也会分配单独的物理空间。 在网上看到还有个细节问题就是，fork之后内核会通过将子进程放在队列的前面，以让子进程先执行，以免父进程执行导致写时复制，而后子进程执行exec系统调用，因无意义的复制而造成效率的下降。 COW详述现在有一个父进程P1，这是一个主体，那么它是有灵魂也就身体的。现在在其虚拟地址空间（有相应的数据结构表示）上有：正文段，数据段，堆，栈这四个部分，相应的，内核要为这四个部分分配各自的物理块。即：正文段块，数据段块，堆块，栈块。至于如何分配，这是内核去做的事，在此不详述。 1、现在P1用fork()函数为进程创建一个子进程P2，内核：（1）复制P1的正文段，数据段，堆，栈这四个部分，注意是其内容相同。（2）为这四个部分分配物理块，P2的：正文段－＞PI的正文段的物理块，其实就是不为P2分配正文段块，让P2的正文段指向P1的正文段块，数据段－＞P2自己的数据段块（为其分配对应的块），堆－＞P2自己的堆块，栈－＞P2自己的栈块。如下图所示：同左到右大的方向箭头表示复制内容。 2、写时复制技术：内核只为新生成的子进程创建虚拟空间结构，它们来复制于父进程的虚拟究竟结构，但是不为这些段分配物理内存，它们共享父进程的物理空间，当父子进程中有更改相应段的行为发生时，再为子进程相应的段分配物理空间。 3、vfork()：这个做法更加火爆，内核连子进程的虚拟地址空间结构也不创建了，直接共享了父进程的虚拟空间，当然了，这种做法就顺水推舟的共享了父进程的物理空间 通过以上的分析，相信大家对进程有个深入的认识，它是怎么一层层体现出自己来的，进程是一个主体，那么它就有灵魂与身体，系统必须为实现它创建相应的实体， 灵魂实体与物理实体。这两者在系统中都有相应的数据结构表示，物理实体更是体现了它的物理意义。传统的fork()系统调用直接把所有的资源复制给新创建的进程。这种实现过于简单并且效率低下，因为它拷贝的数据也许并不共享，更糟的情况是，如果新进程打算立即执行一个新的映像，那么所有的拷贝都将前功尽弃。 Linux的fork()使用写时拷贝（copy-on-write）页实现。写时拷贝是一种可以推迟甚至免除拷贝数据的技术。内核此时并不复制整个进程地址空间，而是让父进程和子进程共享同一个拷贝。只有在需要写入的时候，数据才会被复制，从而使各个进程拥有各自的拷贝。也就是说，资源的复制只有在需要写入的时候才进行，在此之前，只是以只读方式共享。 这种技术使地址空间上的页的拷贝被推迟到实际发生写入的时候。在页根本不会被写入的情况下—举例来说，fork()后立即调用exec()—它们就无需复制了。fork()的实际开销就是复制父进程的页表以及给子进程创建惟一的进程描述符。在一般情况下，进程创建后都会马上运行一个可执行的文件，这种优化可以避免拷贝大量根本就不会被使用的数据（地址空间里常常包含数十兆的数据）。由于Unix强调进程快速执行的能力，所以这个优化是很重要的。这里补充一点：Linux COW与exec没有必然联系 实际上COW技术不仅仅在Linux进程上有应用，其他例如C++的String在有的IDE环境下也支持COW技术，即例如： 12string str1 = \"hello world\";string str2 = str1; 12str1[1]='q';str2[1]='w'; 在开始的两个语句后，str1和str2存放数据的地址是一样的，而在修改内容后，str1的地址发生了变化，而str2的地址还是原来的,这就是C++中的COW技术的应用，不过VS2005似乎已经不支持COW。 原文地址：《Linux写时拷贝技术(copy-on-write)》","updated":"2020-03-13T03:06:29.769Z","categories":[{"name":"操作系统理论","slug":"操作系统理论","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"}]},{"title":"C++类特性","date":"2018-11-09T10:09:30.000Z","path":"2018/11/09/C++类特性/","text":"访问限定符说明 public修饰的成员在类外可以直接被访问 protected和private修饰的成员在类外不能直接被访问(此处protected和private是类似的) 访问权限作用域从该访问限定符出现的位置开始直到下一个访问限定符出现时为止 class的默认访问权限为private，struct为public(因为struct要兼容C)注意：访问限定符只在编译时有用，当数据映射到内存后，没有任何访问限定符上的区别 C++的class与C的struct的区别：C++需要兼容C语言，所以C++中struct可以当成结构体去使用。另外C++中struct还可以用来定义类。和class是定义类是一样的，区别是struct的成员默认访问方式是public，class是struct的成员默认访问方式是private 类的大小的计算一个类的大小，实际就是该类中成员变量之和，当然也要进行内存对齐，注意空类的大小，空类比较特殊，编译器给了空类一个字节来唯一标识这个类。 结构体内存对齐规则: 第一个成员在与结构体偏移量为0的地址处。 其他成员变量要对齐到某个数字（对齐数）的整数倍的地址处。注意：对齐数 = 编译器默认的一个对齐数 与 该成员大小的较小值。VS中默认的对齐数为8，gcc中的对齐数为4 结构体总大小为：最大对齐数（所有变量类型最大者与默认对齐参数取最小）的整数倍。 如果嵌套了结构体的情况，嵌套的结构体对齐到自己的最大对齐数的整数倍处，结构体的整体大小就是所有最大对齐数（含嵌套结构体的对齐数）的整数倍。 this指针的特性 this指针的类型：类类型* const 只能在“成员函数”的内部使用 this指针本质上其实是一个成员函数的形参，是对象调用成员函数时，将对象地址作为实参传递给this形参。所以对象中不存储this指针。 this指针是成员函数第一个隐含的指针形参，一般情况由编译器通过ecx寄存器自动传递，不需要用户传递 类的6个默认成员函数构造函数构造函数是一个特殊的成员函数，名字与类名相同,创建类类型对象时由编译器自动调用，保证每个数据成员都有 一个合适的初始值，并且在对象的生命周期内只调用一次！ 构造函数是特殊的成员函数，需要注意的是，构造函数的虽然名称叫构造，但是需要注意的是构造函数的主要任务并不是开空间创建对象，而是初始化对象。 如果类中没有显式定义构造函数，则C++编译器会自动生成一个无参的默认构造函数，一旦用户显式定义编译器将不再生成！ 无参的构造函数和全缺省的构造函数都称为默认构造函数，并且默认构造函数只能有一个。注意：无参构造函数、全缺省构造函数、我们没写编译器默认生成的构造函数，都可以认为是默认成员函数。 编译器生成默认的构造函数会对自定类型成员调用的它的默认成员函数 12345678910111213141516171819202122232425262728#include&lt;iostream&gt;using namespace std;class Time&#123;public: Time()&#123; cout &lt;&lt; \"Time()\" &lt;&lt; endl; _hour = 0; _minute = 0; _second = 0; &#125;private: int _hour; int _minute; int _second;&#125;;class Date&#123;private: // 基本类型(内置类型) int _year; int _month; int _day; // 自定义类型 Time _t;&#125;;int main()&#123; Date d; return 0;&#125; 析构函数与构造函数功能相反，析构函数不是完成对象的销毁，局部对象销毁工作是由编译器完成的。而对象在销毁时会自动调用析构函数，完成类的一些资源清理工作一个类有且只有一个析构函数。若未显式定义，系统会自动生成默认的析构函数。对象生命周期结束时，C++编译系统系统自动调用析构函数 拷贝构造函数拷贝构造函数也是特殊的成员函数，其特征如下： 拷贝构造函数是构造函数的一个重载形式。 拷贝构造函数的参数只有一个且必须使用引用传参，使用传值方式会引发无穷递归调用 若未显示定义，系统生成默认的拷贝构造函数。 默认的拷贝构造函数对象按内存存储按字节序完成拷贝，这种拷贝我们叫做浅拷贝，或者值拷贝！ 赋值运算符重载运算符重载C++为了增强代码的可读性引入了运算符重载，运算符重载是具有特殊函数名的函数，也具有其返回值类型，函数名字以及参数列表，其返回值类型与参数列表与普通的函数类似。函数名字为：关键字operator后面接需要重载的运算符符号。函数原型：返回值类型 operator操作符(参数列表)注意： 不能通过连接其他符号来创建新的操作符：比如operator@ 重载操作符必须有一个类类型或者枚举类型的操作数 用于内置类型的操作符，其含义不能改变，例如：内置的整型+，不能改变其含义 作为类成员的重载函数时，其形参看起来比操作数数目少1成员函数的操作符有一个默认的形参this，限定为第一个形参! .* 、:: 、sizeof、?:、. 注意以上5个运算符不能重载。 赋值运算符重载 检测是否自己给自己赋值 返回*this 一个类如果没有显式定义赋值运算符重载，编译器也会生成一个，完成对象按字节序的值拷贝。 const成员const修饰类的成员函数将const修饰的类成员函数称之为const成员函数，const修饰类成员函数，实际修饰该成员函数隐含的this指针，表明在该成员函数中不能对类的任何成员进行修改！ const对象可以调用非const成员函数吗？不能 非const对象可以调用const成员函数吗？可以 const成员函数内可以调用其它的非const成员函数吗？不能 非const成员函数内可以调用其它的const成员函数吗？可以 总结：const只能使用const的东西，非const都可以使用！ 取地址及const取地址操作符重载这两个默认成员函数一般不用重新定义 ，编译器默认会生成！这两个运算符一般不需要重载，使用编译器生成的默认取地址的重载即可，只有特殊情况，才需要重载，比如想让别人获取到指定的内容！ 再谈构造函数构造函数体赋值在创建对象时，编译器通过调用构造函数，给对象中各个成员变量一个合适的初始值 12345678910111213class Date&#123;public: Date(int year, int month, int day)&#123; _year = year; _month = month; _day = day; &#125;private: int _year; int _month; int _day;&#125;; 虽然上述构造函数调用之后，对象中已经有了一个初始值，但是不能将其称作为类对象成员的初始化，构造函数体中的语句只能将其称作为赋初值，而不能称作初始化。因为初始化只能初始化一次，而构造函数体内可以多次赋值。 初始化列表初始化列表：以一个冒号开始，接着是一个以逗号分隔的数据成员列表，每个”成员变量”后面跟一个放在括号中的初始值或表达式。 1234567891011121314class Date&#123;public: Date(size_t year, size_t month, size_t day) : _year(year), _month(month), _day(day)&#123; this-&gt;_year = 2018;//这只能叫做赋初值 &#125;private: size_t _year; size_t _month; size_t _day;&#125;; 注意： 每个成员变量在初始化列表中只能出现一次(初始化只能初始化一次) 类中包含以下成员，必须放在初始化列表位置进行初始化：引用成员变量const成员变量类类型成员(该类没有默认构造函数） 尽量使用初始化列表初始化，因为不管你是否使用初始化列表，对于自定义类型成员变量，一定会先使用初始化列表初始化 成员变量在类中声明次序就是其在初始化列表中的初始化顺序，与其在初始化列表中的先后次序无关 explicit关键字构造函数不仅可以构造与初始化对象，对于单个参数的构造函数，还具有类型转换的作用。 1234567891011121314151617#include &lt;iostream&gt;using namespace std;class Date&#123;public: Date(int year) : _year(year)&#123; &#125;private: size_t _year;&#125;;int main()&#123; Date d1(2017); Date d2 = 2018;//在这里编译器默认构造一个匿名对象，然后将这个匿名对象赋值给d2 return 0;&#125; static成员声明为static的类成员称为类的静态成员，用static修饰的成员变量，称之为静态成员变量；用static修饰的成员函数，称之为静态成员函数。静态的成员变量一定要在类外进行初始化！ 静态成员为所有类对象所共享，不属于某个具体的实例 静态成员变量必须在类外定义，定义时不添加static关键字 类静态成员即可用类名::静态成员或者对象.静态成员来访问 静态成员函数没有隐藏的this指针，不能访问任何非静态成员(必须非私有) 静态成员和类的普通成员一样，也有public、protected、private3种访问级别，也可以具有返回值，const修饰符等参数 静态成员函数可以调用非静态成员函数吗？不能 非静态成员函数可以调用类的静态成员函数吗？可以 静态只能调用静态的函数或变量，非静态访问一切 C++11 的成员初始化新玩法非静态成员变量，可以在成员声明时，直接初始化。 12345678class A&#123;private: // 非静态成员变量，可以在成员声明时，直接初始化。 int a = 10; int* p = (int*)malloc(4); static int n;&#125;;int A::n = 10; 友元友元函数假设现在我们尝试去重载operator&lt;&lt;，然后发现我们没办法将operator&lt;&lt;重载成成员函数。因为cout的输出流对象和隐含的this指针在抢占第一个参数的位置。this指针默认是第一个参数也就是左操作数了。但是实际使用中cout需要是第一个形参对象，才能正常使用。所以我们要将operator&lt;&lt;重载成全局函数。但是这样的话，又会导致类外没办法访问成员，那么这里就需要友元来解决。 123456789101112131415161718192021222324252627#include &lt;iostream&gt;using namespace std;class Date&#123;friend ostream&amp; operator&lt;&lt;(ostream&amp; out, const Date&amp; d);public: Date(int y, int m, int d) : year(y), month(m), day(d)&#123; &#125;private: int year; int month; int day;&#125;;ostream&amp; operator&lt;&lt;(ostream&amp; out, const Date&amp; d)&#123; out &lt;&lt; d.year &lt;&lt; \":\" &lt;&lt; d.month &lt;&lt; \":\" &lt;&lt; d.day; return out;&#125;int main()&#123; Date d(2018, 11, 10); cout &lt;&lt; d &lt;&lt; endl;; return 0;&#125; 友元函数可访问类的私有成员，但不是类的成员函数友元函数不能用const修饰友元函数可以在类定义的任何地方声明，不受类访问限定符限制一个函数可以是多个类的友元函数友元函数的调用与普通函数的调用和原理相同 友元类 友元类的所有成员函数都可以是另一个类的友元函数，都可以访问另一个类中的非公有成员。 友元关系是单向的，不具有交换性。比如上述Time类和Date类，在Time类中声明Date类为其友元类，那么可以在Date类中直接访问Time 类的私有成员变量，但想在Time类中访问Date类中私有的成员变量则不行。 友元关系不能传递，如果B是A的友元，C是B的友元，则不能说明C时A的友元。 123456789101112131415161718192021222324252627282930313233343536class Date; // 前置声明class Time&#123; friend class Date; // 声明日期类为时间类的友元类，则在日期类中就直接访问Time类中的私有成员变量public: Time(int hour, int minute, int second) : _hour(hour) , _minute(minute) , _second(second) &#123; &#125;private: int _hour; int _minute; int _second;&#125;;class Date&#123;public: Date(int year = 1900, int month = 1, int day = 1) : _year(year) , _month(month) , _day(day) &#123; &#125; void SetTimeOfDate(int hour, int minute, int second)&#123; // 直接访问时间类私有的成员变量 _t._hour = hour; _t._minute = minute; _t.second = second; &#125;private: int _year; int _month; int _day; Time _t;&#125;; 内部类如果一个类定义在另一个类的内部，这个内部类就叫做内部类。注意此时这个内部类是一个独立的类，它不属于外部类，更不能通过外部类的对象去调用内部类。外部类对内部类没有任何优越的访问权限内部类就是外部类的友元类。注意友元类的定义，内部类可以通过外部类的对象参数来访问外部类中的所有成员。但是外部类不是内部类的友元 内部类可以定义在外部类的public、protected、private都是可以的。 注意内部类可以直接访问外部类中的static、枚举成员，不需要外部类的对象/类名。 sizeof(外部类)=外部类，和内部类没有任何关系。 12345678910111213141516171819202122class A&#123;private: static int k; int h;public: class B&#123; public: void foo(const A&amp; a) &#123; cout &lt;&lt; k &lt;&lt; endl;//OK cout &lt;&lt; a.h &lt;&lt; endl;//OK &#125; &#125;;&#125;;int A::k = 1;int main()&#123; A::B b; b.foo(A()); return 0;&#125; 再谈三大基本特性：封装，继承，多态封装封装就是把客观事物封装成抽象的类，并且类可以把自己的数据和方法只让可信的类或者对象操作，对不可信的进行信息隐藏。一个类就是一个封装了数据以及操作这些数据的代码的逻辑实体。在一个对象内部，某些代码或某些数据可以是私有的，不能被外界访问。通过这种方式，对象对内部数据提供了不同级别的保护，以防止程序中无关的部分意外的改变或错误的使用了对象的私有部分。 继承让某个类型的对象获得另一个类型的对象的属性的方法。它支持按级分类的概念。继承是指这样一种能力：它可以使用现有类的所有功能，并在无需重新编写原来的类的情况下对这些功能进行扩展。 通过继承创建的新类称为“子类”或“派生类”，被继承的类称为“基类”、“父类”或“超类”。继承的过程，就是从一般到特殊的过程。要实现继承，可以通过 “继承”（Inheritance）和“组合”（Composition）来实现。继承概念的实现方式有二类：实现继承与接口继承。实现继承是指直接使用 基类的属性和方法而无需额外编码的能力；接口继承是指仅使用属性和方法的名称、但是子类必须提供实现的能力。 多态多态是指一个类实例的相同方法在不同情形有不同表现形式。多态机制使具有不同内部结构的对象可以共享相同的外部接口。这意味着，虽然针对不同对象的具体操作不同，但通过一个公共的类，它们（那些操作）可以通过相同的方式予以调用!","updated":"2020-03-13T03:06:29.635Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://zouchanglin.cn/tags/C-C/"}]},{"title":"C++11新特性","date":"2018-11-04T10:09:30.000Z","path":"2018/11/04/C++11新特性/","text":"auto关键字C语言中其实就有auto关键字，修饰可变化的量，但是由于平时我们直接使用int a = 10;也是声明变量，编译器已经自动帮我们加上了auto关键字，是C语言中应用最广泛的一种类型，也就是说，省去类型说明符auto的都是自动变量！随着时代进步，Java10中有一个新特性，就是使用var来定义变量，当然前提是类型可推导，语言总是在演化，C++11也是支持了这个新特性，不过在C++11中是auto关键字：使用auto的时候，编译器根据上下文情况，确定auto变量的真正类型！接下来演示一下auto的使用： 1234567891011121314int main() &#123; auto a = 10; auto b = 20; list&lt;string&gt; s; list&lt;string&gt;::iterator be = s.begin(); list&lt;string&gt;::iterator en = s.end(); auto be2 = s.begin(); //很显然使用auto可以减少很多不必要的代码 auto en2 = s.end(); return 0;&#125; 很显然使用auto可以减少很多不必要的代码，但是: auto不能作为函数参数 auto不能直接用来声明数组 auto不能定义类的非静态成员变量 实例化模板时不能使用auto作为模板参数 auto作为函数返回值时，只能用于定义函数，不能用于声明函数 为了避免与C++98中的auto发生混淆，C++11只保留了auto作为类型指示符的用法 for-each新式风格的for循环，在Java中叫做增强for循环，这个特性从JDK1.5开始被支持，随后C++11也支持了这种for循环 1234int arr[] = &#123; 1,3,5,7,9,11 &#125;;for(int i:arr)&#123; cout &lt;&lt; i &lt;&lt; \" \";&#125; for循环迭代的范围必须是确定的：对于数组而言，就是数组中第一个元素和最后一个元素的范围；对于类而言，应该提供begin和end的方法，begin和end就是for循环迭代的范围不能对参数中的数组进行for-each，因为长度不确定 指针空值nullptr在良好的C/C++编程习惯中，声明一个变量时最好给该变量一个合适的初始值，否则可能会出现不可预料的错误，比如未初始化的指针。NULL实际是一个宏，在传统的C头文件(stddef.h)中 1234567#ifndef NULL#ifdef __cplusplus#define NULL 0#else#define NULL ((void *)0)#endif#endif 可以看到，NULL可能被定义为字面常量0，或者被定义为无类型指针(void*)的常量。不论采取何种定义，在使用空值的指针时，都不可避免的会遇到一些麻烦: 1234567891011121314void f(int)&#123; cout&lt;&lt;\"f(int)\"&lt;&lt;endl;&#125;void f(int*)&#123; cout&lt;&lt;\"f(int*)\"&lt;&lt;endl;&#125;int main()&#123; f(0); f(NULL); f((int*)NULL); return 0;&#125; 程序本意是想通过f(NULL)调用指针版本的f(int*)函数，但是由于NULL被定义成0，因此与程序的初衷相悖。 在C++98中，字面常量0既可以是一个整形数字，也可以是无类型的指针(void*)常量，但是编译器默认情况下将其看成是一个整形常量，如果要将其按照指针方式来使用，必须对其进行强转(void *)0。 为了考虑兼容性，C++11并没有消除常量0的二义性，C++11给出了全新nullptr表示空值指针。C++11为什么不在NULL的基础上进行扩展，这是因为NULL以前就是一个宏，而且不同的编译器厂商对于NULL的实现可能不太相同，而且直接扩展NULL，可能会影响以前旧的程序。因此：为了避免混淆，C++11提供了nullptr，即：nullptr代表一个指针空值常量。nullptr是有类型的，其类型为nullptr_t，仅仅可以被隐式转化为指针类型，nullptr_t被定义在头文件中： 1typedef decltype(nullptr) nullptr_t; 注意： 在使用nullptr表示指针空值时，不需要包含头文件，因为nullptr是C++11作为新关键字引入的。 在C++11中，sizeof(nullptr) 与 sizeof((void*)0)所占的字节数相同。 为了提高代码的健壮性，在后续表示指针空值时建议最好使用nullptr。 long long 类型long long 类型实际上没有在C++ 98中存在，而之后被C99标准收录，其实现在市面上大多数编译器是支持 long long 的，但是这个类型正式成为C++的标准类型是在C++11中。标准要求long long至少是64位也就是8个字节。一个字面常量使用LL后缀表示long long类型，使用ULL后缀表示unsigned long long 类型 constexpr定义常量的时候一般使用const来定义，一个常量必须在定义的时候进行初始化，并且之后不可更改。一个常量必须使用一个常量表达式进行初始化，并且在编译期间就可以得到常量的值，但是如何确定一个表达式就是常量表达式呢，这个通常是由程序员自己确定的，所以C++11提供了一个新的关键字constexpr，使用该关键字定义的常量，由编译器检查为其赋值的表达式是否是常量表达式： 123int a = 10;const int i = a;//OKconstexpr int i2 = a;//error 编译器编译的时候就会报错说a并不是常量。显然constexpr关键字将常量表达式的检查转交给编译器处理，而不是程序员自己，所以使用constexpr定义常量要比const安全! 普通的函数一般是不能用来为constexpr常量赋值的，但是C++11允许定义一种constexpr的函数，这种函数在编译期间就可以计算出结果，这样的函数是可以用来为constexpr赋值的。定义constexpr函数需要遵守一些约定，函数的返回类型以及所有形参的类型都应该是字面值，一般情况下函数体中必须有且只有一条return语句。 123456789101112int fun()&#123; //error return 0;&#125;constexpr int fun()&#123; //OK return 0;&#125;int main()&#123; constexpr int ret = fun(); return 0;&#125; 执行初始化的时候编译器将函数的调用替换成结果值，constexpr函数体中也可以出现除了return之外的其他语句，但是这些语句在运行时不应该执行任何操作，例如空语句，using声明等。constexpr函数允许其返回值并非是一个字面值: 123456789101112constexpr int size(int s)&#123; return s * 4;&#125;int a = 20;const int b = 30;constexpr int c = 40;constexpr int si = size(a); //error a是一个变量所以函数返回的是一个可变的值constexpr int si1 = size(20); //ok 函数返回的实际上是一个常量constexpr int si2 = size(b); //okconstexpr int si3 = size(c); //ok 由上可知constexpr函数并不一定返回常量，如果应用于函数的参数是一个常量表达式则返回常量，否则返回变量，而该函数调用到底是一个常量表达式还是非常量表达式则由编译器来判断。这就是constexpr的好处! using类型别名类型别名其实早在C语言中就有了，一般情况下我们使用关键字typedef来声明一个类型的别名，在C++11中增加了另一种声明类型别名的方法就是使用using关键字，using关键字在C++11以前一般用来引用命名空间。 12345typedef int INT; // 右侧符号代表左侧using INT2 = int; // 左侧符号代表右侧INT a = 20;INT2 b = 30; 列表初始化1234567891011121314151617181920212223242526272829303132333435//列表初始化还可以用结构体typedef struct Str &#123; int x; int y;&#125;Str;Str s = &#123; 10,20 &#125;;//列表初始化类,必须是public成员,如果含有私有成员会失败class Cls &#123;public: int x; int y;&#125;;Cls c = &#123; 10,20 &#125;;//vector不仅可以使用列表初始化，还可以使用列表进行赋值，数组不能用列表赋值vector&lt;int&gt;v1=&#123;1,2,3,4,5,6,7,8,9&#125;; // 初始化vector&lt;int&gt;v2;v2=&#123;3,4,5,6,7&#125;; //赋值 //map列表初始化map&lt;string ,int&gt; m = &#123;&#123;\"x\",1&#125;,&#123;\"y\",2&#125;,&#123;\"z\",3&#125;&#125;; //用函数返回初始化列表只展示关键代码，相关头文件自行添加//同理结构体，类，map的返回也可以使用初始化列表返回vector&lt;int&gt; getVector()&#123; return &#123;1,2,3,4,5&#125;;&#125; int main()&#123; vector&lt;int&gt; v = getVector(); cout&lt;&lt;v[0]&lt;&lt;v[1]&lt;&lt;v.size()&lt;&lt;endl; return 0 ;&#125; decltype类型指示符decltype作用于一个表达式，并且返回该表达式的类型，在此过程中编译器分析表达式的类型，并不会计算表达式的值: 123int a = 10;int b = 20;decltype(a+b) c = 50; // OK c的类型就是 a+b 的类型int 对于引用类型decltype有一些特别的地方： 1234int a = 20 ;int &amp;b = a;decltype(b) c ; // Error c是引用类型必须赋值decltype(b) d = a; // OK d是引用类型，指向a 可以看到decltype如果作用于一个引用类型，其得到的还是一个引用类型如果一个表达式是一个解指针引用的操作，decltype得到的也是一个引用类型： 12345a = 20 ;int *p = &amp;a;decltype(*p) c = a; // c的类型是int&amp;c = 50;cout &lt;&lt; a &lt;&lt; endl; // 输出50 当decltype作用于一个变量的时候，变量加不加括号是有区别的，例如： 123int a = 20;decltype(a) b = 30; //ok b的类型是 intdecltype((a)) c = a ; // ok c的类型是int&amp; 其关联变量 a 加上括号之后编译器会把(a)当作是一个表达式处理，而变量是一种可以作为赋值语句左值的表达式，所以会解释成引用类型。 尾置返回类型看看下面这个函数声明： 1int (*func(char x))[10]; 很显然，func函数的参数是一个char类型的x，返回值是一个指向10个int类型的指针数组的指针（就是一个10个元素的数组，每个元素分别指向一个int类型，返回的就是数组首元素地址—&gt;数组指针），这样的定义实在是晦涩难懂，于是C++11新特性中出现了尾置返回类型： 1auto func(char x) -&gt; int(*) [10]; 这种形式将函数的返回类型写在函数声明的最后面，并且在函数形参列表后面加上 -&gt; 符号，然后紧接着是函数需要返回的类型，由于函数的返回类型被放在了形参列表之后，所以在函数名前面使用一个 auto替代。 =default 生成默认构造函数在C++的类中，如果我们没有定义构造函数，编译器会为我们合成默认的无参构造函数，如果我们定义了构造函数，则编译器就不生成默认构造函数了，但是如果我们定义构造函数同时也希望编译器生成默认构造函数呢? C++11中可以通过在构造函数的声明中直接 =default的方式要求编译器生成构造函数。 12345class ClassName&#123; public: ClassName(int x); ClassName()=default; // 显示要求编译器生成构造函数&#125;; 类对象成员的类内初始化1234class ClassName&#123;public: int x = 10; //C++11 之前是不允许的&#125;; lambda表达式123456789int main()&#123; auto add = [](int a, int b)-&gt;int &#123; return a + b; &#125;; int ret = add(1, 2); std::cout &lt;&lt; \"ret:\" &lt;&lt; ret &lt;&lt; std::endl; return 0;&#125; 缺省参数&lt;不在C++11新特性内&gt;这个只是为了我做一些笔记，这个并不属于C++11性特性 半缺省参数必须从右往左依次来给出，不能间隔着给，站在编译器的角度考虑这个问题 缺省参数不能在函数声明和定义中同时出现 缺省参数在实现与头文件分离的时候缺省参数定义只能出现在头文件中，如果头文件未定义缺省参数，那么即使在实现的时候定义缺省参数也是无法编译的 综上：头文件定义了缺省参数实现的时候就不能写缺省参数，头文件没有定义缺省参数，实现的时候更不能自己加缺省参数！ 参考链接：《C++11常用特性的使用经验总结》《C++11新特性梳理》《C++11新特性整理》","updated":"2020-03-13T03:06:29.632Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://zouchanglin.cn/tags/C-C/"}]},{"title":"Linux下打包静动态库","date":"2018-10-29T10:09:30.000Z","path":"2018/10/29/Linux下打包静动态库/","text":"静态库和动态库概念静态库在Linux下是.a的后缀名，在windows下是.lib的后缀名，程序在编译链接的时候吧库代码链接到可执行文件中，运行时不再需要静态库 动态库在Linux下是.so ，在windows下是.dll，程序在运行的时候才去链接动态库的代码，多个程序共享库的代码！一个与动态库链接的可执行文件仅仅包含它用到的函数的入口地址的一个表，而不是外部函数所在目标文件的整个机器码在可执行文件运行以前，外部函数的机器码，由操作系统从磁盘上复制到内存中，这个过程就是动态链接动态库可以在多个程序直接共享，所以使得可执行文件变得更小，节省磁盘空间，操作系统采用虚拟内存机制允许物理内存中的一份动态库被要用到该库的所有进程共用，节省了内存和磁盘空间 打包静态库与动态库打包静态库假设有以下代码，我们需要把add和sub打包成mymath静态库！ 1234567891011121314151617[zcl@localhost 25_code]$ lsadd.c add.h main.c sub.c sub.h[zcl@localhost 25_code]$ gcc -c add.c -o add.o[zcl@localhost 25_code]$ gcc -c sub.c -o sub.o[zcl@localhost 25_code]$ ar -rc libmymath.a add.o sub.o[zcl@localhost 25_code]$ lsadd.c add.h add.o libmymath.a main.c sub.c sub.h sub.o[zcl@localhost 25_code]$ ar -tv libmymath.a rw-rw-r-- 1000/1000 1240 Oct 28 03:58 2018 add.orw-rw-r-- 1000/1000 1240 Oct 28 03:58 2018 sub.o[zcl@localhost 25_code]$ gcc main.c -L. -lmymath[zcl@localhost 25_code]$ lsadd.c add.h add.o a.out libmymath.a main.c sub.c sub.h sub.o[zcl@localhost 25_code]$ ./a.out add_ret = 5sub_ret = 3[zcl@localhost 25_code]$ 库搜索路径 从左到右搜索-L指定的目录 由环境变量指定的目录（LIBRARY_PATH） 由系统指定的目录 /usr/lib、/usr/local/lib 打包动态库1234567891011121314[zcl@localhost 25_code]$ lsadd.c add.h libmymath.a main.c sub.c sub.h[zcl@localhost 25_code]$ gcc -fPIC -c add.c sub.c[zcl@localhost 25_code]$ gcc -shared -o libmymath.so add.o sub.o[zcl@localhost 25_code]$ lsadd.c add.h add.o libmymath.a libmymath.so main.c sub.c sub.h sub.o[zcl@localhost 25_code]$ gcc -c main.c -o main.o[zcl@localhost 25_code]$ gcc main.o -o a2.out -L. -lmymath[zcl@localhost 25_code]$ lsa2.out add.c add.h add.o libmymath.a libmymath.so main.c main.o sub.c sub.h sub.o[zcl@localhost 25_code]$ export LD_LIBRARY_PATH=.[zcl@localhost 25_code]$ ./a2.out add_ret = 5sub_ret = 3 其它的添加动态库方式：1、往/lib和/usr/lib里面加东西，是不用修改/etc/ld.so.conf文件的，但是添加完后需要调用下ldconfig，不然添加的library会找不到。2、如果添加的library不在/lib和/usr/lib里面的话，就一定要修改/etc/ld.so.conf文件，往该文件追加library所在的路径，然后也需要重新调用下ldconfig命令。3、如果添加的library不在/lib或/usr/lib下，但是却没有权限操作写/etc/ld.so.conf文件的话，这时就需要往export里写一个全局变量LD_LIBRARY_PATH，就可以了。参考文章：《Linux动态链接库的使用》《linux下动态链接库》","updated":"2020-03-13T03:06:29.672Z","categories":[{"name":"操作系统实战","slug":"操作系统实战","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"}]},{"title":"软硬链接与Linux文件系统","date":"2018-10-26T10:09:30.000Z","path":"2018/10/26/软硬链接与Linux文件系统/","text":"想要理解硬链接和软链接必须要了解一下什么是Linux的文件系统 文件分类 普通文件 (-)这个不用说，常见的音频、视频、文本、可执行程序都是普通文件 目录文件 (d)如果是要查看目录，需要读权限；如果要进入目录，需要该目录具有可执行权限；如果要在目录里修改或者增加文件，那么需要写权限 字符设备文件 (c)提供连续的数据流，应用程序可以顺序读取，不支持随机存取。键盘、调制解调器等等都是字符设备文件，在你按键的时候系统只能一个一个从键盘上读取字符，这样的设备就是字符设备 块设备文件 (b)应用程序可以随机访问设备数据，程序可自行确定读取数据的位置。硬盘、软盘、CD-ROM驱动器和闪存都是典型的块设备，应用程序可以寻址磁盘上的任何位置，并由此读取数据。 套接字文件 (s)这类文件通常用在网络数据连接。可以启动一个程序来监听客户端的要求，客户端就可以通过套接字来进行数据通信。第一个属性为 [s]，最常在 /var/run目录中看到这种文件类型 链接文件 (l)这个在本文后面将说到！ 管道文件 (p)管道文件可以用于进程间通信，至于什么是管道我会在Linux进程间通信的博客中说到！ 文件应该是两部分构成文件信息+文件内容 ,stat命令可以查看文件的具体信息 123456789101112131415161718[zcl@localhost ~]$ lltotal 12drwxrwxr-x. 28 zcl zcl 4096 Oct 24 21:56 Codedrwxr-xr-x. 3 zcl zcl 34 Oct 25 05:02 Desktopdrwxr-xr-x. 2 zcl zcl 6 Oct 5 00:07 Documentsdrwxr-xr-x. 2 zcl zcl 6 Oct 5 00:07 Downloads-rw-rw-r--. 1 zcl zcl 827 Oct 5 00:12 install.sh-rw-rw-r--. 1 zcl zcl 60 Oct 11 06:46 makefile[zcl@localhost ~]$ stat Code File: ‘Code’ Size: 4096 Blocks: 8 IO Block: 4096 directoryDevice: fd00h/64768d Inode: 3270049 Links: 28Access: (0775/drwxrwxr-x) Uid: ( 1000/ zcl) Gid: ( 1000/ zcl)Context: unconfined_u:object_r:user_home_t:s0Access: 2018-10-25 22:29:03.493269180 -0400Modify: 2018-10-24 21:56:01.089329792 -0400Change: 2018-10-24 21:56:01.089329792 -0400 Birth: - Access：最后访问时间Modif：文件内容最后修改时间Change：属性最后修改时间 inode 所以，新建一个文件的主要操作是： 存储属性内核首先找到一个空闲的inode，在这里是34192477，内核把文件信息记录到其中 存储数据该文件存储在三个磁盘块，内核找到了三个空闲块，29，57，1228，将内核缓冲区数据复制到29，下一块复制到57，以此类推 记录分配情况文件按顺序29，57，1228存放，内核在inode上的磁盘分布区记录了上述块列表 添加文件名到目录新的文件名叫做myfile。内核将入口（34192477，myfile）添加到目录文件，文件名核inode之间的对应关系将文件名和文件的内容及属性连接起来 硬链接和软链接上面我们了解了inode，是不是每个文件都有自己的独立的inode 呢？也不一定 硬链接在Linux上可以将多个文件名对应同一个inode，那么这个就是硬链接 1234567891011121314[tim@xpu ~]$ touch myfile[tim@xpu ~]$ ln myfile mylink[tim@xpu ~]$ ls -li myfile mylink1067027 -rw-rw-r-- 2 tim tim 0 Oct 26 17:38 myfile1067027 -rw-rw-r-- 2 tim tim 0 Oct 26 17:38 mylink[tim@xpu ~]$ lltotal 16drwxrwxr-x 2 tim tim 4096 Oct 16 18:32 11_codedrwxrwxr-x 2 tim tim 4096 Oct 16 18:32 12_codedrwxrwxr-x 2 tim tim 4096 Oct 1 22:09 lhl-rw-rw-r-- 2 tim tim 0 Oct 26 17:38 myfile-rw-rw-r-- 2 tim tim 0 Oct 26 17:38 mylinkdrwxrwxr-x 3 tim tim 4096 Oct 2 18:41 zcl[tim@xpu ~]$ 我们可以看到myfile和mylink的inode号是一样的，那么这就属于硬链接，所以myfile与mylink共用一个inode，所以所对应的物理设备也是只有一份文件，同样的我们可以看出来myfile和mylink的硬链接数为2，接下来说说目录文件的硬链接数目，每个目录中的子目录都有 . 和 .. ，. 就表示当前目录， .. 就表示上一级目录，所以一个空目录都包含两个硬链接数，如果包含子目录的话那么硬链接数还应该加上子目录的个数，因为子目录中中的每个目录都含有一个 .. 与父目录硬链接，使用ln命令实现文件之间的硬链接，使用方法在上述代码中已经包含！ 软链接使用ln -s选项可以建立软链接，软链接有自己独立的inode，软链接保存了其代表的文件的绝对路径，是另外一种文件，在硬盘上有独立的区块，访问时替换自身路径。所以可以把软链接看成是Windows底下的快捷方式! 123456789101112[tim@xpu code]$ touch myfile[tim@xpu code]$ ln myfile mylink[tim@xpu code]$ lltotal 0-rw-rw-r-- 2 tim tim 0 Oct 26 18:00 myfile-rw-rw-r-- 2 tim tim 0 Oct 26 18:00 mylink[tim@xpu code]$ ln -s myfile _mylink [tim@xpu code]$ ls -litotal 01067046 -rw-rw-r-- 2 tim tim 0 Oct 26 18:00 myfile1067046 -rw-rw-r-- 2 tim tim 0 Oct 26 18:00 mylink1067047 lrwxrwxrwx 1 tim tim 6 Oct 26 18:01 _mylink -&gt; myfile 接下来加入我们删除myfile，硬链接mylink是正常的，但是软链接却报警告： 不难理解， 对于硬链接只要还存在硬链接那么即使删除其中一个，那么inode也不会释放，那么磁盘数据也不会释放，对于软链接来说，软链接由于是一个独立的文件保存了其指向文件的路径，所以只要myfile被删除，那么路径也就没有意义了！","updated":"2020-03-13T03:06:29.770Z","categories":[{"name":"操作系统理论","slug":"操作系统理论","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"}]},{"title":"工厂设计模式","date":"2018-10-23T10:09:30.000Z","path":"2018/10/23/工厂设计模式/","text":"六大设计原则既然说到设计模式那就顺便回顾一下六大设计原则，六大设计原则是心法，二十三种设计模式是内功，下面只是简明的叙述了一下，后面还会有更详细的说明！一 、 类单一职责原则：Single Responsibility Principle (SRP)一个类只有一个引起这个类变化的原因。即一个类只完成一个功能，如果做不到一个类只完成一个功能，最少要保证一个方法只完成一个功能。 二、依赖倒置原则：Dependency Inversion Principle (DIP)高层组件应该依赖抽象而不依赖具体，即面向接口编程，一般依赖的成员变量或者参数都应该是抽象的不应该是具体的。 三、里氏替换原则：Liskov Substitution Principle (LSP)凡是父类出现的地方都可以用子类代替并且原功能没有发生变化，子类不应该覆盖父类的非抽象方法。 四、迪米特法则：Least Knowledge Principle (LKP)一个类要尽量的封装自己，一个对象应该对其他对象有最少的了解，一个类只需要知道自己需要耦合或者调用类的public方法即可。 五、接口隔离原则：Interface Segregation Principle (ISP)一个接口完成的功能尽可能的单一，不要让一个接口承担过多的责任。 六、开闭原则：The Open-Closed Principle (OCP)对扩展开放，对修改闭合。 工厂模式概述如何将实例化具体类的代码从客户端中抽离，或者封装起来？这就需要工厂模式：工厂模式是我们最常用的实例化对象模式了，是用工厂方法代替new操作的一种模式。工厂模式的核心作用就是把new操作解耦合！ 一、简单工厂模式简单工厂模式：专门定义一个类用来创建其它类的实例，被创建的实例通常都具有共同的父类。Spring-BeanFactory就用到了简单工厂模式！ 模式中包含的角色及其职责： 一个抽象产品类（接口） 多个具体产品类 一个工厂类（new 操作在此工厂中进行）客户端通过工厂类获取具体实例 以购买电脑为例子：这里我们相当于是创建生产电脑的工厂，客户需要购买什么样的电脑，只要输入类型编号就可以获取该电脑。将类的实例化交给工厂，易于new操作的解耦合！ 12345678910111213141516171819202122232425262728293031323334353637383940import java.util.Scanner;//定义的产品抽象类abstract class Computer&#123; public abstract void fun();&#125;//具体产品子类class SufeBook extends Computer&#123; public void fun()&#123; System.out.println(\"This is SufeBook\"); &#125;&#125;//具体产品子类class MacBookPro extends Computer&#123; public void fun()&#123; System.out.println(\"This is MacBookPro\"); &#125;&#125;//工厂类class ComputerFactory&#123; private ComputerFactory()&#123;&#125; public static Computer getComuter(String pcType)&#123; Computer ret = null; if(pcType.equals(\"mac\"))&#123; ret = new MacBookPro(); &#125;else if(pcType.equals(\"sufe\"))&#123; ret = new SufeBook(); &#125; return ret; &#125;&#125;public class Demo&#123; public static void main(String[] args) &#123; Scanner sc = new Scanner(System.in); System.out.println(\"请输入笔记本\"); Computer ret = ComputerFactory.getComuter(sc.next()); ret.fun(); &#125;&#125; 简单工厂模式的优缺点在这个模式中，工厂类是整个模式的关键所在。它包含必要的判断逻辑，能够根据外界给定的信息，决定究竟应该创建哪个具体类的对象。用户在使用时可以直接根据工厂类去创建所需的实例，而无需了解这些对象是如何创建以及如何组织的。有利于整个软件体系结构的优化。 不难发现，简单工厂模式的缺点也正体现在其工厂类上，由于工厂类集中了所有实例的创建逻辑，所以“高内聚”方面做的并不好。另外，当系统中的具体产品类不断增多时，可能会出现要求工厂类也要做相应的修改，扩展性并不很好，添加具体产品需要修改工厂违反OCP开放封闭原则！ 二、工厂方法模式定义一个用来创建对象的接口，让子类决定实例化哪一个类，让子类决定实例化延迟到子类!Spring-FactoryBean就用到的是工厂方法模式！工厂方法模式同样属于类的创建型模式又被称为多态工厂模式 。工厂方法模式的意义是定义一个创建产品对象的工厂接口，将实际创建工作推迟到子类当中。核心工厂类不再负责产品的创建，这样核心类成为一个抽象工厂角色，仅负责具体工厂子类必须实现的接口，这样进一步抽象化的好处是使得工厂方法模式可以使系统在不修改具体工厂角色的情况下引进新的产品。 模式中包含的角色及其职责： 抽象工厂（Creator）角色 工厂方法模式的核心，任何工厂类都必须实现这个接口。 具体工厂（ Concrete Creator）角色 具体工厂类是抽象工厂的一个实现，负责实例化产品对象。 抽象（Product）角色 工厂方法模式所创建的所有对象的父类，它负责描述所有实例所共有的公共接口。 具体产品（Concrete Product）角色 工厂方法模式所创建的具体实例对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//抽象产品类abstract class Computer&#123; public abstract void fun();&#125;//具体产品类class MacBookPro extends Computer &#123; public void fun()&#123; System.out.println(\"This is a MacBookPro!\"); &#125;&#125;//具体产品类class SufeBook extends Computer&#123; public void fun()&#123; System.out.println(\"This is a SufeBookPro!\"); &#125;&#125;//抽象工厂接口interface ComputerFactory &#123; Computer createComputer();&#125;//实现工厂接口的具体类class AppleFactory implements ComputerFactory&#123; public Computer createComputer()&#123; return new MacBookPro(); &#125;&#125;//实现工厂接口的具体类class MicrosoftFactory implements ComputerFactory&#123; public Computer createComputer()&#123; return new SufeBook(); &#125;&#125;public class Demo&#123; public static void main(String[] args) &#123; ComputerFactory mf = new MicrosoftFactory(); Computer mfc = mf.createComputer(); mfc.fun(); ComputerFactory apple = new AppleFactory(); Computer mac = apple.createComputer(); mac.fun(); &#125;&#125; 工厂方法模式和简单工厂模式比较：工厂方法模式与简单工厂模式在结构上的不同不是很明显。工厂方法类的核心是一个抽象工厂类，而简单工厂模式把核心放在一个具体类上。工厂方法模式之所以有一个别名叫多态性工厂模式是因为具体工厂类都有共同的接口，或者有共同的抽象父类。 当系统扩展需要添加新的产品对象时，仅仅需要添加一个具体对象以及一个具体工厂对象，原有工厂对象不需要进行任何修改，也不需要修改客户端，很好的符合了OCP开放封闭原则。而简单工厂模式在添加新产品对象后不得不修改工厂方法，扩展性不好。 工厂方法模式退化后可以演变成简单工厂模式。 三、抽象工厂模式抽象工厂模式是所有形态的工厂模式中最为抽象和最其一般性的。抽象工厂模式可以向客户端提供一个接口，使得客户端在不必指定产品的具体类型的情况下，能够创建多个产品族的产品对象。 有多少个产品等级结构，就会在工厂角色中发现多少个工厂方法。每一个产品等级结构中有多少个具体的产品，就有多少个产品族，也就会在工厂等级结构中发现多少个具体工厂。在这里我使用水果举例子： 模式中包含的角色及其职责1.抽象工厂（Creator）角色 抽象工厂模式的核心，包含对多个产品结构的声明，任何工厂类都必须实现这个接口。2.具体工厂（ Concrete Creator）角色 具体工厂类是抽象工厂的一个实现，负责实例化某个产品族中的产品对象。3.抽象产品（Product）角色 抽象模式所创建的所有对象的父类，它负责描述所有实例所共有的公共接口。4.具体产品（Concrete Product）角色 抽象模式所创建的具体实例对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126//水果抽象接口public interface Fruit &#123; void get();&#125;//水果工厂接口public interface FruitFactory &#123; //实例化Apple public Fruit getApple(); //实例化Banana public Fruit getBanana();&#125;//苹果抽象类public abstract class Apple implements Fruit &#123; @Override public abstract void get();&#125;//香蕉抽象类public abstract class Banana implements Fruit &#123; @Override public abstract void get();&#125;//北方苹果具体类public class NorthApple extends Apple &#123; public void get() &#123; System.out.println(\"NorthApple\"); &#125;&#125;//南方香蕉具体类public class NorthBanana extends Banana &#123; public void get() &#123; System.out.println(\"NorthBanana\"); &#125;&#125;//新增加的温室苹果public class GreenhouseApple extends Apple &#123; public void get() &#123; System.out.println(\"GreenhouseApple\"); &#125;&#125;//新增的温室香蕉类public class GreenhouseBanana extends Banana &#123; public void get() &#123; System.out.println(\"greenhouseBabanana\"); &#125;&#125;//南方苹果具体类public class SouthApple extends Apple &#123; public void get() &#123; System.out.println(\"SouthApple\"); &#125;&#125;//北方香蕉具体类public class NorthBanana extends Banana &#123; public void get() &#123; System.out.println(\"NorthBanana\"); &#125;&#125;//北方水果工厂类public class NorthFruitFactory implements FruitFactory &#123; public Fruit getApple() &#123; return new NorthApple(); &#125; public Fruit getBanana() &#123; return new NorthBanana(); &#125;&#125;//南方水果工厂类public class SouthFruitFactory implements FruitFactory &#123; public Fruit getApple() &#123; return new SouthApple(); &#125; public Fruit getBanana() &#123; return new SouthBanana(); &#125;&#125;//温室具体工厂类public class GreenhouseFruitFactory implements FruitFactory &#123; //返回GreenhouseApple public Fruit getApple() &#123; return new GreenhouseApple(); &#125; //返回GreenhouseBanana public Fruit getBanana() &#123; return new GreenhouseBanana(); &#125;&#125;public class Demo&#123; public static void main(String[] args) &#123; FruitFactory northfruitFactory = new NorthFruitFactory(); Fruit apple = northfruitFactory.getApple(); Fruit banana = northfruitFactory.getBanana(); apple.get(); banana.get(); System.out.println(\"---------------------------\"); FruitFactory southFruitFactory = new SouthFruitFactory(); Fruit apple2 = southFruitFactory.getApple(); Fruit banana2 = southFruitFactory.getBanana(); apple2.get(); banana2.get(); System.out.println(\"--------------------------\"); GreenhouseFruitFactory greenhouseFruitFactory = new GreenhouseFruitFactory(); Fruit apple3 = greenhouseFruitFactory.getApple(); Fruit banana3 = greenhouseFruitFactory.getBanana(); apple3.get(); banana3.get(); &#125;&#125; 这样比工厂方法模式更加抽象，抽象工厂模式是所有形态的工厂模式中最为抽象和最具一般性的一种形态。抽象工厂模式是指当有多个抽象角色时，使用的一种工厂模式。抽象工厂模式可以向客户端提供一个接口，使客户端在不必指定产品的具体的情况下，创建多个产品族中的产品对象。根据里氏替换原则，任何接受父类型的地方，都应当能够接受子类型。因此，实际上系统所需要的，仅仅是类型与这些抽象产品角色相同的一些实例，而不是这些抽象产品的实例。换言之，也就是这些抽象产品的具体子类的实例。工厂类负责创建抽象产品的具体子类的实例!","updated":"2020-03-13T03:06:29.736Z","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://zouchanglin.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[]},{"title":"系统IO接口","date":"2018-10-18T10:09:30.000Z","path":"2018/10/18/系统IO接口/","text":"一、fopen函数12#include &lt;stdio.h&gt;FILE *fopen(const char *path, const char *mode); 参数说明：path：要打开的文件路径+文件名mode：打开模式，下面是第二个参数的说明来自CentOS 7：man 3 fopen 选项 说明 译文 r Open text file for reading,The stream is positioned at the beginning of the file. 打开只读文本文件，流位于文件的开头。 r+ Open for reading and writing. The stream is positioned at the beginning of the file. 打开可读可写的文本文件，流位于文件的开头。 w Truncate file to zero length or create text file for writing.The stream is positioned at the beginning of the file. 将文件清空或创建用于写入的文本文件。流位于文件的开头。 w+ Open for reading and writing.The file is created if it does not exist, otherwise it is truncated.The stream is positioned at the beginning of the file. 打开可读可写的文本文件，如果文件不存在，则创建该文件，否则将被截断，流位于文件的开头。 a Open for appending (writing at end of file).The file is created if it does not exist.The stream is positioned at the end of the file. 打开以追加（在文件末尾写入）。如果文件不存在，则创建该文件。流位于文件的末尾。 a+ Open for reading and appending（writing at end of file).The file is created if it does not exist.The initial file position for reading is at the beginning of the file，but output is always appended to the end of the file. 打开读取和追加（在文件结尾写入）。如果文件不存在，则创建该文件。用于读取的初始文件位置在文件的开头，但是输出总是附加在文件的结尾。 关于fseek、ftell、rewind函数对于文件的读写方式，C 语言不仅支持简单地顺序读写方式，还支持随机读写（即只要求读写文件中某一指定的部分）。对顺序读写方式来说，随机读写方式需要将文件内部的位置指针移动到需要读写的位置再进行读写，这通常也被称为文件的定位。 rewindrewind 函数用于将文件内部的位置指针重新指向一个流（数据流或者文件）的起始位置。这里需要注意的是，这里的”指针”表示的不是文件指针，而是文件内部的位置指针。即随着对文件的读写，文件的位置指针（指向当前读写字节）向后移动。而文件指针指向整个文件，如果不重新赋值，文件指针不会发生改变。 12#include &lt;stdio.h&gt;void rewind(FILE *fp); 从上面的函数原型可以看出，rewind 并没有返回值，因此也无法做安全性检查。因此，应该尽量使用 fseek 来替换 rewind 函数，从而以验证流已经成功地回绕。 fseek相对于 rewind 函数而言，fseek 函数的功能更加强大，它用来设定文件的当前读写位置，从而可以实现以任意顺序访问文件的不同位置，以实现文件的随机访问。 12#include &lt;stdio.h&gt;int fseek(FILE *fp,long offset,int from); fseek的返回值如果该函数执行成功，fp 将指向以 from 为基准，偏移 offset 个字节的位置，函数的返回值为 0；如果该函数执行失败（比如 offset 超过文件自身大小），则不改变 fp 指向的位置，函数的返回值为 -1，并设置 errno 的值，可以用 perror 函数来输出错误信息。 fseek的参数对于 fseek 函数中的参数：第一个参数 fp 为文件指针；第二个参数 offset 为偏移量，它表示要移动的字节数，整数表示正向偏移，负数表示负向偏移；第三个参数 from 表示设定从文件的哪里开始偏移，取值范围：SEEK_SET 表示从文件起始位置增加 offset 个偏移量为新的读写位置；SEEK_CUR 表示从目前的读写位置增加 offset 个偏移量为新的读写位置；SEEK_END 表示将读写位置指向文件尾后，再增加 offset 个偏移量为新的读写位置。 调用 fseek 函数的文件指针 fp 应该指向已经打开的文件，否则将会出现错误。 fseek 函数一般用于二进制文件，当然也可以用于文本文件。需要特别注意的是，当 fseek 函数用于文本文件操作时，一定要注意回车换行的情况。因为在一般浏览工具（如 UltraEdit）中，回车换行被视为两个字符 0x0D 和 0x0A，但真实的文件读写和定位却按照一个字符 0x0A 进行处理。因此，在碰到此类问题时，可以考虑将文件整个读入内存，然后在内存中手工插入 0x0D的方法，这样可以达到较好的处理效果。 fseek 函数只返回执行的结果是否成功，并不返回文件的读写位置。因此，你可以使用 ftell 函数来取得当前文件的读写位置。 ftell12# include&lt;stdio.h&gt;long ftell(FILE *fp); 该函数用于得到文件位置指针当前位置相对于文件首的偏移字节数。在随机方式存取文件时，由于文件位置频繁前后移动，程序不容易确定文件的当前位置。在使用 fseek 函数后，再调用函数 ftell 就能非常容易地确定文件的当前位置。示例代码: 123456789long getfilelength(FILE *fp)&#123; long curpos = 0L; long length = 0L; curpos = ftell(fp); fseek(fp, 0L, SEEK_END); length = ftell(fp); fseek(fp, curpos, SEEK_SET); return length;&#125; 二、Linux系统接口：open123456#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt; int open(const char *pathname, int flags);int open(const char *pathname, int flags, mode_t mode); 参数说明：pathname：要打开或者要创建的文件 flags：打开文件时，可以传入多个参数选项，用下面的一个或者多个常量进行 或 运算，这样就可以根据二进制位来判断打开文件的模式 O_EDONLY：只读打开 O_WRONLY：只写打开 O_RDWR：读写打开这三个常量，必须指定一个且只能指定一个 O_CREAT：若文件不存在，则创建它，需要使用mode选项来指定文件的访问权限 O_APPEND：追加写入 返回值：成功：新打开的文件描述符 mode参数mode参数表示设置文件访问权限的初始值，和用户掩码umask有关，比如0644表示-rw-r–r–，也可以用S_IRUSR、S_IWUSR等宏定义按位或起来表示。要注意的是，有以下几点 umaskumask与chmod是配套使用的，umask默认情况下的umask值是002，可以直接用umask命令查看，文件权限由open的mode参数和当前进程的umask掩码共同决定。 从图中可以看出，只要umask设置的为1的二进制位，在新建文件的时候就不会加上这些权限！所以第三个参数是在第二个参数中有O_CREAT时才作用，如果没有创建新文件，则第三个参数可以忽略！ 接口使用示例写入数据： 123456789101112131415161718192021222324252627#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;int main()&#123; umask(0); int fd = open(\"myfile\", O_WRONLY|O_CREAT, 0664); if(fd &lt; 0)&#123; perror(\"open\"); return 1; &#125; int count = 5; const char *msg = \"hello bit!\\n\"; int len = strlen(msg); while(count--)&#123; int ret = write(fd, msg, len); printf(\"实际写入数据%d字节\\n\", ret); &#125; close(fd); return 0;&#125; 读取数据： 12345678910111213141516171819202122232425262728293031#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;int main()&#123; int fd = open(\"myfile\", O_RDONLY); if(fd &lt; 0)&#123; perror(\"open\"); return 1; &#125; const char *msg = \"hello bit!\\n\"; char buf[1024]; while(1)&#123; ssize_t s = read(fd, buf, strlen(msg));//返回实际读到的字节数 if(s &gt; 0)&#123; printf(\"%s\", buf); &#125;else&#123; break; &#125; &#125; close(fd); return 0;&#125; 三、文件描述符与FILE结构体文件描述符就是open函数成功之后的值，本质就是一个数字！Linux进程默认有3个缺省打开的文件描述符，分别是标准输入0，标准输出1，标准错误2，对应的物理设备分别是：键盘、显示器、显示器 进程怎么知道打开了那些文件呢？下面是PCB(task_struct)的一部分查看task_struct结构体请看：《深入理解进程》中的task_struct，很显然，Linux的进程是通过files_struct 类型的一个属性来管理打开得文件列表： 而现在知道，文件描述符就是从0开始的小整数。当我们打开文件时，操作系统在内存中要创建相应的数据结构来描述目标文件。于是就有了file结构体。表示一个已经打开的文件对象。而进程执行open系统调用，所以必须让进程和文件关联起来。每个进程都有一个指针*files，指向一张表files_ struct，该表最重要的部分就是包涵一个指针数组，每个元素都是一个指向打开文件的指针!所以，本质上，文件描述符就是该数组的下标。所以，只要拿着文件描述符，就可以找到对应的文件！ 文件描述符的分配规则：从0开始分配最小的，可用的！ 看代码理解一下： 1234567891011121314151617181920212223242526272829303132333435#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;int fun()&#123; int fd = open(\"myfile\", O_RDONLY); if(fd &lt; 0)&#123; perror(\"open\"); return 1; &#125; printf(\"fd = %d\\n\", fd); close(fd); return 0;&#125;int fun2()&#123; close(0); int fd = open(\"myfile\", O_RDONLY); if(fd &lt; 0)&#123; perror(\"open\"); return 1; &#125; printf(\"fd = %d\\n\", fd); close(fd); return 0;&#125;int main()&#123; //fun(); fun2(); return 0;&#125; 调用fun()的时候打印3，调用fun2()的时候打印0，所以从0开始分配最小的，可用的！ 文件描述符与重定向1234567891011121314int main()&#123; close(1); int fd = open(\"myfile\", O_WRONLY|O_CREAT, 00644); if(fd &lt; 0)&#123; perror(\"open\"); return 1; &#125; printf(\"fd = %d\\n\", fd); fflush(stdout); close(fd); return 0;&#125; 从上面的代码不难看出：本来应该打印到屏幕的内容却输出到了文件中，这就实现了重定向！从下图中可以看出： 不难得出，要完成输出重定向需要先关闭1号文件描述符所定义的标准输出，然后用其他的文件来占用这个1号文件描述符，所以输出结果便转移到了普通文件中，键盘、屏幕也是文件，所以这也再次体现了在Linux下：一切皆文件所以很久上面的理论：要完成输入重定向，只要关闭0号文件描述符，然后用其他的文件来占用0号文件描述符，这样的话输入重定向就很容易完成了：先准备一个作为输入的文件(\\n即是Linux下的回车换行)： 123456789101112int main()&#123; close(0); int fd = open(\"myfile2\", O_RDONLY); if(fd &lt; 0)&#123; perror(\"open\"); return 1; &#125; char buf[1024] = &#123;0&#125;; scanf(\"%s\", buf); printf(\"buf = %s\\n\", buf); return 0;&#125; 运行结果 从结果不难看出，其实只要关闭了0号文件描述符，然后打开的文件就会分配到0号文件描述符，所以这样就完成了输入重定向，这样的话追加重定向输入其实也不难，只要把文件指针在写入文件的时候使用O_APPEND参数即可完成！ FILE结构体FILE结构体是C库中封装的描述文件的结构体： 12345678910111213141516171819202122232425262728293031323334353637383940414243struct _IO_FILE &#123; int _flags; /* High-order word is _IO_MAGIC; rest is flags. */#define _IO_file_flags _flags //缓冲区相关 /* The following pointers correspond to the C++ streambuf protocol. */ /* Note: Tk uses the _IO_read_ptr and _IO_read_end fields directly. */ char* _IO_read_ptr; /* Current read pointer */ char* _IO_read_end; /* End of get area. */ char* _IO_read_base; /* Start of putback+get area. */ char* _IO_write_base; /* Start of put area. */ char* _IO_write_ptr; /* Current put pointer. */ char* _IO_write_end; /* End of put area. */ char* _IO_buf_base; /* Start of reserve area. */ char* _IO_buf_end; /* End of reserve area. */ /* The following fields are used to support backing up and undo. */ char *_IO_save_base; /* Pointer to start of non-current get area. */ char *_IO_backup_base; /* Pointer to first valid character of backup area */ char *_IO_save_end; /* Pointer to end of non-current get area. */ struct _IO_marker *_markers; struct _IO_FILE *_chain; int _fileno; //文件描述符#if 0 int _blksize;#else int _flags2;#endif _IO_off_t _old_offset; /* This used to be _offset but it's too small. */#define __HAVE_COLUMN /* temporary */ /* 1+column number of pbase(); 0 is unknown. */ unsigned short _cur_column; signed char _vtable_offset; char _shortbuf[1]; /* char* _save_gptr; char* _save_egptr; */ _IO_lock_t *_lock;#ifdef _IO_USE_OLD_IO_FILE&#125;; 可见，FILE结构体中封装了文件描述符 示例代码12345678910111213141516#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;int main()&#123; const char *msg0 = \"hello printf\\n\"; const char *msg1 = \"hello fwrite\\n\"; const char *msg2 = \"hello write\\n\"; printf(\"%s\", msg0); fwrite(msg1, strlen(msg0), 1, stdout); write(1, msg2, strlen(msg2)); fork(); return 0;&#125; 如果只是打印出来，很显然只是打印了三次，但是如果是重定向到一个文件中，printf与fwrite都输出了两次，只有write始终只输出一次，这是什么原因呢？肯定和缓冲区有关系！！！先说几种缓冲的区别：行缓冲：意思就会说只是缓冲一行，当行结束的时候缓冲区就满了，就会刷新流数据，printf()就是典型的行缓冲！全缓冲：意思就是只有缓冲区写满了才会刷新流，除非我们自己主动刷新无缓冲：不存在缓冲区，也就是没事每刻都在刷新流！ 原因分析 一般写入文件的时候就是全缓冲，但是打印到显示器的时候就是行缓冲，当重定向到普通文件的时候行缓冲变成了全缓冲 放在缓冲区的数据不会立即刷新，甚至fork之后也不会刷新 进程退出后才刷新，写入文件中 fork（）之后父进程和子进程会发生数据拷贝，所以当父进程准备刷新的时候子进程也有了相同的一份数据 write函数没有变化，说明wirte函数没有缓冲区通过分析可知，系统IO接口是没有缓冲区的，而C库的IO函数却有缓冲区，这样的缓冲区是用户级缓冲区，为了提升性能，操作系统也会提供内核缓冲区！","updated":"2020-03-13T03:06:29.758Z","categories":[{"name":"操作系统实战","slug":"操作系统实战","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"}]},{"title":"函数重载实现原理","date":"2018-10-16T10:09:30.000Z","path":"2018/10/16/函数重载实现原理/","text":"函数重载方法重载 （overload）C++允许多个函数拥有相同的名字，只要它们的参数列表不同就可以，这就是函数的重载(Function Overloading)，借助重载，一个函数名可以有多种用途。 永远记住一点：函数重载只与参数列表有关，与返回值类型无关 1234567891011121314int fun(int a,int b)&#123; return 0;&#125;int fun(int a,char b)&#123; return 0;&#125;void fun()&#123; &#125;int fun(int a, int b, int c)&#123; return 0;&#125; 使用 objdump -S a.out 命令便可以通过a.out查看编译器编译出的汇编代码： 12345678910111213141516171819202122232425262728293031323334353637383940410000000000400674 &lt;_Z3funii&gt;: 400674: 55 push %rbp 400675: 48 89 e5 mov %rsp,%rbp 400678: 89 7d fc mov %edi,-0x4(%rbp) 40067b: 89 75 f8 mov %esi,-0x8(%rbp) 40067e: b8 00 00 00 00 mov $0x0,%eax 400683: c9 leaveq 400684: c3 retq 0000000000400685 &lt;_Z3funic&gt;: 400685: 55 push %rbp 400686: 48 89 e5 mov %rsp,%rbp 400689: 89 7d fc mov %edi,-0x4(%rbp) 40068c: 89 f0 mov %esi,%eax 40068e: 88 45 f8 mov %al,-0x8(%rbp) 400691: b8 00 00 00 00 mov $0x0,%eax 400696: c9 leaveq 400697: c3 retq 0000000000400698 &lt;_Z3funv&gt;: 400698: 55 push %rbp 400699: 48 89 e5 mov %rsp,%rbp 40069c: c9 leaveq 40069d: c3 retq 000000000040069e &lt;_Z3funiii&gt;: 40069e: 55 push %rbp 40069f: 48 89 e5 mov %rsp,%rbp 4006a2: 89 7d fc mov %edi,-0x4(%rbp) 4006a5: 89 75 f8 mov %esi,-0x8(%rbp) 4006a8: 89 55 f4 mov %edx,-0xc(%rbp) 4006ab: b8 00 00 00 00 mov $0x0,%eax 4006b0: c9 leaveq 4006b1: c3 retq 00000000004006b2 &lt;main&gt;: 4006b2: 55 push %rbp 4006b3: 48 89 e5 mov %rsp,%rbp 4006b6: b8 00 00 00 00 mov $0x0,%eax 4006bb: c9 leaveq 4006bc: c3 retq 由此可见g++编译器在编译cpp源文件的时候，对于同名称的函数做了区分，添加了相应的函数修饰，修饰之后的函数名称不同，所以是可以区分的！同样的道理我们可以看看C语言是否支持函数重载： 123456789#include&lt;stdio.h&gt;void fun(int a, int b)&#123;&#125;int main()&#123; return 0;&#125; 对应的汇编代码如下 123456789101112131415161718190000000000400474 &lt;fun&gt;: 400474: 55 push %rbp 400475: 48 89 e5 mov %rsp,%rbp 400478: 89 7d fc mov %edi,-0x4(%rbp) 40047b: 89 75 f8 mov %esi,-0x8(%rbp) 40047e: c9 leaveq 40047f: c3 retq 0000000000400480 &lt;main&gt;: 400480: 55 push %rbp 400481: 48 89 e5 mov %rsp,%rbp 400484: b8 00 00 00 00 mov $0x0,%eax 400489: c9 leaveq 40048a: c3 retq 40048b: 90 nop 40048c: 90 nop 40048d: 90 nop 40048e: 90 nop 40048f: 90 nop 由此可见对于C语言是没有函数修饰的，所以C是不支持函数重载的！ extern “C”有时候在C++工程中可能需要将某些函数按照C的风格来编译，在函数前加extern “C”，意思是告诉编译器，将该函数按照C语言规则来编译: 123extern \"C\" int add(int a, char b) &#123; return 0;&#125; 如果是头文件与实现分离的情况下，只要头文件写了extern &quot;C&quot;，源文件可写可不写，但是只要头文件没写extern &quot;C&quot; ,源文件中就不应该写extern &quot;C&quot;，否则会报链接错误 函数重载与函数重写(覆写)的区别方法重写（覆盖）override：发生在有继承关系的类中，子类定义了与父类完全相同的方法（方法名、返回值、参数列表全都一样），被重写的方法必须比父类中的方法权限更高！ 缺省参数缺省参数顾名思义，就是说参数列表中定义了默认的参数： 1234567891011#include&lt;iostream&gt;int fun(int a, int b = 10)&#123; //定义缺省参数 return a+b;&#125;int main()&#123; std::cout &lt;&lt; fun(20) &lt;&lt; std::endl; //30 std::cout &lt;&lt; fun(20, 20) &lt;&lt; std::endl; //40 return 0;&#125; 缺省参数的使用注意事项：1、缺省参数必须定义在参数列表的最右边，这个比较容易理解，如果你把缺省参数定义在中间的话，编译器也就不知道到底哪些参数是缺省的error: default argument missing for parameter 2 of ‘int fun(int, int)’ 2、缺省参数不能同时在函数声明和定义中出现，只能二者留其一，这个也是比较好理解的，站在编译器的角度看问题，要想让编译器明白，首先我们自己的明白到底该用哪一个值作为默认值 3、缺省值必须是常量或者全局变量 4、C语言并不支持缺省参数 指针与引用1234567891011121314151617int main()&#123; const int a = 10; const int&amp; b = a; int c = 20; const int&amp; d = c; const int x = 10; //int&amp; y = x; error 非const引用不能引用const变量 int e = 10; //double&amp; f = e; error const double&amp; f = e; //之所以可以这样定义,后面解释 return 0;&#125; 相同点 都是地址的概念 指针指向一块内存，它的内容是所指内存的地址；引用是某块内存的别名 区别 指针是一个实体，而引用仅是个别名； 引用只能在定义时被初始化一次，之后不可变；指针可变；可以理解为引用从定义那一刻就已经不能被改变 const的引用只能去引用const的变量，使用const引用非const变量是错误的，但是有一中情况除外： 12int e = 10;const double&amp; f = e; 这种情况是这样的，在计算机想把 int 转为 double 的时候会创建一个临时的double变量，而就在这个时引用便引用的是这块临时存储区域，所以没有报错，但是这样的一块临时区域本来是不应该被引用的 引用不能为空，指针可以为NULL sizeof (引用) 得到的是所指向的变量（对象）的大小，而 sizeof 指针 得到的是指针本身（所指向的变量或对象的地址）的大小 指针和引用的自增（++）运算意义不一样，指针++是加上数据类型的大小，而引用++就是原变量加1，因为引用就是给原来的变量起别名！ 使用规则 引用被创建的同时必须被初始化（指针则可以在任何时候被初始化）。 不能有NULL 引用，引用必须与合法的存储单元关联（指针则可以是NULL）。 一旦引用被初始化，就不能改变引用的关系（指针则可以随时改变所指的对象） 如果函数返回时，离开函数作用域后，其栈上空间已经还给系统，因此不能用栈上的空间作为引用类型返回。如果以引用类型返回，返回值的生命周期必须不受函数的限制(即比函数生命周期长)","updated":"2020-03-13T03:06:29.723Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://zouchanglin.cn/tags/C-C/"}]},{"title":"常见排序的总结","date":"2018-10-05T11:20:00.000Z","path":"2018/10/05/常见排序的总结/","text":"插入排序基本思想每一步将一个待排序的元素，按其排序码的大小，插入到前面已经排好序的一组元素的合适位置上去，直到元素全部插完为止 直接插入排序 代码实现123456789101112131415161718//插入排序void InsertSort(int *arr, int n)&#123; int index = 0; assert(arr); for(index = 1; index &lt; n; index++) &#123; //将当前数据往前插入 int end = index-1; int temp = arr[index]; while(end &gt;= 0 &amp;&amp; temp &lt; arr[end]) &#123; arr[end+1] = arr[end]; --end; &#125; arr[end+1] = temp; &#125;&#125; 下图只是演示到将16排序到前面的过程 直接插入排序的优化12345678910111213141516171819202122232425262728293031//插入排序void InsertSort_OP(int *arr, int n)&#123; int index = 0; for (index = 1; index &lt; n; index++) &#123; int temp = arr[index]; //通过二分查找 找出待插入元素的位置 int left = 0; int right = index - 1; while (left &lt;= right) &#123; int mid = left + ((right - left) &gt;&gt; 1); if (temp &gt;= arr[mid]) left = mid + 1; else right = mid - 1; &#125; //将当前数据往前插入 int end = index - 1; //搬移元素 while (end &gt;= left) &#123; arr[end + 1] = arr[end]; --end; &#125; arr[left] = temp; &#125;&#125; 由于之前的序列都是有序的，这样的话就不用一个一个进行比较，只要先用二分查找找到插入的位置，这样的话本来需要搬运元素只需要搬运一次就行，很多元素不需要比较一次就搬运一次，这样会提高效率！ 总结元素集合越接近有序，直接插入排序算法的时间效率越高！最优情况下：时间效率为$O(n)$最差情况下：时间复杂度为$O(n^2)$空间复杂度：$O(1)$，它是一种稳定的排序算法 希尔排序又称缩小增量排序，是对直接插入排序的优化很容易看出，经过这样的分组之后，许多比较大的数字都放在了后面，小数字都放在了前面，只要gap不断减小，分组不断变小，最后直到gap减到1的时候就和直接插入排序没什么区别了！ 代码实现123456789101112131415161718192021void ShellSort(int* arr,int len)&#123; int gap = len; assert(arr); while(gap&gt;1) &#123; gap = gap/3+1; int cur = 0; for(cur = gap;cur&lt;len;++cur) &#123; int end = cur-gap; int tmp = arr[cur]; while(end&gt;=0 &amp;&amp; tmp&lt;arr[end]) &#123; arr[end+gap] = arr[end]; end -= gap; &#125; arr[end+gap] = tmp; &#125; &#125;&#125; 总结希尔排序是一种不稳定的排序，时间复杂度：$N^{1.25} 到 1.6N^{1.25}$ 选择排序基本思想假设要排升序，每一趟选择一个最大的数字与最后的元素交换，这里所说的最后一个元素是会逐步向前调整的！ 代码实现12345678910111213141516171819//选择排序void SelectSort(int *arr, int n)&#123; int index = 0; assert(arr); for (index = 0; index &lt; n; index++) &#123; int maxPos = 0; int i = 1; //找最大元素的位置 for (i = 1; i &lt; n-index; i++) &#123; if (arr[i] &gt; arr[maxPos]) maxPos = i; &#125; if (maxPos != (n - index - 1)) Swap(&amp;arr[maxPos], &amp;arr[n-index-1]); &#125;&#125; 基本选择排序的优化优化为每趟寻找最大值的同时找到最小值 123456789101112131415161718192021222324252627282930313233//选择排序的优化void SelectSort_OP(int *arr, int n)&#123; int begin = 0; int end = n-1; assert(arr); while(begin &lt; end) &#123; int minindex = begin; int maxindex = begin; //分别找到最大和最小的下标 int i = 0; for(i = begin;i &lt;= end; ++i) &#123; if(arr[i]&gt;arr[maxindex]) &#123; maxindex = i; &#125; if(arr[i]&lt;arr[minindex]) &#123; minindex = i; &#125; &#125; //把最小的放在前面，最大的放在后面 Swap(&amp;arr[begin], &amp;arr[minindex]); if(begin == maxindex)//修正 maxindex = minindex; Swap(&amp;arr[end], &amp;arr[maxindex]); ++begin; --end; &#125;&#125; 只不过这种优化的方式需要注意一点（见下图），如果最小的元素恰好是最大的元素需要插入的位置，此时就需要将minPos设置为原来max的位置，因为最小值已经和maxPos位置的值交换了！ 总结时间复杂度：$n^2$空间复杂度：$O(1)$这是一种不稳定的排序 堆排序堆排序有两个关键点 根据数组去建堆 交换首尾元素向下调整 代码实现123456789101112131415161718192021222324252627282930313233343536373839void HeapAdjust(int *arr,int root,int len)&#123; //child是左孩子的下标 int child = root*2+1; assert(arr); while(child&lt;len) &#123; // 比较左孩子和右孩子,child指向大的孩子 if(child+1&lt;len &amp;&amp; arr[child+1]&gt;arr[child]) &#123; child++; &#125; // 1.若大的孩子节点大于根节点,则不再需要调整,跳出循环 // 2.否则,交换孩子节点和根节点,将根节点继续往下调整 if (arr[child] &gt; arr[root]) &#123; Swap(&amp;arr[child], &amp;arr[root]); root = child; child = child * 2 + 1; &#125; else return; &#125;&#125;//堆排序void HeapSort(int *arr, int len)&#123; int i = (len - 2)&gt;&gt;1; assert(arr); //建堆 for( ; i&gt;=0; i--) HeapAdjust(arr, i, len); //排序 for(i = len-1;i &gt; 0; i--) &#123; Swap(&amp;arr[i], &amp;arr[0]); HeapAdjust(arr, 0, i); &#125;&#125; 总结堆排序的时间复杂度：$N/2 *{log N}$其实就是$logN$空间复杂度：$O(1)$稳定性：不稳定 交换排序冒泡排序冒泡排序的思想非常简单，如下图，两两比较，每趟排序总可以把最大的放在最后面或者最小的值放在最后面，只需要进行（假设元素个数为n）n-1趟冒泡，便可以排序完成！ 代码实现123456789101112131415161718192021// 冒泡排序及其优化void BubbleSort(int* arr, int len)&#123; int index = 0; int end = 0; assert(arr); for(end = len-1;end&gt;0;end--) &#123; int flag = 0; for(index=0;index&lt;end;index++) &#123; if(arr[index]&gt;arr[index+1]) &#123; Swap(&amp;arr[index], &amp;arr[index+1]); flag = 1; &#125; &#125; if(flag == 0) break; &#125;&#125; 总结冒泡排序最好情况时间复杂度$O(n)$，冒泡排序最坏情况下时间复杂度 $O(1)$冒泡排序空间复杂度$O(1)$冒泡排序是一种稳定的排序算法 快速排序基本思想任取待排序元素序列中的某元素作为基准值，按照该排序码将待排序集合分割成两子序列，左子序列中所有元素均小于基准值，右子序列中所有元素均大于基准值，然后最左右子序列重复该过程，直到所有元素都排列在相应位置上为止。这样的话左边都是比基准值小的，在右边的都是比基准值大的，根据分治的思想，可以再把左边的序列选出一个基准值，在右边的序列也选择一个基准值，左边的排好了，右边的排好了，整个序列也就排好了！ 代码实现_hoare版本12345678910111213141516171819202122232425262728293031//快速排序int PartSort(int *a, int left, int right)&#123; int key = a[right]; int begin = left; int end = right - 1; while (begin &lt; end) &#123; //找比基准值大的 while ((begin &lt; end) &amp;&amp; (a[begin] &lt;= key)) begin++; //找比基准值小的 while ((begin &lt; end) &amp;&amp; (a[end] &gt;= key)) end--; if (begin&lt;end) Swap(&amp;a[begin], &amp;a[end]); &#125; //最后别忘记把基准值和相遇点交换 if (begin != right - 1) Swap(&amp;a[begin], &amp;a[right-1]); return begin;&#125;void QuickSort(int *a, int left, int right)&#123; assert(a); if (left &gt;= right) return; int div = PartSort(a, left, right); QuickSort(a, left, div); QuickSort(a, div + 1, right);&#125; 前后指针法这个方式其实不难理解，就是每次将key值移动到中间，在key左边的值都是比key小的值，key右边的值都是比key大的值，这样做分组的的话通过递归就可以完成排序！ 12345678910111213141516171819202122232425int PartSort3(int *array, int left, int right)&#123; int key = array[right - 1]; int cur = left; int pre = cur - 1; while (cur &lt; right)//说明区间的元素还没有遍历完成 &#123; if ((array[cur] &lt; key) &amp;&amp; (++pre != cur)) Swap(&amp;array[cur], &amp;array[pre]); ++cur; &#125; if (++pre != right) Swap(&amp;array[pre], &amp;array[right-1]); return pre;&#125;void QuickSort3(int *a, int left, int right)&#123; assert(a); if (left &gt;= right) return; int div = PartSort3(a, left, right); QuickSort(a, left, div - 1); QuickSort(a, div + 1, right);&#125;","updated":"2020-04-05T02:57:12.632Z","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序算法","slug":"排序算法","permalink":"https://zouchanglin.cn/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"}]},{"title":"《沉睡中的大学生》人民日报","date":"2018-10-04T10:09:30.000Z","path":"2018/10/04/《沉睡中的大学生》人民日报/","text":"职场上，这样的人也同样不在少数。上班时踩着点走进公司大门，下班前一小时就心不在焉。玩玩手机，刷刷微博，手里的工作能拖则拖。实在拖不了就勉强自己加个班，但一定要发条朋友圈，问：“你见过深夜十二点的写字楼吗？”自我感动到不行，却忽略了这本是白天早就应该完成的工作。 工作，不必认真，能应付上司即可； 能力，不思进取，不被炒鱿鱼就行； 至于工资，只要心怀梦想，总有一天会涨的。 这样的人，不是真“佛系”，而是依然会羡慕别人升职加薪，却拿着3千的工资，做着月薪5万的梦。 拿着父母血汗钱养老的年轻人有些人，二十多岁就开始养老了。一杯咖啡慢悠悠度过一整天，不再学习知识，看着别人加班工作，还要嘲笑一声“傻子”，将安于现状视为“知足常乐”。而可悲的是，有多少月薪三千，就心满意足的人，过着“岁月静好”的生活，却是依靠父母提供经济支持。 根据中国老龄科研中心的统计，中国目前有30%的年轻人依靠“啃老”生活，65%以上的家庭存在着“啃老”的现象。一些成年子女带给父母的经济压力，甚至比他们未成年时更大。 今年7月，武汉一位81岁的老人向社会求助，称自己的儿子已经在家宅了很多年了，不肯工作。他年纪大了，身体也不好，怕哪天离开了，儿子的生活就没着落了。老人说，他每个月有五千多的退休金，自己平日买药和日常开支，也花不了多少，只是因为小儿子的状况，家里日子过得非常紧张。而面对老人的担心，儿子却表现得很反感，甚至指责老人“多管闲事”。 本该自立的年纪，不仅没有能力给家人幸福，还要依赖父母才能生活。这些患了“精神癌症的年轻人”，心安理得地“啃老”，却不肯为自己的未来奋斗。 不久前，一位已是两个孩子母亲的三十七岁女硕士在论坛发帖求职。她毕业于国内顶尖的大学，在外企工作近十年，因为部门关闭被裁员，只能重新找工作。她的要求并不高，短期内月薪三千就可以。她在帖子中这样描述自己的工作经历：做过科研合作管理，但只是“打杂”；本专业的注册证书没考下来；考了个日语1级，却不能口头交流；英语还行，但也只是考研时英语成绩过得去。很长时间过去了，还是没结果。她说：“我承认我很失败，没有在这么长的职业生涯里磨砺好我的翅膀，所以现在才这么凄惨。” 有人评论：“在职场上混日子，迟早让职场把你混了。” 不进则退的道理，放之四海而皆准。所有成功的事业都是时间和汗水熬出来的，你偷过的懒，迟早会变成打脸的巴掌。最近，中年危机成了一个热门话题。 中国劳动关系学院行政管理教研室主任刘文军认为，中年人容易被淘汰，主要是因为他们知识结构已经基本固定，学习新知识新技能的效率较低，转型太慢。但也不是所有人都在中年危机到来时一筹莫展，那些有一技之长的人，即使遭遇职场危机，也能轻松化解。 蔡玉洁是一名一线生产车间的技术员，在不到40岁的时候丢了饭碗。但她下决心重新出发，通过自学和参加培训班，考取了会计和计算机相关资格证书。凭借这些新本领，她找到了一份会计工作。待遇甚至比原来还好。在企业从事员工选拔聘用工作的肖成萍说：“社会发展这么快，企业也要不断求新求变。如果平时没有忧患意识，放松学习，不管年龄是大还是小，被淘汰都是必然的。” 英国哲学家怀特海曾说：缺乏进取的精神，就意味着堕落。 BBC根据剑桥大学研究者的数据分析了 365 种职业在未来被淘汰的概率，AI技术越来越发达，“电话推销员”被机器人取代的几率接近100%。其次是打字员、会计、保险业务员、银行职员…… 淘汰率较高的工作都无需天赋，经过大量重复性、机械性训练就可以轻松掌握。而在“如何避免被机器人取代”的建议里，专家说，必须培养自己的终身学习计划，提升职业技能。在未来，有创新能力、更侧重于人类本性的工作者才不容易被替代。 人比人能气死人，可若是连和人比的资本和机会都没了，岂不更惨。 走出懒丧穷的死循环微博曾上有一个反鸡汤语录，其中点赞很高的一句是：“努力不一定成功，但不努力一定很舒服。”还在懒着的年轻人，从葛优瘫到悲伤蛙、从长腿的咸鱼到“佛系”青年，每天的生活都充斥着负能量。 “反正努力工作也未必能升职加薪，不如随便应付应付，空出时间来看看电影逛逛街多舒服。” 倪杰是同济大学的学霸，他学的汽车系在当时是号称“铁饭碗”的专业，倪杰毕业后不久，却选择了转行，用了一年多，通过了国家司法考试，成为了一名律师。2013年，他再次转行创业，做起了手游。许多人不理解，觉得他“善变”，工程师也好、律师也好，都是很令人羡慕的职业，他却一次又一次选择转行，甚至去自己创业。 而在倪杰自己看来，能看清上升空间的职业不是他想要的，他想要做看不到上升空间的职业。“善变”的他其实从来没有变过。他一直坚持着深入骨髓的“不安于现状”。虽然每一次转行，都要付出更多的努力，但正是这种勇往直前的心态，让倪杰的路，越走越宽。倪杰有很明确的目标，他知道自己想要什么，并且有毅力去践行决心，永远不满足现状，永远在冲破舒适区。 《奇葩说》有一期的辩题是“我没有上进心，有错吗？”当然，有人说，我就是有一颗平常心。生活嘛，过得去就好。工作不肯较真，一时兴起买来的技能书早就落了灰，列了无数个新年计划，却没有一个执行超过三天。体力和精力都足够，却在别人加班熬夜抠细节的时候，选择睡觉玩游戏。只有领工资的时候会满腹牢骚，感叹自己“怀才不遇”，却在应该努力的时候告诉自己：“顺其自然吧，安逸最重要。”一边喝着“毒”鸡汤，一边喊着“咸鱼翻身之后还是咸鱼”。 想努力却害怕拼尽全力也不如别人，于是选择放弃。意志慢慢消沉，颓废成了生活常态，越懒越丧，越丧越穷。 年轻就是拼命学习的资本网上曾有人做过一份针对60岁以上老人的调查：你这一生最后悔的一件事是什么？有75%的人为自己年轻时不够努力，一事无成，所以感到后悔。 年轻的时候，人的身体、精神都处于巅峰状态，如果不在这个时候拼命工作，等到体力、精力都不足以支撑你去拼搏的时候，再后悔，就迟了。我们常常听到有人说：“读这么多书有什么用？能派上用场吗？”、“拼命工作有什么用？你能比那些富二代更有钱吗？”也许都不能，但不可否认的是，越是月薪高的人，越不会容忍自己懒惰，因为他们知道，只有拼命工作，不断学习，才能永不落伍。 雅虎曾调查过一些全球顶尖科技公司CEO的作息时间，结果显示，他们中大多数人都有一个共同特点：早起。苹果现任CEO蒂姆·库克每天4:30就会起床处理工作邮件，之后是健身。在公司，他往往是第一个到的人。雅虎前任CEO梅丽莎·梅耶尔在接受采访时表示：她每天的睡眠时间只有4到6小时。有人问，已经成功了，干嘛还那么拼？对于这些成功人士而言，拼命早已成了一种习惯，这是提升自己的最好方式，也是让他们成为佼佼者的资本。奥斯卡最佳男配角摩根·弗里曼被人称为“美国最优秀的演员”之一，然而他是到了30岁才第一次出演歌舞剧。虽然大器晚成，但他并没有因为错过了“青春饭”而懊恼。出道以后，他尝试各种不同的角色，从仆人到总统，从囚犯到特工。57岁时，他因出演《肖申克的救赎》而获得第三次奥斯卡提名；68岁，凭借《百万美元宝贝》获得了奥斯卡最佳男配角奖。如今已经80岁的摩根·弗里曼，依然活跃在荧幕上。 从什么时候开始都不晚。作家杨昌溢曾说：“后来我才明白，要过上简单，安逸，自由的生活，前提依然是得赚到足够令你安心的钱。这个世界是现实的，只有你用努力和自我，令它柔软。” 你可以说服自己满足于月薪3千，但你的身后还有那么多人。安于现状的确很舒服，但想要给父母更好的生活、让爱人不会为一件衣服犹豫不决，你必须拼命工作。没什么需要怕的，因为年轻，就是允许失败的资本。 你加过的班、读过的书、学过的技能都不会白费，所以奋力拼搏吧，它们默默储存下的力量，会在未来某一个时间出现，帮你度过难关。不知道从什么时候开始，“学霸”、“学渣”、“学弱”（有学霸的心，但总取得学渣般的成绩）等一些名词开始流行。那些年，你认为高考分数将决定你一生的命运，于是你拿着自己的高考成绩寻找自己梦想的大学，把分数当做赌注，与梦想一决高下。可是后来，怎样了呢？ 这些场景你是否觉得熟悉上课时：清醒没有发呆的多，发呆没有睡觉的多，睡觉没有玩手机的多；下课时，自修没有吃零食多，吃零食没有看连续剧多，看连续剧没有游戏多。如此这般，就业时的失败怎能不比成功多？ 考试时：不给范围就不会考试，给了范围也只是复印同学准备的答案。你如果是老板，会雇用你自己这样的员工吗？ 毕业前:上大学前填报志愿，你说不知道自己的兴趣特长，好吧，大学毕业找工作了，同样不知道自己的兴趣特长。自己都不认识自己，还有谁能认识你？ 专业课:学技术不肯动手，学理论不肯动脑。等待你的除了失业还能是什么？你说，你修完了《计算机基础》，但真实水平却连个PPT都做不好。你的竞争力在哪里？ 你说，你修了两年英语，然而，你的水平却连与外国人日常对话都打怵。有哪家用人单位需要你？你说，你修了《思想修养》，但你根本就没听。你敢说，除了课堂上睡眠的抗干扰能力得到提升外，在思想修养和道德品德方面，得到了应有的提升吗？你说，你修过《阅读与写作》，但你读的是手机，你写的是微信。对语文，自己都没信心，你还想指望别人对你有信心？ 实习要让父母开假证明，评先进要让父母找关系，补办证书要父母跑学校。找工作的时候，你能有一分坦然和自信吗？双休日你起来吃早饭吗？连吃饭都不会，还有谁会相信你会干活？军训的时候叠过被子，军训过后还叠过几次？ 唯一投入的是游戏，耗时最多的是游戏，而你的自荐信对此又只字不提。自己做的连自己都不认可，世界上，还有谁会认可你？讲大道理的时候你口若悬河，伸手要钱的时候你撒娇耍赖。你可以欺负你的父母亲，世界也能任由你欺负吗？离开了电脑你还能做什么？离开了游戏你还喜欢什么？离开了家你还能到哪里去？离开了父母还有谁会给你送水端饭？对于这些问题，你都找不到答案，你还想找到前程吗？图书馆里没有你的人影，运动场上没有你的人影，公益场上更没有你的人影。你退化的不是肌肉，你退化的不是责任感，你退化的是最基本的生存能力。找工作时，在工作岗位上的最大价值在于不可替代。责任心、吃苦精神、写作水平、做事能力、专业修养、操作技术、学问素养、人际处理，有哪一方面是你的看家本领？有哪一点是他人不可替代的？你不失业谁失业？有没有想过当年那些不如你的人也许有一天会超过你，L同学本科就读的大学是一所普通得不能再普通的高校。上大一的时候，身边大部分的同学都是挣脱出高考枷锁的飞鸟，渴望自由。大学对于他们来说，就好像是进了一个没有人管的”游乐场“，一进了”游乐场“的大门，他们便飞奔进去，奔向各自想要玩的游乐项目。他们参加各种各样的社团，又或者去光明正大地恋爱。只是，课本知识对于他们来说，只要不挂科，大学便是圆满结束了。你们是不是也有过这样的经历，也有过这样的同窗。谁会偶然给你高薪，又偶然让你过上想要的生活，但总有一些人进了”游乐场“，不是选择先玩耍，而是选择先苦练技能。而我们常常嘲笑这样的人。L同学就是这样的姑娘。从刚踏入校园的那一刻开始，她就和图书馆紧密相连。总之，在学校里见到她的时候，不是在教室的前排，就是在图书馆。虽然她每一次都坐在第一排，但是很多科目，她常常没有那些进了游乐场就疯玩的人强。同学们甚至会偷偷嘲笑她，花了那么多的时间，不过是和大家差不多的分数，甚至还不如耍小聪明的同学。“也许，看上去花了这么多的时间，最后出来的结果相差并不太大，甚至看不出什么差别，因为考试的偶然性很大。但是，我们不仅仅这一次考试，如果我们考一次试追求小聪明、考研也追求偶然性、找工作要偶然通过，那么谁又会偶然给你高薪，偶然让你过上你想要的生活呢？” 明明一同起跑却走向了不同的人生旅途有时你会突然发现，当时看上去和你差不多的人，甚至是不如你的人，原来蕴藏着如此巨大的能量。而这样的能量，是无法用一次的考试成绩，用她所在的大学，甚至用她的智商来衡量的。而其他人，那些还在“游乐场”里疯玩的孩子，等到游乐场打烊，等到灯光都熄灭的时候，甚至还没回过神来，究竟发生了什么。L同学如愿考上了名牌大学的研究生，终于去了她想要的学校，过她想要的生活。再后来，硕士毕业，她找到了一份给应届生的special offer，刚入职年收入就有30万。很多本科的同学都惊呆了，看着普普通通只知道死读书的她，究竟是怎么有了这么大的能量，在四年后、六年后与其他人走向了不同的人生旅途。 熬过的辛苦最后都换成了幸福。有人说，大学是一场长跑，这四年时光，有的人从一开始就踏上了其他跑道，也有人挤到了前面的队伍，是时间让大家变得不一样。 自我感言虽然我不符合上面说的那种生活状态，但是看着身边的同学的确是那种状态，有感而发故转载此文，别到了未来想起来当年碌碌无为的大学而后悔！！","updated":"2020-03-13T03:06:29.714Z","categories":[{"name":"生活","slug":"生活","permalink":"https://zouchanglin.cn/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"生活","slug":"生活","permalink":"https://zouchanglin.cn/tags/%E7%94%9F%E6%B4%BB/"}]},{"title":"进程的系统编程接口","date":"2018-10-02T10:09:30.000Z","path":"2018/10/02/进程的系统编程接口/","text":"进程创建fork&amp;vforkfork与vfork同样都是创建子进程，但是注意两者的区别： fork()：子进程拷贝父进程的数据段，代码段， vfork ()：子进程与父进程共享数据段 fork()：父子进程的执行次序不确定 vfork()： 保证子进程先运行，在调用exec 或exit 之前与父进程数据是共享的,在它调用exec或exit 之后父进程才可能被调度运行 vfork()：保证子进程先运行，在她调用exec 或exit 之后父进程才可能被调度运行。如果在调用这两个函数之前子进程依赖于父进程的进一步动作，则会导致死锁 1234567891011121314151617181920#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;int g_val = 100;int main(int argc, char *argv[])&#123; pid_t pid = vfork(); if(pid == 0)&#123; sleep(5); g_val = 200; printf(\"%d\\n\",g_val); exit(0); &#125;else&#123; printf(\"%d\\n\",g_val); &#125; return 0;&#125; 可以看出，vfork出来的子进程实际上和父进程共享数据段！ 进程终止进程退出的三种状况 代码运行完毕、结果正确 代码运行完毕、结果不正确 从main函数返回 调用exit() 调用_exit() 代码异常终止 信号终止： 1kill -9 _exit()123#include &lt;unistd.h&gt;void _exit(int status);参数：status定义了进程的终止状态，父进程通过wait来获取该值 exit()12#include &lt;stdlib.h&gt;void exit(int status); exit()最后也会调用_exit()，只不过在调用_exit()之前做了些其他的事情： 执行用户通过atexit()或者on_exit()定义的清理函数 关闭所有打开的流，所有的缓存数据均被写入 调用_exit() return 退出return退出是一种更常见的退出，执行 等同于，因为调用main函数的函数会把main函数的返回值当做的参数 进程等待 子进程退出，父进程如果不管不顾，就可能造成僵尸进程的问题，进而造成内存泄漏。 另外，进程一旦变成僵尸状态，那就刀枪不入，kill -9 也无能为力，因为谁也没有办法杀死一个已经死去的进程。 最后，父进程派给子进程的任务完成的如何，我们需要知道。子进程运行完成，结果对还是不对，或者是否正常退出。 父进程通过进程等待的方式，回收子进程资源，获取子进程退出信息 wait方法1234567#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;pid_t wait(int *status);返回值： 成功返回被等待进程的pid，失败返回-1参数： 输出型参数，获取子进程退出状态，不用关心此状态则设置为NULL waitpid方法12345678910111213#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;pid _ t waitpid ( pid _ t pid , int * status , int options ) ;返回值： 当正常返回的时候 waitpid 返回收集到的子进程的进程ID 如果设置了选项 wNOHANG ，而调用中 waitpid 发现没有已退出的子进程可收集，则返回0 如果调用中出错，则返回一 1 ，这时 errno 会被设置成相应的值以指示错误所在；参数： pid : pid ＝ -1 ，等待任一个子进程，与 wait 等效 pid ＞0 等待其进程ID与 pid 相等的子进程。 status: WIFEXITED(status) ：若为正常终止子进程返回的状态，则为真。（查看进程是否是正常退出) status: WEXITSTATUS(status) ：若 WIFEXITED非零，提取子进程退出码。（查看进程的退出码） options :WNOHANG：若 pid 指定的子进程没有结束，则 waitpid()函数返回0，不予以等待。若正常结束，则返回该子进程的ID 如果子进程已经退出，调用 wait和waitpid 时， wait/waitpid 会立即返回，并且释放资源，获得子进程退出信息。如果在任意时刻调用 wait和waitpid ，子进程存在且正常运行，则进程可能阻塞。如果不存在该子进程，则立即出错返回。 获取子进程statuswait 和 waitpid ，都一个 status 参数，该参数是一个输出型参数，由操作系统填充。如果传递 NULL ，表示不关心子进程的退出状态信息。否则，操作系统会根据该参数，将子进程的退出信息反馈给父进程。status 不能简单的当作整形来看待，可以当作位图来看待，具体细节如下图（只研究 status 低 16 比特位） : 先看这样一段代码： 12345678910111213141516171819202122232425#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;int main(int argc, char *argv[])&#123; pid_t pid = fork(); if(pid &lt; 0)&#123; printf(\"fork error\"); exit(1); &#125;else if(pid == 0)&#123; sleep(20); exit(10); &#125;else&#123; int st = 0; int ret = wait(&amp;st); if(ret&gt;0 &amp;&amp; (st &amp; 0X7F)==0)&#123;//正常退出 printf(\"child exit code: %d\\n\",(st&gt;&gt;8&amp;0XFF)); &#125;else if(ret &gt; 0)&#123;//异常退出 printf(\"sig code: %d\\n\",st&amp;0X7F); &#125; &#125; return 0;&#125; 进程阻塞式等待方式123456789101112131415161718192021222324#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;int main(int argc, char *argv[])&#123; pid_t pid = fork(); if(pid == 0)&#123; printf(\"child is run, pid is %d\\n\", getpid()); sleep(5); exit(27); &#125;else&#123; int status = 0; pid_t ret = waitpid(-1,&amp;status,0);//阻塞式等待5s printf(\"this is test forwait\\n\"); if(WIFEXITED(status) &amp;&amp; ret == pid)&#123; printf(\"wait child 5s success,child return code is %d\\n\",WEXITSTATUS(status)); &#125;else&#123; printf(\"wait child failed,return .\\n\"); return 1; &#125; &#125; return 0;&#125; 进程的非阻塞等待1234567891011121314151617181920212223242526272829303132333435#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/wait.h&gt;int main()&#123; pid_t pid; pid = fork(); if(pid &lt; 0)&#123; printf(\"%s fork error\\n\", __FUNCTION__); return 1; &#125;else if(pid == 0)&#123; printf(\"child is run, pid is :%d\\n\", getpid()); sleep(5); exit(1); &#125;else&#123; int status = 0; pid_t ret = 0; do&#123; ret = waitpid(-1, &amp;status, WNOHANG);//非阻塞式等待 if(ret == 0)&#123; printf(\"child is running\\n\"); &#125; sleep(1); &#125;while(ret == 0); if(WIFEXITED(status) &amp;&amp; ret == pid)&#123; printf(\"wait child 5s success, child return code is :%d\\n\", WEXITSTATUS(status)); &#125;else&#123; printf(\"wait child failed, return \\n\"); return 1; &#125; &#125; return 0;&#125; 进程程序替换用fork创建子进程后执行的是和父进程相同的程序(但有可能执行不同的代码分支)，子进程往往要调用一种exec函数以执行另一个程序。当进程调用一种exec函数时，该进程的用户空间代码和数据完全被新程序替换，从新程序的启动例程开始执行。调用exec并不创建新进程，所以调用exec前后该进程的id并未改变。 替换函数1234567#include &lt;unistd.h&gt;int execl(const char *path, const char *arg, ...);int execlp(const char *file, const char *arg, ...);int execle(const char *path, const char *arg, ..., char * const envp[]);int execv(const char *path, char *const argv[]);int execvp(const char *file, char *const argv[]);int execvpe(const char *file, char *const argv[], char *const envp[]); 这些函数如果调用成功则加载新的程序从启动代码开始执行，不再返回。如果调用出错则返回一1所以exec函数只有出错的返回值而没有成功的返回值。这么多函数如何区分？ 函数名 参数格式 是否带路径 是否使用当前环境变量 execl 列表 不是 是 execlp 列表 是 是 execle 列表 不是 不是，需自己组装环境变量 execv 数组 不是 是 execvp 数组 是 是 execve 数组 不是 不是，需自己组装环境变量 只有execve才是真正的系统调用，其它五个函数最终都是调用execve 利用前面的知识写一个shellshell的运行原理其实不难，就是从标准输入读入命令和命令参数然后开启一个子进程去执行这个程序，根据程序替换的原理，我们只要使用execve这个系统接口去把要执行的程序的代码段和数据段进行替换，便把一个磁盘上的程序加载到了内存中，变成了进程！此时，我们自己编写的shell变成了该进程父进程！ 主要分为以下几个步骤：1、获取命令行2、解析命令行3、建立一个子进程（fork）4、子进程程序替换（execvp）5、父进程等待子进程退出（wait） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;unistd.h&gt;#include &lt;sys/wait.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;errno.h&gt;char* argv[8];int argc = 0;void do_parse(char *buf)&#123; int i; int status = 0; for(argc=i=0;buf[i];i++)&#123; if(!isspace(buf[i]) &amp;&amp; status == 0)&#123; argv[argc++] = buf+i; status = 1; &#125;else if(isspace(buf[i]))&#123; status = 0; buf[i] = 0; &#125; &#125; argv[argc] = NULL;&#125;void do_execute(void)&#123; pid_t pid = fork(); switch(pid)&#123; case -1: perror(\"fork\"); exit(EXIT_FAILURE); break; case 0: execvp(argv[0], argv); perror(\"execvp\"); exit(EXIT_FAILURE); default: &#123; int st; while(wait(&amp;st) != pid); &#125; &#125;&#125;int main(void)&#123; char buf[1024] = &#123;&#125;; while(1)&#123; scanf(\"%[^\\n]%*c\", buf); do_parse(buf); do_execute(); &#125; return 0;&#125; 函数与进程之间的相似性能exec/exit就像call/return一个C程序有很多函数组成。一个函数可以调用另外一个函数，同时传递给它一些参数。被调用的函数执行一定的操作，然后返回一个值。每个函数都有他的局部变量，不同的函数通过call/return系统进行通信。这种通过参数和返回值在拥有私有数据的函数间通信的模式是结构化程序设计的基础。Linux鼓励将这种应用于程序之内的模式扩展到程序之间。如下图: 一个C程序可以fork/exec另一个程序，并传给它一些参数。这个被调用的程序执行一定的操作，然后通过exit(n)来返回值。调用它的进程可以通过wait (&amp;ret)来获取exit的返回值。 popen/system和fork的区别system函数原型12#include &lt;stdlib.h&gt;int system(const char *command); system()会调用fork()产生子进程，由子进程来调用/bin/sh-c string来执行参数string字符串所代表的命令，此命令执行完后随即返回原调用的进程。在调用system()期间SIGCHLD信号会被暂时搁置，SIGINT和SIGQUIT信号则会被忽略。调用/bin/sh来执行参数指定的命令，/bin/sh 一般是一个软连接，指向某个具体的shell。 实际上system()函数执行了三步操作： fork一个子进程； 在子进程中调用exec函数去执行command； 在父进程中调用wait去等待子进程结束。 返回值： 如果exec执行成功，即command顺利执行，则返回command 通过 exit或 return的返回值。(注意 ：command 顺利执行不代表执行成功，当参数中存在文件时，不论这个文件存不存在，command 都顺利执行) 如果exec执行失败，也即command没有顺利执行，比如被信号中断，或者command命令根本不存在， 返回 127 如果 command为 NULL， 则 system 返回非 0 值. 对于fork失败，system()函数返回-1。 popen()函数 创建一个管道用于进程间通信，并调用shell，因为管道被定义为单向的。所以 type 参数只能定义成只读或者只写, 不能是两者同时, 结果流也相应的是只读或者只写.函数原型： 123#include &lt;stdio.h&gt;FILE *popen(const char *command, const char *type);int pclose(FILE *stream); 函数功能：popen()会调用fork()产生子进程，然后从子进程中调用/bin/sh-c来执行参数command的指令。这个进程必须由 pclose 关闭。 command参数：command 参数是一个字符串指针, 指向的是一个以null结束符结尾的字符串, 这个字符串包含一个shell命令. 这个命令被送到 /bin/sh 以 -c 参数 执行, 即由 shell 来执行 type 参数 也是一个指向 以 null 结束符结尾的 字符串的指针参数type可使用“r”代表读取，“w”代表写入。依照此type值，popen()会建立管道连到子进程的标准输出设备或标准输入设备，然后返回一个文件指针。 随后进程便可利用此文件指针来读取子进程的输出设备或是写入到子进程的标准输入设备中。 返回值：若成功则返回文件指针，否则返回NULL，错误原因存于errno中 区别1.system 在执行期间，调用进程会一直等待 shell 命令执行完成(waitpid)，但是 popen 无需等待 shell 命令执行完成就返回了。可以理解为，system为串行执行，popen 为并行执行。2.popen 函数执行完毕后必须调用 pclose 来对所创建的子进程进行回收，否则会造成僵尸进程的情况。3.popen 没有屏蔽 SIGCHLD ，如果我们在调用时屏蔽了 SIGCHLD ，如果在 popen 和 pclose 之间调用进程又创建了其他子进程并调用进程注册了 SIGCHLD 来处理子进程的回收工作，那么这个回收工作会一直阻塞到 pclose 调用。 参考：《调研popen/system, 理解这两个函数和fork的区别》《popen/system, 理解这两个函数和fork的区别》","updated":"2020-03-13T03:06:29.773Z","categories":[{"name":"操作系统实战","slug":"操作系统实战","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"}]},{"title":"必须避免的C语言大坑","date":"2018-09-28T10:09:30.000Z","path":"2018/09/28/必须避免的C语言大坑/","text":"变量的定义位置先上一段看来比较正常的代码： 12345678910#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int main(void)&#123; printf(\"Tim\\n\"); int a = 10; printf(\"a = %d\\n\",a); system(\"pause\"); return 0;&#125; 由于我的环境是VisualStudio2013，所以上述程序可以完美执行，并且是编译0警告通过，但是在VisualStudio2008的环境下这样写是完全错误的，C89规定，在任何执行语句之前，在块的开头声明所有局部变量。但是在C99以及C++中则没有这个限制，即在首次使用之前，可在块的任何位置都可以声明变量。例如下面的写法对于C89标准才是正确的： 12345678910111213#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;int main(void)&#123; printf(\"sssssssss\\n\"); &#123; int a = 10; printf(\"a = %d\\n\", a); &#125; system(\"pause\"); return 0;&#125; 当然为了达到更好兼容性，建议都把局部变量写在代码块的开始位置！！ define与typedef1、函数式宏定义的参数没有类型，预处理器只负责做形式上的替换，而不做参数类型检查，所以危险性高；但因为省去了函数的调用，返回，释放，所以效率比自定义函数高；2、调用真正函数的代码和调用函数式宏定义的代码编译生成的指令不同。如果MAX是个普通函数，那么它的函数体return a &gt; b ? a : b; 要编译生成指令，代码中出现的每次调用也要编译生成传参指令和call指令。而如果MAX是个函数式宏定义，这个宏定义本身倒不必编译生成指令，但是代码中出现的每次调用编译生成的指令都相当于一个函数体，而不是简单的几条传参指令和call指令。所以，使用函数式宏定义编译生成的目标文件会比较大。3、在执行复杂功能时，如递归，函数式宏定义往往会导致较低的代码执行效率。尽管函数式宏定义和普通函数相比有很多缺点，但只要小心使用还是会显著提高代码的执行效率，毕竟省去了分配和释放栈帧、传参、传返回值等一系列工作，因此那些简短并且被频繁调用的函数经常用函数式宏定义来代替实现。 ①define定义符号 1#define MAX 100 ②定义比较长的关键字 1#define reg register ③用更加形象的的符号替代另一种实现 1#define do_forever for( ; ; ) ④在写switch语句的时候自动把break加上 1#define CASE break;case ⑤打印日志 1#define DEBUG_PRINT &gt;printf(“file:%s\\tline:%d\\tdate:%s\\ttime:%s\\n”,FILE,_LINE__,DATE,TIME) 使用宏时候的提示：所有对于数值表达式求值得宏定义都应该用这种方式加上括号，避免在使用宏的时候由于参数中的操作符之间不可预料的相互作用。 define 替换在程序中扩展#define定义符号和宏时，需要涉及几个步骤。在调用宏时，首先对参数进行检查，看看是否包含任何由#define定义的符号。如果是，它们首先被替换。替换文本随后被插入到程序中原来文本的位置。对于宏，参数名被他们的值替换。最后，再次对结果文件进行扫描，看看它是否包含任何由#define定义的符号。如果是，就重复上述处理过程。注意：宏参数和#define 定义中可以出现其他#define定义的变量。但是对于宏，不能出现递归。当预处理器搜索#define定义的符号的时候，字符串常量的内容并不被搜索。 #与##使用”#”把宏参数变成对应的字符串 1234567#define PRINT(FORMAT, VALUE)\\ printf(\"the value of \"#VALUE\" is \"FORMAT\"\\n\", VALUE) int main(void)&#123; //这里只有当字符串作为宏参数的时候才可以把字符串放在字符串中 PRINT(\"%d\", 5 + 10); system(\"pause\"); return 0; &#125; “##”可以把位于它两边的符号合成一个符号。它允许宏定义从分离的文本片段创建标识符。注意：这样的连接必须产生一个合法的标识符。否则其结果就是未定义的。接下来说说宏与函数宏通常被应用于执行简单的运算。比如在两个数中找出较大的一个： 1#define GET_MAX(a,b) a&gt;b?a:b 用于调用函数和从函数返回的代码可能比实际执行这个小型计算工作所需要的时间更多。所以宏比函数在程序的规模和速度方面更胜一筹。更为重要的是函数的参数必须声明为特定的类型。所以函数只能在类型合适的表达式上使用。反之这个宏怎可以适用于整形、长整型、浮点型等可以用于来比较的类型。宏是类型无关的 和函数相比宏的缺点每次使用宏的时候，一份宏定义的代码将插入到程序中。除非宏比较短，否则可能大幅度增加程序的长度。宏是没法调试的。宏由于类型无关，也就不够严谨。宏可能会带来运算符优先级的问题，导致程容易出现错宏的其他注意事项:宏可以传类型，但是函数不行 12345678910#define _CRT_SECURE_NO_WARNINGS #include&lt;stdio.h&gt; #include&lt;stdlib.h&gt; #define MALLOC(num, type)\\ (type *)malloc(num * sizeof(type))int main(void)&#123; int* p = MALLOC(3, int); system(\"pause\"); return 0; &#125; 带副作用的宏参数 12345678#define MAX(a,b) ((a)&gt;(b)?(a):(b))int main(int argc,char** argv)&#123; int a = 5; int b = 8; int z = MAX(a++,b++); printf(\"a=%d b=%d z=%d\\n\",a,b,z);&#125; 上例就说明了带副作用的宏参数的危害有时不把宏写成全部大写，是为了让别人把它当做函数理解 函数和宏的对比 属性 #defien宏 函数 代码长度 每次使用时，宏代码都被插入到程序中，除了非常短的宏之外，程序的长度将大幅度增长 函数代码只会出现在同一个地方，每次使用这个函数时都会调用这段代码 执行速度 更快 存在函数弹栈、压栈的开销 操作符优先级 宏参数的求值实在所有周围表达式的上下文环境里，除非它们加上括号，否侧邻近操作符的优先级可能会产生不可预料的后果 函数参数只在函数调用时求值一次，它的结果值将传递给函数，表达式的求值很容易预测 参数求值 参数每次用于宏定义时， 它们都将重新求值，具有副作用的参数可能会产生不可预料的结果 参数在函数调用前只求值一次，在函数中多次使用参数并不会带至多种求值过程，参数的副作用并不会造成任何的问题 参数类型 宏与类型无关，只要对参数的操作是合法的，它可以适用于任何参数类型 函数参数与类型有关，如果参数的类型不同，就需要使用不同的函数，即使它们的逻辑任务是相同的 调试 宏在预处理阶段就已经替换，所以无法执行调试 函数是可以调试的 其他 宏不具备函数的性质，不能递归 函数可以递归 浮点数与其他数字的比较C语言中浮点数同0不能直接用==比较，只能看浮点数与0值的误差，原因如下：float是浮点数，存的是近似值，当用来表示0的时候，有可能计算结果是0，但是由于精度问题，实际上存储的是一个和0很接近的值，而==做判断的话只要不是完全相等就返回假，所以用==判断float有可能出错。 数组越界问题下面一段程序： 1234567891011121314#define _CRT_SECURE_NO_WARNINGS#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;int main(void)&#123; long i; long arr[10]; for (i = 0; i &lt;= 12; i++)&#123; arr[i] = 0; printf(\"hehe\\n\"); &#125; system(\"pause\"); return 0;&#125; 看起来只是一个数组越界访问的问题，但是会引起死循环， 字符究竟是什么样的整数我们需要把一个字符值转换为一个较大的整数的时候，这个问题才变得重要起来。而在其他情况下，结果都是已经定义：多余的位将简单的被“丢弃”丢弃编译器在将char类型为int类型的时候，需要做出选择：应该将字符作为有符号整数还是应该是无符号整数处理？如果是前一种情况，编译器在将char类型的数扩展到int类型时，应该同时赋值符号位，如果是后一种情况，编译器只需要在多余的位上用0填充即可。如果说一个字符的最高位是1，编译器是将该字符当成是有符号数还是当做无符号数呢？这个选择非常重要，它决定着一个8字符的取值范围是从-128到127还是从0到255.而这一点，又会反过来影响到程序员岁哈希表或者转换表的设计方式。如果编程者最关注的一个高位是1 的字符其数值究竟是正还是负，可将字符声明为无符号字符（unsigned char）。这样无论什么编译器将该字符转换为整数的时候只需要多余的位填充为0即可，而如果声明为一般的字符变量，那么有些编译器上可能会作为有符号处理，而另一些编译器又会作为无符号数处理。与此相关的一个常见错误认识是：如果c是一个字符变量，使用（unsigned ）c就可以得到与c等价的一个无符号整数，但是这是会失败的，因为将字符c转化为无符号整数时，c将首先被转化为一个int型整数，因此可能会得到一个非预期的结果。下面说说正确的方式：使用（unsigned char） c ，因为一个unsigned char 类型的字符在转化为无符号整数时无需首先转化为int型整数，而是直接进行转换。接下来再说说getchar()函数，先看下面一段代码： 123456789#include&lt;stdio.h&gt;int main(void)&#123; char c; while ((c = getchar()) != EOF)&#123; putchar(c); &#125; return 0;&#125; 这个程序表面上是把标准输入复制到标准输出，实则不然，原因是c被声明为char类型，而getchar（）的返回值是int类型，这意味着c可能无法容纳下所有可能的字符，特别是可能无法容纳EOF，但是还好在VS下的EOF是-1 使用char类型的变量去接收返回值为int型的结果，最终会有3种可能：1、某些合法的字符被截断后使得c的取值与EOF相同，这样的话程序将在赋值文件的过程中就终止；2、c根本不可能取到EOF这个值，那么程序将陷入死循环；3、程序会正常工作，但完全是因为巧合，尽管getchar()的返回值赋值给c的时候会发生截断，尽管while语句中比较运算符的操作数不是getchar()的返回值，而是截断后的值c。然而令人惊讶的是许多编译器对上述表达式的实现并不正确。这些编译器确实对getchar()的返回值做了截断处理，并把低端字节部分赋值给了变量c。但是它们在表达式中并不是比较c与EOF的值，而是比较getchar()函数的返回值与EOF，如果编译器采用的是红色字段的方法，那么程序完全可以正常运行！ 求平均数的正确姿势当然这个就不单单是c语言的坑了，其他语言中也会出现！比如我们现在要写一个二分查找的代码，好吧，我承认这段代码大家都能写出来，可是….谁会写出没BUG的那种？ 1234567891011121314151617int binary(int[] arr, int date, int length)&#123; int min = 0; int max = length - 1; int mid = 0; while(min &lt;= max)&#123; mid = (min + max)/2; if(arr[mid] &gt; data)&#123; max = mid - 1; &#125;else if(arr[mid &lt; data])&#123; min = mid + 1; &#125;else&#123; return mid; &#125; &#125; return -1;&#125; 很多人可能一上手就写出这种代码了，很明显这是非常容易出现Bug的程序，问题就出在第七行,在min和max很大的时候会出现数据溢出，从而导致程序出错。第一篇二分搜索论文在1946年发表，但是这个错误直到1962年才被发现，中途用了16年的时间才发现并且纠正了错误！ 错误很明显，但是怎么改进呢？方法一：加法变减法 1234567891011121314151617int binary(int[] arr, int date, int length)&#123; int min = 0; int max = length - 1; int mid = 0; while(min &lt;= max)&#123; mid = min + (max - min)/2; if(arr[mid] &gt; data)&#123; max = mid - 1; &#125;else if(arr[mid &lt; data])&#123; min = mid + 1; &#125;else&#123; return mid; &#125; &#125; return -1;&#125; 方法二：位运算求平均数 123456789101112131415161718int binary(int[] arr, int date, int length)&#123; int min = 0; int max = length - 1; int mid = 0; while(min &lt;= max)&#123; //此处注意：&gt;&gt; 是右移, &gt;&gt;&gt; 是无符号右移 mid = min + ((max - min) &gt;&gt;&gt; 1)/2; if(arr[mid] &gt; data)&#123; max = mid - 1; &#125;else if(arr[mid &lt; data])&#123; min = mid + 1; &#125;else&#123; return mid; &#125; &#125; return -1;&#125;","updated":"2020-03-13T03:06:29.739Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://zouchanglin.cn/tags/C-C/"}]},{"title":"深入理解进程","date":"2018-09-27T10:09:30.000Z","path":"2018/09/27/深入理解进程/","text":"现代计算机体系结构冯·诺依曼结构要了解进程的概念得先从计算机的体系结构说起，首先了解一些世界上用得最多的计算机体系结构：冯·诺依曼结构(还有其他的计算机体系结构：如哈佛结构) 冯·诺曼结构处理器具有以下几个特点：必须有一个存储器；必须有一个控制器；必须有一个运算器，用于完成算术运算和逻辑运算；必须有输入和输出设备，用于进行人机通信 存储设备对比 上图从容量、传输速度、价格上来作比较，可以看出来为什么我们平时见到的计算机为什么硬盘几百G甚至几个T，而内存却只有8G或者16G，内存的IO速度是非常快的，跟硬盘的IO速度是 数量级 的差距，和内存相比寄存器就更快了，也是数量级的差距，于是出现了缓存，现在(2018/09/27)都是三级缓存，也就几M的大小，每次CPU在执行一些指令的时候会将需要的数据放在缓存中，其实就相当于是一个过渡元件！ 操作系统的定位 操作系统本质上就是一款软件，一款搞管理的软件，操作系统管理软件、管理硬件，为了安全操作系统不会让用户直接操作硬件，而是对外提供一套接口：也就是我们常用的系统调用接口 什么是进程早期的内存比较小，但是伴随着应用程序(可以理解为安装包)越来越大，现在的计算机至少都是500M内存，连500M的都很少见了。为什么应用程序越大需要的内存也越大？这与冯·诺依曼计算机体系结构有关： 首先我们都学过C语言，C程序也是一个文件，既然是文件那就是在磁盘上放着的，磁盘并不属于冯诺依曼结构中的一部分，磁盘属于外部设备，这一点需要注意，因为在冯诺依曼计算机体系中只有运算器、控制器、存储器、输入输出设备，运算器和控制器集成在CPU中，存储器实际上是内存，这也就意味着没有硬盘计算机也是可以正常工作的：《网吧电脑为什么没有硬盘 那没硬盘的电脑怎么运行？》 可以看出计算机在执行任务的时候，都是把应用程序加载到内存中，CPU会去内存中取数据、取指令然后才执行，这也就是为什么网吧的电脑没有硬盘也可以正常使用，只要在开机的时候把操作系统加载到内存中(操作系统也是一个软件)，然后要执行某个游戏的时候再次请求服务器将游戏也加载到内存中即可！通过上面的论述我们得出一个初步结论：一个应用程序想要被CPU执行必须要先加载到内存，这个被加载到内存的程序就叫做一个进程 操作系统怎么维护进程当你在听音乐的时候同时也可以编辑文档，还可以挂着TIM，很显然不止一个程序在执行，既然执行一个应用程序需要把它加载到内存中，那么当前肯定不止一个进程，每个程序一旦加载到内存中就是一个进程，那么这么多的进程如何维护呢？ PCBPCB的全称是：Processing Control Block，翻译过来叫做进程控制块操作系统是根据PCB来对并发执行的进程进行控制和管理的。 PCB存放着操作系统用于描述进程情况及控制进程运行所需的全部信息，PCB本质上就是一个结构体，这个结构体里面封装了描述进程的全部信息，它使一个在多道程序环境下不能独立运行的程序成为一个能独立运行的基本单位或一个能与其他进程并发执行的进程。所以这就叫做： 并发那么什么是并行呢？并行指的是多核CPU同时执行多个任务，可见并发并不是真的同时执行，并行才是真的同时执行！如果仅仅是把程序的代码和数据拷贝到内存中毫无意义，操作系统是无法管理好这个进程的，于是出现了PCB，进程之间是相互独立的，每个进程都对应一个PCB！在Linux下描述进程的结构体叫做task_struct，接下来看看它的源码地址是：https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h描述task_struct的位置在第593行（2019/09/27） 源码有点长，操作系统想要管理好这么多的进程必须把控进程的每一个信息，所以这就和学校管理学生一样，学号、地址、电话、身份证号….信息量很大的情况下必须封装成一个结构体来管理！我们选择部分主要的内容看一下： 结构体成员 作用 标识符 描述本进程的唯标一符，用来区别其他进程 状态 任务状态，退出代码，退出信号等。 优先级 相对于其他进程的优先级 程序计数器 程序中即将被执的下一条指令的地址 内存指针 包括程序代码和进程相关数据的指针，还有和其他进程共享的内存块的指针 上下文数据 进程执行时处理器的寄存器中的数据 I／O状态信息 包括显式的I/O请求,分配给进程的I／O设备和被进程使用的文件列表 记账信息 可能包括处理器时间总和，使用的时钟数总和，时间限制，记账号等 其他信息 … 所有运在系统的进程都以 task_struct 链表的形式存在内核里。进程的信息可以通过 /proc 系统 件夹查看。要获取PID为400的进程信息，你需要查看 / proc/400 这个件夹。 多数进程信息同样可以使 top和ps这些户集具来获取 获取进程ID与父进程ID1234567#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;int main() &#123; printf(\"pid=%d ppid=%d\\n\", getpid(), getppid()); return 0;&#125; 初识forkman 2 fork之后：fork() creates a new process by duplicating the calling process. The new process is referred to as the child process. The calling process is referred to as the parent process.The child process and the parent process run in separate memory spaces. At the time of fork() both memory spaces have the same content. Memory writes, file mappings(mmap(2)), and unmappings (munmap(2)) performed by one of the processes do not affect the other. 综合上面的意思来讲就是fork函数是用来开辟子进程的，fork() 函数正常的话对父进程返回子进程的id，对子进程返回0，返回-1则表示开辟子进程失败，所以一般使用if的结构开辟子进程： 12345678910111213141516171819202122#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;error.h&gt;int main()&#123; int id = 0; //获取当前进程的进程ID和父进程 printf(\"pid:%d ppid=%d\\n\",getpid(),getppid()); //开辟子进程 id = fork(); if(id &lt; 0)&#123; perror(\"fork failed\\n\"); return -1; &#125;else if( id == 0) printf(\"Child,id = %d, ppid = %d\\n\", getpid(), getppid()); else&#123; printf(\"Parent,id = %d, ppid = %d\\n\",getpid(), getppid()); &#125; return 0;&#125; 很显然，子进程的ppid与父进程的id是一致的，那么父进程的ppid又是谁呢？是Bash，记得开始学习Linux的时候老师讲过shell就是外壳程序，而Linux下面的shell就是叫做Bash，也就是命令行解释器，每当我们用Bash执行一条指令的时候，Bash就会开启一个子进程去完成需要被执行的指令！ 进程状态Linux内核源代码的解释：为了弄明白正在运行的进程是什么意思，我们需要知道进程的不同状态。一个进程可以有几个状态，在Linux内核里,进程有时候也叫做任务。 下面的状态在kernel源代码里定义： 12345678910111213/** The task state array is a strange \"bitmap\" of reasons to sleep. Thus \"running\" is zero, and * you can test for combinations of others with simple bit tests.*/static const char * const task_state_array[] = &#123;\"R (running)\", /* 0 */\"S (sleeping)\", /* 1 */\"D (disk sleep)\", /* 2 */\"T (stopped)\", /* 4 */\"t (tracing stop)\", /* 8 */\"X (dead)\", /* 16 */\"Z (zombie)\", /* 32 */&#125;; 进程状态说明 R运行状态(running)并不意味着进程一定在运行行中,它表明进程要么是在运行中要么在运行队列里里，这个其实不难理解，因为对于单核CPU而言每个单位时间里只能运行一个进程，为了看似表面上同时执行多个任务，CPU会在多个进程之间来回切换，速度非常快以至于我们是察觉不到CPU的切换，也就造成了我们误以为在同时运行多个任务的假象！ S睡眠状态(sleeping)意味着进程在等待事件完成(这里里的睡眠有时候也叫做可中断睡眠(interruptible sleep)，也就是说可以随时被唤醒，或者被杀死都有可能！ D磁盘休眠状态(Disk sleep)有时候也叫不可中断睡眠状态(uninterruptible sleep),在这个状态的进程通常会等待IO的结束，如果IO一直没有结束这个进程是无法结束的！处在这种状态的进程不接受外来的任何信号，就算使用kill -9也不可以杀掉，如果长时间未响应就说明IO出了问题！比如说我开了8个进程同时访问一个IO，访问的时候势必会加锁来保护资源，那么，当一个进程正在访问的时候，其他进程如果在等待锁，那么就会进入disk sleep，当你执行kill，它不会立即响应，当锁满足条件的时候才可能响应信号。 T停止止状态(stopped)可以通过发送 SIGSTOP 信号给进程来停止止(T)进程。这个被暂停的进程可以通过发送 SIGCONT 信号让进程继续运行。 X死亡状态(dead)这个状态只是一个返回状态，你不会在任务列表里看到这个状态 修改进程状态通过kill -l命令查看可以发送的信号： 比如我们经常使用的kill -9 pid就是向ID为pid的进程发送9号信号，9号信号对应的是SIGKILL Z(zombie)-僵尸进程僵死状态(Zombies)是一个比较特殊的状态。当进程退出并且父进程(使用用wait()系统调用用,后面讲)没有读取到子进程退出的返回代码时就会产生僵死(尸)进程僵死进程会以终止状态保持在进程表中,并且会一直在等待父进程读取退出状态代码。所以，只要子进程退出，父进程还在运行，但父进程没有读取子进程状态,子进程进入Z状态模拟僵尸进程： 接下来我们写一个shell脚本来监视这两个进程的情况 12while :; do ps aux|grep test.out|grep -v grep;sleep 1; echo \"#######################\";done 可以看到父进程还没有结束的时候子进程却死掉了，子进程在死掉的时候由于PCB是不会释放的，这样就没有进程来回收这个子进程，最终导致的结果就是内存泄漏!进程的退出状态必须被维持下去，因为他要告诉关心它的进程(父进程),你交给我的任务,我办的怎么样了。可父父进程如果一直不读取,那子进程就一直处于Z状态?是的!维护退出状态本身就是要用数据维护，也属于进程基本信息，所以保存在task_struct(PCB)中,换句话说,Z状态一直不退出，PCB一直都要维护?是的!那一个父进程创建了很多子进程，就是不回收，是不是就会造成内存资源的浪费?是的!因为数据结构对象本身身就要占用用内存，想想C中定义一个结构体变量(对象)，是要在内存的某个位置进行行开辟空间! 孤儿进程接下来说说孤儿进程，顾名思义孤儿进程就是没有父进程的进程，如果父进程比子进程先退出，那么这个子进程就是孤儿进程了，下面使用代码模拟一下孤儿进程： 1234567891011121314151617181920212223#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;int main(int argc, char *argv[])&#123; int pid = fork(); if(pid &lt; 0)&#123; perror(\"fork failed...\"); return -1; &#125;else if(pid == 0)&#123; printf(\"child[%d], my parentid[%d]..\\n\", getpid(), getppid()); sleep(5); printf(\"child[%d], my parentid[%d]..\\n\", getpid(), getppid()); &#125; else&#123; printf(\"parent[%d]...\\n\", getpid()); sleep(2); exit(0); &#125; return 0;&#125; 由图中可以看出，子进程还没有退出但是父进程已经退出了，于是子进程的ID变成了2915，2915号进程是/lib/systemd/systemd --user，但是远程链接的结果却是： 其实Ubuntu自带的终端是个桌面软件，如果不在图形界面下运行就变成了1！ 进程优先级优先级概述cpu资源分配的先后顺序,就是指进程的优先权(priority)。优先权高的进程有优先执行权利。配置进程优先权对多任务环境的linux很有用用,可以改善系统性能。还可以把进程运行行到指定的CPU上,这样一一来,把不重要的进程安排到某个CPU,可以大大大大改善系统整体性能 PRI and NIPRI也还是比比较好理解的，即进程的优先级,或者通俗点说就是程序被CPU执行的先后顺序,此值越小进程的优先级别越高高，那NI呢？就是我们所要说的nice值了，其表示进程可被执行的优先级的修正数值，PRI值越小越快被执行，那么加入nice值后，将会使得PRI变为PRI(new)=PRI(old)+nice 这样，当nice值为负值的时候,那么该程序将会优先级值将变小，即其优先级会变高,则其越快被执行所以，调整进程优先级,在Linux下,就是调整进程nice值，nice其取值范围是-20至19,一共40个级别！ 修改进程优先级的命令 启动进程前调整: nice(开始执行程序就指定nice值:) 1nice -n -5 .&#x2F;test.out 调整已存在进程的nice: renice 1renice -5 -p 5200 &#x2F;&#x2F;PID为5200的进程nice设为-5 使用top命令更改已存在进程的nice进入top后按r-&gt;输入进程PID-&gt;输入nice值 其他概念 竞争性: 系统进程数目众多，而而CPU资源只有少量,甚至至1个,所以进程之间是具有竞争属性的。为了高效完成任务，更合理竞争相关资源，便具有了优先级 独立性: 多进程运行，需要独享各种资源，多进程运行期间互不干扰 并行: 多个进程在多个CPU下分别同时运行，这称之为并行 环境变量环境变量(environment variables)一般是指在操作系统中用来指定操作系统运行环境的一些参数如:我们在编写C/C++代码的时候，在链接的时候，从来不知道我们的所链接的动态静态库在哪里，但是照样可以链接成功，生成可执行程序，原因就是有相关环境变量帮助编译器进行查找。环境变量通常具有某些特殊用途,还有在系统当中通常具有全局特性！ 常见的环境变量PATH: 指定命令的搜索路径HOME: 指定用户的主工作目录(即用用户登陆到Linux系统中时,默认的目录)HISTSIZE: 指保存历史命令记录的条数SHELL: 当前Shell，它的值通常是/bin/bash 和环境变量有关的命令 echo: 显示某个环境变量值 export: 设置一个新的环境变量 env: 显示所有环境变量 unset: 清除环境变量 set: 显示本地定义的shell 环境变量的组织形式 每个程序都会收到一张环境表,环境表是一个字符指针数组，每个指针指向一个以’\\0’结尾的环境字符串 通过代码获取环境变量 程序地址空间先看这样一段代码： 1234567891011121314151617181920212223#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;int g_val = 0;int main(int argc, char *argv[])&#123; pid_t id = fork(); if(id &lt; 0)&#123; perror(\"fork failed\"); return 0; &#125; else if(id == 0)&#123; g_val = 100; printf(\"child[%d],val[%d],address[%p]\\n\", getpid(), g_val, &amp;g_val); &#125;else&#123; sleep(3); printf(\"parent[%d],val[%d],address[%p]\\n\", getpid(), g_val, &amp;g_val); &#125; sleep(1); return 0;&#125; 全局变量g_val 在内存中很显然有两份，这个不难理解，父进程和子进程都有自己的一份变量，所以即使子进程修改了g_val 也是不会影响到父进程中的g_val，但是为什么打印出来的地址是一样的？这就引出来程序地址空间的概念：由此可见打印出的地址都是虚拟地址，是操作系统将这写虚拟地址转化为了实际的物理地址！ 进程地址空间其实说程序地址空间是不准确的，应该叫做进程地址空间早期的内存管理机制： 要运行一个程序，会把这些程序全都装入内存，当计算机同时运行多个程序时，必须保证这些程序用到的内存总量要小于计算机实际物理内存的大 进程地址空间不隔离。由于程序都是直接访问物理内存，所以恶意程序可以随意修改别的进程的内存数据，以达到破坏的目的 内存使用效率低。在A和B都运行的情况下，如果用户又运行了程序C，而程序C需要15M大小的内存才能运行，而此时系统只剩下4M的空间可供使用，所以此时系统必须在已运行的程序中选择一个将该程序的数据暂时拷贝到硬盘上，释放出部分空间来供程序C使用，然后再将程序C的数据全部装入内存中运行 程序运行的地址不确定。当内存中的剩余空间可以满足程序C的要求后，操作系统会在剩余空间中随机分配一段连续的20M大小的空间给程序C使用，因为是随机分配的，所以程序运行的地址是不确定的，这种情况下，程序的起始地址都是物理地址，而物理地址都是在加载之后才能确定。 分页&amp;虚拟地址空间 其实从图中可以看出：虚拟地址空间只不过是操作系统建立了页表，把虚拟地址和实际物理内存建立了联系而已！ 进程调度算法既然操作系统中有那么多的进程，那CPU应该先调用哪一个呢？这就涉及到进程的调度算法，这里看Linux2.6内核为研究对象： 一个 CPU 拥有一个 runqueue 如果有多个 CPU 就要考虑进程个数的负载均衡(优先级)问题普通优先级： 100 ~ 139 （我们都是普通的优先级，想想 nice 值的取值范围实时优先级：这个可以不用关心 活动队列 时间片还没有结束的所有进程都按照优先级放在该队列 n_active ：总共有多少个运行状态的进程 queue [ 140] ：一个元素就是一个进程队列，相同优先级的进程按照FIFO 规则进行排队调度，所以，数组下标就是优先级！ 从该结构中，选择一个最合适的进程，过程是怎么的呢？ 从 0 下表开始遍历queue[140 ] 找到第一个非空队列，该队列必定为优先级最高的队列 拿到选中队列的第一个进程，开始运行，调度完成！ 遍历 queue [ 1 40 ］时间复杂度是常数！但还是太低效了！ bitmaP [ 5 ] ：一共 1 40 个优先级，一共 1 40 个进程队列，为了提高查找非空队列的效率，就可以用 5 * 32 个比特位表示队列是否为空，这样，便可以大大提高查找效率！ 过期队列 过期队列和活动队列结构一模一样 过期队列上放置的进程都是时间片耗尽的进程 当活动队列上的进程都被处理完毕之后，对过期队列的进程进行时间片重新计算 active指针expired active 指针永远指向活动队列 expired 指针永远指向过期队列可是活动队列上的进程会越来越少，过期队列上的进程会越来越多，因为进程时间片到期时一直都存在的。 没关系，在合适的时候，只要能够交换 active 指针和 expired 指针的内容，就相当于有具有了一批新的活动进程！ 常见的进程调度算法 时间片轮转调度算法（RR）：给每个进程固定的执行时间，根据进程到达的先后顺序让进程在单位时间片内执行，执行完成后便调度下一个进程执行，时间片轮转调度不考虑进程等待时间和执行时间，属于抢占式调度。优点是兼顾长短作业；缺点是平均等待时间较长，上下文切换较费时。适用于分时系统。 先来先服务调度算法（FCFS）：根据进程到达的先后顺序执行进程，不考虑等待时间和执行时间，会产生饥饿现象。属于非抢占式调度，优点是公平，实现简单；缺点是不利于短作业。 优先级调度算法（HPF）：在进程等待队列中选择优先级最高的来执行。 多级反馈队列调度算法：将时间片轮转与优先级调度相结合，把进程按优先级分成不同的队列，先按优先级调度，优先级相同的，按时间片轮转。优点是兼顾长短作业，有较好的响应时间，可行性强，适用于各种作业环境。 高响应比优先调度算法：根据“响应比=（进程执行时间+进程等待时间）/ 进程执行时间”这个公式得到的响应比来进行调度。高响应比优先算法在等待时间相同的情况下，作业执行的时间越短，响应比越高，满足段任务优先，同时响应比会随着等待时间增加而变大，优先级会提高，能够避免饥饿现象。优点是兼顾长短作业，缺点是计算响应比开销大，适用于批处理系统。 参考资料 《Linux的进程优先级 NI 和 PR》 《进程调度算法》 《五种进程调度算法的总结》 《https://www.jianshu.com/p/3bb1cdd44ef0》","updated":"2020-03-13T03:06:29.753Z","categories":[{"name":"操作系统理论","slug":"操作系统理论","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"}]},{"title":"哈希与海量数据处理","date":"2018-09-23T10:09:30.000Z","path":"2018/09/23/哈希与海量数据处理/","text":"哈希切割、Top K问题 问题一：给一个超过100G大小的log file, log中存着IP地址, 设计算法找到出现次数最多的IP地址？问题二：与上题目条件相同，如何找出Top K的IP？问题三：直接用Linux命令如何实现？ 对于问题一： 很显然100G的文件直接存入内存中的代价太高了，所以我们需要将文件切分为1000份，逐个加载到内存中 将切分后的每个文件中的IP加载到哈希表，由于需要统计次数，所以需要使用哈希表的 Key-Value 模型，使用哈希函数将IP转换为位置int index = IPHash(char* ip)%1000 这样的话相同的IP就会映射到相同的位置，执行Value++即可统计次数 最后只要一个Key对应着最大的Value即是我们想要的结果 对于问题二：要想统计次数最多的前K个IP，这里需要建K个元素的小堆，每次选出的IP与堆顶元素比较如果大于堆顶元素就替换掉堆顶元素并且向下调堆，这样到最后堆中的数据就是出现次数最多的前K个IP对于问题三：cat logfile | sort -r | uniq | awk NR==排行数先使用sort -r进行次数由多到少的排序，再使用uniq去重，最后使用awk获取结果 位图应用 问题四：给定100亿个整数，设计算法找到只出现一次的整数？问题五：给两个文件，分别有100亿个整数，只有1G内存，如何找到两个文件的交集？ 对于问题四：分析一下100亿个数字需要的内存：4×100 ×10^8 / 1024/1024/1024 = 37.252G很显然与题目一类似，可以将这100亿个数字分100份分别存储在文件中，通过int index = NumHash(int num) %100 哈希切分，在每份文件中找出只出现一次的数字，最后将这100分文件结果归纳在一起即可找出！但是可以使用位图来解决这种 存在与否、出现一次 的问题，如果是判断存不存在之类的问题可以使用一个位就可以表示，如：0表示不存在，1表示存在，在本题目中统计只出现一次则需要使用一个位来表示，0表示不存在或者出现多次，1表示只出现一次！ 对于问题五：也是只有1G内存，在这里两个文件则需要使用两个位图，然后根据两个位图的交集，也就是将两个位图&amp;操作来求出两个文件的交集！ 位图变形 问题六：1个文件有100亿个int,1G内存,设计算法找到出现次数不超过2次的所有整数 对于问题六：这个问题实际上是问题四的变形，分析可知不超过两次的情况直接使用两个比特位来完成，00表示0次，01表示一次，10表示两次，11表示超过两次，这样处理问题无论怎么变化都是有效的！ 布隆过滤器 问题七：给两个文件,分别有100亿个query(查询),我们只有1G内存,如何找到两个文件交集?分别给出精确算法和近似算法?问题八：如何扩展BloomFilter使得它支持删除元素的操作？问题九：如何扩展BloomFilter使得它支持计数操作？ 对于问题七： 精确算法现有100亿条查询，预估每一条大小为20字节，算出大小为96.7G，很显然需要特殊的处理，思想和问题一是一致的，先使用哈希函数做分割，可以把这两个文件分别分割为1000个文件，这样问题就变成了逐个求这1000个文件和另外1000个文件的交集，最后再整合结果即可！ 近似算法使用布隆进行过滤，将其中的某一个文件映射到位图当中，取另一文件的内容进行比对，如果不存在，那么就不是交集，如果存在，可视为交集！注意：虽然布隆在映射时会映射多个位置，但判断是否在位图中存在时还是可能出现误差，故此方法为近似算法！ 对于问题八和问题九：布隆的删除和计数可归为一类问题，原本布隆是一个元素映射到多个位置上，这个位置上的值是一个Key,现在将其改为数据存在的个数，每当映射到相同的位置，该位置上的数进行加1，最后每个位置上的值表示出现某一元素映射到该位的次数！标准的BloomFilter只支持插入和查找两种操作，如果表达的集合是静态集合的时候，在初始化集合大小，确定hash k的个数，控制错误率的基础上，BloomFilter可以很好的满足需求。但是对于一些动态集合，BloomFilter不满足需求了，其不支持删除操作，现引入Counting BloomFilter，能解决相关问题，具体见：https://blog.csdn.net/fuwei736349065/article/details/75642933 倒排索引 问题十：给上千个文件,每个文件大小为1K—100M。给n个词,设计算法对每个词找到所有包含它的文件,你只有100K内存 这种做法就是根据文件中的Key去建立哈希表，采用拉链法，凡是包含key的文件都会被挂载到对应的位置，搜索引擎就是这种做法，很显然那些能被百度排在最前面的位置的网页很可能就是在拉链的前几个位置，所以会呈现在前面！","updated":"2020-03-13T03:06:29.727Z","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"哈希","slug":"哈希","permalink":"https://zouchanglin.cn/tags/%E5%93%88%E5%B8%8C/"}]},{"title":"神奇的卷积核","date":"2018-08-31T10:09:30.000Z","path":"2018/08/31/神奇的卷积核/","text":"关于卷积卷积是分析数学中一种重要的运算。卷积是一种线性运算,图像处理中常见的mask运算都是卷积，广泛应用于图像滤波。高斯变换就是用高斯函数对图像进行卷积，卷积操作是图像变换的基础。 如果我们称 (f*g) (n)是f、g的卷积： 连续型卷积的公式 离散型卷积的公式 从中我们可以得出 假设现有两个骰子，随机掷骰子点数和为4 那么这样的概率就是f(1)*g(3)+f(2)*g(2)+f(3)*g(1) ,符合： 下面这张图可以很好的帮助理解卷积： 用一个模板和一幅图像进行卷积，对于图像上的一个点，让模板的原点和该点重合，然后模板上的点和图像上对应的点相乘，然后各点的积相加，就得到了该点的卷积值。对图像上的每个点都这样处理。由于大多数模板都是对称的，所以模板不旋转。卷积是一种积分运算，用来求两个曲线重叠区域面积。可以看作加权求和，可以用来消除噪声、特征增强。把一个点的像素值用它周围的点的像素值的加权平均代替。卷积是一种线性运算,图像处理中常见的mask运算都是卷积，广泛应用于图像滤波。卷积关系最重要的一种情况，就是在信号与线性系统或数字信号处理中的卷积定理。利用该定理，可以将时间域或空间域中的卷积运算等价为频率域的相乘运算，从而利用FFT等快速算法，实现有效的计算，节省运算代价。 卷积核这个核本质上是一个大小固定，由数值参数构成的数组，数组的标定点通常位于数组的中心。数组的大小被称为核支撑。单就技术而言，核支撑实际上仅仅由核数组的非零部分组成。上图中不停移动的白色块变是卷积核！ opencv的图像卷积函数：cvFilter2D、filter2D()cvFilter2D() —— opencv中的c语言函数src：输入图像.dst：输出图像.kernel：卷积核, 单通道浮点矩阵. 如果想要应用不同的核于不同的通道，先用 cvSplit 函数分解图像到单个色彩通道上，然后单独处理。anchor：核的锚点表示一个被滤波的点在核内的位置。 锚点应该处于核内部。缺省值 (-1,-1) 表示锚点在核中心。函数 cvFilter2D 对图像进行线性滤波，支持 In-place 操作。当核运算部分超出输入图像时，函数从最近邻的图像内部象素差值得到边界外面的象素值。 123void cvFilter2D( const CvArr* src, CvArr* dst, const CvMat* kernel, CvPoint anchor = cvPoint(-1,-1)); filter2D() ——— opencv中的c++函数InputArray src: 输入图像OutputArray dst: 输出图像，和输入图像具有相同的尺寸和通道数量int ddepth: 目标图像深度，如果没写将生成与原图像深度相同的图像。原图像和目标图像支持的当ddepth输入值为-1时，目标图像和原图像深度保持一致InputArray kernel: 卷积核(或者是相关核), 一个单通道浮点型矩阵。如果想在图像不同的通道使用不同的kernel，可以先使用split()函数将图像通道事先分开Point anchor: 内核的基准点(anchor)，其默认值为(-1,-1)说明位于kernel的中心位置。基准点即kernel中与进行处理的像素点重合的点double delta: 在储存目标图像前可选的添加到像素的值，默认值为0int borderType: 像素向外逼近的方法，默认值是BORDER_DEFAULT,即对全部边界进行计算。该函数使用于任意线性滤波器的图像，支持就地操作。当其中心移动到图像外，函数可以根据指定的边界模式进行插值运算。函数实质上是计算kernel与图像的相关性而不是卷积! 123CV_EXPORTS_W void filter2D( InputArray src, OutputArray dst, int ddepth, InputArray kernel, Point anchor=Point(-1,-1), double delta=0, int borderType=BORDER_DEFAULT ); 原理对于图像的每一个像素点，计算它的邻域像素和滤波器矩阵的对应元素的乘积，然后加起来，作为该像素位置的值，这样就完成了滤波过程！ 卷积与相关对图像和滤波矩阵进行逐个元素相乘再求和的操作就相当于将一个二维的函数移动到另一个二维函数的所有位置，这个操作就叫卷积或者相关。卷积和相关的差别是，卷积需要先对滤波矩阵进行180的翻转，但如果矩阵是对称的，那么两者就没有什么差别了。图像卷积操作的本质是矩阵卷积。某些特殊的卷积核会使图像产生特殊的效果： 图像处理下面把均把相关性计算看成是卷积运算，从本质上来说其实就是卷积运算，相当于把矩阵旋转了180°所以看成是卷积！ 边缘检测卷积核 图像锐化卷积核123-1, -1, -1,-1, 9, -1,-1, -1, -1 图像的锐化和边缘检测很像，首先找到边缘，然后把边缘加到原来的图像上面，这样就强化了图像的边缘，使图像看起来更加锐利了。这两者操作统一起来就是锐化滤波器了，也就是在边缘检测滤波器的基础上，再在中心的位置加1，这样滤波后的图像就会和原始的图像具有同样的亮度了，但是会更加锐利。 更加强调边缘的卷积核1231, 1, 1, 1, -7, 1, 1, 1, 1 浮雕效果的卷积核123-6, -3, 0, -3, 1, 3, 0, 3, 6 模糊图像的卷积核Smooth/Blur 是图像处理中最简单和常用的操作之一，使用该操作的原因之一就为了给图像预处理时候减低噪声，使用Smooth/Blur操作其背后是数学的卷积计算 卷积过程：6x6上面是个3x3的窗口，从左向右，从上向下移动，黄色的每个像个像素点值之和取平均值赋给中心红色像素作为它卷积处理之后新的像素值。每次移动一个像素格。 归一化滤波（均值滤波） 高斯滤波 均值模糊 1blur(Mat src, Mat dst, Size(xradius, yradius), Point(-1,-1)); 高斯模糊 1GaussianBlur(Mat src, Mat dst, Size(11, 11), sigmax, sigmay); 其中Size（x, y）, x, y 必须是正数而且是奇数 123456789101112131415161718192021222324252627282930313233343536373839#include &lt;iostream&gt;#include &lt;opencv2\\opencv.hpp&gt;using namespace cv;using namespace std;int main(int argc, char *argv[]) &#123; Mat src = imread(\"C:\\\\Users\\\\Tim\\\\Desktop\\\\Image\\\\book.png\"); namedWindow(\"src_window\", CV_WINDOW_AUTOSIZE); imshow(\"src_window\", src); Mat dst = Mat::zeros(src.size(), src.type()); //定义卷积核 //边缘检测 //Mat C = (Mat_&lt;double&gt;(3, 3) &lt;&lt; -1, -2, -1, 0, 0, 0, 1, 2, 1); // Sobel算子 横向边缘检测 //Mat C = (Mat_&lt;double&gt;(3, 3) &lt;&lt; -1, 0, 1, -2, 0, 2, -1, 0, 1); // Sobel算子 纵向边缘检测 //Mat C = (Mat_&lt;double&gt;(3, 3) &lt;&lt; -1, -1, -1, 0, 0, 0, 1, 1, 1); // Prewitt算子 横向边缘检测 //Mat C = (Mat_&lt;double&gt;(3, 3) &lt;&lt; -1, 0, 1, -1, 0, 1, -1, 0, 1); // Prewitt算子 纵向边缘检测 //Mat C = (Mat_&lt;double&gt;(3, 3) &lt;&lt; -1, -1, -1, -1, 8, -1, -1, -1, -1); // Laplacian算子 边缘检测 //图像锐化 //Mat C = (Mat_&lt;double&gt;(3, 3) &lt;&lt; -1, -1, -1, -1, 9, -1, -1, -1, -1); //Mat C = (Mat_&lt;double&gt;(3, 3) &lt;&lt; 1, 1, 1, 1, -7, 1, 1, 1, 1);//更加强调边缘的锐化 //浮雕 //Mat C = (Mat_&lt;double&gt;(3, 3) &lt;&lt; -6, -3, 0, -3, 1, 3, 0, 3, 6); filter2D(src, dst, src.depth(), C, Point(-1, -1)); namedWindow(\"dst_window\", CV_WINDOW_AUTOSIZE); imshow(\"dst_window\", dst); moveWindow(\"src_window\", 1, 1); moveWindow(\"dst_window\", 1, 1); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;opencv2\\opencv.hpp&gt;using namespace cv;using namespace std;int main(int argc, char *argv[])&#123; //Mat src = imread(\"C:\\\\Users\\\\Tim\\\\Desktop\\\\Image\\\\a.jpg\"); Mat src = imread(\"C:\\\\Users\\\\Tim\\\\Desktop\\\\test.png\"); if (src.empty())&#123; cout &lt;&lt; \"load image filed...\" &lt;&lt; endl; return -1; &#125; namedWindow(\"src_window\", CV_WINDOW_AUTOSIZE); imshow(\"src_window\", src); namedWindow(\"dst_window\", CV_WINDOW_AUTOSIZE); Mat dst; //3*3均值滤波模糊 blur(src, dst, Size(3, 3), Point(-1, -1)); //GaussianBlur(src, dst, Size(3, 3), 11, 11);//高斯滤波 imshow(\"dst_window\", dst); waitKey(0); return 0;&#125; 其他的几种滤波方式中值滤波中值滤波对椒盐噪声有很好的抑制作用椒盐噪声也称为脉冲噪声，是图像中经常见到的一种噪声，它是一种随机出现的白点或者黑点，可能是亮的区域有黑色像素或是在暗的区域有白色像素（或是两者皆有）。盐和胡椒噪声的成因可能是影像讯号受到突如其来的强烈干扰而产生、类比数位转换器或位元传输错误等。例如失效的感应器导致像素值为最小值，饱和的感应器导致像素值为最大值。 中值滤波器中值滤波器（Median filtering） 如其名所隐含的，它将一个像素的值用该像素邻域中强度值的中间值来取代（计算中间值的过程中，也会将该像素的原始值包含），中值滤波器在处理盐和胡椒噪声上能提供绝佳的噪声降低效能。 伪中值滤波器为了改进中值滤波器的计算速率，伪中值滤波器（Pseudo-median filtering） 以近似的方法算出中间值。 双边滤波 均值模糊无法克服边缘像素信息丢失缺陷。原因是均值滤波是基于平均权重 高斯模糊部分克服了该缺陷，但是无法完全避免，因为没有考虑像素值的不同 高斯双边模糊 – 是边缘保留的滤波方法，避免了边缘信息丢失，保留了图像轮廓不变 双边滤波就是在对像素进行卷积时，不单单用位置（定义域）信息，还要用到值域信息。你看看高斯卷积的模板，就能明白什么是位置信息。值域信息就是当前像素与邻域像素的差别，差别越大（也就是边界位置），权重越小，这个小权重施加到高斯模板上，就会让高斯权重变小，模糊变弱，也就起到了在边界处弱化高斯模糊的作用，双边滤波的保边作用就是这样实现的。而在平坦区域，值域与领域像素差别小，几乎为零（指数函数用到了），那么权重最大接近1，施加到高斯权重上几乎对高斯不起作用，也就是在平坦区实际执行的就是高斯滤波。计算公式如下： 其中w(i,j,k,l)的计算方法如下: d函数是根据像素距离选择权重，距离越近权重越大，这一点和方框滤波，高斯滤波方式相同。而r函数则是根据像素的差异来分配权值。如果两个像素值越接近，即使相距较远，也比差异大而距离近的像素点权重大。正是r函数的作用，使得边缘，即相距近但差异大的像素点的特性得以保留! 相关API中值模糊medianBlur（Mat src, Mat dest, ksize）双边模糊bilateralFilter(src, dest, d=15, 150, 3);5 –计算的半径，半径之内的像数都会被纳入计算，如果提供-1 则根据sigma space参数取值50 – sigma color 决定多少差值之内的像素会被计算3 – sigma space 如果d的值大于0则声明无效，否则根据它来计算d值中值模糊的ksize大小必须是大于1而且必须是奇数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;iostream&gt;#include &lt;opencv2\\opencv.hpp&gt;using namespace cv;using namespace std;int main(int argc, char **argv)&#123; Mat src = imread(\"C:\\\\Users\\\\Tim\\\\Desktop\\\\Image\\\\notbeautify1.png\"); //Mat src = imread(\"C:\\\\Users\\\\Tim\\\\Desktop\\\\Image\\\\AI.png\"); Mat dst; if (src.empty()) &#123; cout &lt;&lt; \"load image filed...\" &lt;&lt; endl; return -1; &#125; namedWindow(\"src_window\", CV_WINDOW_AUTOSIZE); imshow(\"src_window\", src); //中值滤波去掉椒盐噪声 //medianBlur(src, dst, 3); //双边滤波 bilateralFilter(src, dst, 15, 100, 3); namedWindow(\"medianBlur_dst\", CV_WINDOW_AUTOSIZE); imshow(\"medianBlur_dst\", dst); Mat gblur; GaussianBlur(src, gblur, Size(15, 15), 3, 3); namedWindow(\"HelloWorld\",CV_WINDOW_AUTOSIZE); namedWindow(\"bglur_dst\", CV_WINDOW_AUTOSIZE); imshow(\"bglur_dst\", gblur); //通过掩模提升对比度 Mat retImg; Mat kernal = (Mat_&lt;int&gt;(3, 3) &lt;&lt; 0, -1, 0, -1, 5, -1, 0, -1, 0); filter2D(dst, retImg, dst.depth(), kernal, Point(-1, -1), 0); namedWindow(\"retImg\", CV_WINDOW_AUTOSIZE); imshow(\"retImg\", retImg); waitKey(0); return 0;&#125;","updated":"2020-03-13T03:06:29.755Z","categories":[{"name":"图像处理","slug":"图像处理","permalink":"https://zouchanglin.cn/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}],"tags":[{"name":"openCV","slug":"openCV","permalink":"https://zouchanglin.cn/tags/openCV/"}]},{"title":"Mat与基本图形绘制","date":"2018-08-30T10:09:30.000Z","path":"2018/08/30/Mat与基本图形绘制/","text":"Mat对象与IplImage对象 Mat对象OpenCV2.0之后引进的图像数据结构、自动分配内存、不存在内存泄漏的问题，是面向对象的数据结构。分了两个部分，头部与数据部分 IplImage是从2001年OpenCV发布之后就一直存在，是C语言风格的数据结构，需要开发者自己分配与管理内存，对大的程序使用它容易导致内存泄漏问题，下面是IplImage结构体的定义： 1234567891011121314151617181920212223242526typedef struct _IplImage&#123; int nSize; // IplImage大小 int ID; // 版本 (=0) int nChannels; // 大多数OPENCV函数支持1,2,3 或 4 个通道 int alphaChannel; // 被OpenCV忽略 int depth; // 像素的位深度，主要有以下支持格式： IPL_DEPTH_8U, IPL_DEPTH_8S, IPL_DEPTH_16U,IPL_DEPTH_16S, IPL_DEPTH_32S,IPL_DEPTH_32F 和IPL_DEPTH_64F char colorModel[4]; // 被OpenCV忽略 char channelSeq[4]; // 同上 int dataOrder; // 0 - 交叉存取颜色通道, 1 - 分开的颜色通道.只有cvCreateImage可以创建交叉存取图像 int origin; // 图像原点位置： 0表示顶-左结构,1表示底-左结构 int align; // 图像行排列方式 (4 or 8)，在 OpenCV 被忽略，使用 widthStep 代替 int width; // 图像宽像素数 int height; // 图像高像素数 struct _IplROI *roi; // 图像感兴趣区域，当该值非空时，只对该区域进行处理 struct _IplImage *maskROI; // 在 OpenCV中必须为NULL void *imageId; // 同上 struct _IplTileInfo *tileInfo; //同上 int imageSize; // 图像数据大小(在交叉存取格式下ImageSize=image-&gt;height*image-&gt;widthStep），单位字节 char *imageData; // 指向排列的图像数据 int widthStep; // 排列的图像行大小，以字节为单位 int BorderMode[4]; // 边际结束模式, 在 OpenCV 被忽略 int BorderConst[4]; // 同上 char *imageDataOrigin; // 指针指向一个不同的图像数据结构（不是必须排列的），是为了纠正图像内存分配准备的 &#125; IplImage; Mat对象使用基本概念通道把图像分解成一个或多个颜色成分！ 单通道：一个像素点只需一个数值表示，只能表示灰度，0为黑色 三通道：RGB模式，把图像分为红绿蓝三个通道，可以表示彩色，全0表示黑色 四通道：在RGB基础上加上alpha通道，表示透明度，alpha=0表示全透明 双通道：双通道图像不常见，通常在程序处理中会用到，如傅里叶变换，可能会用到，一个通道为实数，一个通道为虚数，主要是编程方便 通过画图板的各种格式可以保存出不同的类型 深度深度即位数（比特数） 位深：一个像素点所占的总位数，也叫像素深度、图像深度等 位深 = 通道数 × 每个通道所占位数 256色图：n位的像素点可以表示2^n种颜色，称2^n色图，n=8时为256色图 8位RGB与8位图：前者的位数指每个通道所占的位数，后者指整个像素点共占的位数 8位RGB是一个24位图，也称为真彩 Mat对象构造函数 Mat()无参数构造方法 Mat(int rows, int cols, int type)创建行数为 rows，列数为 col，类型为 type 的图像 Mat(Size size, int type)创建大小为 size，类型为 type 的图像 Mat(int rows, int cols, int type, const Scalar&amp; s)创建行数为 rows，列数为 col，类型为 type 的图像，并将所有元素初始化为值s Mat(Size size, int type, const Scalar&amp; s)创建大小为 size，类型为 type 的图像，并将所有元素初始化为值 s Mat(const Mat&amp; m)将m赋值给新创建的对象，此处不会对图像数据进行复制，m和新对象共用图像数据，属于浅拷贝 Mat(int rows, int cols, int type, void* data, size_t step=AUTO_STEP)创建行数为rows，列数为col，类型为type的图像，此构造函数不创建图像数据所需内存，而是直接使用data所指内存，图像的行步长由 step指定 Mat(Size size, int type, void* data, size_t step=AUTO_STEP)创建大小为size，类型为type的图像，此构造函数不创建图像数据所需内存，而是直接使用data所指内存，图像的行步长由step指定 Mat(const Mat&amp; m, const Range&amp; rowRange, const Range&amp; colRange)创建的新图像为m的一部分，具体的范围由rowRange和colRange指定，此构造函数也不进行图像数据的复制操作，新图像与m共用图像数据 Mat(const Mat&amp; m, const Rect&amp; roi)创建的新图像为m的一部分，具体的范围roi指定，此构造函数也不进行图像数据的复制操作，新图像与m共用图像数据 例如：Mat M(2,2,CV_8UC3, Scalar(0,0,255))这些构造函数中，很多都涉及到类型type。type可以是CV_8UC1，CV_16SC1，CV_64FC4 等其中前两个参数分别表示行(row)跟列(column)、第三个CV_8UC3中的8表示每个通道占8位、U表示无符号、C表示Char类型、3表示通道数目是3，第四个参数是向量表示初始化每个像素值是多少，向量长度对应通道数目一致如果你需要更多的通道数，需要用宏 CV_8UC(n) ，例如：Mat M(3,2, CV_8UC(5)) 创建行数为 3，列数为 2，通道数为 5 的图像。 Mat对象常用方法 部分复制：一般情况下只会复制Mat对象的头和指针部分，不会复制数据部分 12Mat A= imread(\"XXX\");Mat B(A) // 只复制头信息，浅拷贝 完全复制：如果想把Mat对象的头部和数据部分一起复制，可以通过如下两个API实现 123Mat F = A.clone(); Mat G;A.copyTo(G); Mat对象分为头部与数据部分，赋值操作和拷贝构造函数只会复制头部，要想用深拷贝只能使用clone() 、 copyTo(Mat dst,int type) ! 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include &lt;iostream&gt;#include &lt;opencv2\\opencv.hpp&gt;using namespace cv;using namespace std;int main(int argc,char **argv)&#123; Mat src = imread(\"C:/Users/Tim/Desktop/Image/a.jpg\"); if (src.empty()) &#123; std::cout &lt;&lt; \"load image filed\" &lt;&lt; std::endl; return -1; &#125; namedWindow(\"input window\",CV_WINDOW_AUTOSIZE); imshow(\"input window\", src); //通过ctreat函数创建 Mat m_c; m_c.create(4, 4, CV_8UC2); cout &lt;&lt; m_c &lt;&lt; endl; //定义了一个dst对象的时候只是创建了Mat对象的头部 Mat dst; dst = Mat(src.size(), src.type()); dst = Scalar(127, 0, 255); namedWindow(\"output window\", CV_WINDOW_AUTOSIZE); imshow(\"output window\", dst); //都是深拷贝 dst = src.clone(); src.copyTo(dst); //获取通道数 cvtColor(src, dst, CV_BGR2GRAY); cout &lt;&lt; \"src.channels():\" &lt;&lt; src.channels() &lt;&lt; endl; cout &lt;&lt; \"dst.channels():\" &lt;&lt; dst.channels() &lt;&lt; endl; //获取首行像素指针 const uchar* firstRow = dst.ptr&lt;uchar&gt;(0); //获取行像素、列像素 int row = dst.rows; int col = dst.cols; //创建行数为 rows，列数为 col，类型为 type 的图像，并将所有元素初始化 Mat m(5, 5, CV_8UC3, Scalar(0, 0, 255)); cout &lt;&lt; m &lt;&lt; endl; namedWindow(\"smail\", CV_WINDOW_AUTOSIZE); imshow(\"smail\", m); //定义小数组 Mat C = (Mat_&lt;double&gt;(3, 3) &lt;&lt; 0, -1, 0, -1, 5, -1, 0, -1, 0); cout &lt;&lt; C &lt;&lt; endl; waitKey(0); return 0;&#125; 这是创建的5*5的矩阵 关于Scalar查看源码opencv3源码， 发现Scalar做成了模板类，其中有如下构造函数：可以看到，Scalar是一个由长度为4的数组作为元素构成的结构体，Scalar最多可以存储四个值，没有提供的值默认是0。Scalar常用的使用场景如下： 1Mat M(7,7,CV_32FC2,Scalar(1,3)); 上面的代码表示：创建一个2通道，且每个通道的值都为（1, 3），深度为32，7行7列的图像矩阵。CV_32F表示每个元素的值的类型为32位浮点数，C2表示通道数为2，Scalar（1,3）表示对矩阵每个元素都赋值为（1, 3），第一个通道中的值都是1，第二个通道中的值都是3.dst = Scalar(127, 0, 255) 上述代码中颜色转换过来就是这样的： Mat对象使用-四个要点 输出图像的内存是自动分配的 使用OpenCV的C++接口，不需要考虑内存分配问题 赋值操作和拷贝构造函数只会复制头部分 使用clone与copyTo两个函数实现数据完全复制 绘制形状与文字Point与ScalarPoint表示2D平面上一个点，其成员就是 x,y 坐标 Scalar表示四个元素的向量，表示 RGB 三个通道 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114#include &lt;iostream&gt;#include &lt;opencv2\\opencv.hpp&gt;using namespace cv;using namespace std;Mat bgImage;void MyLines();void MyRectangle();void MyEllipse();void MyCircle();void MyPolyon();void Random();int main(int argc, char **argv)&#123; bgImage = imread(\"C:\\\\Users\\\\Tim\\\\Desktop\\\\Image\\\\a.jpg\"); if (bgImage.empty())&#123; cout &lt;&lt; \"load image filed...\" &lt;&lt; endl; return -1; &#125; MyLines(); MyRectangle(); MyEllipse(); MyCircle(); MyPolyon(); //绘制文字 //背景图、文字、起始点、字体（前提是系统必须支持设定的字体）、字体放大倍数、颜色、线粗、线条类型 putText(bgImage, \"HelloWorld\", Point(30, 50), CV_FONT_HERSHEY_COMPLEX, 2.0, Scalar(0, 0, 255), 2, 8); namedWindow(\"src_window\", CV_WINDOW_AUTOSIZE); imshow(\"src_window\", bgImage); //Random(); waitKey(0); return 0;&#125;//绘制线条void MyLines()&#123; Point p1 = Point(0, 0); Point p2 = Point(410, 624); Scalar color = Scalar(0, 0, 255); //背景图、直线两头坐标、颜色、线粗、 //line(bgImage, p1, p2, color, 4, LINE_8); //line(bgImage, p1, p2, color, 4, LINE_4); line(bgImage, p1, p2, color, 4, LINE_AA);//LINE_AA是无锯齿&#125;//绘制矩形void MyRectangle()&#123; //起始坐标点，宽和高 Rect rect = Rect(120, 20, 200, 200); Scalar color = Scalar(0, 255, 0); rectangle(bgImage, rect, color, 2, LINE_AA);&#125;//绘制椭圆void MyEllipse()&#123; Scalar color = Scalar(255, 0, 0); //背景图、中心点坐标、长轴和短轴、椭圆的倾斜度、（0-360就是绘制完整椭圆）、颜色、线粗、无锯齿 ellipse(bgImage, Point(bgImage.cols / 2, bgImage.rows / 2), Size(bgImage.cols / 4, bgImage.rows / 8), 45, 0, 180, color, 2, LINE_AA);&#125;//绘制圆void MyCircle()&#123; Scalar color = Scalar(255, 255, 0); //背景图、圆心、半径 Point center = Point(bgImage.cols / 2, bgImage.rows / 2); circle(bgImage, center, 150, color, 2, LINE_8);&#125;//绘制多边形void MyPolyon()&#123; //定义好多边形顶点的二维数组 Point pts[1][5]; pts[0][0] = Point(100, 100); pts[0][1] = Point(120, 180); pts[0][2] = Point(220, 200); pts[0][3] = Point(150, 80); pts[0][4] = Point(100, 100); const Point* ppts[] = &#123; pts[0] &#125;; int npt[] = &#123; 5 &#125;; Scalar color = Scalar(0, 255, 255); fillPoly(bgImage, ppts, npt, 1, color, 8);&#125;//绘制随机线条void Random()&#123; RNG rng(12345);//设置随机种子 Point pt1; Point pt2; Mat bg = Mat::zeros(bgImage.size(), bgImage.type()); namedWindow(\"random\", CV_WINDOW_AUTOSIZE); for (size_t i = 0; i &lt;10000; i++) &#123; //确保随机数的范围 pt1.x = rng.uniform(0, bg.cols); pt2.x = rng.uniform(0, bg.cols); pt1.y = rng.uniform(0, bg.rows); pt2.y = rng.uniform(0, bg.rows); waitKey(100); Scalar color = Scalar(rng.uniform(0, 255), rng.uniform(0, 255), rng.uniform(0, 255)); line(bg, pt1, pt2, color, 1, 8); imshow(\"random\", bg); &#125;&#125; 随机线条","updated":"2020-03-13T03:06:29.686Z","categories":[{"name":"图像处理","slug":"图像处理","permalink":"https://zouchanglin.cn/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}],"tags":[{"name":"openCV","slug":"openCV","permalink":"https://zouchanglin.cn/tags/openCV/"}]},{"title":"图像亮度对比度调整","date":"2018-08-29T10:09:30.000Z","path":"2018/08/29/图像亮度对比度调整/","text":"读写像素读一个GRAY像素点的像素值（CV_8UC1）1234//方式一Scalar intensity = img.at&lt;uchar&gt;(y, x); //方式二Scalar intensity = img.at&lt;uchar&gt;(Point(x, y)); 读一个BGR像素点的像素值12345678910//读取整形Vec3b intensity = src.at&lt;Vec3b&gt;(x, y);int b = intensity [0];int g = intensity [1];int r = intensity [2];//读取浮点型Vec3f intensity = img.at&lt;Vec3f&gt;(y, x); float blue = intensity.val[0]; float green = intensity.val[1]; float red = intensity.val[2]; Vec3b与Vec3F Vec3b对应三通道的顺序是blue、green、red的uchar类型数据。 Vec3f对应三通道的float类型数据 把CV_8UC1转换到CV32F1实现如下：src.convertTo(dst, CV_32F); 修改像素值灰度图像img.at(y, x) = 128; RGB三通道图像img.at(y,x)[0]=128; // blueimg.at(y,x)[1]=128; // greenimg.at(y,x)[2]=128; // red 空白图像赋值img = Scalar(0); ROI选择Rect r(10, 10, 100, 100);Mat smallImg = img(r); 获取灰度图像素点接下来用灰度图的反色作为示例： 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;iostream&gt;#include &lt;opencv2\\opencv.hpp&gt;using namespace cv;using namespace std;int main(int argc, char **argv)&#123; Mat src = imread(\"C:\\\\Users\\\\Tim\\\\Desktop\\\\Image\\\\a.jpg\"); if (src.empty()) &#123; cout &lt;&lt; \"load image filed...\" &lt;&lt; endl; return -1; &#125; namedWindow(\"src_window\", CV_WINDOW_AUTOSIZE); imshow(\"src_window\", src); Mat src_gray; cvtColor(src, src_gray, CV_BGR2GRAY); namedWindow(\"src_gray_window\", CV_WINDOW_AUTOSIZE); imshow(\"src_gray_window\", src_gray); int height = src_gray.rows; int width = src_gray.cols; //单通道 for (int row = 0; row &lt; height;row++)&#123; for (int col = 0; col &lt; width; col++) &#123; //给每一个像素点取反 int gray = src_gray.at&lt;uchar&gt;(row, col); src_gray.at&lt;uchar&gt;(row, col) = 255 - gray; &#125; &#125; //展示灰度图的反色图 namedWindow(\"gray_window\",CV_WINDOW_AUTOSIZE); imshow(\"gray_window\", src_gray); waitKey(0); return 0;&#125; 获取三通道图像素点对于三通道的像素点获取，以获取原图的反色图为例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include &lt;iostream&gt;#include &lt;opencv2\\opencv.hpp&gt;using namespace cv;using namespace std;int main(int argc, char **argv)&#123; Mat src = imread(\"C:\\\\Users\\\\Tim\\\\Desktop\\\\Image\\\\a.jpg\"); if (src.empty()) &#123; cout &lt;&lt; \"load image filed...\" &lt;&lt; endl; return -1; &#125; namedWindow(\"src_window\", CV_WINDOW_AUTOSIZE); imshow(\"src_window\", src); Mat src_gray; cvtColor(src, src_gray, CV_BGR2GRAY); int height = src_gray.rows; int width = src_gray.cols; //三通道图像初始化 Mat dst; dst.create(src.size(), src.type()); height = src.rows; width = src.cols; //获取通道数 int cn = src.channels(); for (int row = 0; row &lt; height; row++) &#123; for (int col = 0; col &lt; width; col++) &#123; if (cn == 1) &#123; //单通道图的处理，这块也就是上面的代码 int gray = src_gray.at&lt;uchar&gt;(row, col); src_gray.at&lt;uchar&gt;(row, col) = 255 - gray; &#125; else if(cn == 3)&#123; //从原图中读取 int b = src.at&lt;Vec3b&gt;(row, col)[0]; int g = src.at&lt;Vec3b&gt;(row, col)[1]; int r = src.at&lt;Vec3b&gt;(row, col)[2]; //写入目标图像 dst.at&lt;Vec3b&gt;(row, col)[0] = 255 - b; dst.at&lt;Vec3b&gt;(row, col)[1] = 255 - g; dst.at&lt;Vec3b&gt;(row, col)[2] = 255 - r; &#125; &#125; &#125; //使用API进行操作 //bitwise_not(src, dst) namedWindow(\"gray_window\",CV_WINDOW_AUTOSIZE); imshow(\"gray_window\", dst); waitKey(0); return 0;&#125; 上面的反色，使用API也可以做到：bitwise_not(src, dst) 三通道图转为灰度图的其他方式 通过把原图中的像素点设定为像素点中的最大或者最小值也是可以达到转换为灰度图的效果！ 图像混合理论-线性混合操作 相关API (addWeighted)参数1：输入图像Mat – src1参数2：输入图像src1的alpha值参数3：输入图像Mat – src2参数4：输入图像src2的alpha值参数5：gamma值参数6：输出混合图像注意点：两张图像的大小和类型必须一致才可以 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;iostream&gt;#include &lt;opencv2\\opencv.hpp&gt;using namespace cv;using namespace std;int main(int argc, char **argv)&#123; Mat src1 = imread(\"C:\\\\Users\\\\Tim\\\\Desktop\\\\Image\\\\a.jpg\"); Mat src2 = imread(\"C:\\\\Users\\\\Tim\\\\Desktop\\\\Image\\\\b.jpg\"); Mat ret;//输出图像 if (src1.empty() || src2.empty()) &#123; cout &lt;&lt; \"load image filed...\" &lt;&lt; endl; return -1; &#125; namedWindow(\"src1\", CV_WINDOW_AUTOSIZE); imshow(\"src1\", src1); namedWindow(\"src2\", CV_WINDOW_AUTOSIZE); imshow(\"src2\", src2); double alpha = 0.5; //两张图片大小一致、类型一致 if (src1.rows == src2.rows &amp;&amp; src1.cols == src2.cols &#123; addWeighted(src1, alpha, src2, (1.0 - alpha), 0, ret); //add(src1, src2, dst, Mat()); //multiply(src1, src2, dst, 1.0); namedWindow(\"ret\", CV_WINDOW_AUTOSIZE); imshow(\"ret\", ret); &#125; else &#123; cout &lt;&lt; \"Images vary in size or type!\" &lt;&lt; endl; return -1; &#125; waitKey(0); return 0;&#125; 这是两张图片各占50%的权重合成的图像： 对比度和亮度调整图像亮度本质上图像中每个像素的亮度，每个像素的亮度本质上RGB值的大小，RGB值为0，则像素点为黑色，RGB都为255时像素点最亮，为白色。对比度则是不同像素点之间的差值，差值越大，对比度越明显。图像变换可以看作如下： 像素变换 – 点操作 邻域操作 – 区域 调整图像亮度和对比度属于像素变换-点操作，搞清楚原理下面开始撸代码: 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;iostream&gt;#include &lt;opencv2\\opencv.hpp&gt;using namespace cv;using namespace std;int main(int argc, char **argv)&#123; Mat src = imread(\"C:\\\\Users\\\\Tim\\\\Desktop\\\\Image\\\\a.jpg\"); Mat dst; if (src.empty()) &#123; cout &lt;&lt; \"load image filed...\" &lt;&lt; endl; return -1; &#125; char *input_win = \"input_win\"; namedWindow(input_win, CV_WINDOW_AUTOSIZE); imshow(\"input_win\", src); int height = src.rows; int wight = src.cols; //构建与src同等大小的空白图 dst = Mat::zeros(src.size(), src.type()); //差异倍数 float alpha = 0.4f; //增益变量 float beta = 0.0f; for (int row = 0; row &lt; height; row++) &#123; for (int col = 0; col &lt; wight; col++) &#123; //根据通道数目来判断处理方式 if (src.channels() == 3)&#123; float b = src.at&lt;Vec3b&gt;(row, col)[0]; float g = src.at&lt;Vec3b&gt;(row, col)[1]; float r = src.at&lt;Vec3b&gt;(row, col)[2]; dst.at&lt;Vec3b&gt;(row, col)[0] = saturate_cast&lt;uchar&gt;(b*alpha + beta); dst.at&lt;Vec3b&gt;(row, col)[1] = saturate_cast&lt;uchar&gt;(g*alpha + beta); dst.at&lt;Vec3b&gt;(row, col)[2] = saturate_cast&lt;uchar&gt;(r*alpha + beta); &#125; else if (src.channels() == 1) &#123; float v = src.at&lt;uchar&gt;(row, col); dst.at&lt;uchar&gt;(row, col) = saturate_cast&lt;uchar&gt;(v*alpha + beta); &#125; &#125; &#125; //输出图像 char *output_win = \"output_win\"; namedWindow(output_win, CV_WINDOW_AUTOSIZE); imshow(output_win, dst); waitKey(0); return 0;&#125; 从代码中可以看出：如果将值乘上差异倍数，那么像素点之间的差异倍数（alpha）越大，像素点之间的差异也就越大，这样导致的直接结果就是图像的对比度增强，增益变量越大，那么像素本身的值在增大，导致的结果就是亮度增强 演示效果 重要的API Mat new_image = Mat::zeros(image.size(), image.type()); 创建一张跟原图像大小和类型一致的空白图像、像素值初始化为0 saturate_cast(value)确保值大小范围为0~255之间 Mat.at(y,x)[index]=value给每个像素点每个通道赋值","updated":"2020-03-13T03:06:29.728Z","categories":[{"name":"图像处理","slug":"图像处理","permalink":"https://zouchanglin.cn/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}],"tags":[{"name":"openCV","slug":"openCV","permalink":"https://zouchanglin.cn/tags/openCV/"}]},{"title":"OpenCV矩阵掩模","date":"2018-08-27T10:09:30.000Z","path":"2018/08/27/OpenCV矩阵掩模/","text":"OpenCV是计算机视觉开源库，主要算法涉及图像处理和机器学习相关方法。是Intel公司贡献出来的，俄罗斯工程师贡献大部分C/C++代码。官网：https://opencv.org/ 从这里 https://opencv.org/releases.html 你可以下载到自己想要的版本！ 环境搭建本人使用的时VisualStudio2015+OpenCV3.4.1-vc14(vc14对应的VisualStudio版本就是VS2015)，配置环境变量，根据自己的路径来配置 测试环境新建VS空项目，注意是改为X64的解决方案，因为直接下载的exe实际上就是一个自解压文件，里面的库都是在X64的环境下编译好的，如何编译自己的VS对应的OpenCV参考：https://www.bilibili.com/video/av17968786，根据自己的路径配置即可！ 包括头文件D:\\opencv3.4\\build\\includeD:\\opencv3.4\\build\\include\\opencvD:\\opencv3.4\\build\\include\\opencv2库文件D:\\opencv3.4\\build\\x64\\vc14\\lib链接器-附加依赖项opencv_world310d.lib 注意自己的图片路径，如果其他代码与图片所示一致，那么恭喜你的OpenCV开发环境搭建OK了！ 图像有关的API加载图像cv::imreadimread功能是加载图像文件成为一个Mat对象(什么是Mat对象接下来的文章中将会说到)第一个参数表示图像文件名称(路径)第二个参数，表示加载的图像是什么类型，支持常见的三个参数值 IMREAD_UNCHANGED (&lt;0) 表示加载原图，不做任何改变 IMREAD_GRAYSCALE ( 0)表示把原图作为灰度图像加载进来 IMREAD_COLOR (&gt;0) 表示把原图作为RGB图像加载进来 注意：OpenCV支持JPG、PNG、TIFF等常见格式图像文件加载例如加载灰度图： 显示图像 cv::namedWindos 与cv::imshownamedWindos功能是创建一个OpenCV窗口，它是由OpenCV自动创建与释放，无需手动销毁它。常见用法namedWindow(“窗口标题”, WINDOW_AUTOSIZE) WINDOW_AUTOSIZE会自动根据图像大小，显示窗口大小，不能人为改变窗口大小 WINDOW_NORMAL,跟QT集成的时候会使用，允许修改窗口大小。 imshow根据窗口名称显示图像到指定的窗口上去，第一个参数是窗口名称，第二参数是Mat对象 修改图像 (cv::cvtColor)cvtColor的功能是把图像从一个彩色空间转换到另外一个色彩空间，有三个参数，第一个参数表示源图像、第二参数表示色彩空间转换之后的图像、第三个参数表示源和目标色彩空间如：COLOR_BGR2HLS 、COLOR_BGR2GRAY 等这是原图转换为COLOR_BGR2HLS色彩空间： 这是原图转换为COLOR_BGR2GRAY 色彩空间： 有时容易出现这样的错误 这个问题一般是由于将已经是灰度图的图片继续转为灰度图时引起的，所以在读取图片转为灰度色彩空间的时候要保证读取到的原图是彩色图像，如果使用灰度图的参数读取那么再次灰度化的时候就会抱这种错误！ 保存图像(cv::imwrite)保存图像文件到指定目录路径，只有8位、16位的PNG、JPG、Tiff文件格式而且是单通道或者三通道的BGR的图像才可以通过这种方式保存，保存PNG格式的时候可以保存透明通道的图片可以指定压缩参数！ 12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;opencv2\\opencv.hpp&gt;#include &lt;math.h&gt;using namespace std;using namespace cv;int main(int argc, char* argv[])&#123; //加载图像，默认IMREAD_UNCHANGED //Mat src = imread(\"C:/Users/Tim/Desktop/Image/a.jpg\"); Mat src = imread(\"C:/Users/Tim/Desktop/Image/a.jpg\", IMREAD_GRAYSCALE); //以灰度图读取 //判断是否加载成功 if (src.empty()) &#123; cout &lt;&lt; \"load image filed...\" &lt;&lt; endl; return -1; &#125; //新建一个OpenCV窗口，大小根据图片自动调节 namedWindow(\"Test Opencv\", CV_WINDOW_AUTOSIZE); //显示图像 imshow(\"Test Opencv\", src); namedWindow(\"Output Window\", CV_WINDOW_AUTOSIZE); Mat outImg; //色彩空间装换 //cvtColor(src, outImg, CV_BGR2HLS); cvtColor(src, outImg, COLOR_BGR2GRAY); imshow(\"Output Window\", outImg); //将Mat对象写入文件 imwrite(\"C:/Users/Tim/Desktop/Image/a2.png\",outImg); waitKey(0); return 0;&#125; 矩阵的掩膜操作掩膜操作解释所谓掩膜其实就是一个矩阵，然后根据这个矩阵重新计算图片中像素的值，掩模也叫作kernal下图就是通过掩模提高图片对比度的计算公式，根据掩模把图片的对比度提高： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;opencv2\\opencv.hpp&gt;#include &lt;iostream&gt;#include &lt;math.h&gt;using namespace cv;int main(int argc,char* argv[])&#123; Mat src; Mat dst; src = imread(\"C:/Users/Tim/Desktop/Image/e.jpg\"); if (!src.data) &#123; std::cout &lt;&lt; \"load iamge filed...\" &lt;&lt; std::endl; return -1; &#125; namedWindow(\"input image\", CV_WINDOW_AUTOSIZE); imshow(\"input image\", src); //获取图像的列数,一定不要忘记图像的通道数 int cols = (src.cols-1) * src.channels(); int rows = src.rows; //获取通道数目 int offsetx = src.channels(); //生成一个和源图像大小相等类型相同的全0矩阵 dst = Mat::zeros(src.size(),src.type()); //获取起始时间 double start = getTickCount(); for (int row = 1; row &lt; (rows - 1); row++) &#123; //获取每一个通道的像素指针 const uchar* prev = src.ptr&lt;uchar&gt;(row - 1); const uchar* cur = src.ptr&lt;uchar&gt;(row); const uchar* next = src.ptr&lt;uchar&gt;(row + 1); uchar* output = dst.ptr&lt;uchar&gt;(row); for (int col = offsetx; col &lt; cols; col++)&#123; //output[col] = (5 * cur[col] - (cur[col - offsetx] + cur[col + offsetx] + prev[col] + next[col]));//注意像素值范围在0~255之间！ output[col] = saturate_cast&lt;uchar&gt;(5 * cur[col] - (cur[col - offsetx] + cur[col + offsetx] + prev[col] + next[col])); &#125; &#125; //获取处理时间 double run_time = (cvGetTickCount() - start)/getTickFrequency(); std::cout &lt;&lt; run_time&lt;&lt; std::endl; namedWindow(\"output image\", CV_WINDOW_AUTOSIZE); imshow(\"output image\", dst); waitKey(0); return 0;&#125; 获取图像像素指针Mat.ptr (int i = 0)获取像素矩阵的指针，索引i表示第几行，从0开始计行数。获得当前行指针const uchar* current= myImage.ptr(row );获取当前像素点P(row, col)的像素值 p(row, col) =current[col] saturate_cast &lt; uchar &gt;123saturate_cast&lt;uchar&gt;（-100），返回0saturate_cast&lt;uchar&gt;（288），返回255saturate_cast&lt;uchar&gt;（100），返回100 这个函数的功能是确保RGB值得范围在0255之间!如果不使用这个函数进行转换，将会得到如下所示的效果，这是由于部分像素点在经过公式转化之后范围已经不再0255之间，所以需要用到此函数： 经过校正之后的处理效果： filter2D实现掩膜参数一：源图像参数二：最终输出的图像,需要再定义一个Mat类变量参数三：为像素深度，两个像素深度一定要相同，否则出错src.depth() 或者 -1参数四：掩膜 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;opencv2\\opencv.hpp&gt;#include &lt;iostream&gt;#include &lt;math.h&gt;using namespace cv;int main(int argc,char* argv[])&#123; Mat src; Mat dst; src = imread(\"C:/Users/Tim/Desktop/Image/e.jpg\"); if (!src.data) &#123; std::cout &lt;&lt; \"load iamge filed...\" &lt;&lt; std::endl; return -1; &#125; namedWindow(\"input image\", CV_WINDOW_AUTOSIZE); imshow(\"input image\", src); //获取图像的列数,一定不要忘记图像的通道数 int cols = (src.cols-1) * src.channels(); int rows = src.rows; //获取通道数目 int offsetx = src.channels(); //生成一个和源图像大小相等类型相同的全0矩阵 dst = Mat::zeros(src.size(),src.type()); //获取起始时间 double start = getTickCount(); //定义一个掩模 //Mat kernal = (Mat_&lt;char&gt;(3, 3) &lt;&lt; 0, 1, 0, -1, 5, -1, 0, -1, 0);//对比度增强 Mat kernal = (Mat_&lt;char&gt;(3, 3) &lt;&lt; 0, -1, 0, -1, 6, -1, 0, -1, 0);//提高锐度 filter2D(src, dst, src.depth(), kernal); //获取处理时间 double run_time = (cvGetTickCount() - start)/getTickFrequency(); std::cout &lt;&lt; run_time&lt;&lt; std::endl; namedWindow(\"output image\", CV_WINDOW_AUTOSIZE); imshow(\"output image\", dst); waitKey(0); return 0;&#125; 掩模的主要应用 实现图像对比度调整 提取感兴趣区,用预先制作的感兴趣区掩模与待处理图像相乘,得到感兴趣区图像,感兴趣区内图像值保持不变,而区外图像值都为0掩膜是一种图像滤镜的模板，实用掩膜经常处理的是遥感图像。当提取道路或者河流，或者房屋时，通过一个n*n的矩阵来对图像进行像素过滤，然后将我们需要的地物或者标志突出显示出来。这个矩阵就是一种掩膜 屏蔽作用,用掩模对图像上某些区域作屏蔽,使其不参加处理或不参加处理参数的计算,或仅对屏蔽区作处理或统计 结构特征提取,用相似性变量或图像匹配方法检测和提取图像中与掩模相似的结构特征 特殊形状图像的制作 各种掩膜的作用3x3 邻域平均 全是1 3x3高斯均值滤波器filters(0) = 1: filters(1) = 2:filters(2) = 1filters(3) = 2: filters(4) = 4: filters(5) = 2filters(6) = 1: filters(7) = 2: filters(8) = 1 拉普拉斯1型滤波器 高通边缘检测器掩膜filters(0) = -1: filters(1) = 0: filters(2) = -1filters(3) = 0: filters(4) = 4: filters(5) = 0filters(6) = -1: filters(7) = 0: filters(8) = -1 锐化 (中锐化：filters(4) = 5 ， 高锐化：filters(4) = 6)filters(0) = 0: filters(1) = -1: filters(2) = 0filters(3) = -1: filters(4) = 6: filters(5) = -1filters(6) = 0: filters(7) = -1: filters(8) = 0 垂直掩膜filters(0) = 3: filters(1) = -6: filters(2) = 3filters(3) = 3: filters(4) = -6: filters(5) = 3filters(6) = 3: filters(7) = -6: filters(8) = 3 水平掩膜filters(0) = 3: filters(1) = 3: filters(2) = 3filters(3) = -6: filters(4) = -6: filters(5) = -6filters(6) = 3: filters(7) = 3: filters(8) = 3 对角线掩膜filters(0) = 3: filters(1) = 3: filters(2) = -6filters(3) = 3: filters(4) = -6: filters(5) = 3filters(6) = -6: filters(7) = 3: filters(8) = 3 高斯滤镜5x5f(0) = 1: f(1) = 4: f(2) = 6: f(3) = 4: f(4) = 1f(5) = 4: f(6) = 16: f(7) = 24: f(8) = 16: f(9) = 4f(10) = 6: f(11) = 24: f(12) = 36: f(13) = 24: f(14) = 6f(15) = 4: f(16) = 16: f(17) = 24: f(18) = 16: f(19) = 4f(20) = 1: f(21) = 4: f(22) = 6: f(23) = 4: f(24) = 1 根据掩模的设定不同效果也就不同，比如上述代码中一个掩模是增强对比度，一个掩模是增强锐度： 注意代码中的如何计算图片的处理时间，以及如何生成全0矩阵！","updated":"2020-03-13T03:06:29.701Z","categories":[{"name":"图像处理","slug":"图像处理","permalink":"https://zouchanglin.cn/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}],"tags":[{"name":"openCV","slug":"openCV","permalink":"https://zouchanglin.cn/tags/openCV/"}]},{"title":"Validate做表单校验","date":"2018-08-23T10:09:30.000Z","path":"2018/08/23/Validate做表单校验/","text":"用户在注册的时候，会有个表单页面，然后有些选项是必填的，有些要填的内容是有规范的，这些都要在用户提交之前进行验证才行，如果不符合要求，需要在右边显示一个友好的提示，让用户修改。此时Validate做表单校验的特效药就发挥作用了！如果我们自己来写js代码的话，那么可就显得有些冲复造轮子了，所以现在Validate这个库可以很好的帮我们省略自己去写js校验代码，Validate这个库属于jQuery的插件库！ Validate下载Validate的官网里面有示例代码，看这个就可以：https://jqueryvalidation.org 官网的介绍是：这个jQuery插件简化了客户端表单验证，同时还提供了大量的自定义选项。如果您正在从头开始构建校验代码，而且当您尝试将某些内容集成到具有大量现有代码的现有应用程序中时，它将是一个不错的选择。该插件捆绑了一组有用的验证方法，包括URL和电子邮件验证，同时提供API来编写自己的方法。所有捆绑方法都带有英语的默认错误消息，并翻译成其他37种语言。由此可见它的强大之处了！ Validate特点 内置验证规则：拥有必填、数字、email、url和信用卡号码等19类内置验证规则 自定义验证规则：可以很方便的自定义验证规则 简单强大的验证信息提示：默认了验证信息提示，并提供自定义覆盖默认提示信息的功能 实时验证：可以通过keyup或bulr事件触发验证，而不仅仅在表单提交的时候验证 Validate使用1234&lt;!--注意导入的顺序--&gt;&lt;script type=\"text/javascript\" src=\"js/jquery-1.8.3.js\" &gt;&lt;/script&gt;&lt;script type=\"text/javascript\" src=\"js/jquery.validate.min.js\" &gt;&lt;/script&gt;&lt;script type=\"text/javascript\" src=\"js/messages_zh.js\" &gt;&lt;/script&gt; 这是我的简易示例表单： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Validate使用示例&lt;/title&gt; &lt;!--注意导入的顺序--&gt; &lt;script type=\"text/javascript\" src=\"js/jquery-1.8.3.js\" &gt;&lt;/script&gt; &lt;script type=\"text/javascript\" src=\"js/jquery.validate.min.js\" &gt;&lt;/script&gt; &lt;script type=\"text/javascript\" src=\"js/messages_zh.js\" &gt;&lt;/script&gt; &lt;script&gt; $(function()&#123; $(\"#checkForm\").validate(&#123; rules:&#123; username:&#123; required:true,//不能为空 minlength:5,//长度必须大于等于5 maxlength:10,//最大长度是10 &#125;, password:&#123; required:true, digits:true, minlength:6, &#125;, password2:&#123; required:true, digits:true, equalTo:\"[name='password']\" &#125; &#125;, messages:&#123; username:&#123; required:\"用户名不能为空\", minlength:\"用户名不得少于5个字符\", maxlength:\"用户名最多10个字符\" &#125;, password:&#123; required:\"密码不能为空\", digits:\"密码必须是整数\", minlength:\"密码不得少于6位\" &#125;, password2:&#123; required:\"确认密码不能为空\", equalTo:\"两次输入密码不一致\" &#125; &#125; &#125;); &#125;); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=\"#\" id=\"checkForm\"&gt; &lt;table border=\"0px\"&gt; &lt;tr&gt; &lt;td&gt;用户名&lt;/td&gt; &lt;td&gt;&lt;input type=\"text\" name=\"username\"/&gt;&lt;br&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;密码&lt;/td&gt; &lt;td&gt;&lt;input type=\"password\" name=\"password\"/&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;确认密码&lt;/td&gt; &lt;td&gt;&lt;input type=\"password\" name=\"password2\"/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;邮箱&lt;/td&gt; &lt;td&gt;&lt;input type=\"email\" name=\"email\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=\"submit\" value=\"提交\" /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 下面是校验效果 重点就在于设定校验的规则，常见的属性： 属性相关设定 属性 释义 required:true 必须输入的字段。 remote:”check.php” 使用 ajax 方法调用 check.php 验证输入值。 email:true 必须输入正确格式的电子邮件。 url:true 必须输入正确格式的网址。 date:true 必须输入正确格式的日期。日期校验 ie6 出错，慎用。 dateISO:true 必须输入正确格式的日期（ISO），例如：2009-06-23，1998/01/22。只验证格式，不验证有效性。 number:true 必须输入合法的数字（负数，小数）。 digits:true 必须输入整数。 creditcard: 必须输入合法的信用卡号。 equalTo:”#field” 输入值必须和 #field 相同。 accept: 输入拥有合法后缀名的字符串（上传文件的后缀）。 maxlength:5 输入长度最多是 5 的字符串（汉字算一个字符）。 minlength:10 输入长度最小是 10 的字符串（汉字算一个字符）。 rangelength:[5,10] 输入长度必须介于 5 和 10 之间的字符串（汉字算一个字符）。 range:[5,10] 输入值必须介于 5 和 10 之间。 max:5 输入值不能大于 5。 min:10 输入值不能小于 10","updated":"2020-03-13T03:06:29.712Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"前端","slug":"前端","permalink":"https://zouchanglin.cn/tags/%E5%89%8D%E7%AB%AF/"}]},{"title":"链表相关问题","date":"2018-07-25T10:09:30.000Z","path":"2018/07/25/链表相关问题/","text":"题目一 题目一：假设有如下的复杂链表，每个节点都有next指针和random指针，random指针是随机指向，请完成这条复杂链表的复制！ 定义复杂链表节点 123456typedef struct ComplexNode&#123; DataType _data; struct ComplexNode* _next; struct ComplexNode* _random;&#125; ComplexNode; 需要的功能 12345678//产生新节点ComplexNode* BuyComplexNode(DataType x);//打印此复杂链表void PrintComplexList(ComplexNode* plist);//复制复杂链表ComplexNode* CopyList(ComplexNode* list); 复制链表的图片分析 绿色箭头便是random指针，指向任意，那么如何来复制这条复杂链表呢？ 思路说明 1、先先断开原有链表，插入新节点，如图所示：5后面插入新节点5，4后面插入新节点4，….新节点1后面补成NULL2、 复制随机指针，这里的随机指针如何才能找到之前的random指针的指向呢？很简单，原来的节点的random指针指向的节点的next所指向的地方就是新节点的random指向应该指向的地方，这样我们就可以很方便的完成随机指针指向的复制3、 分离两条链表，这个也是比较容易的，让一个指针连续走两步，执行链接！ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677//复杂链表的复制。ComplexNode* BuyComplexNode(DataType x)&#123; ComplexNode* node = (ComplexNode*)malloc(sizeof(ComplexNode)); assert(node); node-&gt;_next = NULL; node-&gt;_random = NULL; node-&gt;_data = x; return node;&#125;//打印这个复杂链表void PrintComplexList(ComplexNode* plist)&#123; ComplexNode* cur = plist; while (cur != NULL) &#123; printf(\"%d:\", cur-&gt;_data); if (cur-&gt;_random != NULL) printf(\"(%d)--&gt;\", cur-&gt;_random-&gt;_data); else &#123; printf(\"(NULL)--&gt;\"); &#125; cur = cur-&gt;_next; &#125; printf(\"Over\\n\");&#125;//实现复制链表，返回复制后的新链表。ComplexNode* CopyList(ComplexNode* list)&#123; //1、在当前节点的后面插入一个当前节点的数据 ComplexNode* cur = list; ComplexNode* newlist = NULL; ComplexNode* cp = NULL; ComplexNode* next = list-&gt;_next; assert(list != NULL); while (cur != NULL) &#123; ComplexNode* newNode = BuyComplexNode(cur-&gt;_data); newNode-&gt;_next = next; cur-&gt;_next = newNode; cur = next; if (next != NULL) next = cur-&gt;_next; &#125; //2、调整插入节点的random指针 cur = list; cp = cur-&gt;_next; while (cur != NULL) &#123; if (cur-&gt;_random != NULL) cp-&gt;_random = cur-&gt;_random-&gt;_next; cur = cp-&gt;_next; if (cur != NULL) cp = cur-&gt;_next; &#125; //3、拆除链表 cur = list; cp = cur-&gt;_next; //确定新链表的起始位置 newlist = cp; while (cur != NULL) &#123; cur-&gt;_next = cp-&gt;_next; if (cur-&gt;_next != NULL) cp-&gt;_next = cur-&gt;_next-&gt;_next; cur = cur-&gt;_next; cp = cp-&gt;_next; &#125; return newlist;&#125; 测试用例 1234567891011121314151617181920212223242526272829303132void test()&#123; ComplexNode* plist = NULL; ComplexNode* newlist = NULL; ComplexNode* p1 = BuyComplexNode(5); ComplexNode* p2 = BuyComplexNode(4); ComplexNode* p3 = BuyComplexNode(3); ComplexNode* p4 = BuyComplexNode(2); ComplexNode* p5 = BuyComplexNode(1); plist = p1; p1-&gt;_next = p2; p2-&gt;_next = p3; p3-&gt;_next = p4; p4-&gt;_next = p5; p5-&gt;_next = NULL; p1-&gt;_random = p3; p2-&gt;_random = p1; p3-&gt;_random = NULL; p4-&gt;_random = p2; p5-&gt;_random = p4; printf(\"原表：\"); PrintComplexList(plist); newlist = CopyList(plist); printf(\"新表：\"); PrintComplexList(newlist); printf(\"原表：\"); PrintComplexList(plist);&#125; 效果演示 题目二 题目二：两个递增排序的链表，合并这两个链表并使新链表中的节点仍然是递增排序的！如下图所示： 注意问题 这是一个经常被各公司采用的面试题，最容易犯两种错误： 一是在写代码时没有对合并的过程想清楚，导致合并出来的链表不是想要的结果！ 二是代码的鲁棒性存在问题，程序一旦有特殊的链表就会崩溃 合并思路 1、先考虑考虑特殊情况，包括输入空链表，两条链表是同一条链表等等情况需要在一开始就考虑好2、确定新链表的头结点，谁小就把谁当做头结点3、使用一个指针来维护尾节点，方便尾部插入元素，每插入一个元素就向后移动4、谁小就把谁插到尾部，然后继续比较5、注意其中任何一条链表结束不要忘记另一条还没有结束的链表，要在尾部补上 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//合并两个有序链表,合并后依然有序 pList Merge(pList list1, pList list2)&#123; pList newlist = NULL; //tail表示链表的尾部 pNode tail = NULL; //先考虑特殊情况 if (list1 == list2) return NULL; if (list1 == NULL) return list2; if (list2 == NULL) return list1; //确定头节点 if (list1-&gt;data &lt; list2-&gt;data) &#123; newlist = list1; list1 = list1-&gt;next; &#125; else&#123; newlist = list2; list2 = list2-&gt;next; &#125; //两条链表中找较小的元素尾插 tail = newlist; while ((list1 != NULL) &amp;&amp; (list2 != NULL)) &#123; if (list1-&gt;data &lt; list2-&gt;data)&#123; tail-&gt;next = list1; list1 = list1-&gt;next; &#125; else&#123; tail-&gt;next = list2; list2 = list2-&gt;next; &#125; tail = tail-&gt;next; &#125; //能走到这里说明其中一条链表已经结束了 if (list1 == NULL) &#123; tail-&gt;next = list2; &#125; else&#123; tail-&gt;next = list1; &#125; return newlist;&#125; 测试用例 12345678910111213141516171819202122232425void test()&#123; int i = 0; pList plist1 = NULL; pList plist2 = NULL; pList plist3 = NULL; InitList(&amp;plist1); InitList(&amp;plist2); for (i = 0; i &lt;= 10; i++) &#123; if (i % 2 == 0) PushBack(&amp;plist1, i); else &#123; PushBack(&amp;plist2, i); &#125; &#125; PushBack(&amp;plist1, 12); PushBack(&amp;plist1, 13); PrintList(plist1); PrintList(plist2); plist3 = Merge(plist1, plist2); PrintList(plist3);&#125; 递归的方式更易理解 其实很容易看出来，只要确定了第一个节点，我们可以把它看成是第一个节点与剩下两条链表的合并结果的合并，这样的话只需要有限次的递归便可以完成这个看似复杂的问题！ 1234567891011121314151617181920212223242526//递归写法pNode Merge_R(pList list1, pList list2)&#123; pList newlist = NULL; //先考虑特殊情况 if (list1 == list2) return NULL; if (list1 == NULL) return list2; if (list2 == NULL) return list1; //确定头结点 if (list1-&gt;data &lt; list2-&gt;data) &#123; newlist = list1; list1-&gt;next = Merge_R(list1-&gt;next, list2); &#125; else &#123; newlist = list2; list2-&gt;next = Merge_R(list2-&gt;next, list1); &#125; return newlist;&#125; 考点 考察分析问题能力，考察指针操作能力，应该透彻分析问题形成清晰的思路，才能够写出正确的代码！ 考察代码的鲁棒性，需要考虑到很多特殊操作，尤其是空指针的情况和空链表的情况处理！ 题目三 题目三：现给出两个有序单链表，求出这两条链表的交集 函数实现结果演示 这个总体思路也是比较简单的，既然是有序的，那么就可以直接根据大小比对，大的节点先放着，小的往后走看能否遇到和大的节点一样的data，相等的话就直接输出，这样每一个元素都不会漏掉了！ 1234567891011121314151617181920212223//求两个有序单链表交集(差集)void UnionSet(pList list1, pList list2)&#123; if (list1 == NULL || list2 == NULL) return; while (list1 &amp;&amp; list2) &#123; if (list1-&gt;data &lt; list2-&gt;data) &#123; list1 = list1-&gt;next; &#125; else if (list1-&gt;data &gt; list2-&gt;data) &#123; list2 = list2-&gt;next; &#125; else &#123; printf(\"%d \", list1-&gt;data); list1 = list1-&gt;next; list2 = list2-&gt;next; &#125; &#125;&#125; 根据这个思路求差集也是很简单的，只需要把data不相等的节点的值输出就可以了！ 题目四 题目四：判断两个链表是否相交，若相交，求交点。（假设链表不带环） 假设这就是两条相交的链表，交点为5，现给出两条链表的头结点plist1和plist2，求出交点： 首先判断两条链表是否相交 两条相交的链表只能是 V 字形状或者 Y 字形，所以遍历最后肯定是同一个节点，由此便可以得出两条链表是否相交，还是要注意特殊情况下的判断，保证代码的鲁棒性！ 12345678910111213141516171819//判断两个链表是否相交，若相交，求交点。（假设链表不带环） int CheckCross(pList list1, pList list2)&#123; pNode end1 = NULL; pNode end2 = NULL; //任意一个为空指针就不可能相交 if (list1 == NULL || list2 == NULL) return 0; end1 = list1; end2 = list2; while (end1-&gt;next != NULL) end1 = end1-&gt;next; while (end2-&gt;next != NULL) end2 = end2-&gt;next; return end1 == end2;&#125; 题目五返回两条相交链表的交点 这个也不难，思路就是看两条链表谁更长，让较长的先走（长链表的长度-短链表的长度的绝对值）步，这样再同时开始走，直到地址相同的时候肯定就是交点！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445//相交的话返回交点pNode GetGrossNode(pList list1, pList list2)&#123; int len1 = 0; int len2 = 0; //长度之差 int gap = 0; pNode p1 = list1; pNode p2 = list2; //先求出两条链表的长度 while (p1 != NULL) &#123; len1++; p1 = p1-&gt;next; &#125; while (p2 != NULL) &#123; len2++; p2 = p2-&gt;next; &#125; gap = abs(len1 - len2); //让较长的链表先走gap步 if (len1 &gt; len2)&#123; while (gap--) list1 = list1-&gt;next; &#125; else &#123; while (gap--) list2 = list2-&gt;next; &#125; //两个链表同时开走，只要遇到地址一样的节点那么就是相交的点 while (list1 != NULL) &#123; list1 = list1-&gt;next; list2 = list2-&gt;next; if (list1 == list2) return list1; &#125; return NULL;&#125; 题目六 题目六：判断单链表是否带环？ 这个问题比较容易解决，我们只需要使用快慢指针的方式便可以解决该问题，快指针(一次走两步)、慢指针一次走一步，如果链表带环这两个指针肯定会相遇，由于快指针走了两步，所以还是要考虑到节点个数是奇数还是偶数，所以【fast-&gt;next != NULL】也是要判断的！ 12345678910111213141516//不带环返回NULL，带环返回相遇点pNode CheckCycle(pList plist)&#123; pNode fast = plist; pNode slow = plist; if (plist == NULL) return NULL; while ((fast != NULL) &amp;&amp; (((fast-&gt;next) != NULL))) &#123; fast = fast-&gt;next-&gt;next; slow = slow-&gt;next; if (fast == slow) return fast; &#125; return NULL;&#125; 题目七 问题七：求链表的环的长度，参数为相遇点** 从相遇点的下一个节点开始计数，直到重新回到相遇点即是换的长度！ 12345678910111213141516171819//求环的长度，参数为相遇点int GetCircleLength(pNode meet)&#123; pNode cur = NULL; int len = 1; assert(meet != NULL); assert(meet-&gt;next != NULL); cur = meet-&gt;next; while (cur != meet) &#123; len++; cur = cur-&gt;next; &#125; return len;&#125; 题目八 题目八：求入口点位置，参数为相遇点 这个相对前面两个来说比较难想，假设不带环的部分长度是x，从入口点到两个指针的相遇点的长度为y，环的长度为L，我们可以得到如图所示的公式 即慢指针走的步数乘以2就是快指针走的距离，我们可以得出x+y是个常数K*L（也就是k个环的长度），则x总是环的长度的倍数减去y，也就是说慢指针一个从链表的起始位置走，另一个慢指针从相遇点开始走，它们总会在入口点相遇！ 12345678910111213141516//求入口点，参数为相遇点pNode GetCycleEntryNode(pList list, pNode meetNode)&#123; pList cur = list; if (list == NULL) return NULL; if (meetNode == NULL) return NULL; while (cur != meetNode) &#123; cur = cur-&gt;next; meetNode = meetNode-&gt;next; &#125; return cur; &#125; 测试用例 123456789101112131415161718192021222324252627void test()&#123; int i = 0; pList plist = NULL; pNode pos = NULL; pNode entrance = NULL; InitList(&amp;plist); for (i = 1; i &lt;= 5; i++) &#123; PushBack(&amp;plist, i); &#125; //带环 Find(plist, 5)-&gt;next = Find(plist, 3); pos = CheckCycle(plist); if (pos != NULL)&#123; printf(\"带环，相遇点为 = %d\\n\", pos-&gt;data); printf(\"环的长度是：%d\\n\", GetCircleLength(pos)); entrance = GetCycleEntryNode(plist, pos); printf(\"环的入口点是：%d\\n\", entrance-&gt;data); &#125; else &#123; printf(\"不带环\\n\"); &#125;&#125; 题目九 题目九：查找单链表的中间节点，要求只能遍历一次链表 这个思路比较简单，利用快慢指针法即可，一个快指针【一次走两步】，一个慢指针【一次走一步】，只要快指针走到结束为止，慢指针恰好就在链表的中间！只不过快指针一次走两步，需要考虑链表元素个数为奇数个的问题，所以结束标志不只是快指针走到NULL位置，快指针的NEXT走到NULL也算是结束： 1234567891011121314pNode FindMidNode(pList head)&#123; pNode fast = head; pNode slow = head; if (head == NULL || head-&gt;next == NULL) return head; while ((fast != NULL) &amp;&amp; (fast-&gt;next != NULL)) &#123; fast = fast-&gt;next-&gt;next; slow = slow-&gt;next; &#125; return slow;&#125; 题目十 题目十：查找单链表的倒数第k个节点，要求只能遍历一次链表 这个与上面的查找中间节点的方式是一样的，也是使用两个指针只不过这次都是使用慢指针的方式，让一个指针先走k步，然后另一个指针才能开始走，直到先走的指针走到链表结尾，此时后走的指针刚好走到倒数第k个节点 1234567891011121314151617181920//查找单链表的倒数第k个节点，要求只能遍历一次链表 pNode FindLastKNode(pList *pplist, int k)&#123; pNode first = *pplist; pNode catch = *pplist; int num = 0; assert(pplist != NULL); if (*pplist == NULL) return NULL; while (first != NULL) &#123; first = first-&gt;next; if (num++ &gt;= k) &#123; catch = catch-&gt;next; &#125; &#125; return catch; &#125; 还是要注意特殊情况的处理！ 题目十一 题目十一：冒泡排序链表 1、比较相邻的元素。如果第一个比第二个大，就交换他们两个。2、对每一对相邻元素做同样的工作，从开始第一对到结尾的最后一对。在这一点，最后的元素应该会是最大的数。3、针对所有的元素重复以上的步骤，除了最后一个。4、持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 普通的数组的冒泡排序 1234567891011121314151617181920212223void fun()&#123; int arr[] = &#123; 5, 4, 3, 2, 1 &#125;; int i = 0; int j = 0; int tmp = 0; int len = sizeof(arr) / sizeof(arr[0]); for (j = 0; j &lt; len - 1; j++) &#123; for (i = 0; i &lt; len - 1 - j; i++) &#123; if (arr[i] &gt; arr[i + 1]) &#123; tmp = arr[i]; arr[i] = arr[i + 1]; arr[i + 1] = tmp; &#125; &#125; &#125; for (i = 0; i &lt; len; i++)&#123; printf(\"%d \",arr[i]); &#125;&#125; 普通数组的冒泡排序使用指针实现 12345678910111213141516171819202122232425void fun02()&#123; int arr[] = &#123; 5, 4, 3, 2, 1 &#125;; int *start = &amp;arr[0]; int *start_next = start + 1; int *end = &amp;arr[4]; int tmp = 0; while (&amp;arr[0] &lt;= end) &#123; start = &amp;arr[0]; start_next = start + 1; while (start_next &lt; end-1) &#123; if (*start &gt; *start_next) &#123; tmp = *start; *start = *start_next; *start_next = tmp; &#125; start++; start_next = start + 1; &#125; end = start; &#125;&#125; 链表的冒泡排序 这样的话根据此方法便可以对链表进行排序了!这个方法与使用指针对数组进行排序是一样的，只不过与之不停的地方在于数组要找到下一个元素的节点是非常容易的，对于链表可以使用next指针！ 12345678910111213141516171819202122232425262728293031//单链表排序（冒泡排序） void BubbleSort(pList * pplist)&#123; pNode pCur = NULL; pNode pnext = NULL; pNode tail = NULL; DataType tmp = 0; assert(pplist != NULL); assert(*pplist != NULL); //外层循环 while (tail != (*pplist)) &#123; pCur = *pplist; pnext = pCur-&gt;next; while (pnext != tail) &#123; if (pCur-&gt;data &gt; pnext-&gt;data) &#123; tmp = pCur-&gt;data; pCur-&gt;data = pnext-&gt;data; pnext-&gt;data = tmp; &#125; pCur = pnext; pnext = pnext-&gt;next; &#125; //tail指针前移 tail = pCur; &#125;&#125; 优化 如果是空链表或者只有一个元素的链表则不进行排序 如果某一趟排完之后就有序，那么直接跳出循环不再排序 1234567891011121314151617181920212223242526272829303132333435363738394041//单链表排序（冒泡排序） void BubbleSort(pList * pplist)&#123; pNode pCur = NULL; pNode pnext = NULL; pNode tail = NULL; DataType tmp = 0; //冒泡排序算法优化，定义标志位 int flag = 0; assert(pplist != NULL); //只有一个元素或者是空链表时候不进行排序 if (*pplist == NULL || (*pplist)-&gt;next == NULL) return; //外层循环 while (tail != (*pplist)) &#123; pCur = *pplist; pnext = pCur-&gt;next; while (pnext != tail) &#123; if (pCur-&gt;data &gt; pnext-&gt;data) &#123; //交换了之后修改标志位 flag = 1; tmp = pCur-&gt;data; pCur-&gt;data = pnext-&gt;data; pnext-&gt;data = tmp; &#125; pCur = pnext; pnext = pnext-&gt;next; &#125; //tail指针前移 tail = pCur; if (flag == 0) //未经改变的时候直接跳出循环 break; &#125;&#125; 算法稳定性 冒泡排序就是把小的元素往前调或者把大的元素往后调。比较是相邻的两个元素比较，交换也发生在这两个元素之间。所以，如果两个元素相等，我想你是不会再无聊地把他们俩交换一下的；如果两个相等的元素没有相邻，那么即使通过前面的两两交换把两个相邻起来，这时候也不会交换，所以相同元素的前后顺序并没有改变，所以冒泡排序是一种稳定排序算法。 题目十二 题目十二：查找单链表的中间节点，要求只能遍历一次链表 这个思路比较简单，利用快慢指针法即可，一个快指针【一次走两步】，一个慢指针【一次走一步】，只要快指针走到结束为止，慢指针恰好就在链表的中间！只不过快指针一次走两步，需要考虑链表元素个数为奇数个的问题，所以结束标志不只是快指针走到NULL位置，快指针的NEXT走到NULL也算是结束： 1234567891011121314pNode FindMidNode(pList head)&#123; pNode fast = head; pNode slow = head; if (head == NULL || head-&gt;next == NULL) return head; while ((fast != NULL) &amp;&amp; (fast-&gt;next != NULL)) &#123; fast = fast-&gt;next-&gt;next; slow = slow-&gt;next; &#125; return slow;&#125; 题目十三 题目十三：查找单链表的倒数第k个节点，要求只能遍历一次链表 这个与上面的查找中间节点的方式是一样的，也是使用两个指针只不过这次都是使用慢指针的方式，让一个指针先走k步，然后另一个指针才能开始走，直到先走的指针走到链表结尾，此时后走的指针刚好走到倒数第k个节点 1234567891011121314151617181920//查找单链表的倒数第k个节点，要求只能遍历一次链表 pNode FindLastKNode(pList *pplist, int k)&#123; pNode first = *pplist; pNode catch = *pplist; int num = 0; assert(pplist != NULL); if (*pplist == NULL) return NULL; while (first != NULL) &#123; first = first-&gt;next; if (num++ &gt;= k) &#123; catch = catch-&gt;next; &#125; &#125; return catch; &#125; 还是要注意特殊情况的处理！ 题目十四 题目十四：约瑟夫环问题 约瑟夫环：JosephCycle，约瑟夫环是一个数学的应用问题：已知n个人（以编号1，2，3…n分别表示）围坐在一张圆桌周围。从编号为k的人开始报数，数到m的那个人出列；他的下一个人又从1开始报数，数到m的那个人又出列；依此规律重复下去，直到圆桌周围的人全部出列。 图片示例 这样如何往复的淘汰直到只剩下最后一个人为幸存者，使用链表即可完成： 1234567891011121314151617void test()&#123; pList plist = NULL; pNode pos = NULL; int i = 0; InitList(&amp;plist); for (i = 1; i &lt;= 41; i++)&#123; PushBack(&amp;plist, i); &#125; //形成环 pos = Find(plist, 41); pos-&gt;next = plist; //求出41个人中最后两名幸存者 pos = JosephCycle(&amp;plist, 3); printf(\"%d \", pos-&gt;data);//16 printf(\"%d \", pos-&gt;next-&gt;data);//31&#125; 1234567891011121314151617181920212223//单链表实现约瑟夫环 pNode JosephCycle(pList * pplist, int num)&#123; pNode pCur = NULL; pNode del = NULL; int count = 0; assert(pplist != NULL); assert(num &gt;= 2); pCur = *pplist; while (pCur-&gt;next-&gt;next != pCur) &#123; count = num; while (--count) &#123; pCur = pCur-&gt;next; &#125; EraseNotTail(pCur); //根据节点位置删除节点 &#125; return pCur;&#125; 题目十五 题目十五：使用三指针法翻转链表 三指针毫无疑问就是使用三个指针去翻转链表，这种方式可以是我们很容易的实现链表的翻转，首先定义三个指针，分别指向第一个节点、第二个节点、第三个节点，然后由于我们保存了节点的地址，当然也就可以随心所欲的操作这些节点的指向了！ 1234567891011121314151617181920212223242526272829303132// 逆置/反转单链表 void ReverseList(pList* pplist)&#123; pNode pCur = NULL; pNode tmp = NULL; pNode tmp2 = NULL; assert(pplist != NULL); assert(*pplist != NULL); //将三个指针赋值 pCur = *pplist; tmp = pCur-&gt;next; tmp2 = tmp-&gt;next; //原头节点先赋值为NULL pCur-&gt;next = NULL; while (tmp2 != NULL) &#123; //改变指向 tmp-&gt;next = pCur; //3个指针后移 pCur = tmp; tmp = tmp2; tmp2 = tmp2-&gt;next; &#125; //循环完毕最后一步的处理 *pplist = tmp; tmp-&gt;next = pCur;&#125; 题目十六 题目十六：替换删除法、替换插入法 这两个方法也是链表操作中比较常见的方法，但是注意是非尾节点才可以使用替换删除法，替换插入法任何节点均可用！先上代码： 12345678910//删除一个无头单链表的非尾节点 void EraseNotTail(pNode pos)&#123; pNode pCur = NULL; assert(pos != NULL); pCur = pos-&gt;next-&gt;next; pos-&gt;data = pos-&gt;next-&gt;data; free(pos-&gt;next); pos-&gt;next = pCur;&#125; 其实就是本来应该删除pos位置的节点，但是为了方便操作我们只能删除pos后面的节点，于是我们先把pos的next的next节点的位置存储起来，然后将pos的next节点的数据存到pos节点上，接着删除pos的next节点即可： 同样的道理，我们看看替换插入法： 123456789101112131415161718//在无头单链表的一个节点前插入一个节点 void InsertNode(pNode pos, DataType data)&#123; pNode pCur = NULL; pNode newNode = NULL; assert(pos != NULL); newNode = BuyNode(pos-&gt;data); if (newNode == NULL) &#123; printf(\"空间不足\\n\"); return; &#125; pCur = pos-&gt;next; pos-&gt;next = newNode; pos-&gt;data = data; newNode-&gt;next = pCur;&#125; 同样的道理，本来我我们应该把新的节点插入到pos之前，但是由于这样是不好操作的，于是我们先把节点插入到了pos的后面，然后交换pos与新节点中的数据，这样便完成了所谓的“在pos之前插入”，优化方案：直接在产生新节点的时候就使用pos的data来构造节点，这样只需要将参数data赋值给pos的data即可！ 题目十七 问题十七： 逆序打印链表 普通的方式逆序打印链表此方法的思路就是使用两个指针，一个指针tail负责指向上一次刚刚被打印的元素，这样每次pCur指针每次都以tail指针标记作为结束，这样直到tail指针指向头结点的时候就证明已经打印完成了。看看下面这个小电影就能很容易的明白： 12345678910111213141516171819//1. 逆序打印单向链表 void PrintTailToHead(pList plist)&#123; pNode pCur = NULL; pNode tail = NULL; assert(plist != NULL); pCur = plist; while (tail != plist) &#123; while (pCur-&gt;next != tail) &#123; pCur = pCur-&gt;next; &#125; printf(\"%d \", pCur-&gt;data); tail = pCur; pCur = plist; &#125;&#125; 递归的方式逆序打印链表 递归往往可以将复杂问题简单化，这里的极限条件是传入的参数不为空，那么直到将链表末尾的【NULL】传入才会直接返回，这样的话层层递归打印出的便是倒序的输出： 12345678//递归的方式打印void PrintTailToHead_R(pList plist)&#123; if (plist == NULL) return; PrintTailToHead_R(plist-&gt;next); printf(\"%d \",plist-&gt;data);&#125;","updated":"2020-03-13T03:06:29.777Z","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"链表","slug":"链表","permalink":"https://zouchanglin.cn/tags/%E9%93%BE%E8%A1%A8/"}]},{"title":"全概率公式与贝叶斯公式","date":"2018-07-15T10:09:30.000Z","path":"2018/07/15/全概率公式与贝叶斯公式/","text":"一、条件概率公式举个例子，比如让你背对着一个人，让你猜猜背后这个人是女孩的概率是多少？直接猜测，肯定是只有５０％的概率，假如现在告诉你背后这个人是个长头发，那么女的概率就变为９０％。所以条件概率的意义就是，当给定条件发生变化后，会导致事件发生的可能性发生变化。 条件概率由文氏图出发，比较容易理解： P(A|B)表示B发生后A发生的概率，由上图可以看出B发生后，A再发生的概率就是P(AnB)/P(B)，因此P(A|B)=P(AnB)/P(B)，由P(A|B)=P(AnB)/P(B)又可以得到P(AnB) = P(A|B)*P(B)，则可以得到P(AnB) = P(B|A)*P(A) ，由此可得：P(A|B) = P(AnB)/P(B) = P(B|A) * P(A)/P(B)这就是条件概率公式。 假如事件A与B相互独立，那么: P(AnB) = P(A)*P(B) 相互独立：表示两个事件发生互不影响。而互斥：表示两个事件不能同时发生，（两个事件肯定没有交集）。互斥事件一定不独立（因为一件事的发生导致了另一件事不能发生）；独立事件一定不互斥，（如果独立事件互斥， 那么根据互斥事件一定不独立，那么就矛盾了），但是在概率形式上具有一些巧合性，一般地：但是，对于两个独立事件，P(A|B)依然可以等于0，因为事件A或者事件B发生的概率可能为0.所以P(AB) = 0，并不是一定表示互斥。互斥和独立的理解还是要究其真正意义，而不是表达形式。 二、全概率公式先举个例子，小张从家到公司上班总共有三条路可以直达（如下图），但是每条路每天拥堵的可能性不太一样，由于路的远近不同，选择每条路的概率如下： P(L1) = 0.5 、P(L2) = 0.3、P(L3) = 0.2 每天上述三条路不拥堵的概率分别为： P(C1) = 0.3、P(C2) = 0.4、P(C3) = 0.7 假设遇到拥堵会迟到，那么小张从Home到Company不迟到的概率是多少？ 其实不迟到就是对应着不拥堵，设事件Ｃ为到公司不迟到，事件Li为选择第i条路，则 123P(C) &#x3D; P(L1) * P(C|L1) + P(L2) * P(C|L2) + P(L3) * P(C|L3)P(C) &#x3D; P(L1) * P(C1) + P(L2) * P(C2) + P(L3) * P(C3)P(C) &#x3D; 0.5 * 0.2 + 0.3 * 0.4 + 0.2 * 0.7 &#x3D; 0.36 全概率就是表示达到某个目的，有多种方式（或者造成某种结果，有多种原因），问达到目的的概率是多少（造成这种结果的概率是多少）？ 全概率公式： 设事件L1、L2…是一个完备事件组，则对于任意一个事件Ｃ，若有如下公式成立： 1P(C) &#x3D; P(L1)P(C1|L1)….P(Ln)P(Cn|Ln) &#x3D; ∑P(Li)P(C|Li) 这就是全概率公式 三、贝叶斯公式仍旧借用上述的例子，但是问题发生了改变，问题修改为：到达公司未迟到选择第１条路的概率是多少？ 可不是P(L1) = 0.5 ，因为０．５这个概率表示的是，选择第一条路的时候并没有靠考虑是不是迟到，只是因为距离公司近才知道选择它的概率，而现在我们是知道未迟到这个结果，是在这个基础上问你选择第一条路的概率，所以并不是直接就可以得出的。 故有： 123P(L1|C) &#x3D; P(C|L1) * P(L1) &#x2F; P(C)P(L1|C) &#x3D; P(C|L1) * P(L1) &#x2F; P(L1)* P(C|L1) +P(L2)* P(C|L2) + P(L3)* P(C|L3)P(L1|C) &#x3D; 0.2 * 0.5 &#x2F; 0.2 * 0.5 + 0.3 * 0.4 + 0.2 * 0.7 &#x3D; 0.28 所以选择第一条路的概率为0.28 贝叶斯公式就是当已知结果，问导致这个结果的第i原因的可能性是多少？执果索因！ 贝叶斯公式： 在已知条件概率和全概率的基础上，贝叶斯公式是很容易推导的： P(Lk|C) = P(C|Lk)/P(C)于是我们推得出： 1P(Lk|C) &#x3D; P(C|Lk)&#x2F;∑P(Li) * P(C|Li)","updated":"2020-03-13T03:06:29.720Z","categories":[{"name":"数学之美","slug":"数学之美","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/"}],"tags":[{"name":"概率论","slug":"概率论","permalink":"https://zouchanglin.cn/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"}]},{"title":"可变参数源码剖析","date":"2018-07-12T10:09:30.000Z","path":"2018/07/12/可变参数源码剖析/","text":"前言可变参数，顾名思义即参数类型不确定，参数个数不确定(只是表面上个数不确定，实际上还是需要直接或者间接的将参数个数传入)。可变参数的应用场景非常多，例如：求n个数字之和，如果写成普通函数，那么将永远也实现不了这个函数的功能，如果写成可变参数的话就会变得非常简单，不至于出现代码冗余。我们最常用的printf()就是一个实现了可变参数的函数，这种函数基本上不可能写成常规函数，可变参数便成最佳选择！printf函数是个经典的可变参数的例子！ 示例虽然我们对像printf()这样的库函数是如何实现可变参数原理不是很了解，但是我们可以从最简单的示例入手，接下来这个例子演示了如何利用可变参数求n个数字之和： 12345678910111213141516171819202122#define _CRT_SECURE_NO_WARNINGS#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;stdarg.h&gt; int get_add(int num, ...)&#123; va_list arg; va_start(arg, num); int ret = 0; int i = 0; for (i = 0; i &lt; num; i++)&#123; ret += va_arg(arg, int); &#125; return ret;&#125; int main(void)&#123; int ret = get_add(3, 10, 10, 10); //第一个参数为后面的参数个数 printf(\"ret = %d\\n\",ret); system(\"pause\"); return 0;&#125; 分析va_list1typedef char * va_list; 先看看 va_list 的类型,从源码中我们看到 va_list 其实是一个 char * 类型 ，这就像是可变参数列表的表头一样 va_start接下来看看 va_start(arg,num) ，这里把上面得到的字符指针，后移动4个字节，就是跳过num的内存地址 123#define va_start _crt_va_start#define _crt_va_start(ap,v) ( ap = (va_list)_ADDRESSOF(v) + _INTSIZEOF(v) )#define _ADDRESSOF(v) ( &amp;(v) ) 通过 va_arg(arg, int) 是怎么拿到后面的参数的呢？ap表示可变参数指针，而t表示数据类型。使用 ((sizeof(n) + sizeof(int) - 1) &amp; ~(sizeof(int) - 1)) ，传入char，float，double等小于4字节的类型，都会返回4。相应的传入如果类型大小是,5,6,7的话，则返回8。即返回当前一组数中靠近4的倍数的值。 ( (t )((ap += _INTSIZEOF(t)) - _INTSIZEOF(t)) ) 该表达式先让指针ap加上4字节的大小，再把减去4字节大小处所对应的值返回。用t强制类型转换，再解引用,注意此处 t是传入参数的数据类型 va_end12#define va_end _crt_va_end#define _crt_va_end(ap) ( ap = (va_list)0 ) 该语句把整型0强制转换为字符0,然后传给指针ap，由此可知将ap指向空！ 从第一个参数就可以访问到后面的参数，这比较依赖于函数栈帧，后面的参数先压栈，我们只需要得到最后一个压栈的参数得出参数的总体个数和类型，这样就可以逐个访问到后面的参数了，这就是可变参数的实现原理！ 结语可变参数让函数的灵活性大大增加，避免了冗长的函数，也实现了普通函数所不能达到的功能，需要注意的是：有时候第一个参数显示的告诉了后面的参数个数和类型，有时候只是隐式的告诉参数个数和类型（就比如printf（）函数一样），在使用可变参数构造自己的函数的时候务必想清楚是否能根据第一个参数正确访问到后面的参数！","updated":"2020-03-13T03:06:29.726Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://zouchanglin.cn/tags/C-C/"}]},{"title":"一个故事告诉你什么才是好的程序员","date":"2018-07-03T10:09:30.000Z","path":"2018/07/03/一个故事告诉你什么才是好的程序员/","text":"大学生经常思考的一个问题，学底层到底还有没有用？这篇文章非常能说明到底有没有必要把底层学好！ 从一个故事说起在应用开发如此方便的今天，我总是会听到有些人有这样的疑问，“只是做 应用 开发的话，还有没有必要学习诸如操作系统，编译原理这样的课程呢？”，亦或是会听到这样的话，“会用这个框架就行了，它底层是怎么实现的不用去管。”还记得我在大一学 C 语言的时候，就听过有同学说我以后是想从事 Java 开发的，C 语言这种学来应付一下考试就行，指针什么的其他语言又没有，就不用去管啦。 真的是这样吗？刚好今天看到一个有意思的故事，从故事中我看到了答案，这个故事是是艾萨克·阿西莫夫 的科幻巨作《基地》中的一个片段。故事是这样的： 在银河系中，随着战争的蔓延，文明从银河系边缘开始逐渐退化，许多星球虽然还保留着核电站等高科技产品，但是已经不知道它们是如何运作的。 而有这样一颗小行星，我们暂且称之为科技星吧，在大战爆发前它搜集了银河系中的各种科学文献，并且汇聚了一大批的顶尖科学家。这颗小行星没有被卷入战争，而是将技术一直传承下去。 科技星周围的星球觊觎它所拥有的高科技，想将之夺取。而科技星又没有自保的武装力量，在这种情况下，科技星如何自保呢？这里最有意思的地方，正是科技星所使用的科技宗教的战略。 当后来其他星球上的高科技出现问题的时候，会向科技星求救。科技星就会派遣工程师前去维修，但是呢，他们将各种身份都进行包装，比如，工程师不叫做工程师，而是叫做“僧侣”，核电站也不叫做核电站，而是叫“圣殿”，维修也不叫做“维修”，而是叫做“祈祷”，也就是说，对核电站维修这一项工作完全被宗教化了！ 而此时科技星提供的说法是这样，因为这颗星球上的人做了坏事，比如违反法规，发动战争等等，触犯了神灵，所以神灵剥夺了他们使用能源的权力。而如果想要恢复能源，就必须对自己的行为忏悔，祈求神灵的原谅。所以当工程师进入核电站进行维修的时候，所有的星球居民一起下跪祈祷，而当核电站恢复的时候，大家纷纷称颂神的伟大。 为什么那些拥有核电站星球的人们会对来维修的工程师“膜拜祈祷”呢？其根本原因还是在于核电站这样的高科技对他们而言是神秘的，未知的东西。 尽管他们拥有这样高科技的东西，却没有与之匹配的认知和知识储备。 再回过头来看看一开始的问题，你是否明悟了呢？我们也是掌握着上层应用框架这种“高科技”，我们知道怎么去配置，怎么去调用，就像上面故事中普通星球的人知道怎么启动，关闭核电站一样。但一旦出了无法解决的问题，或者是遇到了什么性能瓶颈，似乎我们能做的，只能去各种技术群里，找那些大神“祈祷”了。 再来说说人工智能在今天，人工智能这个名词已经逐渐为人们所熟知。而未来，人工智能的应用场景只会越来越广泛，面向 AI 编程也必然会是一种趋势。 那么现在从事于 Web 或是 Android 等应用开发的程序员需要去学习机器学习或是深度学习相关的知识吗？我的回答是 YES 。有人说我又不想从事于人工智能的开发工作，为什么还要去学它呢呢？我想说的是，为了避免成为上面故事中那些普通星球的居民。再过几年，当你碰到一个会跟你说话的机器人或是更加奇妙的事物的时候，我们应该是对它的一些实现细节感兴趣，会有探究的欲望。而不是在那里感慨着造物主真伟大，竟能造出一个这样神奇的东西。 话又说回来，在机器学习或是深度学习的学习过程中其实也很容易陷入到这种只会调用上层 API 而不知底层原理模型的境地。因为在今天，有很多库类都可以让你轻松实现一条语句就直接使用某个算法模型，所以很多人就不再专注于对底层模型原理的学习。在机器学习的学习过程中，相信大多数人应该都看过这样一张图： 我们来看看这张图中 Hacking Skills 和 Substantive Expertise 的交界处，这里叫 Danger Zone，即危险区。意思是如果你只会编程和调用机器学习的 API，调参数，那么你就处于一种很危险的境地。 结语一个好的程序员，不应当满足于学习到了什么新的技术或者学习了什么新的算法模型。真正有价值的东西，往往是那些人们不乐意去学的底层的，枯燥的内容。 我们应该认识到，单单只会上层应用开发或只会调包调模型而不懂底层原理，那这种开发人员的知识体系便如空中阁楼。看起来华丽壮观，但实际上却地基不稳。一旦出现一点问题这座阁楼便会顷刻崩塌，并且无计可施，只能到处“祈祷”。 对未知的事务保持好奇，不断学习，探究事物的本质和原理。在我看来，这才是程序员之道。原文地址:《一个故事告诉你什么才是好的程序员》","updated":"2020-03-13T03:06:29.715Z","categories":[{"name":"生活","slug":"生活","permalink":"https://zouchanglin.cn/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"生活","slug":"生活","permalink":"https://zouchanglin.cn/tags/%E7%94%9F%E6%B4%BB/"}]},{"title":"Java内置的序列化方式","date":"2018-06-19T10:09:30.000Z","path":"2018/06/19/Java内置的序列化方式/","text":"网络数据传输的是一个二进制的字节数组。把对象序列化为二进制字节数组和把二进制字节数组反序列化为对象的时间加起来，时间越少，性能越高。使用JSON 和XML的居多！ 先看看String类的源码 123456789101112public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 /** use serialVersionUID from JDK 1.0.2 for interoperability */ private static final long serialVersionUID = -6849794470754667710L; //....................&#125; Java的内置序列化方式可以看出String实现了Java的内置序列化接口Serializable，于是接下来利用String类演示一下Java的内置序列化是怎样做到的： 1234567891011121314151617181920212223242526272829303132import java.io.ByteArrayInputStream;import java.io.ByteArrayOutputStream;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;public class Demo &#123; public static void main(String[] args) throws IOException, ClassNotFoundException &#123; String str = new String(\"Hello\"); // 定义一个字节数组输出流 ByteArrayOutputStream os = new ByteArrayOutputStream(); // 对象输出流 ObjectOutputStream out = new ObjectOutputStream(os); // 将对象写入到字节数组输出，进行序列化 out.writeObject(str); byte[] strByte = os.toByteArray(); // 字节数组输入流 ByteArrayInputStream is = new ByteArrayInputStream(strByte); // 执行反序列化，从流中读取对象 ObjectInputStream in = new ObjectInputStream(is); String str2 = (String) in.readObject(); System.out.println(str2); &#125;&#125; 使用Hessian进行序列化123456789101112131415161718192021222324252627282930313233import java.io.ByteArrayInputStream;import java.io.ByteArrayOutputStream;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import com.caucho.hessian.io.HessianInput;import com.caucho.hessian.io.HessianOutput;public class Demo &#123; public static void main(String[] args) throws IOException &#123; String str = new String(\"Hello\"); // 定义一个字节数组输出流 ByteArrayOutputStream os = new ByteArrayOutputStream(); //Hessian的序列化输出 HessianOutput ho = new HessianOutput(os); ho.writeObject(str); byte[] strByte = os.toByteArray(); ByteArrayInputStream is = new ByteArrayInputStream(strByte); //Hessioan的反序列化读取对象 HessianInput hi = new HessianInput(is); String str2 = (String)hi.readObject(); System.out.println(str2); &#125;&#125;","updated":"2020-03-13T03:06:29.666Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"}]},{"title":"冯诺依曼架构","date":"2018-06-18T10:09:30.000Z","path":"2018/06/18/冯诺依曼架构/","text":"冯诺依曼计算机特点 计算机由运算器、控制器、存储器、输入设备、输出设备五大部件组成 指令与数据以同等地位存放在存储器，并可按地址寻访 指令与数据均以二进制数表示 指令由操作码和地址码组成，操作码用来表示操作的性质，地址码用来表示操作数在存储器中的位置 指令在存储器中顺序存放。通常也顺序执行，在特定条件下，可根据运算结果或设定条件改变执行顺序 机器以运算器为中心，输入输出设备与存储器间的数据传递通过运算器完成 细化的计算机组成框架 主存储器 主存储器包括存储体M、各种逻辑部件及控制电路。存储体由许多存储单元组成，每个存储单元包含若干个存储元件（称存储元、存储基元），每个存储元件能寄存以为二进制代码 主存的工作方式是按存储单元的地址号来实现对存储字各位的存（写）、取（读）。这种存取方式称为按地址存取方式，即按地址访问存储器（简称访存） 为了能实现按址访问的方式，主存中还需要配置两个寄存器MAR和MDR MAR（Memory Address Register）是存储器地址寄存器，用来存放欲访问的存储单元的地址，其位数对应存储单元的个数（如MAR为10 位，则由210=1024个存储单元，记为1K）MDR（Memory Data Register）是存储器数据寄存器，用来存放从存储体的某个单元取出的代码或者准备往某个存储单元存入的代码，其位数与存储字长相等 现代计算机为了适应指令和是字长可变，其长度不由存储字长来确定，而是由字节的个数来表示，1个字节（byte）被定义为8位（bit）二进制代码组成，此时存储字长、指令字长、数据字长三者可不相等，但必须是字节整数倍 运算器 运算器最少包括3个寄存器（现代计算机内部往往设有通用寄存器组）和一个算术逻辑部件（ALU）。其中ACC为累加器、MQ为商乘寄存器、X为操作数寄存器，这三个寄存器完成不同运算时，所放的操作数也各不相同 ACC在加法运算中存储被加数以及和；在减法运算中存储被减数以及差；乘法运算中存储乘积高位；在除法运算中存储被除数以及余数，MQ在乘法运算中存储乘数以及乘积低位；在除法运算中存储商，X在加法运算中存储加数；在减法运算中存储减数；在乘法运算中存储被乘数；在除法运算中存储除数 控制器 控制器是计算机的神经中枢，由他指挥各部件自动、协调地工作。具体而言它首先要命令存储器读出一条指令，称为取址过程。接着对该指令分析，指出该指令要完成的操作，并按寻址特征指明操作数的地址，称为分析过程。最后根据操作数所在的地址及指令的操作码完成操作，称为执行过程。 控制器由程序计数器（Program Counter，PC）、指令寄存器（Instruction Register）以及控制单元（Control Unit，CU） PC用来存放当前欲执行指令的地址，它与主存的MAR之间有一条直接通路，并具有自动加1功能，即可自动形成下一条指令的地址。 IR用来存放当前的指令，IR的内容来自主存的MDR，IR中的操作码（OP(IR)）送至CU，记作OP（IR）→CU，用来分析指令；其地址码（Ad(IR)）作为操作数的地址送至存储器的MAR，记作Ad(IR)→MAR CU用来分析当前指令所需要完成的操作，并发出各种微操作命令序列，用以控制所以被控的对象 CPU工作过程 控制单元在时序脉冲的作用下，将指令计数器里所指向的指令地址(这个地址是在内存里的)送到地址总线上去，然后CPU将这个地址里的指令读到指令寄存器进行译码。对于执行指令过程中所需要用到的数据，会将数据地址也送到地址总线，然后CPU把数据读到CPU的内部存储单元(就是内部寄存器)暂存起来，最后命令运算单元对数据进行处理加工。周而复始，一直这样执行下去!","updated":"2020-03-13T03:06:29.722Z","categories":[{"name":"操作系统理论","slug":"操作系统理论","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"},{"name":"Mac","slug":"Mac","permalink":"https://zouchanglin.cn/tags/Mac/"},{"name":"Windows","slug":"Windows","permalink":"https://zouchanglin.cn/tags/Windows/"}]},{"title":"MySQL库表操作","date":"2018-06-08T10:09:30.000Z","path":"2018/06/08/MySQL库表操作/","text":"数据库操作字符集与校验规则当我们创建数据库没有指定字符集和校验规则时，系统使用默认字符集：utf8，校验规则是：utf8_ general_ ci ，这个校验规则中的 ci就是Case insensitive意为不区分大小写 创建一个使用utf8 的字符集，并带校对规则为utf8_general_ci的数据库。 1create database DBName charset&#x3D;utf8 collate utf8_general_ci; 查看系统默认字符集、默认校验规则 12show variables like &#39;character_set_database&#39;; show variables like &#39;collation_database&#39;; 支持的字符集、支持的校验规则 12show charset;show collation; 创建数据库123456789101112131415161718192021mysql&gt; create database myDB;Query OK, 1 row affected (0.90 sec)mysql&gt; show create database myDB;+----------+---------------------------------------------------------------+| Database | Create Database |+----------+---------------------------------------------------------------+| myDB | CREATE DATABASE &#96;myDB&#96; &#x2F;*!40100 DEFAULT CHARACTER SET utf8 *&#x2F; |+----------+---------------------------------------------------------------+1 row in set (0.00 sec)mysql&gt; create database &#96;myDB2&#96;;Query OK, 1 row affected (0.00 sec)mysql&gt; show create database myDB2;+----------+----------------------------------------------------------------+| Database | Create Database |+----------+----------------------------------------------------------------+| myDB2 | CREATE DATABASE &#96;myDB2&#96; &#x2F;*!40100 DEFAULT CHARACTER SET utf8 *&#x2F; |+----------+----------------------------------------------------------------+1 row in set (0.00 sec) MySQL 建议我们关键字使用大写，但是不是必须的。数据库名字的反引号,是为了防止使用的数据库名刚好是关键字/*!40100 DEFAULT CHARACTER SET utf8 */这个不是注释，表示当前Mysql版本大于4.01版本，就执行这句话 修改数据库对数据库的修改主要指的是修改数据库的字符集，校验规则 例如把 mytest 数据库字符集改为 gbk 1alter database mytest charset&#x3D;gbk 删除数据库不要随意删除数据库，否则还是容易从删库到跑路的 1drop databse [if exists] 数据库名称 如果加上if exists那么删除一个不存在的数据库也不会出错，但是会有警告(查看警告就使用show warnings;)，如果不加if exists去删除一个不存在的数据库就会报错 123456789101112mysql&gt; drop database mydbs;ERROR 1008 (HY000): Can&#39;t drop database &#39;mydbs&#39;; database doesn&#39;t existmysql&gt; drop database if exists mydbs;Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; show warnings;+-------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+| Level | Code | Message |+-------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+| Error | 1064 | You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;warning&#39; at line 1 |+-------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 备份和恢复1mysqldump -P3306 -uroot -p -B 数据库名称 &gt; 路径+数据库名称.sql 使用示例 12345678910111213141516171819mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mydb1 || performance_schema || sakila || test || userdata || world |+--------------------+11 rows in set (0.00 sec)mysql&gt; exit;ByeC:\\Users\\15291\\Desktop&gt;mysqldump -P3306 -uroot -p -B mydb1 &gt; .&#x2F;mydb1.sqlEnter password: **** 备份后会生成mydb1.sql文件，通过执行这个sql脚本就会恢复数据库： 1mysql&gt; source C:&#x2F;Users&#x2F;15291&#x2F;Desktop&#x2F;mydb1.sql; 如果备份的不是整个数据库，而是其中的一张表: 1mysqldump -u root -p 数据库名 表名1 表名2 &gt; .&#x2F;mytest.sql 同时备份多个数据库: 1mysqldump -u root -p -B 数据库名1 数据库名2 ... &gt; 数据库存放路路径 如果备份一个数据库时，没有带上-B参数， 在恢复数据库时，需要先创建空数据库，然后使用数据库，再使用source来还原 查看连接情况可以告诉我们当前有哪些用户连接到我们的MySQL，如果查出某个用户不不是你正常登陆的，很有可能数据库被人入侵了。如果发现数据库比较慢时，可以用这个指令来查看数据库连接情况： 1234567mysql&gt; show processlist;+----+------+-----------------+------+---------+------+-------+------------------+| Id | User | Host | db | Command | Time | State | Info |+----+------+-----------------+------+---------+------+-------+------------------+| 9 | root | localhost:55469 | NULL | Query | 0 | init | show processlist |+----+------+-----------------+------+---------+------+-------+------------------+1 row in set (0.04 sec) 表的操作创建表12345CREATE TABLE table_name ( field1 datatype, field2 datatype, field3 datatype) character set 字符集 collate 校验规则 engine 存储引擎; 不同的存储引擎，创建表的文件不一样，这个在Myisam存储引擎中说到过假设users表存储引擎是Myisam，在数据目中有三个不同的文件，分别是：users.frm：表结构users.MYD：表数据users.MYI：表索引 查看表结构12345678910mysql&gt; desc users;+----------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+-------------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || name | varchar(20) | YES | | NULL | || password | char(32) | YES | | NULL | || birthday | date | YES | | NULL | |+----------+-------------+------+-----+---------+-------+4 rows in set (0.03 sec) 修改表/表结构123ALTER TABLE tablename ADD (column datatype [DEFAULT expr][,column datatype]...);ALTER TABLE tablename MODIfy (column datatype [DEFAULT expr][,column datatype]...);ALTER TABLE tablename DROP (column); 使用示例： 在users表的password字段后添加assets字段，类型为varchar(50)，备注为图片路径 123456789101112131415mysql&gt; alter table users add assets varchar(50) comment &#39;图片路径&#39; after password;Query OK, 0 rows affected (0.14 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc users; +----------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+-------------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || name | varchar(20) | YES | | NULL | || password | char(32) | YES | | NULL | || assets | varchar(50) | YES | | NULL | || birthday | date | YES | | NULL | |+----------+-------------+------+-----+---------+-------+5 rows in set (0.02 sec) 修改users表中name字段的类型为varchar(30) 123456789101112131415mysql&gt; alter table users modify name varchar(30);Query OK, 0 rows affected (0.09 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc users;+----------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+-------------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || name | varchar(30) | YES | | NULL | || password | char(32) | YES | | NULL | || assets | varchar(50) | YES | | NULL | || birthday | date | YES | | NULL | |+----------+-------------+------+-----+---------+-------+5 rows in set (0.02 sec) 删除users表中assets字段 1234567891011121314mysql&gt; alter table users drop assets;Query OK, 0 rows affected (0.09 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc users;+----------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+-------------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || name | varchar(30) | YES | | NULL | || password | char(32) | YES | | NULL | || birthday | date | YES | | NULL | |+----------+-------------+------+-----+---------+-------+4 rows in set (0.01 sec) 对user表的名字进行重命名 12mysql&gt; alter table users rename to user;Query OK, 0 rows affected (0.01 sec) 对user表中的name字段进行重命名并更改字段类型 1234567891011121314mysql&gt; alter table user change name xingming varchar(50);Query OK, 0 rows affected (0.09 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc user;+----------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+-------------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || xingming | varchar(50) | YES | | NULL | || password | char(32) | YES | | NULL | || birthday | date | YES | | NULL | |+----------+-------------+------+-----+---------+-------+4 rows in set (0.02 sec) 删除user表 12345678910mysql&gt; drop table user;Query OK, 0 rows affected (0.01 sec)mysql&gt; show tables;+---------------+| Tables_in_db1 |+---------------+| person |+---------------+1 row in set (0.00 sec) Mysql数据类型数值类型tinyint-bigint这是有符号的范围，无符号的返回自行推导，和C的无符号是一致的 在MySQL中，整型可以指定是有符号的和无符号的，默认是有符号的。可以通过UNSIGNED来说明某个字段是无符号的，但是一般遇到存储类型不足以存储数据的大小时，应该换成更大的类型，而不是换成对应的无符号类型！ bitbit[(M)]: 位字段类型。M表示每个值的位数，范围从1到64。如果M被忽略，默认为1注意bit类型，bit字段在显示时，是按照ASCII码对应的值显示！如果我们有这样的值:只存放0或1，这时可以定义bit(1)，这样可以节省空间！ floatfloat[(m, d)] [unsigned]: M指定显示长度，d指定小数位数，占用空间4个字节！ decimal浮点数float与定点数decimal的存储方式的不同决定了他们的用途不同：《浮点数的存储方式》decimal用于保存必须为确切精度的值，很显然底层是用字符串来存储的 1decimal(m, d) [unsigned] 定点数m指定长度，d表示小数点的位数 float表示的精度大约是7位。decimal整数最大位数m为65。支持小数最大位数d是30。如果d被省略，默认为0.如果m被省略，默认是10。建议：如果希望小数的精度高，推荐使用decimal(对于银行这种对小数要求非常高的业务，decimal还是大有用处的) 字符串类型char1char(L) 固定长度字符串，L是可以存储的长度，单位为字符，最大长度值可以为255，这里需要注意的是一个汉字和一个字母均被视为一个字符！ varchar1varchar(L) 可变长度字符串，L表示字符长度，最大长度65535个字节关于varchar(len)，len到底是多大，这个len值，和表的编码密切相关：varchar长度可以指定为0到65535之间的值，但是有两个字节用于记录数据大小，所以说有效字节数65532。当我们的表的编码是utf8时，varchar(n)的参数n最大值是65532/3=21844[因为utf中，一个字符占用3个字节]，如果编码是gbk，varchar(n)的参数n最大是65532/2=32766（因为gbk中，一个字符占用2字节） char和varcahr比较 两者如何选择？如果数据确定长度都一样，就使用定长（char），比如：身份证，手机号，md5如果数据长度有变化,就使用变长(varchar), 比如：名字，地址，但是你要保证最长的能存的进去定长的磁盘空间比较浪费，但是效率高变长的磁盘空间比较节省，但是效率 日期和时间类型常用的日期有如下三个： datetime 时间日期格式 ‘yyyy-mm-dd HH:ii:ss’ 表示范围从1000到9999，占用八字节 date:日期 ‘yyyy-mm-dd’，占用三字节 timestamp：时间戳，从1970年开始的 yyyy-mm-dd HH:ii:ss格式和datetime完全一致，占用四字节 使用示例： 12345678910111213141516171819202122mysql&gt; create table birthday (t1 date, t2 datetime, t3 timestamp);Query OK, 0 rows affected (1.00 sec)mysql&gt; show tables;+---------------+| Tables_in_db1 |+---------------+| birthday || person |+---------------+2 rows in set (0.00 sec)mysql&gt; insert into birthday(t1,t2) values(&#39;1997-7-1&#39;,&#39;2008-8-8 12:1:1&#39;);Query OK, 1 row affected (0.22 sec)mysql&gt; select * from birthday;+------------+---------------------+---------------------+| t1 | t2 | t3 |+------------+---------------------+---------------------+| 1997-07-01 | 2008-08-08 12:01:01 | 2018-12-07 18:57:58 |+------------+---------------------+---------------------+1 row in set (0.00 sec) enum和set语法：enum：枚举，“单选”类型； 1enum(&#39;选项1&#39;,&#39;选项2&#39;,&#39;选项3&#39;,...); 该设定只是提供了若干个选项的值，最终一个单元格中，实际只存储了其中一个值；而且出于效率考虑，这些值实际存储的是“数字”，因为这些选项的每个选项值依次对应如下数字：1,2,3,….最多65535个；当我们添加枚举值时，也可以添加对应的数字编号。 set：集合，“多选”类型； 1set(&#39;选项值1&#39;,&#39;选项值2&#39;,&#39;选项值3&#39;, ...); 该设定只是提供了若干个选项的值，最终一个单元格中，设计可存储了其中任意多个值；而且出于效率考虑，这些值实际存储的是“数字”，因为这些选项的每个选项值依次对应如下数字：1,2,4,8,16,32，…. 最多64个。 不建议在添加枚举值，集合值的时候采用数字的方式，因为不利于阅读。使用示例： 12345678910111213141516mysql&gt; create table votes( -&gt; username varchar(30), -&gt; hobby set(&#39;登山&#39;,&#39;黎狗子吃粑粑&#39;,&#39;运动&#39;), -&gt; gender enum(&#39;男&#39;,&#39;女&#39;));Query OK, 0 rows affected (0.43 sec)mysql&gt; insert into votes values(&#39;LiLiNaNa&#39;,&#39;登山,黎狗子吃粑粑&#39;,&#39;男&#39;);Query OK, 1 row affected (0.09 sec)mysql&gt; select * from votes;+----------+-------------------+--------+| username | hobby | gender |+----------+-------------------+--------+| LiLiNaNa | 登山,黎狗子吃粑粑 | 男 |+----------+-------------------+--------+1 row in set (0.02 sec) 集合查询使用find_ in_ set函数： 1find_in_set(sub,str_list) 如果sub 在str_list 中，则返回下标；如果不在，返回0； str_list 用逗号分隔的字符串。 1234567891011mysql&gt; insert into votes values(&#39;Juse&#39;,&#39;登山&#39;,2);Query OK, 1 row affected (0.07 sec)mysql&gt; select * from votes where find_in_set(&#39;登山&#39;, hobby);+----------+-------------------+--------+| username | hobby | gender |+----------+-------------------+--------+| LiLiNaNa | 登山,黎狗子吃粑粑 | 男 || Juse | 登山 | 女 |+----------+-------------------+--------+2 rows in set (0.09 sec)","updated":"2020-03-13T03:06:29.689Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"}]},{"title":"MySQL存储引擎","date":"2018-06-07T10:09:30.000Z","path":"2018/06/07/MySQL存储引擎/","text":"基本概念所谓安装数据库服务器，只是在机器上安装了一个数据库管理系统程序，这个管理程序可以管理多个数据库，一般开发人员会针对每一个应用创建一个数据库。为保存应用中实体的数据，一般会在数据库中创建多个表，以保存程序中实体的数据。数据库服务器、数据库和表的关系如下： MariaDB数据库管理系统是MySQL的一个分支，主要由开源社区在维护，采用GPL授权许可 MariaDB的目的是完全兼容MySQL，包括API和命令行，使之能轻松成为MySQL的代替品，只是存储引擎不同！ Mysql使用连接Mysql服务1mysql -h 127.0.0.1 -P 3306 -u root -p -h 选项默认是127.0.0.1 -P默认是3306 后面有没有空格都是可以的很多时候我们要使用本地Mysql服务的话，直接简写为： 1mysql -uroot -p 使用services.msc命令可以打开服务管理，来启动和关闭Mysql服务《MySql服务器的启动和关闭》这篇文章就讲述了如何通过命令其启动和关闭Mysql SQL分类DDL数据定义语言，用来维护存储数据的结构代表指令: create、 drop、alter DML数据操纵语言，用来对数据进行行操作代表指令： insert、delete、update DML中又单独分了一个DQL，数据查询语言，代表指令： select DCL数据控制语言，主要负责权限管理理和事务代表指令： grant 、revoke 、commit MySQL架构MySQL 是一个可移植的数据库，几乎能在当前所有的操作系统上运行，如 Unix/Linux、Windows、Mac 和 Solaris。各种系统在底层实现⽅方⾯面各有不不同，但是 MySQL 基本上能保证在各个平台上的物理体系结构的一致性： 说说这张图： Client Connectors 是客户端链接，这个不用细说，就是应用程序与Mysql交互的接口，毕竟Mysql是要为程序提供数据存储服务的，所以必须将操作接口暴露出来，假如你是一个Java开发者，那么JDBC可以轻松链接上Mysql服务，就可以让你的Java程序使用上Mysql提供的服务 Connection Pool这个是连接池，Mysql与外界可能不止有一个连接，多次链接和断开会造成非常大的性能消耗，于是用使用连接池来管理这些链接，这就如Java的线程池来管理线程一样，通过连接池来避免性能损耗 Management Serveices &amp; Utilities是管理服务和工具组件，例如备份恢复、Mysql复制、安全性验证、集群、分区工作台等，下面会演示一个Mysql备份的例子 SQL Interface 就是SQL接口，存储过程、触发器、视图等，接受用户的SQL命令，并且返回用户需要查询的结果。接收DML(data manipulation language)数据操纵语言、DDL(data definition language数据库定义语言、比如select from就是调用SQL Interface Parser 是解析器，SQL命令传递到解析器的时候会被解析器验证和解析。解析器是由Lex和YACC实现的，是一个很长的脚本，将SQL语句分解成数据结构，并将这个结构传递到后续步骤，以后SQL语句的传递和处理就是基于这个结构的，如果在分解构成中遇到错误，那么就说明这个sql语句是不合理的 Optimizer 是查询优化器，SQL语句在查询之前会使用查询优化器对查询进行优化，这个不难理解，假如你有一张info 表中的字段是年龄(很显然这个额字段值是大于0的)，如果你在查询的时候的SQL语句是select * form info where age=-10，那么这条语句经过优化器之后不会再被执行，这就好像优化器知道不可能存在年龄小于0的条目 Caches 是高速缓存， 查询缓存，如果查询缓存有命中的查询结果，查询语句就可以直接去查询缓存中取数据。 通过LRU算法将数据的冷端溢出，未来得及时刷新到磁盘的数据页，叫脏页。 这个缓存机制是由一系列小缓存组成的。比如表缓存，记录缓存，key缓存，权限缓存等 Pluggable Storage Engines 是存储引擎，图中的圆柱体都是存储引擎，Mysql默认的存储引擎是InnoDB，后面谈论存储引擎 FileSystem 就是文件系统，Mysql数据库的数据最终还是要存放到文件中，所以我们可以理解为数据库就是一种帮我们管理数据的软件，处于文件系统的应用程序之间专门提供数据管理的软件，把数据的增删改查以及他的功能做了完美的封装，使用起来安全性更高，更方便我们队数据进行操作 Mysql存储引擎存储引擎是：数据库管理理系统如何存储数据、如何为存储的数据建立索引和如何更新、查询数据等技术的实现方法。MySQL的核心就是插件式存储引擎，支持多种存储引擎，所以你可以看到在Mysql的架构图上存储引擎的小插头，存储引擎是插拔式的，默认是InnoDB（从MySQL5.5.8开始，之前是MyISAM），当然也可以选择其他的存储引擎使用show engines;命令可以查看支持的存储引擎： 接下来说说他们的区别： 接下来说说他们的区别： MyISAM存储引擎MyISAM是MySQL官方提供默认的存储引擎，其特点是不支持事务、表锁和全文索引，对于一些OLAP系统(OLAP 系统强调数据分析，强调SQL执行市场，强调磁盘I/O，强调分区等)，操作速度快。关于《OLAP、OLTP的介绍和比较》 每个MyISAM在磁盘上存储成三个文件。文件名都和表名相同，扩展名分别是.frm（存储表定义）、.MYD (MYData，存储数据)、.MYI (MYIndex，存储索引)。这里特别要注意的是MyISAM不缓存数据文件，只缓存索引文件。 InnoDB存储引擎InnoDB存储引擎支持事务，主要面向OLTP方面的应用，其特点是行锁设置、支持外键，并支持类似于Oracle的非锁定读，即默认情况下读不产生锁。InnoDB将数据放在一个逻辑表空间中。InnoDB通过多版本并发控制来获得高并发性，实现了ANSI标准的4种隔离级别，默认为Repeatable，使用一种被称为next-key locking的策略避免幻读。 对于表中数据的存储，InnoDB采用类似Oracle索引组织表Clustered的方式进行存储。 InnoDB 存储引擎提供了具有提交、回滚和崩溃恢复能力的事务安全。但是对比Myisam的存储引擎，InnoDB 写的处理效率差一些并且会占用更多的磁盘空间以保留数据和索引 NDB存储引擎NDB存储引擎是一个集群存储引擎，类似于Oracle的RAC，但它是Share Nothing的架构，因此能提供更高级别的高可用性和可扩展性。NDB的特点是数据全部放在内存中，因此通过主键查找非常快。 关于NDB，有一个问题需要注意，它的连接(join)操作是在MySQL数据库层完成，不是在存储引擎层完成，这意味着，复杂的join操作需要巨大的网络开销，查询速度会很慢。 Memory (Heap) 存储引擎Memory存储引擎（之前称为Heap）将表中数据存放在内存中，如果数据库重启或崩溃，数据丢失，因此它非常适合存储临时数据。 Archive存储引擎正如其名称所示，Archive非常适合存储归档数据，如日志信息。它只支持INSERT和SELECT操作，其设计的主要目的是提供高速的插入和压缩功能。 Federated存储引擎Federated存储引擎不存放数据，它至少指向一台远程MySQL数据库服务器上的表，非常类似于Oracle的透明网关。 InnoDB与MyISAM应用场景参考：《InnoDB与MyISAM两者的区别》MyISAM管理非事务表。它提供高速存储和检索，以及全文搜索能力。如果应用中需要执行大量的SELECT查询，那么MyISAM是更好的选择。 InnoDB用于事务处理应用程序，具有众多特性，包括ACID事务支持。如果应用中需要执行大量的INSERT或UPDATE操作，则应该使用InnoDB，这样可以提高多用户并发操作的性能 存储引擎相关SQL查看Mysql已提供存储引擎 1show engines; 查看Mysql默认存储引擎: 1show variables like 'storage_engine'; 查看某个表的存储引擎: 1show create table 表名; 修改表的存储引擎： 1alter table 表名 engine=引擎","updated":"2020-03-13T03:06:29.688Z","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zouchanglin.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zouchanglin.cn/tags/MySQL/"}]},{"title":"Linux调度器","date":"2018-05-28T10:09:01.000Z","path":"2018/05/28/Linux调度器/","text":"进程是操作系统虚拟出来的概念，用来组织计算机中的任务。它从诞生到随着CPU时间执行，直到最终消失。不过，进程的生命都得到了操作系统内核的关照。就好像疲于照顾几个孩子的母亲内核必须做出决定，如何在进程间分配有限的计算资源，最终让用户获得最佳的使用体验。内核中安排进程执行的模块称为调度器（scheduler）。这里将介绍调度器的工作方式。 进程状态调度器可以切换进程状态（process state）。一个Linux进程从被创建到死亡，可能会经过很多种状态，比如执行、暂停、可中断睡眠、不可中断睡眠、退出等。我们可以把Linux下繁多的进程状态，归纳为三种基本状态。 就绪（Ready）: 进程已经获得了CPU以外的所有必要资源，如进程空间、网络连接等。就绪状态下的进程等到CPU，便可立即执行。 执行（Running）：进程获得CPU，执行程序。 阻塞（Blocked）：当进程由于等待某个事件而无法执行时，便放弃CPU，处于阻塞状态。 进程创建后，就自动变成了就绪状态。如果内核把CPU时间分配给该进程，那么进程就从就绪状态变成了执行状态。在执行状态下，进程执行指令，最为活跃。正在执行的进程可以主动进入阻塞状态，比如这个进程需要将一部分硬盘中的数据读取到内存中。在这段读取时间里，进程不需要使用CPU，可以主动进入阻塞状态，让出CPU。当读取结束时，计算机硬件发出信号，进程再从阻塞状态恢复为就绪状态。进程也可以被迫进入阻塞状态，比如接收到SIGSTOP信号。 调度器是CPU时间的管理员。Linux调度器需要负责做两件事：一件事是选择某些就绪的进程来执行；另一件事是打断某些执行中的进程，让它们变回就绪状态。不过，并不是所有的调度器都有第二个功能。有的调度器的状态切换是单向的，只能让就绪进程变成执行状态，不能把正在执行中的进程变回就绪状态。支持双向状态切换的调度器被称为抢占式（pre-emptive）调度器。 调度器在让一个进程变回就绪时，就会立即让另一个就绪的进程开始执行。多个进程接替使用CPU，从而最大效率地利用CPU时间。当然，如果执行中进程主动进入阻塞状态，那么调度器也会选择另一个就绪进程来消费CPU时间。所谓的上下文切换（context switch）就是指进程在CPU中切换执行的过程。内核承担了上下文切换的任务，负责储存和重建进程被切换掉之前的CPU状态，从而让进程感觉不到自己的执行被中断。应用程序的开发者在编写计算机程序时，就不用专门写代码处理上下文切换了。 进程的优先级调度器分配CPU时间的基本依据，就是进程的优先级。根据程序任务性质的不同，程序可以有不同的执行优先级。根据优先级特点，我们可以把进程分为两种类别。 实时进程（Real-Time Process）：优先级高、需要尽快被执行的进程。它们一定不能被普通进程所阻挡，例如视频播放、各种监测系统。普通进程（Normal Process）：优先级低、更长执行时间的进程。例如文本编译器、批处理一段文档、图形渲染。普通进程根据行为的不同，还可以被分成互动进程（interactive process）和批处理进程（batch process）。互动进程的例子有图形界面，它们可能处在长时间的等待状态，例如等待用户的输入。一旦特定事件发生，互动进程需要尽快被激活。一般来说，图形界面的反应时间是50到100毫秒。批处理进程没有与用户交互的，往往在后台被默默地执行。 实时进程由Linux操作系统创造，普通用户只能创建普通进程。两种进程的优先级不同，实时进程的优先级永远高于普通进程。进程的优先级是一个0到139的整数。数字越小，优先级越高。其中，优先级0到99留给实时进程，100到139留给普通进程。 一个普通进程的默认优先级是120。我们可以用命令nice来修改一个进程的默认优先级。例如有一个可执行程序叫app，执行命令： 1$nice -n -20 ./app 命令中的-20指的是从默认优先级上减去20。通过这个命令执行app程序，内核会将app进程的默认优先级设置成100，也就是普通进程的最高优先级。命令中的-20可以被换成-20至19中任何一个整数，包括-20 和 19。默认优先级将会变成执行时的静态优先级（static priority）。调度器最终使用的优先级根据的是进程的动态优先级： 1动态优先级 &#x3D; 静态优先级 – Bonus + 5 如果这个公式的计算结果小于100或大于139，将会取100到139范围内最接近计算结果的数字作为实际的动态优先级。公式中的Bonus是一个估计值，这个数字越大，代表着它可能越需要被优先执行。如果内核发现这个进程需要经常跟用户交互，将会把Bonus值设置成大于5的数字。如果进程不经常跟用户交互，内核将会把进程的Bonus设置成小于5的数。 O(n)和O(1)调度器下面介绍Linux的调度策略。最原始的调度策略是按照优先级排列好进程，等到一个进程运行完了再运行优先级较低的一个，但这种策略完全无法发挥多任务系统的优势。因此，随着时间推移，操作系统的调度器也多次进化。 先来看Linux 2.4内核推出的O(n)调度器。O(n)这个名字，来源于算法复杂度的大O表示法。大O符号代表这个算法在最坏情况下的复杂度。字母n在这里代表操作系统中的活跃进程数量。O(n)表示这个调度器的时间复杂度和活跃进程的数量成正比。 O(n)调度器把时间分成大量的微小时间片（Epoch）。在每个时间片开始的时候，调度器会检查所有处在就绪状态的进程。调度器计算每个进程的优先级，然后选择优先级最高的进程来执行。一旦被调度器切换到执行，进程可以不被打扰地用尽这个时间片。如果进程没有用尽时间片，那么该时间片的剩余时间会增加到下一个时间片中。 O(n)调度器在每次使用时间片前都要检查所有就绪进程的优先级。这个检查时间和进程中进程数目n成正比，这也正是该调度器复杂度为O(n)的原因。当计算机中有大量进程在运行时，这个调度器的性能将会被大大降低。也就是说，O(n)调度器没有很好的可拓展性。O(n)调度器是Linux 2.6之前使用的进程调度器。当Java语言逐渐流行后，由于Java虚拟机会创建大量进程，调度器的性能问题变得更加明显。 为了解决O(n)调度器的性能问题，O(1)调度器被发明了出来，并从Linux 2.6内核开始使用。顾名思义，O(1)调度器是指调度器每次选择要执行的进程的时间都是1个单位的常数，和系统中的进程数量无关。这样，就算系统中有大量的进程，调度器的性能也不会下降。O(1)调度器的创新之处在于，它会把进程按照优先级排好，放入特定的数据结构中。在选择下一个要执行的进程时，调度器不用遍历进程，就可以直接选择优先级最高的进程。 和O(n)调度器类似，O(1)也是把时间片分配给进程。优先级为120以下的进程时间片为： 1(140–priority)×20毫秒 优先级120及以上的进程时间片为： 1(140–priority)×5 毫秒 O(1)调度器会用两个队列来存放进程。一个队列称为活跃队列，用于存储那些待分配时间片的进程。另一个队列称为过期队列，用于存储那些已经享用过时间片的进程。O(1)调度器把时间片从活跃队列中调出一个进程。这个进程用尽时间片，就会转移到过期队列。当活跃队列的所有进程都被执行过后，调度器就会把活跃队列和过期队列对调，用同样的方式继续执行这些进程。 上面的描述没有考虑优先级。加入优先级后，情况会变得复杂一些。操作系统会创建140个活跃队列和过期队列，对应优先级0到139的进程。一开始，所有进程都会放在活跃队列中。然后操作系统会从优先级最高的活跃队列开始依次选择进程来执行，如果两个进程的优先级相同，他们有相同的概率被选中。执行一次后，这个进程会被从活跃队列中剔除。如果这个进程在这次时间片中没有彻底完成，它会被加入优先级相同的过期队列中。当140个活跃队列的所有进程都被执行完后，过期队列中将会有很多进程。调度器将对调优先级相同的活跃队列和过期队列继续执行下去。过期队列和活跃队列，如图2所示。 我们下面看一个例子，有五个进程，如表1所示： Linux操作系统中的进程队列（run queue），如表2所示： 那么在一个执行周期，被选中的进程依次是先A，然后B和C，随后是D，最后是E。 注意，普通进程的执行策略并没有保证优先级为100的进程会先被执行完进入结束状态，再执行优先级为101的进程，而是在每个对调活跃和过期队列的周期中都有机会被执行，这种设计是为了避免进程饥饿（starvation）。所谓的进程饥饿，就是优先级低的进程很久都没有机会被执行。 我们看到，O(1)调度器在挑选下一个要执行的进程时很简单，不需要遍历所有进程。但是它依然有一些缺点。进程的运行顺序和时间片长度极度依赖于优先级。比如，计算优先级为100、110、120、130和139这几个进程的时间片长度，如表3所示： 从表格中你会发现，优先级为110和120的进程的时间片长度差距比120和130之间的大了10倍。也就是说，进程时间片长度的计算存在很大的随机性。O(1)调度器会根据平均休眠时间来调整进程优先级。该调度器假设那些休眠时间长的进程是在等待用户互动。这些互动类的进程应该获得更高的优先级，以便给用户更好的体验。一旦这个假设不成立，O(1)调度器对CPU的调配就会出现问题。 完全公平调度器从2007年发布的Linux 2.6.23版本起，完全公平调度器（CFS，Completely Fair Scheduler）取代了O(1)调度器。CFS调度器不对进程进行任何形式的估计和猜测。这一点和O(1)区分互动和非互动进程的做法完全不同。 CFS调度器增加了一个虚拟运行时（virtual runtime）的概念。每次一个进程在CPU中被执行了一段时间，就会增加它虚拟运行时的记录。在每次选择要执行的进程时，不是选择优先级最高的进程，而是选择虚拟运行时最少的进程。完全公平调度器用一种叫红黑树的数据结构取代了O(1)调度器的140个队列。红黑树可以高效地找到虚拟运行最小的进程。 我们先通过例子来看CFS调度器。假如一台运行的计算机中本来拥有A、B、C、D四个进程。内核记录着每个进程的虚拟运行时，如表4所示： 系统增加一个新的进程E。新创建进程的虚拟运行时不会被设置成0，而会被设置成当前所有进程最小的虚拟运行时。这能保证该进程被较快地执行。在原来的进程中，最小虚拟运行时是进程A的1 000纳秒，因此E的初始虚拟运行时会被设置为1 000纳秒。新的进程列表如表5所示： 假如调度器需要选择下一个执行的进程，进程A会被选中执行。进程A会执行一个调度器决定的时间片。假如进程A运行了250纳秒，那它的虚拟运行时增加。而其他的进程没有运行，所以虚拟运行时不变。在A消耗完时间片后，更新后的进程列表，如表6所示： 可以看到，进程A的排序下降到了第三位，下一个将要被执行的进程是进程E。从本质上看，虚拟运行时代表了该进程已经消耗了多少CPU时间。如果它消耗得少，那么理应优先获得计算资源。 按照上述的基本设计理念，CFS调度器能让所有进程公平地使用CPU。听起来，这让进程的优先级变得毫无意义。CFS调度器也考虑到了这一点。CFS调度器会根据进程的优先级来计算一个时间片因子。同样是增加250纳秒的虚拟运行时，优先级低的进程实际获得的可能只有200纳秒，而优先级高的进程实际获得可能有300纳秒。这样，优先级高的进程就获得了更多的计算资源。 以上就是调度器的基本原理，以及Linux用过的几种调度策略。调度器可以更加合理地把CPU时间分配给进程。现代计算机都是多任务系统，调度器在多任务系统中起着顶梁柱的作用！ 原文地址：《调度器简介，以及Linux的调度策略》","updated":"2020-03-13T03:06:29.677Z","categories":[{"name":"操作系统理论","slug":"操作系统理论","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"}]},{"title":"结构体、位段与联合体","date":"2018-05-26T16:00:00.000Z","path":"2018/05/27/结构体、位段与联合体/","text":"结构体和指针是数据结构的根基，所以这篇博客这算是对结构体有一个重新的认识，主要内容包括：匿名结构体、结构体的自引用、结构体的不完整声明、结构体内存对齐、位段的使用、联合体的应用场景等等。 匿名结构体匿名结构体简言之就是没有名字的结构体，在结构体的时候就已经定义它的具体结构体对象。以后再也不允许创建新的结构体。这是我遇到的第一个坑，先看看下面这段代码： 12345678910111213141516171819202122232425#define _CRT_SECURE_NO_WARNINGS#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt; struct&#123; int age; int id;&#125;s; struct&#123; int age; int id;&#125;*p; int main(void)&#123; p = &amp;s; p-&gt;age = 20; p-&gt;id = 9; printf(\"id = %d ,age = %d\\n\", s.id, s.age); system(\"pause\"); return 0;&#125; 接着还是将VS的 #define _CRT_SECURE_NO_WARNINGS 去掉果然还是报出警告： warning C4133: “=”: 从“”到“”的类型不兼容这段代码居然连警告都没有直接跑起来了，结果发现是我冤枉了VS编译器，由于之前使用scanf和strcpy等函数的时候VS老是报警告说使用这些函数是不安全的，于是乎我果断在前面加了一句： #define _CRT_SECURE_NO_WARNINGS ,直接导致了VS没有报出警告信息。刚开始没有发现这个问题，接着在Linux下的gcc下跑了一回，警告! 原因：虽然两个结构体的成员都是一模一样的，但是都是匿名结构体，两个没有结构体标签，编译器认为上边的两个类型不同，所以这个操作时会报警告，但是由于部分编译器对这种情况的检查不严格，所以仍然是可以得出正确的结果，但是我们只需要明白这属于非法操作就行了!。 结构体的自引用结构体的自引用就是在结构中包含一个类型为该结构体本身的成员 1234567891011//错误写法typedef struct Node&#123; Node* p; int id;&#125;Node;//错误写法struct Node&#123; Node p; int id;&#125;; ①这种引用是非法的，这里的目的是使用typedef为结构体创建一个别名Node。但是这里是错误的，因为类型名的作用域是从语句的结尾开始，而在结构体内部是不能使用的，因为还没定义。 ②这种引用是非法的，因为成员p是另外一个完整的结构，其内部还将包含它自己的成员p.这第二个成员又是另一个完整的结构，它仍将包含自己的成员p，这样重复下去将永无止境。就像一个永远没有出口的递归！ 1234567891011//正确写法typedef struct Node&#123; struct Node* p; int id;&#125;Node; //正确写法struct Node&#123; struct Node* p; int id;&#125;; 这个声明和前面那个声明的区别在于p现在是一个指针而不是结构，编译器在结构的长度确定之前就已经知道了指针的长度，所以其自引用是合法的。 结构体的不完整声明结构体的不完整声明就是如果两个结构体互相包含，则需要对其中一个结构体进行不完整声明。比如在A结构体成员中包含B结构体指针，在B结构体成员中包含A结构体指针，但是总是得有一个在前面声明，所以就有了不完整声明！ 1234567891011121314//结构体不完整声明struct B; struct A&#123; //结构体A中包含指向结构体B的指针 struct B* pB;&#125;; struct B&#123; //结构体B中包含指向结构体A的指针 struct A* pA;&#125;; 结构体内存对齐结构体内存对齐的概念比较重要，也是面试中经常考到的问题！结构体内存对齐：元素是按照定义顺序一个一个放到内存中去的，但并不是紧密排列的。从结构体存储的首地址开始，每个元素放置到内存中时，它都会认为内存是按照自己的大小来划分的，因此元素放置的位置一定会在自己宽度的整数倍上开始。 结构体内存对齐的好处 效率原因:CPU访问某个数据时，要求其存储地址必须是相应数据类型的自然边界。对于存储地址不在其相应类型自然边界的数据，不支持非对齐数据访问的CPU，会导致CPU异常；即使是支持非对齐数据访问的CPU，也会严重影响程序效率，因为需要多次访问才可以拿到完整的的数据！内存对齐这种做法相当于是在 拿空间换时间！ 移植性原因:不是所有的硬件平台都能访问任意地址上的任意数据的；某些硬件平台只能在某些地址处取某些特定类型的数据，否则抛出硬件异常。 结构体内存对齐的规则1、第一个成员放在与结构体变量偏移量为0的地址处2、剩下的其他成员对齐到对齐数的整数倍地址处。对齐数就是编译器默认对齐数与该成员大小的较小值，VS的编译器默认值是8，Linux的gcc编译器是4。更改方法：在结构体struct之前加上#pragma pack(对齐数)，在struct之后加上#pragma pack;便可以设置两条指令之间的结构体的对齐参数。注意对齐参数不能任意设置，只能是内置类型已有的字节数，如设置为1、2、4…3、结构体的总大小为最大对齐数的整数倍 4、如果有嵌套了结构体的情况，嵌套的结构体对齐到自身的最大对齐数的整数倍处，结构体的总大小就是所有对齐数中最大对齐数的整数倍 123456789101112131415161718192021222324#define _CRT_SECURE_NO_WARNINGS#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include &lt;stddef.h&gt; typedef struct S&#123; int a; char b; double c; char d;&#125;S; int main(void)&#123; printf(\"%d\\n\", offsetof(S, a)); printf(\"%d\\n\", offsetof(S, b)); printf(\"%d\\n\", offsetof(S, c)); printf(\"%d\\n\", offsetof(S, d)); printf(\"sizeof(struct S) = %d\\n\",sizeof(S)); system(\"pause\"); return 0;&#125; 我们是如何得知结构体某个成员相对于结构体起始位置的偏移量呢？需要一个宏：offsetof,这个宏的设置比较巧妙，首先将0地址强制转换为type类型的指针，然后就可以定位到member在结构体中偏移位置，编译器把0当做有效地址，认为0是type指针的起始地址，这样就立刻得出了偏移量! 1#define offsetof(type,menber) (size_t)&amp;(((type*)0)-&gt;member) 第一个成员放在与结构体变量偏移量为0的地址处，现在可用偏移为4偏移，接下来存char b; 由于4是1的倍数，故而，b占用4偏移，接下来可用偏移为5偏移，接下来该存double c; 由于5不是8的倍数，所以向后偏移5，6，7，都不是8的倍数，偏移到8时，8是8的倍数，故而c从8处开始存储，占用8，9，10，11，12，13，14，15偏移，现在可用偏移为16偏移，最后该存char d ;因为16是1的倍数，故d占用16偏移，接下来在整体向后偏移一位，现处于17偏移，min(默认对齐参数，类型最大字节数)=8；因为17不是8的倍数，所以继续向后偏移18…23都不是8的倍数，到24偏移处时，28为8的整数倍，故而，该结构体大小为24个字节。接下来我们再看这样一个结构体: 123456789101112131415161718192021222324#define _CRT_SECURE_NO_WARNINGS#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include &lt;stddef.h&gt; typedef struct S&#123; double a; int b; char c; char d;&#125;S; int main(void)&#123; printf(\"%d\\n\", offsetof(S, a)); printf(\"%d\\n\", offsetof(S, b)); printf(\"%d\\n\", offsetof(S, c)); printf(\"%d\\n\", offsetof(S, d)); printf(\"sizeof(struct S) = %d\\n\",sizeof(S)); system(\"pause\"); return 0;&#125; 这说明我们在设计结构体的时候应该尽量让小的成员贴在一起，避免不必要的空间浪费！ 结构体的应用场景1、一般当内置内存无法满足用户需要，没有合适类型对应对象时，需要封装特定的类型2、当函数有多个参数时，返回值过多，需要封装特定类型，将参数打包返回。 位段C语言允许在一个结构体中以位为单位来指定其成员所占内存长度，这种以位为单位的成员称为“位段”或称“位域”( bit field) 。利用位段能够用较少的位数存储数据。一个位段必须存储在同一存储单元中，不能跨两个单元。如果第一个单元空间不能容纳下一个位段，则该空间不用，而从下一个单元起存放该位段。(见下例)1.位段声明和结构体类似2.位段的成员必须是int、unsigned int、signed int3.位段的成员名后边有一个冒号和一个数字 123456789101112struct A&#123; int a : 2; //先开辟4个字节的空间，也就是32个比特位 //a占掉2个比特位，32-2=30 int b : 5; //b占掉5个比特位，30-5=25 int c : 10; //c占掉10个比特位，25-10=15 int d : 30; //d占30个比特位，前边开辟的4个字节已经不够用了，因此在开辟四个字节&#125;; 位段无跨平台性1.int位段被当成是有符号还是无符号是不确定的2.位段中最大位的数目不能确定3.位段中的成员在内存中是从右向左还是从左向右分配的不确定4.当一个结构包含两个位段，第二个位段成员比较大，放不下在第一个位段剩余的为时，舍弃还是利用第二个位段成员是不确定的 位段应用场景由于位段比较节省内存，通常用于网络数据包的封装信息，在网络此次发达的时代，为了减缓网络拥堵提交网络访问速率，在封装数据包头部信息的时候通常是采用位段的方式来存储数据，减少网络流量！ 联合体在进行某些算法的C语言编程的时候，需要使几种不同类型的变量存放到同一段内存单元中。也就是使用覆盖技术，几个变量互相覆盖。这种几个不同的变量共同占用一段内存的结构，在C语言中，被称作“共用体”类型结构，简称共用体，也叫联合体。 联合体大小的计算1.联合的大小至少是最大成员的大小2.当最大成员大小不是最大对齐数的整数倍时，就要对齐要最大对齐数的整数倍 12345678910111213141516union un1&#123; int i; char arr[6];&#125;;union un2&#123; int i; short arr[5];&#125;;int main(void)&#123; printf(\"sizeof(un1) = %d\\n\",sizeof(union un1)); //8 printf(\"sizeof(un1) = %d\\n\", sizeof(union un2));//12 system(\"pause\"); return 0;&#125; 联合体判断大小端1234567891011//返回1则是小端存储、返回0则是大端存储int decide()&#123; union un&#123; int i; char c; &#125;; union un u1; u1.i = 1; return (int)u1.c;&#125; 巧用联合体与结构体12345678910111213141516171819202122232425#define _CRT_SECURE_NO_WARNINGS#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;string.h&gt;#include&lt;assert.h&gt; typedef struct S2&#123; unsigned char a; unsigned char b; unsigned char c; unsigned char d;&#125;S2;typedef union S&#123; long num; S2 s1;&#125;S; int main(void)&#123; S s; s.num = 2378912378; printf(\"%d.%d.%d.%d\\n\", s.s1.a, s.s1.b, s.s1.c, s.s1.d ); system(\"pause\"); return 0;&#125; 利用联合体这种巧妙地存储结构就可以轻松的将数据拆分出来，这绝对是一个非常有用的技巧！ 本次对结构体、联合体、位段进行了比较综合的复习，温故而知新，啊哈哈！","updated":"2020-03-13T03:06:29.759Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://zouchanglin.cn/tags/C-C/"}]},{"title":"浅谈函数栈帧","date":"2018-05-21T15:29:01.000Z","path":"2018/05/21/浅谈函数栈帧/","text":"先说说函数栈帧的概念，函数栈帧又叫函数运行时堆栈，栈帧也叫过程活动记录，是编译器用来实现函数调用的一种数据结构。这个该概念说起来比较抽象，简单的说就是函数在被调用时的一块空间，这个空间由esp寄存器和ebp寄存器共同维护。首先应该明白，栈是从高地址向低地址延伸的。每个函数的每次调用，都有它自己独立的一个栈帧，这个栈帧中维持着所需要的各种信息。寄存器ebp指向当前的栈帧的底部(高地址)，寄存器esp指向当前的栈帧的顶部(低地址)。 main函数的调用程序的入口必须是main吗？每当我们在点击一个(或者在命令行打开)一个C程序的时候，程序立马就能运行起来，从刚开始学习C语言的时候我们都直到main函数是程序的入口。至于为什么这么说完全是因为ANSIC就是这么规定的，一个程序的执行并不一定需要main函数，但是一定需要一个入口，做过单片机的同学应该深有体会。起来从CPU角度来看，将要执行的指令地址放在程序计数器里，程序需要执行必然需要一个入口地址。通用的可执行文件格式总会指定一个入口地址，这样操作系统才可以调度这样一个程序执行指令。所谓的main函数，就是执行时把这个程序装入任务调度器中，调度器执行调度的入口函数而已，而main函数只是个程序员和调度器之间的约定。 mainCRTStartup函数：扯得有点远了，在VC++下，连接器对控制台程序设置的入口函数是 mainCRTStartup，mainCRTStartup再调用main函数，mainCRTStartup所做的初始化准备工作，例如获取命令行参数、获取环境变量值，是通过调用相应的Windows系统调用来实现的。 123456789101112131415#include&lt;stdio.h&gt;int Add(int x, int y)&#123; int z = x + y; return z;&#125;int main(void)&#123; int a = 10; int b = 20; int c = Add(a, b); printf(\"c = %d\\n\",c); return 0;&#125; 下面是其反汇编代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778int Add(int x, int y)&#123;009613D0 push ebp 009613D1 mov ebp,esp 009613D3 sub esp,0CCh 009613D9 push ebx 009613DA push esi 009613DB push edi 009613DC lea edi,[ebp-0CCh] 009613E2 mov ecx,33h 009613E7 mov eax,0CCCCCCCCh 009613EC rep stos dword ptr es:[edi] int z &#x3D; x + y;009613EE mov eax,dword ptr [x] 009613F1 add eax,dword ptr [y] 009613F4 mov dword ptr [z],eax return z;009613F7 mov eax,dword ptr [z] &#125;009613FA pop edi 009613FB pop esi 009613FC pop ebx 009613FD mov esp,ebp 009613FF pop ebp 00961400 ret ------------------- main.c ------------------------int main(void)&#123;00961410 push ebp 00961411 mov ebp,esp 00961413 sub esp,0E4h 00961419 push ebx 0096141A push esi 0096141B push edi 0096141C lea edi,[ebp-0E4h] 00961422 mov ecx,39h 00961427 mov eax,0CCCCCCCCh 0096142C rep stos dword ptr es:[edi] int a &#x3D; 10;0096142E mov dword ptr [a],0Ah int b &#x3D; 20;00961435 mov dword ptr [b],14h int c &#x3D; Add(a, b);0096143C mov eax,dword ptr [b] 0096143F push eax 00961440 mov ecx,dword ptr [a] 00961443 push ecx 00961444 call _Add (09610E6h) 00961449 add esp,8 0096144C mov dword ptr [c],eax printf(&quot;c &#x3D; %d\\n&quot;,c);0096144F mov esi,esp printf(&quot;c &#x3D; %d\\n&quot;,c);00961451 mov eax,dword ptr [c] 00961454 push eax 00961455 push 965858h 0096145A call dword ptr ds:[969118h] 00961460 add esp,8 00961463 cmp esi,esp 00961465 call __RTC_CheckEsp (0961140h) system(&quot;pause&quot;);0096146A mov esi,esp 0096146C push 965864h 00961471 call dword ptr ds:[969110h] 00961477 add esp,4 0096147A cmp esi,esp 0096147C call __RTC_CheckEsp (0961140h) return 0;00961481 xor eax,eax &#125;00961483 pop edi 00961484 pop esi 00961485 pop ebx 00961486 add esp,0E4h 0096148C cmp ebp,esp 0096148E call __RTC_CheckEsp (0961140h) 00961493 mov esp,ebp 00961495 pop ebp 00961496 ret 由此也可以看出编程语言的进化历程，要是现在还是由我们来写这些汇编代码，想想都难受… 汇编指令简述数据传送指令这部分指令包括通用数据传送指令MOV、条件传送指令CMOVcc、堆栈操作指令PUSH/PUSHA/PUSHAD/POP/POPA/POPAD、交换指令XCHG/XLAT/BSWAP、地址或段描述符选择子传送指令LEA/LDS/LES/LFS/LGS/LSS等。注意，CMOVcc不是一条具体的指令，而是一个指令簇，包括大量的指令，用于根据EFLAGS寄存器的某些位状态来决定是否执行指定的传送操作。 在本例中，可以将MOV指令理解为赋值语句，例如 mov ebp，esp就是将esp中的值赋给ebp；push指令理解为压栈，例如 push ebx 就是将ebx寄存器压栈(入栈)，同样的道理，pop ebx 就是将ebx寄存器弹栈(出栈)，例如 lea edi,[ebp-0E4h] 就是取源操作数地址的偏移量，并把它传送到目的操作数所在的单元 整数和逻辑运算指令这部分指令用于执行算术和逻辑运算，包括加法指令ADD/ADC、减法指令SUB/SBB、加一指令INC、减一指令DEC、比较操作指令CMP、乘法指令MUL/IMUL、除法指令DIV/IDIV、符号扩展指令CBW/CWDE/CDQE、十进制调整指令DAA/DAS/AAA/AAS、逻辑运算指令NOT/AND/OR/XOR/TEST等。 在本例中用到了 add和sub 指令，例如add esp,8，就是给esp寄存器中的值加上8，执行加法运算；还有一个是cmp指令，cmp是比较指令,cmp的功能是相当于减法指令,只是不保存结果.cmp指令执行后,将对标志寄存器产生影响.其他相关指令通过识别这些被影响的标志寄存器来得知比较结果. 串操作指令这部分指令用于对数据串进行操作，包括串传送指令MOVS、串比较指令CMPS、串扫描指令SCANS、串加载指令LODS、串保存指令STOS，这些指令可以有选择地使用REP/REPE/REPZ/REPNE和REPNZ的前缀以连续操作。 本例中使用到了rep stos等等lea edi,[ebp-0C0h]mov ecx,30hmov eax,0CCCCCCCChrep stos dword ptr es:[edi]rep指令的目的是重复其上面的指令.ECX的值是重复的次数.STOS指令的作用是将eax中的值拷贝到ES:EDI指向的地址.LEA叫做loadEffectiveAddress，加载有效的地址 其他指令在本例中未出现，故在此不再赘述 函数栈帧解析看完上述的汇编指令简述，再次看到汇编语言应该就不那么陌生了，接下来正式进入函数栈帧的分析环节main的函数栈帧 由前面我们已经知道，是mainCRTStartup函数调用的main函数，main函数终究也是一个函数，是函数调用就肯定需要空间的，由esp寄存器和ebp寄存器来维护这块空间，这块空间就叫函数栈帧 1234567891000961410 push ebp 00961411 mov ebp,esp 00961413 sub esp,0E4h 00961419 push ebx 0096141A push esi 0096141B push edi 0096141C lea edi,[ebp-0E4h] 00961422 mov ecx,39h 00961427 mov eax,0CCCCCCCCh 0096142C rep stos dword ptr es:[edi] 首先将ebp寄存器压栈，然后将ebp的位置移向了esp的位置，esp有向上移动了04E个位置，然后将ebx、esi、edi寄存器压栈，然后让edi寄存器加载红线所示的地址，从edi的位置开始copy，重复拷贝ecx次数据，一次拷贝的大小是双字节(4个字节)，拷贝到eax里面! 0CCCCCCCC在被解析的时候就是乱码，对应的ACCSI码也就是”烫”,所以我们在使用未初始化的变量的时候又时会打印出”烫烫烫…”这也就不足为奇了 变量的创建以及函数调用时参数拷贝： 123456789int a &#x3D; 10;0096142E mov dword ptr [a],0Ah int b &#x3D; 20;00961435 mov dword ptr [b],14h int c &#x3D; Add(a, b);0096143C mov eax,dword ptr [b] 0096143F push eax 00961440 mov ecx,dword ptr [a] 00961443 push ecx 可以看出，首先创建了a和b变量并做了相应的初始化操作，然后将b的值放入到eax寄存器中，然后将eax寄存器压栈，同样把a变量放入ecx寄存器中，然后将ecx压栈: 也可以得出结论： C语言中所有的参数传递都是值传递，形式参数只是实参的一份临时拷贝 后面的参数先被压入栈中，即参数传递的顺序是右向左的 Add函数的函数的函数栈帧 1200961444 call _Add (09610E6h) 00961449 add esp,8 call指令先将下一条指令语句的给地址存起来了，方便在函数执行完毕之后执行下一条语句，这一点至关重要，一个函数执行完毕一定要跳到下一条语句的地方继续执行才可以。Add函数栈帧的创建于main函数栈帧的创建与初始化操作是一样的，同样的道理Add函数也是采用同样的方式初始化 12345678910009613D0 push ebp 009613D1 mov ebp,esp 009613D3 sub esp,0CCh 009613D9 push ebx 009613DA push esi 009613DB push edi 009613DC lea edi,[ebp-0CCh] 009613E2 mov ecx,33h 009613E7 mov eax,0CCCCCCCCh 009613EC rep stos dword ptr es:[edi] 1234567891011&#x2F;&#x2F;接下来就是创建Z变量并且执行加法操作将结果返回00DE13EE mov eax,dword ptr [x] 00DE13F1 add eax,dword ptr [y] 00DE13F4 mov dword ptr [z],eax 00DE13F7 mov eax,dword ptr [z]00DE13FA pop edi 00DE13FB pop esi 00DE13FC pop ebx 00DE13FD mov esp,ebp 00DE13FF pop ebp 00DE1400 ret 首先将x的值放入eax寄存器中，然后执行加法运算将结果放入eax寄存器中，然后把eax寄存器中的值放入变量z，然后将z的值存入eax寄存器作为返回，接着将edi、esi、ebx三个寄存器分别弹栈，又将ebp移向了esp的位置，此时Add函数的栈帧销毁，代表整个Add函数的结束！当pop ebp的时候，将ebp弹栈，此时下面正好就是Call指令下一条指令的地址，这样就能接着Add的函数的结束继续执行下面的代码了！ 1200961449 add esp,8 0096144C mov dword ptr [c],eax 然后把eax寄存器中的值放入变量c，这样就拿到了Add函数的返回值，所以说返回值是寄存器带回来的！ VC6.0之坑1234567891011121314#include &lt;stdio.h&gt;void fun()&#123; int tmp = 10; int *p = (int*)(* (&amp;tmp+1)); * (p-1) = 20;&#125;int main(void)&#123; int a = 0; fun(); printf(\"a = %d\\n\",a); return 0;&#125; 在VC6.0的编译器下打印结果是20（VC6.0是多么可怕从这里也就可以看出了…），只要了解函数栈帧的话本题就不难解释，tmp的地址加一再解引用刚好就是main函数的ebp的地址，然后p-1得到的就是main函数中a的地址，这样的话打印出20也就不足为奇了！ 函数栈帧的理解还是很重要的，其中涉及到函数栈帧的初始化，参数传递，返回值传递等等问题，学习函数栈帧就是明白函数的整个调用过程，还是非常重要的一部分！","updated":"2020-03-13T03:06:29.749Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://zouchanglin.cn/tags/C-C/"}]},{"title":"Listener和Filter","date":"2018-04-29T16:00:00.000Z","path":"2018/04/30/Listener和Filter/","text":"监听器 listener其中 servlet规范包括三个技术点：servlet listener filter 什么是监听器？监听器就是监听某个对象的的状态变化的组件 监听器的相关概念：事件源：被监听的对象 —– 三个域对象 request、session、servletContext监听器：监听事件源对象 事件源对象的状态的变化都会触发监听器 —6+2注册监听器：将监听器与事件源进行绑定响应行为：监听器监听到事件源的状态变化时 所涉及的功能代码 —程序员编写代码 监听器有哪些？第一维度：按照被监听的对象划分：ServletRequest域 HttpSession域 ServletContext域第二维度：监听的内容分：监听域对象的创建与销毁的 监听域对象的属性变化的 监听三大域对象的创建与销毁监听ServletContext域的创建与销毁的监听器：ServletContextListenerServlet域的生命周期何时创建：服务器启动创建何时销毁：服务器关闭销毁 监听器的编写步骤（重点）a、编写一个监听器类去实现监听器接口b、覆盖监听器的方法c、需要在web.xml中进行配置—注册 监听的方法：1234567891011121314public class MyServletContextListener implements ServletContextListener&#123; @Override //监听context域对象的销毁 public void contextDestroyed(ServletContextEvent con) &#123; System.out.println(\"context销毁...\"); &#125; @Override //监听context域对象的创建 public void contextInitialized(ServletContextEvent arg0) &#123; System.out.println(\"context创建...\"); &#125;&#125; 1234&lt;!-- 在web.xml中注册监听器 --&gt;&lt;listener&gt; &lt;listener-class&gt;com.xpu.create.MyServletContextListener&lt;/listener-class&gt;&lt;/listener&gt; ServletContextListener监听器的主要作用 初始化的工作：初始化对象 初始化数据 —- 加载数据库驱动 连接池的初始化 加载一些初始化的配置文件 — spring的配置文件 任务调度—-定时器—-Timer/TimerTask 监听Httpsession域的创建于销毁的监听器：HttpSessionListenerHttpSession对象的生命周期何时创建：第一次调用request.getSession时创建 何时销毁：服务器关闭销毁、session过期、手动销毁 HttpSessionListener的方法123456789101112public class MyHTTPSessionListener implements HttpSessionListener &#123; @Override public void sessionCreated(HttpSessionEvent se) &#123; System.out.println(\"sessionCreated:\"+se.getSession().getId()); &#125; @Override public void sessionDestroyed(HttpSessionEvent se) &#123; System.out.println(\"sessionDestroyed:\"+se.getSession().getId()); &#125;&#125; 监听ServletRequest域创建与销毁的监听器：ServletRequestListenerServletRequest的生命周期何时创建：每一次请求都会创建request 何时销毁：请求结束 ServletRequestListener的方法123456789101112public class MyServletRequqestListener implements ServletRequestListener &#123; @Override public void requestDestroyed(ServletRequestEvent sre) &#123; System.out.println(\"创建request\"); &#125; @Override public void requestInitialized(ServletRequestEvent sre) &#123; System.out.println(\"销毁request\"); &#125;&#125; 监听三大域对象的属性变化域对象的通用的方法setAttribute(name,value) 方法创建或改变某个新属性 getAttribute(name) 方法通过名称获取属性的值 removeAttribute(name) 方法通过名称删除属性的值 ServletContextAttibuteListener的方法1234567891011121314151617181920public class MyServletContextAttributeListener implements ServletContextAttributeListener&#123; @Override public void attributeAdded(ServletContextAttributeEvent scae) &#123; System.out.println(scae.getName());//放到域中的name System.out.println(scae.getValue());//放到域中的value &#125; @Override public void attributeRemoved(ServletContextAttributeEvent scae) &#123; System.out.println(scae.getName());//获得修改前的name System.out.println(scae.getValue());//获得修改前的value &#125; @Override public void attributeReplaced(ServletContextAttributeEvent scae) &#123; System.out.println(scae.getName());//修改域中的name System.out.println(scae.getValue());//修改域中的value &#125;&#125; HttpSessionAttributeListener、ServletRequestAriibuteListenr监听器方法同上，故此不再赘述 与session中的绑定的对象相关监听器与session中的绑定的对象相关监听器即是对象感知监听器 绑定状态：就一个对象被放到session域中 解绑状态：就是这个对象从session域中移除了 钝化状态：是将session内存中的对象持久化（序列化）到磁盘 活化状态：就是将磁盘上的对象再次恢复到session内存中 当用户很多时，怎样对服务器进行优化？这就涉及到对象的钝化与活化，只要把内存中的对象存储到磁盘中就可以从很大程度上减轻内存的消耗，从而达到服务器优化的目的！ 绑定与解绑的监听器：HttpSessionBindingListener1234567891011121314public class Person implements HttpSessionBindingListener&#123; @Override //绑定的方法 public void valueBound(HttpSessionBindingEvent event) &#123; System.out.println(\"Person 被绑定\"); &#125; @Override //解绑的方法 public void valueUnbound(HttpSessionBindingEvent event) &#123; System.out.println(\"Person 解绑\"); &#125;&#125; 123456789protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; HttpSession session = request.getSession(); Person p = new Person(\"tom\", 19); //触发绑定的监听器方法valueBound() session.setAttribute(\"person\", p); //触发解除绑定的监听器方法valueUnbound() session.removeAttribute(\"person\");&#125; 钝化与活化的监听器:HttpSessionActivationListener1234567891011121314public class Students implements HttpSessionActivationListener,Serializable&#123; private String name; private int age; //钝化（内存----&gt;硬盘） @Override public void sessionWillPassivate(HttpSessionEvent se) &#123; System.out.println(\"Student对象被钝化了！\"); &#125; //活化（硬盘----&gt;内存） @Override public void sessionDidActivate(HttpSessionEvent se) &#123; System.out.println(\"Student对象被活化了！\"); &#125;&#125; 想要实现对象的钝化和活化的时候需要实现Serializable接口，这个属于对象序列化的接口就不赘述了 123456789public class AServlet extends HttpServlet &#123; protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; HttpSession session = request.getSession(); Students students = new Students(\"tom\", 19); session.setAttribute(\"students\", students); System.out.println(\"students被放入session域中\"); &#125;&#125; 首先我们访问这个AServlet，然后关掉服务器，注意：不要点击控制台那个停止，如图所示： 此时session域的对象会转储到apache-tomcat-7.0.92\\work\\Catalina\\localhost\\你的工程路径这个文件夹中 重新启动服务器后访问BServlet的时候，对象便会活化，重新加载到内存，监听器监听到后便会执行相关的函数： 123456789public class BServlet extends HttpServlet &#123; protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; //从session域中获得students HttpSession session = request.getSession(); Students attribute = (Students)session.getAttribute(\"students\"); System.out.println(attribute.getName()+\":\"+attribute.getAge()); &#125;&#125; 可以通过配置文件指定对象钝化时间 —&gt; 对象多长时间不用被钝化 在META-INF下创建一个context.xml： 12345678910&lt;Context&gt; &lt;!-- maxIdleSwap:session中的对象多长时间不使用就钝化(以分钟为单位) --&gt; &lt;!-- directory:钝化后的对象的文件写到磁盘的哪个目录下 配置钝化的对象文件在 work/catalina/localhost/钝化文件 --&gt; &lt;Manager className=\"org.apache.catalina.session.PersistentManager\" maxIdleSwap=\"1\"&gt; &lt;Store className=\"org.apache.catalina.session.FileStore\" directory=\"mydirectory\" /&gt; &lt;/Manager&gt;&lt;/Context&gt; 过滤器 filterfilter的简介过滤器可以拦截所有访问web资源的请求或响应操作。执行过滤任务的对象，这些任务是针对对某一资源（servlet 或静态内容）的请求或来自某一资源的响应执行的，抑或同时针对这两者执行 ，是对客户端访问资源的过滤，符合条件放行，不符合条件不放行，并且可以对目标资源访问前后进行逻辑处理，下面是Filter的基本使用流程： 编写一个过滤器的类实现Filter接口 实现接口中尚未实现的方法(着重实现doFilter方法) 在web.xml中进行配置(主要是配置要对哪些资源进行过滤) filter生命周期及其APIFilter接口有三个方法，并且这个三个都是与Filter的生命相关的方法 init(Filterconfig)：代表filter对象初始化方法 filter对象创建时执行 doFilter(ServletRequest,ServletResponse,FilterCha)：代表filter执行过滤的核心方法，如果某资源在已经被配置到这个filter进行过滤的话，那么每次访问这个资源都会执行doFilter方法 destory()：代表是filter销毁方法 当filter对象销毁时执行该方法 Filter对象的生命周期： Filter何时创建：服务器启动时就创建该filter对象 Filter何时销毁：服务器关闭时filter销毁 123456789101112131415161718192021public class QuickFilter1 implements Filter&#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; System.out.println(\"init\"); &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; //拦截别人的访问 System.out.println(\"quick1 running ...\"); //放行请求 chain.doFilter(request, response); &#125; @Override public void destroy() &#123; System.out.println(\"destroy\"); &#125;&#125; web.xml文件的配置： 注意：Filter的过滤顺序是按照filter-mapping的顺序过滤的 1234567891011121314&lt;filter&gt; &lt;filter-name&gt;QuickFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.xpu.web.filter.QuickFilter1&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;QuickFilter1&lt;/filter-name&gt; &lt;!-- 默认过滤所有url --&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;QuickFilter2&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; Filter的API详解init(FilterConfig)方法其中参数config代表 该Filter对象的配置信息的对象，内部封装是该filter的配置信息。 假设web.xml配置了如下的Filter： 123456789&lt;filter&gt; &lt;display-name&gt;QuickFilter3&lt;/display-name&gt; &lt;filter-name&gt;QuickFilter3&lt;/filter-name&gt; &lt;filter-class&gt;com.xpu.web.filter.QuickFilter3&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;aaa&lt;/param-name&gt; &lt;param-value&gt;AAA&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt; 那么在filterConfig中就油API可以获取这些配置： 123456789101112//Filter的过滤顺序是按照filter-mapping的顺序过滤的public void init(FilterConfig fConfig) throws ServletException &#123; //fConfig维护着Filter的配置信息 System.out.println(fConfig.getFilterName());//&lt;filter-name&gt;QuickFilter3&lt;/filter-name&gt; //获取初始化参数 System.out.println(fConfig.getInitParameter(\"aaa\")); //AAA //获取servletContext ServletContext servletContext = fConfig.getServletContext(); System.out.println(\"init\");&#125; destory()方法filter对象销毁时执行 doFilter方法1doFilter(ServletRequest,ServletResponse,FilterChain) 其中的参数： ServletRequest/ServletResponse：每次在执行doFilter方法时 web容器负责创建一个request和一个response对象作为doFilter的参数传递进来。该request个该response就是在访问目标资源的service方法时的request和response。 FilterChain：过滤器链对象，通过该对象的doFilter方法可以放行该请求 Filer的配置url-pattern配置时： 完全匹配 /sertvle1 目录匹配 /aaa/bbb/* 使用的最多的方法 /user/*：访问前台的资源进入此过滤器 /admin/*：访问后台的资源时执行此过滤器 扩展名匹配 *.txt 、*.jsp 注意：url-pattern可以使用servlet-name替代，也可以混用 dispatcher：该属性指定了对某种访问方式的过滤： REQUEST：默认值，代表直接访问某个资源时执行filter FORWARD：转发时才执行filter INCLUDE: 包含资源时执行filter ERROR：发生错误时 进行跳转是执行filter Filter的作用 公共代码的提取 可以对request和response中的方法进行增强(装饰者模式/动态代理) 这个特点用一个示例来解决get请求或者post请求乱码的问题 对于POST请求： 123456//第一种request.setCharacterEncoding(\"utf-8\");//第二种request.setCharacterEncoding(this.getServletContext().getInitParameter(\"charset\"));//备注：这种获取方式是因为在web.xml中进行了如下配置 12345&lt;!-- 设置编码 --&gt;&lt;context-param&gt; &lt;param-name&gt;charset&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt;&lt;/context-param&gt; 对于GET请求： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class EncodingFilter implements Filter&#123; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; //在传递request之前对request的getParameter方法进行增强 /* * 装饰者模式(包装) * * 1、增强类与被增强的类要实现统一接口 * 2、在增强类中传入被增强的类 * 3、需要增强的方法重写 不需要增强的方法调用被增强对象的 */ //被增强的对象 HttpServletRequest req = (HttpServletRequest) request; //增强对象 EnhanceRequest enhanceRequest = new EnhanceRequest(req); chain.doFilter(enhanceRequest, response); &#125; @Override public void destroy() &#123; &#125; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; &#125;&#125;class EnhanceRequest extends HttpServletRequestWrapper&#123; private HttpServletRequest request; public EnhanceRequest(HttpServletRequest request) &#123; super(request); this.request = request; &#125; //对getParaameter增强 @Override public String getParameter(String name) &#123; String parameter = request.getParameter(name);//乱码 if(value == null || value.trim().equals(\"\"))&#123; value=\"\"; &#125; try &#123; parameter = new String(parameter.getBytes(\"iso8859-1\"),\"UTF-8\"); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; return parameter; &#125;&#125; 进行权限控制 下面是一个使用了过滤器自动登录的示例： 过滤器的核心代码：（其他业务逻辑如上图所示） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class AutoLoginFilter implements Filter&#123; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest req = (HttpServletRequest) request; HttpServletResponse resp = (HttpServletResponse) response; HttpSession session = req.getSession(); //获得cookie中用户名和密码 进行登录的操作 //定义cookie_username String cookie_username = null; //定义cookie_password String cookie_password = null; //获得cookie Cookie[] cookies = req.getCookies(); if(cookies!=null)&#123; for(Cookie cookie : cookies)&#123; //获得名字是cookie_username和cookie_password if(\"cookie_username\".equals(cookie.getName()))&#123; cookie_username = cookie.getValue(); //恢复中文用户名 cookie_username = URLDecoder.decode(cookie_username, \"UTF-8\"); &#125; if(\"cookie_password\".equals(cookie.getName()))&#123; cookie_password = cookie.getValue(); &#125; &#125; &#125; //判断username和password是否是null if(cookie_username!=null&amp;&amp;cookie_password!=null)&#123; //登录的代码 UserService service = new UserService(); User user = null; try &#123; user = service.login(cookie_username,cookie_password); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; //将登录的用户的user对象存到session中 session.setAttribute(\"user\", user); &#125; //放行 chain.doFilter(req, resp); &#125; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; &#125; @Override public void destroy() &#123; &#125; &#125;","updated":"2020-03-13T03:06:29.683Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://zouchanglin.cn/tags/JavaWeb/"}]},{"title":"Linux常用命令","date":"2018-04-08T16:00:00.000Z","path":"2018/04/09/Linux常用命令/","text":"本次介绍如下命令：du、 df、 top、 free、 pstack、su、sudo、 adduser、 passwd du命令Linux下命令，统计目录（或文件）所占磁盘空间的大小！ 格式：du [选项] [文件]完整格式：du [-abcDhHklmsSx][-L &lt;符号连接&gt;][-X &lt;文件&gt;][--block-size][--exclude=&lt;目录或文件&gt;][--max-depth=&lt;目录层数&gt;][--help][--version][目录或文件] 参数说明 -a或-all 显示目录中个别文件的大小。-b或-bytes 显示目录或文件大小时，以byte为单位。-c或–total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和。-k或–kilobytes 以KB(1024bytes)为单位输出。-m或–megabytes 以MB为单位输出。-s或–summarize 仅显示总计，只列出最后加总的值。-h或–human-readable 以K，M，G为单位，提高信息的可读性。-x或–one-file-xystem 以一开始处理时的文件系统为准，若遇上其它不同的文件系统目录则略过。-L&lt;符号链接&gt;或–dereference&lt;符号链接&gt; 显示选项中所指定符号链接的源文件大小。-S或–separate-dirs 显示个别目录的大小时，并不含其子目录的大小。-X&lt;文件&gt;或–exclude-from=&lt;文件&gt; 在&lt;文件&gt;指定目录或文件。–exclude=&lt;目录或文件&gt; 略过指定的目录或文件。-D或–dereference-args 显示指定符号链接的源文件大小。-H或–si 与-h参数相同，但是K，M，G是以1000为换算单位。-l或–count-links 重复计算硬件链接的文件。 使用示例接下来看看这样一个现象，大小不一样，对于.viminfo 这个文件 ls 查看信息查出是862字节，为什么到du命令下就变成4k了？ 原因解释：因为底层的内存管理是以页面为单位，Linux内核的默认页面大小就是4k，至于为什么这个设计，详见：《为什么Linux内核页面最小单位是4K》注意最下面的是一个总结值，也可以使用-s直接显示总结值，对显示结果排序： df命令df命令作用是列出文件系统的整体磁盘空间使用情况。可以用来查看磁盘已被使用多少空间和还剩余多少空间。df命令显示系统中包含每个文件名参数的磁盘使用情况，如果没有文件名参数，则显示所有当前已挂载文件系统的磁盘空间使用情况 ，在默认情况下，磁盘空间是以1KB为单位进行显示的，但是，如果POSIXLY_CORRECT环境变量被设置为true，这种情况下默认使用512字节为单位显示！ 格式： df [选项] [文件名]参数说明 -a：–all，显示所有的文件系统，包括虚拟文件系统-B：–block-size，指定单位大小。比如1k，1m等-h：–human-readable，以人们易读的GB、MB、KB等格式显示。-H：–si，和-h参数一样，但是不是以1024，而是1000，即1k=1000，而不是1k=1024。-i：–inodes，不用硬盘容量，而是以inode的数量来显示-k：以KB的容量显示各文件系统，相当于–block-size=1k。-m：以KB的容量显示各文件系统，相当于–block-size=1m。-l：–local，只显示本地文件系统。–no-sync：在统计使用信息之前不调用sync命令(默认)。-sync：在统计使用信息之前调用sync命令。-P：–portability，使用POSIX格式显示-t：–type=TYPE，只显示指定类型的文件系统.-T：–print-type，显示文件系统类型-x：–exclude-type=TYPE，不显示指定类型的文件系统–help：显示帮助信息。–version：显示版本信息。 Filesystem：代表该文件系统时哪个分区，所以列出的是设备名称。1K-blocks：说明下面的数字单位是1KB，可利用-h或-m来改变单位大小，也可以用-B来设置。Used：已经使用的空间大小。Available：剩余的空间大小。Use%：磁盘使用率。如果使用率在90%以上时，就需要注意了，避免磁盘容量不足出现系统问题，尤其是对于文件内容增加较快的情况(如/home、/var/spool/mail等)。Mounted on：磁盘挂载的目录，即该磁盘挂载到了哪个目录下面。 top命令实时动态地查看系统的整体运行情况，是一个综合了多方信息监测系统性能和运行信息的实用工具。通过top命令所提供的互动式界面，用热键可以管理。 格式：top [选项]参数说明 -b：以批处理模式操作-c：显示完整的治命令-d：屏幕刷新间隔时间-I：忽略失效过程-s：保密模式-S：累积模式-i&lt;时间&gt;：设置间隔时间-u&lt;用户名&gt;：指定用户名-p&lt;进程号&gt;：指定进程-n&lt;次数&gt;：循环显示的次数 top命令中一些字段的含义VIRT：virtual memory usage 虚拟内存 进程“需要的”虚拟内存大小，包括进程使用的库、代码、数据等 假如进程申请100m的内存，但实际只使用了10m，那么它会增长100m，而不是实际的使用量 1RES：resident memory usage 常驻内存 进程当前使用的内存大小，但不包括swap out 包含其他进程的共享 如果申请100m的内存，实际使用10m，它只增长10m，与VIRT相反 关于库占用内存的情况，它只统计加载的库文件所占内存大小 1SHR：shared memory 共享内存 除了自身进程的共享内存，也包括其他进程的共享内存 虽然进程只使用了几个共享库的函数，但它包含了整个共享库的大小 计算某个进程所占的物理内存大小公式：RES – SHR swap out后，它将会降下来 1DATA：真实内存 数据占用的内存。如果top没有显示，按f键可以显示出来。 真正的该程序要求的数据空间，是真正在运行中要使用的。 显示完整命令示例 显示完整命令 top -c以批处理模式显示程序信息 top -b以累积模式显示程序信息 top -S设置信息更新次数 top -n 2(表示更新两次后终止更新显示)设置信息更新时间 top -d 3(表示更新周期为3秒)显示指定的进程信息 top -p 139(显示进程号为139的进程信息，CPU、内存占用率等)显示更新十次后退出 top -n 10使用者将不能利用交谈式指令来对行程下命令 top -s将更新显示二次的结果输入到名称为 top.log 的档案里 top -n 2 -b &lt; top.log 使用示例 free命令Linux free命令用于显示内存状态。free指令会显示内存的使用情况，包括实体内存，虚拟的交换文件内存，共享内存区段，以及系统核心使用的缓冲区等。 格式：free [-选项][-s &lt;间隔秒数&gt;]###参数说明 -b 以Byte为单位显示内存使用情况。-k 以KB为单位显示内存使用情况。-m 以MB为单位显示内存使用情况。-o 不显示缓冲区调节列。-s&lt;间隔秒数&gt; 持续观察内存使用状况。-t 显示内存总和列。-V 显示版本信息 详细解释 我们可以发现，free命令会输出这4行，第二行是站在操作系统的角度去看待的内存使用情况，也就是总共有2054224KB（默认情况下单位是KB），换算过来大约是1.96G，这些内存中只有1917816KB被使用了，剩余为136804KB，0.13G 可用。shared是共享的意思，表示几个进程的共享内存，buffer缓冲区之意，cached也是缓存的意思，接下来说说他们的区别，先看看外语解释： A buffer is something that has yet to be “written” to disk.A cache is something that has been “read” from the disk and stored for later use. 翻译过来就是：缓冲区是尚未被“写入”到磁盘的东西。缓存是从磁盘上“读取”并存储以供以后使用的东西。 buffer是用于存放要输出到disk（块设备）的数据的，而cache是存放从disk上读出的数据。这二者是为了提高IO性能的，并由OS管理。Linux和其他成熟的操作系统（例如windows），为了提高IO的性能，总是要多cache一些数据，这也就是为什么cached memory比较大，而free比较小的原因！ free输出的第三行是从一个应用程序的角度看系统内存的使用情况。对于-buffers/cache，表示一个应用程序认为系统被用掉多少内存；对于+buffers/cache，表示一个应用程序认为系统还有多少内存；因为被系统cache和buffer占用的内存可以被快速回收，所以通常+buffers/cache比-buffers/cache会大很多！ pstack命令格式：pstack &lt;-PID&gt;显示每个进程的栈跟踪。pstack 命令必须由相应进程的属主或 root 运行。可以使用 pstack 来确定进程挂起的位置。此命令允许使用的唯一选项是要检查的进程的 PID。需要先安装此工具： 这个命令在排查进程问题时非常有用，比如我们发现一个服务一直处于work状态（如假死状态，好似死循环），使用这个命令就能轻松定位问题所在；可以在一段时间内，多执行几次pstack，若发现代码栈总是停在同一个位置，那个位置就需要重点关注，很可能就是出问题的地方! 作用归纳 查看线程数(比pstree, 包含了详细的堆栈信息) 能简单验证是否按照预定的调用顺序/调用栈执行 采用高频率多次采样使用时, 能发现程序当前的阻塞在哪里, 以及性能消耗点在哪里 能反映出疑似的死锁现象(多个线程同时在wait lock, 具体需要进一步验证) su命令Linux su命令用于变更为其他使用者的身份，除 root 外，需要键入该使用者的密码。使用权限：所有使用者 格式：su [-fmp] [-c command] [-s shell] [--help] [--version] [-] [USER [ARG]]参数说明 -f 或 –fast 不必读启动档（如 csh.cshrc 等），仅用于 csh 或 tcsh-m -p 或 –preserve-environment 执行 su 时不改变环境变数-c command 或 –command=command 变更为帐号为 USER 的使用者并执行指令（command）后再变回原来使用者-s shell 或 –shell=shell 指定要执行的 shell （bash csh tcsh 等），预设值为 /etc/passwd 内的该使用者（USER） shell–help 显示说明文件–version 显示版本资讯–l 或 –login 这个参数加了之后，就好像是重新 login 为该使用者一样，大部份环境变数（HOME SHELL USER等等）都是以该使用者（USER）为主，并且工作目录也会改变，如果没有指定USER ，内定是 root USER 欲变更的使用者帐号 ARG 传入新的 shell 参数** sudo命令Linux sudo命令以系统管理者的身份执行指令，也就是说，经由 sudo 所执行的指令就好像是 root 亲自执行。使用权限：在 /etc/sudoers 中有出现的使用者。参数说明 -V 显示版本编号-h 会显示版本编号及指令的使用方式说明-l 显示出自己（执行 sudo 的使用者）的权限-v 因为 sudo 在第一次执行时或是在 N 分钟内没有执行（N 预设为五）会问密码，这个参数是重新做一次确认，如果超过 N 分钟，也会问密码-k 将会强迫使用者在下一次执行 sudo 时问密码（不论有没有超过 N 分钟）-b 将要执行的指令放在背景执行-p prompt 可以更改问密码的提示语，其中 %u 会代换为使用者的帐号名称， %h 会显示主机名称-u username/#uid 不加此参数，代表要以 root 的身份执行指令，而加了此参数，可以以 username 的身份执行指令（#uid 为该 username 的使用者号码）-s 执行环境变数中的 SHELL 所指定的 shell ，或是 /etc/passwd 里所指定的 shell-H 将环境变数中的 HOME （家目录）指定为要变更身份的使用者家目录（如不加 -u 参数就是系统管理者 root ）command 要以系统管理者身份（或以 -u 更改为其他人）执行的指令 sudo命令的特点 sudo能够限制用户只在某台主机上运行某些命令。 sudo提供了丰富的日志，详细地记录了每个用户干了什么。它能够将日志传到中心主机或者日志服务器。 sudo使用时间戳文件来执行类似的“检票”系统。当用户调用sudo并且输入它的密码时，用户获得了一张存活期为5分钟的票（这个值可以在编译的时候改变）。 sudo的配置文件是sudoers文件，它允许系统管理员集中的管理用户的使用权限和使用的主机。它所存放的位置默认是在/etc/sudoers，属性必须为0440。 adduser命令Linux adduser命令用于新增使用者帐号或更新预设的使用者资料。 adduser 与 useradd 指令为同一指令（经由符号连结 symbolic link）。 使用权限：系统管理员。 adduser是增加使用者。相对的，也有删除使用者的指令，userdel 格式：adduser [login ID]完全格式： 1adduser -D [-g default_group] [-b default_home] [-f default_inactive] [-e default_expire_date] [-s default_shell] 参数说明-c comment 新使用者位于密码档（通常是 /etc/passwd）的注解资料 -d home_dir 设定使用者的家目录为 home_dir ，预设值为预设的 home 后面加上使用者帐号 loginid -e expire_date 设定此帐号的使用期限（格式为 YYYY-MM-DD），默认值为永久有效 -f inactive_time 范例 使用示例添加一个一般用户 1useradd name 为添加的用户指定相应的用户组 useradd ? g root kk 添加用户kk，并指定用户所在的组为root用户组 创建一个系统用户 useradd ?r kk 创建一个系统用户kk 为新添加的用户指定/home目录 useradd-d /home/myf kk 新添加用户kk，其home目录为/home/myf，当用户名kk登录主机时，系统进入的默认目录为/home/myf passwd命令Linux passwd命令用来更改使用者的密码！ 格式：passwd [-k] [-l] [-u [-f]] [-d] [-S] [username]参数说明 必要参数-d 删除密码-f 强制执行-k 更新只能发送在过期之后-l 停止账号使用-S 显示密码信息-u 启用已被停止的账户-x 设置密码的有效期-g 修改群组密码-i 过期后停止用户账号选择参数–help 显示帮助信息–version 显示版本信息 使用示例删除Tim的密码 passwd -d Tim Linux下的命令分类","updated":"2020-03-13T03:06:29.674Z","categories":[{"name":"操作系统实战","slug":"操作系统实战","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"}]},{"title":"浅析指针与数组","date":"2018-04-04T06:00:29.000Z","path":"2018/04/04/浅析指针与数组/","text":"指针是C/C++的精华，如果未能很好的掌握指针也基本等于没学，本篇主要内容有：数组指针、指针数组、函数指针、函数指针数组、指向函数指针数组的指针、指针与数组的区别、多维数组与多级指针。暂且不要觉得这些概念比较复杂，且听我逐一道来！ 数组指针很多人开始把数组指针和指针数组搞不清楚，本人第一次听到这两个名词也是有点晕，但是要搞清楚并不难，数组指针就是一个指针，一个指向数组的指针、而指针数组就是一个数组，一个存放指针的数组。 数组指针的定义数组指针毕竟是个指针，而且是指向数组的指针。我们知道整型叫int，指向整型的指针就是int*。同样的数组也是一种类型，但是准确的说数组的类型有很多种，元素个数不同或者元素类型不同那么数组的类型就不同。 123456789101112#define _CRT_SECURE_NO_WARNINGS int main(void)&#123; int num = 10; int* pNum = &amp;num; //整型变量的地址使用整型指针来保存 int arr[] = &#123; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 &#125;; int(*pArr)[10] = &amp;arr; //数组的地址使用数组指针来保存 return 0;&#125; 其中pArr指向数组arr，int是arr中元素的类型，[10]则表示了指向数组元素的个数， 数组类型是由其中存储的元素类型和元素个数共同决定的： 任何一个条件不匹配都会导致编译的时候类型不兼容！ 数组指针的使用数组指针意义就是指向一个数组，接下来看一个示例: 1234567891011121314151617#define _CRT_SECURE_NO_WARNINGS#include&lt;stdio.h&gt; int main(void)&#123; int arr[] = &#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&#125;; int(*pArr)[10] = &amp;arr;//数组的地址使用数组指针来保存 int i = 0; for (i = 0; i &lt; 10; i++) &#123; printf(\"%d \",(* pArr)[i]); &#125; return 0;&#125; 这个基本上看不出数组指针的意义何在，反倒变得更加麻烦了，其实数组指针多用于二维数组传参。数组指针既然是一个指针，那么就是用来接收地址，在传参时就接收数组的地址，所以数组指针在作为函数参数时对应的实参应该是二维数组。 123456789101112131415#define _CRT_SECURE_NO_WARNINGS#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;void fun(int(*pArr)[4])//子函数中的形参，指针数组&#123; //TUDO...&#125;int main(void)&#123; int a[3][4] = &#123; 0 &#125;; //主函数中定义的二维数组 fun(a); //主函数调用子函数的实参，是二维数组的首元素首地址 system(\"pause\"); return 0;&#125; 主函数调用子函数的实参，是二维数组的首元素地址，但是二维数组首元素是个一维数组，必须传入一维数组的地址，这个时候我们发现使用 int * 类型作为函数的形参当然是不行的，我们需要的是一个一维数组的地址，也就是指向一维数组的指针，这个时候才是数组指针的用武之地！ 很多人在写以一维数组传参的时候写成整型指针 int *，这一点问题都没有,因为参数是匹配的一维数组的首元素地址确实是 int * ,但是接着遇到二维数组传参就会出错，很多人把二维数组传参写成了 int **，这种做法是绝对错误的。 数组作为参数传递我们指针，在C语言中只是存在值传递（地址传递也只是值拷贝），我们在传递单个值得时候就会发生值拷贝，但是如果我们在传递数组的时候也逐个拷贝，如果只是很少的元素还好，一旦元素多了那效率就太低了，于是C中在传递数组的时候只是将数组的首元素地址给传递过去，避免了拷贝数组，效率自然就高了！注意：当一维数组作为函数参数的时候，编译器总是把它解析成一个指向其首元素地址的指针所以当我们传递二维数组的时候，它的首元素是一个以为数组，传递过去的自然也就是一个指向一维数组的指针！ 指针数组指针数组的定义指针数组 —— 本质为数组，只不过这是一个存储指针的数组,接下来看看这个存储了4个 int* 类型指针的数组 1234567891011int main(void)&#123; int* pInt1 = NULL; int* pInt2 = NULL; int* pInt3 = NULL; int* pInt4 = NULL; int* arr[4] = &#123;pInt1, pInt2, pInt3, pInt4&#125;; return 0;&#125; 上面这个指针的数组的大小在32位平台下就是16个字节，64位平台下是32字节，无论指针指向什么内容，指针的大小只是和平台有关，所以数组的大小是非常容易确定的！ 1234567891011#define _CRT_SECURE_NO_WARNINGS#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt; int main(void)&#123; char* arr[4] = &#123;\"AAA\", \"BBB\", \"CCC\", \"DDD\"&#125;; //TUDO... return 0;&#125; 有时候我们常见这种写法，其实也不难理解，只要想明白它们在内存中的布局就可以轻松解决问题 内存 内容 权限 栈区 函数中的普通变量 可读 可写 堆区 动态申请的内存 可读 可写 静态变量区 static修饰的变量 可读 可写 数据区 用于初始化变量的常量 只读 指针数组的使用指针数组常用在main函数传参，在写主函数时，参数有两个，一个确定参数个数，一个这是指针数组用来接收每个参数（字符串）的地址 12345int main(int argc,char* argv[])&#123; //TUDO... return 0;&#125; 此时可以想象内存映像图，主函数的栈区有一个叫argv的数组，这个数组的元素是你输入的参数的地址，指向着只读数据区。 指针数组对应着二级指针这个其实也不难理解，我们看看main函数的另一种写法： 12345int main(int argc,char** argv)&#123; //TUDO... return 0;&#125; 这和传递一个普通数组的思想一样，不能传递整个数组过去，如果数组很大，这样内存利用率很低，所以应该传递数组的首地址，用一个指针接收这个地址。因此，指针数组对应着二级指针 12345678910111213141516171819202122#define _CRT_SECURE_NO_WARNINGS#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt; void fun(char* arr[])&#123; //TUDO...&#125; void fun2(char** arr)&#123; //TUDO...&#125;int main(void)&#123; char* arr[4] = &#123;\"AAA\", \"BBB\", \"CCC\", \"DDD\"&#125;; fun(arr); fun2(arr); return 0;&#125; 数组名当参数传递表示首元素地址，而指针数组的元素本来就是指针，所以接受参数的时候使用二级指针是完全正确的！！！ 函数指针函数指针也是一个指针，只不过这个指针是指向函数的指针。C在编译时，每一个函数都有一个 入口地址 ，该入口地址就是函数指针所指向的地址。有了指向函数的指针变量后，可用该指针变量调用函数，就如同用指针变量可引用其他类型变量一样，在这些概念上是大体一致的。函数指针有两个用途：调用函数和做函数的参数。 普通函数指针普通函数指针的简单使用呢，参考示例: 1234567891011121314151617181920212223242526272829#define _CRT_SECURE_NO_WARNINGS#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt; int add(int num1,int num2)&#123; return num1 + num2;&#125; int main(void)&#123; int(*pAdd)(int, int) = NULL; pAdd = &amp;add; pAdd = add; int ret = 0; ret = pAdd(2, 3); printf(\"ret = %d\\n\",ret); ret = (* pAdd)(2, 3); printf(\"ret = %d\\n\", ret); system(\"pause\"); return 0;&#125; 函数指针使用的时候无需加*解引用，获取函数地址的时候直接使用函数名和对函数名取地址是一样的。地址指向函数的指针变量没有++和--运算，这一点要切记！ 可变参数函数指针12345678910111213141516171819202122232425262728#define _CRT_SECURE_NO_WARNINGS#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;stdarg.h&gt; void fun(int num,...)&#123; //TODO...&#125; int main(void)&#123; void(*pFun)(int, ...) = NULL; pFun = &amp;fun; pFun = fun; pFun(3, 11, 22, 33); (* pFun)(3, 11, 22, 33); pFun(4, 11, 22, 33, 44); (* pFun)(4, 11, 22, 33, 44); system(\"pause\"); return 0;&#125; 可变参数函数指针与普通函数指针用法是一致的！ 指针函数和函数指针指针函数是指返回值是指针的函数，即本质是一个函数。我们知道函数都有返回类型，只不过指针函数返回类型是某一类型的指针。返回类型可以是任何基本类型和复合类型。 函数指针的重要作用：回调函数回调函数：回调函数是一个通过函数指针调用的函数，回调函数不是由该函数的实现方直接调用，而是在特定的事件或条件发生时由另外的一方进行调用，用于对该事件或条件进行响应。 示例一这个示例演示了非常简单的回调函数机制，首先我们定义了一个求3个数字加法的函数add，接着定义了求平均值的函数get_average，由于get_average的函数参数列表中有一个参数类型是函数指针，并且我们在使用的时候将add函数放入了这个函数指针中，所以导致的必然结果就是调用get_average函数必然会去调用add函数，这就是回调函数机制！ 12345678910111213141516171819#define _CRT_SECURE_NO_WARNINGS#include&lt;stdio.h&gt; int add(int a, int b, int c)&#123; return a + b + c;&#125; double get_average(int a, int b, int c, int(*pAdd)(int, int, int) )&#123; int ret = pAdd(a, b, c); return ret / 3;&#125; int main(void)&#123; double ret = get_average(12, 15, 17, add); return 0;&#125; 示例二qsort函数的使用 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#define _CRT_SECURE_NO_WARNINGS#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;string.h&gt; typedef struct Stu&#123; int age; char name[20];&#125;Stu; //通过年龄来比较int cmp_age(const void * e1, const void * e2)&#123; if ((((Stu*)e1)-&gt;age) &gt; (((Stu*)e2)-&gt;age)) &#123; return (((Stu*)e1)-&gt;age) - (((Stu*)e2)-&gt;age); &#125; return 0;&#125;//通过名字来排序int cmp_name(const void * e1, const void * e2)&#123; return strcmp(((Stu*)e1)-&gt;name, ((Stu*)e2)-&gt;name);&#125; void print_Stu(Stu arr[], int len)&#123; int i = 0; for (i = 0; i &lt; len; i++) &#123; printf(\"[age = %d,name = %s] \\n\", arr[i].age, arr[i].name ); &#125;&#125; int main(void)&#123; Stu arr[] = &#123; &#123; 21, \"tim\" &#125;, &#123; 16, \"lilililala\" &#125;, &#123; 20, \"joker\" &#125;, &#123; 18, \"avinla\" &#125; &#125;; int len = sizeof(arr) / sizeof(arr[0]); qsort(arr, len, sizeof(Stu), cmp_age); print_Stu(arr, len); printf(\"-------------------------\\n\"); qsort(arr, len, sizeof(Stu), cmp_name); print_Stu(arr, len); system(\"pause\"); return 0;&#125; 很显然我们是对结构体进行排序，并不是简单地int或者double型，自定义的结构体类型当然只有我们自己才知道排序规则，调用的两次比较函数一次是通过年龄比较、一次是通过名称比较，然后同一个函数是怎样明确我们的比较规则的呢？这当然就是回调函数的功劳了，我们只需要改变函数指针的值就可以吧我们定义的规则传进去，这样对再复杂的结构体只要我自定义排序规则，将这个“规则”函数传入即可完成排序！ 示例三自定义实现qsort，首先我们看看qsort的参数说明： void qsort ( void base, size_t num, size_t size, int ( comparator ) ( const void , const void ) ); 1、第一个参数 base 是需要排序的目标数组名(或者也可以理解成开始排序的地址，因为可以写&amp;s[i]这样的表达式)2、第二个参数 num 是 参与排序的目标数组元素个数3、第三个参数 width 是单个元素的大小(或者目标数组中每一个元素长度），推荐使用sizeof(s[0])这样的表达式) 4、第四个参数 compare 比较函数,自定义的算法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#define _CRT_SECURE_NO_WARNINGS#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;string.h&gt;#include&lt;assert.h&gt; typedef struct Stu&#123; int age; char name[20];&#125;Stu; //通过年龄来比较int cmp_age(const void * e1, const void * e2)&#123; if ((((Stu*)e1)-&gt;age) &gt; (((Stu*)e2)-&gt;age)) &#123; return (((Stu*)e1)-&gt;age) - (((Stu*)e2)-&gt;age); &#125; return 0;&#125;//通过名字来排序int cmp_name(const void * e1, const void * e2)&#123; return strcmp(((Stu*)e1)-&gt;name, ((Stu*)e2)-&gt;name);&#125; void print_Stu(Stu arr[], int len)&#123; int i = 0; for (i = 0; i &lt; len; i++) &#123; printf(\"[age = %d,name = %s] \\n\", arr[i].age, arr[i].name ); &#125;&#125;void _swap(char *e1,char *e2,int size)&#123; assert((e1 != NULL) &amp;&amp; (e2 != NULL)); while (size--) &#123; char tmp = *e1; *e1 = *e2; *e2 = tmp; e1++; e2++; &#125;&#125;void my_qsort(void * base, int num, int size, int(*cmparator) (const void *, const void *))&#123; int i = 0; int j = 0; assert(base != NULL); for (i = 0; i &lt; num-1; i++) &#123; for (j = 0; j &lt; num - i -1; j++) &#123; if (cmparator((char*)base+j*size,(char*)base+(j+1)*size)&gt;0) &#123; //执行交换 _swap((char*)base + j*size, (char*)base + (j + 1)*size ,size); &#125; &#125; &#125;&#125;int main(void)&#123; Stu arr[] = &#123; &#123; 21, \"tim\" &#125;, &#123; 16, \"lilililala\" &#125;, &#123; 20, \"joker\" &#125;, &#123; 18, \"avinla\" &#125; &#125;; int len = sizeof(arr) / sizeof(arr[0]); my_qsort(arr, len, sizeof(Stu), cmp_age); print_Stu(arr, len); printf(\"-------------------------\\n\"); my_qsort(arr, len, sizeof(Stu), cmp_name); print_Stu(arr, len); system(\"pause\"); return 0;&#125; 函数指针的小练习 (* (void(*)())0)();把0强制类型转换为函数指针类型，具体类型为(void(*)),然后对这个函数指针解引用就把它当成了函数使用 void( *signal(int,void(* )(int)))(int)signal是一个函数声明，signal有两个参数，一个是int，一个是函数指针该函数指针指向的函数有一个int参数，返回值类型为voidsignal函数的返回值类型为一个函数指针该函数指针指向的函数有一个int参数，返回类型为void 函数指针数组的应用：转移表函数指针数组的概念其实不难理解，本质还是一个数组，这个数组中存放的元素类型是函数指针。 转移表其实就是和状态相关，我们在实际应用中使用if-else结构或者switch语句进行一些状态的切换。但是如果遇到比较复杂情况，转移次数达到数百次或者数千次，如果再使用if-else结构或者switch语句，维护起这个软件系统，工作量将会相当大。这个时候可以采用”转移表”来避免这个情况。 我们要实现一个计算器程序，我们可能就会用到这样的switch分支结构： 123456789101112switch(operation) &#123; case ADD: result=add(a,b);break; case SUB: result=sub(a,b);break; case MUL: result=mul(a,b);break; case DIV: result=div(a,b);break; ..... &#125; 如果这个计算器要实现的功能很多，那么将有很多这样的语句，可维护性很差。如果我们将具体的数值操作与选择操作的代码分开将会提高代码的可读性。这种情况下，我们需要建立一个“转移表”。在建立转移表之间需要对涉及到的函数提前声明，然后建立转移表，对于上面的可以这么修改： 那么建立的转移表如下： 1double (*operation_fun[])(double,double)=&#123;add,sub,mul,div,......&#125;; 在调用的时候可以这样操作： 12double result;result=operation_fun[operation](a,b); 上面两句可以替换switch语句，使得程序的可维护性大大增强！ 指向函数指针数组的指针指向函数指针数组的指针是一个指针 ，指针指向一个数组 ，数组的元素都是函数指针，如 void (*(*p)[10]) )(void) 这样的形式它表示:一个指向有10个元素 、每个元素为指向一个返回值为空的函数的数组的这样一个指针! 指针与数组的区别两种情况：①定义为数组、声明为指针，错误②定义为指针、声明为数组，错误注意：声明一个变量时是不会创建空间的！ 二维数组和二级指针参数二维数组和二级指针参数的等效关系 数组参数 等效的指针参数 数组的数组:char arr[3][4] 数组的指针:char(*p)[10] 指针数组:char *arr[5] 指针的指针:char ** p 在C语言中，当一位数组作为函数参数的时候，编译器总是把它解析成为一个指向其首元素地址的指针。但是这条规则并不是递归的，也就是说只有一维数组才是如此，当数组超过一维时，将第一维改写为指向数组元素首地址的指针之后，后面的维再也不可改写。比如:a[3][4][5]作为参数时可以被修改为(*p)[4][5]) 至于超过二维数组和超过二级的指针，由于本身很少使用，在此不做讨论！ 指针和数组的对比 在《天龙八部》八部中，乔峰血战聚贤庄，一套平凡的太祖长拳打得虎虎生威，在场英雄无不佩服至极，这是其苦练的结果！C语言是程序员的内功，无论招式如何华丽只有内功深厚才是正道，注意这一点等于掌握编程的半壁江山！！！","updated":"2020-03-13T03:06:29.748Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://zouchanglin.cn/tags/C-C/"}]},{"title":"最冤枉的sizeof","date":"2018-04-01T01:28:23.000Z","path":"2018/04/01/最冤枉的sizeof/","text":"之前一直以为sizeof(char)、sizeof(int)…居然一直以为sizeof是函数，其实sizeof在C中只是一个关键字！！！ 之前一直以为sizeof(char)、sizeof(int)…居然一直以为sizeof是函数，其实sizeof在C中只是一个运算符！！！ 之前一直以为sizeof(char)、sizeof(int)…居然一直以为sizeof是函数，其实sizeof在C中只是一个操作符！！！ 在 Pascal 语言中，sizeof() 是一种内存容量度量函数，功能是返回一个变量或者类型的大小（以字节为单位）① 在 C 语言中，sizeof() 是一个判断数据类型或者表达式长度的运算符（以字节为单位） 在Pascal 语言与C语言中，对 sizeof() 的处理都是在编译阶段进行; ② sizeof内部的表达式不参与运算 指针记录了另一个对象的地址。既然是来存放地址的，那么它当然等于计算机内部地址总线的宽度。所以在32位计算机中，一个指针变量的返回值必定是4（注意结果是以字节为单位），但是，在64位系统中指针变量的sizeof结果为8 注意sizeof和strlen的区别：1、strlen(char*)函数求的是字符串的实际长度，直到遇到第一个’\\0’，然后就返回计数值，且不包括’\\0’。而sizeof()函数返回的是变量声明后所占的内存数，不是实际长度 2、sizeof操作符的结果类型是size_t，它在头文件中typedef为unsigned int类型。该类型保证能容纳实现所建立的最大对象的字节大小 3、sizeof是算符，strlen是函数 4、sizeof可以用类型做参数，strlen只能用char*做参数，且必须是以’’\\0’’结尾的 5、sizeof还可以用函数做参数，比如：short f(); printf(“%d\\n”,sizeof(f())); 结果是sizeof(short)，即2 6、数组做sizeof的参数不退化，传递给strlen就退化为指针了 7、大部分编译程序在编译的时候就把sizeof计算过了是类型或是变量的长度这就是sizeof(x)可以用来定义数组维数的原因 8、strlen的结果要在运行的时候才能计算出来，是用来计算字符串的长度，不是类型占内存的大小 9、sizeof后如果是类型必须加括弧，如果是变量名可以不加括弧。这是因为sizeof是个操作符不是个函数 10、数组作为参数传给函数时传的是指针而不是数组，传递的是数组的首地址","updated":"2020-03-13T03:06:29.743Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://zouchanglin.cn/tags/C-C/"}]},{"title":"C语言关键字","date":"2018-03-27T16:00:00.000Z","path":"2018/03/28/C语言关键字/","text":"首先看看这份我总结的这份C语言大纲，大概也就知道C语言的关键字处在那个地位了，同时也可以作为一份复习的资料，虽然C语言看起来就这么一些知识点，但是我只能说我看到的知识C语言的冰山一角，想要彻底了解C语言，还是需要多看看书，甚至可以尝试去写一个C的编译器，也算是一大壮举了，语言的特性只有编译器的设计者最清楚！ 今天从关键字开始说起：C语言的关键字共有32个，根据关键字作用，可分其为数据类型关键字、控制语句关键字、存储类型关键字和其它关键字四类。 1、数据类型关键字（12个）： char ：声明字符型变量或函数 double ：声明双精度变量或函数 enum ：声明枚举类型 float：声明浮点型变量或函数 int： 声明整型变量或函数 long ：声明长整型变量或函数 short ：声明短整型变量或函数 signed：声明有符号类型变量或函数 struct：声明结构体变量或函数 union：声明联合数据类型 unsigned：声明无符号类型变量或函数 void ：声明函数无返回值或无参数，声明无类型指针（基本上就这三个作用） 2、控制语句关键字（12个）：循环语句 for：一种循环语句(可意会不可言传） do ：循环语句的循环体 while ：循环语句的循环条件 break：跳出当前循环 continue：结束当前循环，开始下一轮循环条件语句 if: 条件语句 else ：条件语句否定分支（与 if 连用） goto：无条件跳转语句选择语句 switch :用于开关语句 case：开关语句分支 default：开关语句中的“其他”分支返回语句 return ：子程序返回语句（可以带参数，也看不带参数） 3、存储类型关键字（4个） auto ：声明自动变量 一般不使用 extern：声明变量是在其他文件正声明（也可以看做是引用变量） register：声明寄存器变量 static ：声明静态变量 ，static修饰全局变量的时候改变全局变量作用范围，是的全局变量只能在当前文件使用 4、其它关键字（4个）： const ：声明只读变量 sizeof：计算数据类型长度 typedef：用以给数据类型取别名 volatile：说明变量在程序执行中可被隐含地改变 特殊说明一下几个原来不熟悉的关键字： union ：声明联合体类型 extern：声明变量是在其他文件正声明（也可以看做是引用变量） register：声明寄存器变量，但只是建议编译器对变量声明为寄存器变量，这完全取决于编译器，有时就算不声明register，编译器也会把变量声明为寄存器变量来提高运算效率 volatile：这个关键字的作用是防止编译器将变量优化为寄存器变量，保证变量的内存可见性。精确地说就是，优化器在用到这个变量时必须每次都小心地重新读取这个变量的值，而不是使用保存在寄存器里的备份","updated":"2020-03-13T03:06:29.643Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://zouchanglin.cn/tags/C-C/"}]},{"title":"使用服务注册特殊广播接收者","date":"2018-03-09T11:08:56.000Z","path":"2018/03/09/使用服务注册特殊广播接收者/","text":"Android中的广播：系统在运行过程中，会发生很多事件，系统为了让其他应用知道系统发生了这个事件，会发送一个对应事件的广播，比如：电量改变，收到短信，拨打电话，屏幕解锁，系统开机，只有注册一个广播接收者，就可以接收到系统发送的广播。 屏幕锁屏和解锁、电量改变等广播属于安卓系统中操作特别频繁的广播事件，若在MainActivity中注册,当MainActivity销毁时无法接收广播，所以应该在服务中去注册广播接收者，必须使用代码注册！ 首先这是定义的广播接收者： 123456789101112131415public class MyReceiver extends BroadcastReceiver &#123; @Override public void onReceive(Context context, Intent intent) &#123; //获取当前事件类型 String action = intent.getAction(); if(\"android.intent.action.SCREEN_OFF\".equals(action))&#123; //屏幕锁屏 System.out.println(\"屏幕锁屏\"); &#125;else if(\"android.intent.action.SCREEN_ON\".equals(action))&#123; //屏幕解锁 System.out.println(\"屏幕解锁\"); &#125; &#125;&#125; 动态注册广播的Service: 123456789101112131415161718192021222324252627282930313233343536import android.app.Service;import android.content.Intent;import android.content.IntentFilter;import android.os.IBinder; public class ScreenService extends Service &#123; private MyReceiver myReceiver; public ScreenService() &#123; &#125; @Override public IBinder onBind(Intent intent) &#123; return null; &#125; @Override public void onCreate() &#123; //获取MyReceiver实例 myReceiver = new MyReceiver(); //添加Action IntentFilter filter = new IntentFilter(); filter.addAction(\"android.intent.action.SCREEN_OFF\"); filter.addAction(\"android.intent.action.SCREEN_ON\"); //动态注册广播 registerReceiver(myReceiver,filter); super.onCreate(); &#125; @Override public void onDestroy() &#123; //当服务销毁的时候取消注册广播 unregisterReceiver(myReceiver); super.onDestroy(); &#125;&#125; MainActivity在加载的时候就开启服务： 12345678910111213141516package useservice.xpu.nevergiveup.serviceresgitreceiver; import android.content.Intent;import android.support.v7.app.AppCompatActivity;import android.os.Bundle; public class MainActivity extends AppCompatActivity &#123; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); startService(new Intent(getApplicationContext(), ScreenService.class)); &#125;&#125; 之后别忘记配置一下Service 1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;manifest xmlns:android=\"http://schemas.android.com/apk/res/android\" package=\"useservice.xpu.nevergiveup.serviceresgitreceiver\"&gt; &lt;application android:allowBackup=\"true\" android:icon=\"@mipmap/ic_launcher\" android:label=\"@string/app_name\" android:roundIcon=\"@mipmap/ic_launcher_round\" android:supportsRtl=\"true\" android:theme=\"@style/AppTheme\"&gt; &lt;activity android:name=\".MainActivity\"&gt; &lt;intent-filter&gt; &lt;action android:name=\"android.intent.action.MAIN\" /&gt; &lt;category android:name=\"android.intent.category.LAUNCHER\" /&gt; &lt;/intent-filter&gt; &lt;/activity&gt; &lt;service android:name=\".ScreenService\"/&gt; &lt;/application&gt; &lt;/manifest&gt;","updated":"2020-03-13T03:06:29.720Z","categories":[{"name":"移动开发","slug":"移动开发","permalink":"https://zouchanglin.cn/categories/%E7%A7%BB%E5%8A%A8%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://zouchanglin.cn/tags/Android/"}]},{"title":"程序员偷偷深爱的9个不良编程习惯","date":"2018-03-07T10:09:24.000Z","path":"2018/03/07/程序员偷偷深爱的9个不良编程习惯/","text":"哈哈，这篇文章还是非常能说明问题的，实际开发中必须要注意的地方！下面这9个编码习惯，虽然在编程规则中是被驳斥的，但我们很多人就是会不由自主地使用它们。我们曾经都做过这样的事情：当妈妈不注意的时候，偷偷地吃糖果零食，然后导致有了蛀牙。同样的，我们都违背过一些编程的基本规则，并且都会坚定地表示这种行为是不可取的。但我们就是偷偷爱着这些不良的编程习惯。 我们对所谓的编程规则嗤之以鼻，输出的代码也很糟糕——但我们依然活着。编程上帝没有下闪电劈死我们，我们的电脑也没有爆炸。事实上，只要我们能编译和发布代码，客户似乎就很满意了。 这是因为糟糕的编程不像安装电路或者摸老虎屁股那样有直接的危害性。大多数时间里它也是可以工作的。规则通常是作为一种指导或格式上的建议，并没有硬性规定一定要遵守，也不会导致代码马上死掉。当然，你的代码可能会被人耻笑，甚至可能大家公开嘲笑你，不过，这种挑战惯例的行为可以让人增加一点颠覆传统的快感，哪怕是在不经意间。 为了让问题变得更加复杂，有时候违反规则反而更好。（一般人我不告诉他！）出来的代码会更干净，甚至可能会更快和更简单。规则通常显得太过于宽泛，有技巧的程序员可以通过打破这些规则来提高代码。不要告诉你的老板，这对你的编码生涯会很有意义。 下面这9个编码习惯，虽然在编程规则中是被驳斥的，但我们很多人就是会不由自主地使用它们。 编程习惯No. 1：使用goto关于禁止使用goto可以追溯到许多结构化编程工具还未面世的时代。如果程序员想要创建一个循环或跳到另一段程序中，那么他们需要输入goto后再跟一个行号。过了几年之后，编译器团队让程序员使用字符串标签取代行号。这在当时被认为是一个热门的新功能。 有的人认为这会导致“意大利面条式代码”。代码会变得不可读，并且很难理解代码的执行路径。线程混乱，缠缠绵绵到天涯。Edsger Dijkstra就三令五申地表示应该禁止这个命令，他有一份诙谐的手稿，题目为《Goto语句害人不浅》。 但绝对的分支是没有问题的。这就让人纠结了。通常，巧妙的 break 语句和return 语句可提供一个非常干净的关于代码在那个时候执行什么的声明。有时候，添加 goto 到case语句会比更恰当的多级嵌套的if-then-else语句块更易于理解。 也有反例。在苹果的SSL堆栈中的“goto fail”安全漏洞就是最好的例子之一。但是，如果我们能够仔细避免case语句和循环的一些尴尬问题，那么我们就可以嵌入良好的绝对转移，使阅读代码的人更容易明白这是怎么回事。我们可以插入break和return 语句，让每一个人感觉更清洁和更愉快——可能得除了goto的敌视者。 编程习惯No. 2：成功避开文档我的一个朋友有一个非常精明的老板，这位老板虽然从来没有写过任何代码，但却秉持着每一个功能都必须包含在文档中的理念。哪个程序员不提供注释，那么他就会受到惩罚。所以，我的朋友在他的编辑器中联入了一个有点像人工智能的玩意儿，于是乎，他的每一个功能就都有几行“文档”了。因为这位精明的老板还不够聪明到能理解这些注释其实啥意思也没有，所以我的朋友逃过一劫。他的代码常常被作为正式文档。我想，他应该快要升职了！ 许多函数方法，甚至一些类或多或少都能自文档化。冠以insertReservation或cancelReservation或 deleteAll 等名称的函数并不需要多此一举来解释它们的作用。为函数取一个正确的名字往往就足够了。事实上，这比写一段长长的注释要好，因为函数名可以出现在代码中的其他地方。而文档只能默默地呆在某个角落。自文档化的函数名可以改进它们出现的每个文件。 在有些情况下，写文档甚至会导致情况变糟。例如，当代码瞬息万变，团队像疯了似的重构的时候，文档会产生分歧。代码是这样写的，但文档解释的还是四五个版本以前的情况。这类“过时”的文档通常位于代码顶部，有的人会在这里对代码应该发生什么作一个美好总结。因此，尽管重构团队已经仔细修改了相关的注释，但还是会遗漏文件顶部的这段“美好总结”。 当代码和文本出现分歧的时候，注释就变得毫无价值，甚至会产生误导。在这样的情况下，良好的自文档化的代码显然胜出了。 编程习惯No. 3：一行写太多代码老板突然发神经地给团队发了一封讨厌的邮件：为了执行非常严格的风格规定，我们大家都必须重写我们的代码。最神奇的要求是：每个行为或步骤或子句必须各自成行。你不能使用点语法连续调用函数。在一个分支语句中，你不能有两个及以上返回布尔值的子句。如果要定义变量，那么另起一行。如果你正在做一个复杂的计算，那么不要使用括号。每个片段也自成一行。 他认为他的这个法令将能使调试变得更加容易。就像你单步调试代码一样，调试器会一个动作一个动作地前进。这样就不会卡在某一行。而且更容易执行。 但是这样一来，键盘上的回车键烦不胜烦，因为我需要不断地插入行。而且我敢肯定，老板因此还可以到处吹嘘他的团队能写多少行代码。 唉，有时在同一行中声明一堆变量反而更容易；有时把所有的布尔子句放在一起反而更简单——一切都能变得更加紧凑。那也意味着，我们可以在屏幕上看到更多的逻辑而无需滚动鼠标。更易于阅读就意味着理解起来更快。这才是简单的精粹。 编程习惯No. 4：不声明类型那些热爱类型化语言的人认为，如果为每个变量添加明确的数据类型声明，就可以写出更好的、没有错误的代码。花一点时间来拼写类型，能帮助编译器在代码开始运行之前标志愚蠢的错误。可能会让人觉得痛苦，但很有帮助。这是编程中停止bug的一种有备无患的方法。 但是时代变了。许多较新的编译器完全可以智能地通过查看代码来推断类型。它们会向后和向前浏览代码，直到可以肯定这个变量是string 还是int，抑或其他。如果这些被查看的类型不成队列，那么错误标志就会点亮。因此再也不需要我们输入变量的类型了。 这意味着我们现在可以在代码中省略掉一些最简单的声明。代码更清洁，而且阅读代码的人也猜得出for循环中命名为 i 的变量表示一个整数型。 编程习惯No. 5：摇摆不定的代码有的程序员在代码上特别优柔寡断，犹豫不决。先是一开始将值存储为字符串，然后又解析成整数。接着又转换回字符串。这是非常低效的，你甚至可以感觉到CPU在咆哮这种浪费负载的行为。聪明的程序员之所以能快速地编码，是因为他们事先会设计架构，以尽量减少转换。他们的代码能更快地运行是因为他们有一个良好的规划。 但是，不管你信不信，这种摇摆不定的代码有时候也是有意义的。比如说，你有一个非常棒的库，在它专有的黑盒子里能做无数智能的事情。如果库需要字符串的数据，那么你就给它字符串，即使你刚将这个数据转换成为整数型。 当然，你可以重写所有的代码，以尽量减少转换，但是这需要时间。而且，有时候让代码稍微多花点额外时间来运行也未尝不可，因为重写代码需要耗费我们更多的时间。有时，背负这样的技术债务比一开始就正确构建的成本要更低。 有的时候，库不是专有的代码，但那些你以前全部自己写的代码是你独有的。有的时候，再次转换数据比重写库中的所有代码要快得多。所以，就让它这样吧，就让代码摇摆吧。 编程习惯No. 6：编写你自己的数据结构有一个标准规则是，程序员在完成数据结构课程的第二年，不应该写用于存储数据的代码。基本上我们需要的所有的数据结构，已经有人写好了，而且其代码已历经多年的测试和再测试。它和语言捆绑在一起，而且常常是免费的。你的代码只能造就bug。 但有时你会发现数据结构库有点慢。有时它们会迫使我们使用标准的，但于我们的代码却是错误的结构。有时库会把我们推向在使用结构之前重新配置数据的地步。有时库会包含一些所谓有备无患的保护功能，如线程锁，但其实我们的代码并不需要。 如果遇到这种情况，那么就应该着手写我们自己的数据结构。这或许能让你做得更快，做得更多。而且代码会变得更清洁，因为我们不会包括那些多余的用于格式化数据来完成一些功能的代码。 编程习惯No. 7：在中间打破循环有一个规则制定小组宣称，每个循环都应该有一个“常量”，也就是说当这个逻辑语句为true的时候，循环一直执行。当常量一定不会是true的时候，循环才会结束。这是考虑复杂循环的好方法，但它会导致愚蠢的禁令——例如禁止我们在循环中间使用return 和break 语句。这一条也包含在禁止goto语句的规则中。 这个理论是好的，但它通常会导致更复杂的代码。请看下面这个简单的案例，遍历数组，将找到的元素传递给test函数，并将该元素返回： 12345while (i&lt;a.length)&#123; ... if (test(a[i]) then return a[i]; ... &#125; “循环常量”爱好者会要求我们增加一个布尔变量，命名为notFound，然后这样使用： 12345while ((notFound) &amp;&amp; (i&lt;a.length)&#123; ... if (test(a[i])) then notFound=false; ... &#125; 如果这个布尔值能够合理地命名，那么这就是一段很棒的自文档化的代码，更易于大家理解。但这也增加了复杂性。这意味着你需要分配另一个局部变量，并堵塞寄存器，因为编译器也许还不能足够智能到解决这个问题。 有时候，一个goto 语句或一个跳转会更干净利索。 编程习惯No. 8：使用短变量名（i和x和and也是有意义的）Edgar Allan Poe这位诗人和小说家曾经说过，在一个故事中的每一个词都应该是有内涵的。编码规则也强调如此。变量名应该说明这个变量的所作所为。那些使用驼峰式大小写的方法来写变量名，以表达关于变量细节的Java程序员深以为然，于是一个又一个疯狂长度的变量名出炉了。有些程序员写的变量名，会组合五六个甚至更多的词语。 但有的时候，使用单个字母作为变量名反而会更方便。有时在循环迭代中只使用i或j会更简单。有时使用字母a代表array ，l代表list会更便捷，即使是字母l和数字1看上去很难辨别。 正如这篇文章前面鼓励的是自文档化的代码，而非长长的注释。在上述情况下，单个字母的变量名也是自文档化的。字母 i 是通用的迭代器。只要是程序员立刻就会懂。 编程习惯No. 9：重新定义运算符和函数一些最有趣的编程语言允许你去做一些特别诡异的事情，例如重新定义元素的值，就如同常量一般。例如Python，你可以输入TRUE=FALSE（在Version2.7及之前的版本）。这并不会产生某种逻辑崩溃，或导致宇宙终结——仅仅只是互换了TRUE和FALSE的含义。你也可以在C预处理器和一些其他语言中玩玩类似于这样的危险游戏。还有一些语言允许你重新定义运算符，如加号。 当然这是延伸了，不过有一个观点是，在一个大的代码块内，当重新定义一个或多个所谓的常量时，速度会更快。有时老板会要求代码做一些截然不同的事情。当然，你可以修改代码的每个事件，或者，你可以重新定义。这让你看上去像一个天才。不必重写一个庞大的库，只需翻转一下，就可以做相反的事情了。 这9个习惯就都在这儿了。千万不要轻易尝试，不管它看上去有多牛掰。太危险了——真的，这是实话。","updated":"2020-03-13T03:06:29.756Z","categories":[{"name":"生活","slug":"生活","permalink":"https://zouchanglin.cn/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"生活","slug":"生活","permalink":"https://zouchanglin.cn/tags/%E7%94%9F%E6%B4%BB/"}]},{"title":"JVM内存配置参数说明","date":"2018-02-28T16:00:00.000Z","path":"2018/03/01/JVM内存配置参数说明/","text":"JVM内存划分Xms -Xmx分别设置堆的最小值和最大值，如果要设置成堆的大小可变，那么可以将最大值和最小值设置成不一样，如果要将堆大小固定，那么只需将最大值和最小值设置成一样的就行。jvm中分为堆和方法区，堆又进一步分为新生代和老年代。方法区为永久代，堆中区分的新生代和老年代是为了垃圾回收，新生代中的对象存活期一般不长，而老年代中的对象存活期较长，所以当垃圾回收器回收内存时，新生代中垃圾回收效果较好，会回收大量的内存，而老年代中回收效果较差，内存回收不会太多。 基于以上特性，新生代中一般采用复制算法，因为存活下来的对象是少数，所需要复制的对象少，而老年代对象存活多，不适合采用复制算法，一般是标记整理和标记清除算法。因为复制算法需要留出一块单独的内存空间来以备垃圾回收时复制对象使用，所以将新生代分为eden区和两个survivor区，每次使用eden和一个survivor区，另一个survivor作为备用的对象复制内存区。 所谓的 Copying算法 是空间换时间，而 Mark-Compact算法 则是时间换空间。因为年轻代中的对象基本都是朝生夕死的(80%以上)，所以在年轻代的垃圾回收算法使用的是复制算法（ Copying算法 ）。Copying算法： 在GC开始的时候，对象只会存在于Eden区和名为“From”的Survivor区，Survivor区“To”是空的。紧接着进行GC，Eden区中所有存活的对象都会被复制到“To”，而在“From”区中，仍存活的对象会根据他们的年龄值来决定去向。年龄达到一定值(年龄阈值，可以通过-XX:MaxTenuringThreshold来设置)的对象会被移动到年老代中，没有达到阈值的对象会被复制到“To”区域。经过这次GC后，Eden区和From区已经被清空。这个时候，“From”和“To”会交换他们的角色，也就是新的“To”就是上次GC前的“From”，新的“From”就是上次GC前的“To”。不管怎样，都会保证名为To的Survivor区域是空的。Minor GC会一直重复这样的过程，直到“To”区被填满，“To”区被填满之后，会将所有对象移动到年老代中。Mark-Compact算法： 在工作的时候则需要分别的mark与compact阶段，mark阶段用来发现并标记所有活的对象，然后compact阶段才移动对象，清楚未标记对象来达到compact的目的。如果compact方式是sliding compaction，则在mark之后就可以按顺序一个个对象“滑动”到空间的某一侧。因为已经先遍历了整个空间里的对象图，知道所有的活对象了，所以移动的时候就可以在同一个空间内而不需要多一份空间。 常见配置汇总 堆设置 Xms:初始堆大小 Xmx:最大堆大小 XX:NewSize=n:设置年轻代大小 XX:NewRatio=n:设置年轻代和年老代的比值。如:为3，表示年轻代与年老代比值为1：3，年轻代占整个年轻代年老代和的1/4 XX:SurvivorRatio=n:年轻代中Eden区与两个Survivor区的比值。注意Survivor区有两个。如：3，表示Eden：Survivor=3：2，一个Survivor区占整个年轻代的1/5 XX:MaxPermSize=n:设置持久代大小 收集器设置 XX:+UseSerialGC:设置串行收集器 XX:+UseParallelGC:设置并行收集器 XX:+UseParalledlOldGC:设置并行年老代收集器 XX:+UseConcMarkSweepGC:设置并发收集器 垃圾回收统计信息 XX:+PrintGC XX:+PrintGCDetails XX:+PrintGCTimeStamps Xloggc:filename 并行收集器设置 XX:ParallelGCThreads=n:设置并行收集器收集时使用的CPU数。并行收集线程数。 XX:MaxGCPauseMillis=n:设置并行收集最大暂停时间 XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n) 并发收集器设置 XX:+CMSIncrementalMode:设置为增量模式。适用于单CPU情况。 XX:ParallelGCThreads=n:设置并发收集器年轻代收集方式为并行收集时，使用的CPU数。并行收集线程数。","updated":"2020-03-13T03:06:29.669Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"},{"name":"JVM","slug":"JVM","permalink":"https://zouchanglin.cn/tags/JVM/"}]},{"title":"Java中数组复制的效率比较","date":"2018-02-28T15:39:28.000Z","path":"2018/02/28/Java中数组复制的效率比较/","text":"在开发中，数组复制是经常使用的，很多方法都可以进行数组赋值，但是效率却天差地别：效率最高的是：System.arraycopy(), 下面是它的使用方式的参数说明： 们可以看看它的源代码，它是个native方法，毫无疑问效率最高： 再说说Arrays.copyof()方法，看源代码发现，它还是调用了System.arraycopy()方法： 然后呢，再看看Object类的clone方法： clone()的返回值是Object类型，强制类型转换毫无疑问是降低了效率，但是好歹是native方法，不会存在有特别明明显的差距的。当然自己通过for循环的方式也可以进行数组的复制，但是效率依旧是很低的！所以还是推荐用System.arraycopy() 来进行数组的复制吧！","updated":"2020-03-13T03:06:29.663Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"}]},{"title":"hashCode与equals","date":"2018-02-15T00:25:00.000Z","path":"2018/02/15/hashCode与equals/","text":"平时我们在项目里经常会用到HashMap这个数据结构，所以在面试的时候一定会被问这个问题∶你有没有重写过hashCode方法？你在使用HashMap时有没有重写hashCode和equals方法？你是怎么写的？ 那么为什么要重写hashCode与equals方法，这两个方法起着什么作用呢？当我们往HashMap里放key时，首先会调用这个对象的hashCode方法计算它的hash值，随后把key放入hash值所指引的内存位置。 equals()方法equals是超类Object中的一个基本方法，是用来判断一个对象和另一个对象是否具有相同的引用（即内存地址），如果有则返回true，下面是Object类的equals()方法： 123public boolean equals(Object obj) &#123; return (this == obj);&#125; equals()与 == 的区别对于从Object继承而来的equlas方法，与 == 并无区别，都是比较的对象的内存地址。但是我们可以重写equals方法，使其按照我们的要求来进行比较。比如String类就重写了equals方法，比较的是字符串的字符序列，而不是内存地址。 equals()的重写规则 自反性：对于任何非null的引用值x，x.equals(x)应该返回true 对称性：对于任何非null的引用值x与y，当且仅当：y.equals(x)返回true时，x.equals(y)才返回true 传递性：对于任何非null的引用值x、 y与z，如果y.equals(x)返回true，y.equals(z)返回true，那么x.equals(z)也应该返回true 一致性：对于任何非null的引用值x与y，假设对象的equals比较中的信息没有被修改，则多次调用x.equals(y)始终返回true或者始终返回false 对于任何非null的引用值x，x.equals(null)应返回false 上述这些规则在同一个类的两个对象中还是很容易理解的。 hashCode()方法hash code是一种通过对象得出Hash值的方式，在Java中，每个对象都会有一个对应的hashCode。通过算法，算出对象的hashcode，同一个对象的hashcode唯一(前提是对象没有被改变)，但是不同的对象也可能有相同的hashCode。 HashMapHashMap通过计算对象的Hash值判断对象应该在Hash表的哪个链上，通过equals方法判断是否是同一个对象。关键是我们没有重写hashCode方法，调用的仍是Object类的hashCode方法（因为所有的类都是Object的子类），而Object类的hashCode方法返回的hash值其实可以看出是对象的内存地址。 因为在hashMap中，判断key是否相等首先是比较hashCode，然后再用equals比较。我们重写equals方法是为了按我们自己的想法来比较两个对象是否相等。如果不重写hashCode方法，可能出现具有相同含义的不同对象（他们的hashCode不同）被pass掉的情况。而实际上他们应该是相同的key。而如果只重写hashCode不重写equals方法，那么equals只是判断两个对象是否是同一个对象。所以需要同时重写equals和hashCode方法，目的是为了准确定位到我们期望的key。","updated":"2020-04-26T11:59:08.279Z","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://zouchanglin.cn/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"https://zouchanglin.cn/tags/JavaSE/"}]},{"title":"网页启动本地Activity","date":"2018-02-03T08:51:00.000Z","path":"2018/02/03/网页启动本地Activity/","text":"前言Intent这个类在开发中是很常用的类，代表了着一个意图（获取理解为目标、目的），首先我们需要明确一点的就是：任何一个浏览器链接都是一个隐式意图，打开一个浏览器的方式无非就是显式意图和隐式意图，所以我们配置过滤器即可！ 示例首先，工程目录如图： MainActivity和布局文件都不用改，关键是manifests文件中LocalAppAty的配置: 12345678910111213141516171819202122232425262728293031&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;manifest xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" package=\"com.xpu.launchlocalapp\"&gt; &lt;application android:allowBackup=\"true\" android:icon=\"@mipmap/ic_launcher\" android:label=\"@string/app_name\" android:roundIcon=\"@mipmap/ic_launcher_round\" android:supportsRtl=\"true\" android:theme=\"@style/AppTheme\"&gt; &lt;activity android:name=\".MainActivity\"&gt; &lt;intent-filter&gt; &lt;action android:name=\"android.intent.action.MAIN\" /&gt; &lt;category android:name=\"android.intent.category.LAUNCHER\" /&gt; &lt;/intent-filter&gt; &lt;/activity&gt; &lt;activity android:name=\".LocalAppAty\" android:label=\"LocalAppAty\"&gt; &lt;intent-filter&gt; &lt;!--可以被浏览器启动的Activity--&gt; &lt;category android:name=\"android.intent.category.BROWSABLE\"&gt;&lt;/category&gt; &lt;category android:name=\"android.intent.category.DEFAULT\"&gt;&lt;/category&gt; &lt;action android:name=\"android.intent.action.VIEW\"&gt;&lt;/action&gt; &lt;data android:scheme=\"app\" &gt;&lt;/data&gt; &lt;/intent-filter&gt; &lt;/activity&gt; &lt;/application&gt;&lt;/manifest&gt; JavaWeb工程如下，一个很简单的标签： 一般安卓模拟器要访问本机的ip地址，使用10.0.0.2,端口号还是与你的服务器一致，我的是8080： 成功开启： 同时获取到了启动该Activity的信息来源：","updated":"2020-03-13T03:06:29.762Z","categories":[{"name":"移动开发","slug":"移动开发","permalink":"https://zouchanglin.cn/categories/%E7%A7%BB%E5%8A%A8%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://zouchanglin.cn/tags/Android/"}]},{"title":"Ajax异步请求与JSON数据格式","date":"2017-12-21T08:51:00.000Z","path":"2017/12/21/Ajax异步请求与JSON数据格式/","text":"百度的预搜索是怎么实现的呢？如下图： 这个场景应该是大家非常熟悉的吧，为什么我们没有点击搜索但是却可以弹出相关的搜索内容条目呢？其中就用到了ajax引擎！接下来我们就可以看一下这个ajax，哈哈！ 一、Ajax概述什么是同步，什么是异步同步现象：客户端发送请求到服务器端，当服务器返回响应之前，客户端都处于等待卡死状态异步现象：客户端发送请求到服务器端，无论服务器是否返回响应，客户端都可以随 意做其他事情，不会被卡死 Ajax的运行原理页面发起请求，会将请求发送给浏览器内核中的Ajax引擎，Ajax引擎会提交请求到 服务器端，在这段时间里，客户端可以任意进行任意操作，直到服务器端将数据返回 给Ajax引擎后，会触发你设置的事件，从而执行自定义的js逻辑代码完成某种页面功能。 二、js原生的Ajax技术js原生的Ajax其实就是围绕浏览器内内置的Ajax引擎对象进行学习的，要使用js原生的Ajax完成异步操作，有如下几个步骤： 创建Ajax引擎对象 为Ajax引擎对象绑定监听（监听服务器已将数据响应给引擎） 绑定提交地址 发送请求 下面是一个使用原生Ajax的示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\"&gt;&lt;title&gt;Insert title here&lt;/title&gt;&lt;script type=\"text/javascript\"&gt; //异步请求 function fn1()&#123; //(1)创建引擎对象 var xmlhttp = new XMLHttpRequest(); //(2)绑定监听 xmlhttp.onreadystatechange = function()&#123; //(5)接受相应数据 if(xmlhttp.readyState == 4 &amp;&amp; xmlhttp.status == 200)&#123; var res = xmlhttp.responseText; document.getElementById(\"span1\").innerHTML = res; &#125; &#125; //(3)绑定地址 xmlhttp.open(\"GET\", \"/WEB21/ajaxservlet\", true); //(4)发送请求 xmlhttp.send(); &#125; //同步请求 function fn2()&#123; //(1)创建引擎对象 var xmlhttp = new XMLHttpRequest(); //(2)绑定监听 xmlhttp.onreadystatechange = function()&#123; //(5)接受相应数据 if(xmlhttp.readyState == 4 &amp;&amp; xmlhttp.status == 200)&#123; var res = xmlhttp.responseText; document.getElementById(\"span2\").innerHTML = res; &#125; &#125; //(3)绑定地址 xmlhttp.open(\"GET\", \"/WEB21/ajaxservlet\", false); //(4)发送请求 xmlhttp.send(); &#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;input type=\"button\" value=\"异步访问服务器\" onclick=\"fn1()\"/&gt;&lt;span id=\"span1\"&gt;&lt;/span&gt; &lt;br&gt; &lt;input type=\"button\" value=\"同步访问服务器\" onclick=\"fn2()\"/&gt;&lt;span id=\"span2\"&gt;&lt;/span&gt; &lt;br&gt; &lt;input type=\"button\" value=\"测试按钮\" onclick=\"alert()\"/&gt;&lt;/body&gt;&lt;/html&gt; servlet如下： 1234567891011protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; // TODO Auto-generated method stub //response.getWriter().write(\"XPU\"); try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; response.getWriter().write(Math.random()+\"\");&#125; 这样的话会得到下图所示的效果：现象很简单，我的测试按钮其实就是弹出一个空的提示框，当我点击异步访问的时候，浏览器会向服务器的servlet发送一个请求，这个请求是获取一个随机数，而且为了模拟服务器长达3秒的运算我让服务器睡眠了3秒钟，点击异步访问后可以立马点击测试按钮弹出提示框，但是点击同步访问后立马点击测试按钮却要等待3秒才会弹出提示框，而且我点击了多次就弹出了多次提示框，相信这个额例子是非常容易理解同步和异步的特点的！ 为什么要这样做呢？有时候我们在加载网页的时候，可能有些图片非常大，在网速不是很好的情况下需要很长的时间才可以加载，如果使用ajax引擎发起对图片的异步请求，即使在图片还没有加载完毕的情况下也可以使用其他的功能，这就是异步请求的一个应用，接下来用图片说明一下：很显然，如果没有ajax引擎的情况下发起请求而且等到收到响应对于浏览器来说是非常浪费时间的，尤其是发起的请求计算量过大，网速特别慢的时候是非常影响用户体验的，有了ajax引擎替我们发起请求和接受协议，浏览器就有机会去做其他的事情，而不是傻傻的等待！ 原生ajax的GET与POST请求：GET 请求比较简单，如下格式即可（上面的代码中用的就是GET请求） 12xmlhttp.open(\"GET\",\"test1.txt\",true);xmlhttp.send(); 但是使用GET请求可能的到的是缓存结果，为了避免这样的情况出现，应该使用如下示例设置不同的ID： 12xmlhttp.open(\"GET\",\"demo_get.asp?t=\" + Math.random(),true);xmlhttp.send(); 简单地POST请求： 12xmlhttp.open(\"POST\",\"demo_post.asp\",true);xmlhttp.send(); 带参数的POST请求(切记不要忘记添加请求头）： 123xmlhttp.open(\"POST\",\"ajax_test.asp\",true);xmlhttp.setRequestHeader(\"Content-type\",\"application/x-www-form-urlencoded\");xmlhttp.send(\"fname=Bill&amp;lname=Gates\"); 最后说一个关于获取XMLHttpRequest 对象的问题：创建 XMLHttpRequest 对象：所有现代浏览器（IE7+、Firefox、Chrome、Safari 以及 Opera）均内建 XMLHttpRequest 对象。 XMLHttpRequest 对象三个重要的属性 属性 描述 onreadystatechange 存储函数（或函数名），每当 readyState 属性改变时，就会调用该函数。 readyState 存有 XMLHttpRequest 的状态。从 0 到 4 发生变化。0: 请求未初始化 1: 服务器连接已建立 2: 请求已接收 3: 请求处理中 4: 请求已完成，且响应已就绪 status 200: “OK” 404: 未找到页面 创建 XMLHttpRequest 对象的语法： 1variable=new XMLHttpRequest(); 老版本的 Internet Explorer （IE5 和 IE6）使用 ActiveX 对象： 1variable=new ActiveXObject(\"Microsoft.XMLHTTP\"); 为了应对所有的现代浏览器，包括 IE5 和 IE6，请检查浏览器是否支持 XMLHttpRequest 对象。如果支持，则创建 XMLHttpRequest 对象。如果不支持，则创建 ActiveXObject ： 1234567var xmlhttp;if (window.XMLHttpRequest)&#123; // code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest();&#125;else&#123; // code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\");&#125; 使用XMLHttpRequest对象用于在后台与服务器交换数据，应用场景： 在不重新加载页面的情况下更新网页 在页面已加载后从服务器请求数据 在页面已加载后从服务器接收数据 在后台向服务器发送数据 所有异步访问都是靠ajax引擎！ 三、JSON数据格式json是一种与语言无关的数据交换的格式，作用： 使用ajax进行前后台数据交换 移动端与服务端的数据交换 Json的格式与解析json有两种格式：1）对象格式：{“key1”:obj,”key2”:obj,”key3”:obj…}2）数组/集合格式：[obj,obj,obj…] 例如：user对象 用json数据格式表示 1&#123;\"username\":\"zhangsan\",\"age\":28,\"password\":\"123\",\"addr\":\"北京\"&#125; 例如：List 用json数据格式表示 1[&#123;\"pid\":\"10\",\"pname\":\"小米4C\"&#125;,&#123;&#125;,&#123;&#125;] 注意：对象格式和数组格式可以互相嵌套注意：json的key是字符串 jaon的value是Object json的解析：json是js的原生内容，也就意味着js可以直接取出json对象中的数据 Json的转换插件将java的对象或集合转成json形式字符串 json的转换插件是通过java的一些工具，直接将java对象或集合转换成json字符串。常用的json转换工具有如下几种：1）jsonlib2）Gson：google3）fastjson：阿里巴巴4）cJSON：腾讯的 四、Jquery的Ajax技术jquery是一个优秀的js框架，自然对js原生的ajax进行了封装，封装后的ajax的操 作方法更简洁，功能更强大，与ajax操作相关的jquery方法有如下几种，但开发中经常使用的有三种 ： 1$.get(url, [data], [callback], [type])$.post(url, [data], [callback], [type]) url：代表请求的服务器端地址data：代表请求服务器端的数据（可以是key=value形式也可以是json格式）callback：表示服务器端成功响应所触发的函数（只有正常成功返回才执行）type：表示服务器端返回的数据类型（jquery会根据指定的类型自动类型转换）常用的返回类型：text、json、html等 1$.ajax( &#123; option1:value1,option2:value2... &#125; ); async：是否异步，默认是true代表异步data：发送到服务器的参数，建议使用json格式dataType：服务器端返回的数据类型，常用text和jsonsuccess：成功响应执行的函数，对应的类型是function类型type：请求方式，POST/GETurl：请求服务器端地址 下面是一个使用示例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\"&gt;&lt;title&gt;Insert title here&lt;/title&gt;&lt;/head&gt;&lt;script type=\"text/javascript\" src=\"jquery-1.11.3.min.js\"&gt;&lt;/script&gt;&lt;script type=\"text/javascript\"&gt; function fn1() &#123; //get异步访问 $.get( \"/WEB21/ajaxservlet\",//url地址 &#123;\"name\":\"邹长林\",\"age\":20&#125;,//请求参数 function(data)&#123; //成功后的回调函数 alert(data.firstname+\" \"+data.lastname+\" \"+data.age); &#125;, \"json\" ); &#125; function fn2() &#123; $.post( \"/WEB21/ajaxservlet\",//url地址 &#123;\"name\":\"邹长林\",\"age\":20&#125;,//请求参数 function(data)&#123; //成功后的回调函数 alert(data.firstname+\" \"+data.lastname+\" \"+data.age); &#125;, \"json\" ); &#125; function fn3() &#123; $.ajax(&#123; url:\"/WEB21/ajaxservlet\", //请求的地址 type:\"GET\", //请求类型 async:true, //是否同步，默认同步 data:&#123;\"name\":\"tim\", \"age\":18&#125;, //请求的数据，JSON格式 success:function(data)&#123; //请求成功的回调函数 alert(data.firstname); &#125;, error:function()&#123; //请求失败的回调函数 alert(\"请求失败\"); &#125;, dataType:\"json\",//从服务器接受返回的数据类型，一般为JSON或者text &#125;); &#125;&lt;/script&gt;&lt;body&gt; &lt;input type=\"button\" value=\"GET访问服务器\" onclick=\"fn1()\"/&gt; &lt;span id=\"span1\"&gt;&lt;/span&gt; &lt;br&gt; &lt;input type=\"button\" value=\"POST访问服务器\" onclick=\"fn2()\"/&gt; &lt;span id=\"span2\"&gt;&lt;/span&gt; &lt;br&gt; &lt;input type=\"button\" value=\"Ajax访问服务器\" onclick=\"fn3()\"/&gt;&lt;/body&gt;&lt;/html&gt; 在使用Ajax三种请求的时候需要注意的地方：GET方式提交的数据到服务器可能会出现乱码，使用编解码的方式就可以解决POST方式提交的数据已经经过ajax处理了，无需我们再自己处理一遍获取数据的时候的乱码问题也是很好解决的： 1request.setCharacterEncoding(\"UTF-8\");","updated":"2020-03-13T03:06:29.628Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://zouchanglin.cn/tags/JavaWeb/"},{"name":"异步","slug":"异步","permalink":"https://zouchanglin.cn/tags/%E5%BC%82%E6%AD%A5/"}]},{"title":"忍受简单的能力","date":"2017-12-07T03:51:00.000Z","path":"2017/12/07/忍受简单的能力/","text":"我关上灯，对女儿说：「闭上眼睛，别乱动了。」 女儿立刻大声抗议：「可是我睡不着！」 我只好又强调了一遍：「我只是请你闭上眼睛，别乱动。」 我从来没说过「请你尽快睡着」，那是我女儿的脑补。我当然挺高兴的，她能脑补出那句话来，说明她起码脑子不笨，能够猜到一个指令之后的真实意图。但同时我也有担心。因为这一点聪明，她入睡可能就会困难一点。「闭上眼睛，别乱动」是一条很简单的指令，是我认为她充分有能力做到的。但她不安于这条指令，而去考虑「即便我照着做了，我可能还是睡不着」。这样的思考，对当下有害无益。 同样的事情在我的工作中也常常遇到。比如说，我跟学生辅导员讲，危机干预中哪些危险的信号是需要注意的。有的辅导员就会问：「可是李老师，有的学生其实有危险的想法，但他就是憋着不说，也根本不表达这些信号，我们怎么去识别呢？」我说：「那是另外一种情况了，但我刚刚讲的不是这种情况，我刚刚讲的你都记住了吗？」他说那都很简单。我说：「请你复述一遍？」结果他可能说不上来。 他们的注意力都集中在「要是这种方法不行呢？」，反而忽略了对「这种方法」本身的吸收。其实，我教的是更简单，更常见的情形，从现实性来讲，比他们考虑的那些例外情况更重要，也更有开展工作的空间。可以说他们是因小失大。 我做咨询的时候，有时候要教来访者尝试一些不一样的说话方式。比如，用更坚定的语气表达拒绝。但是教完之后，他们常常不能真的付诸实践。下一次咨询的时候他们不说自己练习时遇到的困难，而是深入思考：「万一」对方根本不听我的，「万一」对方如何如何纠缠，「万一」对方表现得更强硬，又该怎么办？……假如我们就这些话题展开讨论，完全还可以讨论十次二十次。脑子更快的人，甚至一听我讲完就忧心忡忡地想到：「要是一直拒绝别人的要求，以后会不会就没朋友了？」 他距离「一直拒绝别人的要求」还远得很呢，但他已经在担心了。 老师都喜欢教聪明的学生，因为他们脑子反应很快，就可以省很多时间。但是太聪明了也不好。因为脑子太快了，需要身体用工夫的地方，就有种种困难。 聪明是在头脑中加速的过程。当我匀速前进的时候，聪明的孩子就在思考：他下一步会走向哪里？你看，我明明还在这一步，但是在聪明人眼里，下一步等于已经有了。他们思考的速度，快于我实际的步速。到我真的在走下一步时，他们的想法也许已经发展为：「这条路通向何方？」他们绝不会满足于跟随我的步伐，而要直接预见到我的终点。再然后，他们会猜测：为什么要到那里？到了之后又会如何？今天还有什么其它打算？这种思维的推进，大刀阔斧，我的路还没走到一半，他们在脑子里说不定已经演绎完了我的一天。换到上课的情境，就是我刚说了上半句，学生就已然猜到了下半句，可能就连一堂课要讲的全部内容，都落到了他们的预知之内。 据我所知，这样的学生上课很容易走神…… 对于聪明人来说，最难以忍受的情况不是一件事有多难，而是纯粹的简单。没有难度挑战的任务，会让他们感到无所着力，继而注意涣散，不得已靠着「举一反三」之类的小花样来自我提神。重复的练习是他们的死穴。——你去问一个健身教练，他多半就见过不少这样的客户：他们一个动作只要重复一两遍，就会开始琢磨：「这个练习真的管用吗？」「这里面真正关键的元素在哪里？」「练完这个，下一步练什么？」借着这些天马行空的思考，他们才能松口气，从当下的枯燥中解脱出来。 而思维上的变化多端，就造成了行动层面的进步迟缓。 就拿我女儿的例子来说，「闭上眼睛，别乱动」是她入睡的第一步，而「睡着」则是第N步。她在第一步的阶段担心第N步的结果，反而连第一步也做不到。 所以我认识的学生里面，除了少部分天赋异禀的奇才之外，真正最影响一个人的成就的因素，可能不是智商，也不是努力，而在于他有多「踏实」。踏实的人做一件事，是一件事；学一样东西，就学得到一样东西。你只要看一门课最开始的时候，讲一些最简单的知识，哪些人可以不厌其烦地听进去，他们未来就算没有什么惊人的成就，也都不会混得太差。而聪明人往往已失去了耐心，都趴在桌子上睡觉。 骐骥一跃，不能十步；驽马十驾，功在不舍。 但趴在桌子上睡觉，还不算是最糟糕的学习状态。我自己上课时也睡过无数，就我的经验来说，当然什么也没学到，但起码知道没学到东西。更可怕的情况是自我催眠，感觉自己在学，实则空空如也。一种典型的催眠方式，就是用手机把每一页PPT都拍下来，之后该开小差照样开小差。他们以为自己「学到了」，但无非是在手机里储存了一堆只在考试前才会看一遍的照片而已。有时我会禁止学生照相，但他们还是会把电脑搬到教室里。一边听课，一边噼里啪啦地打字。这样也算很努力。但他们努力把课堂的内容敲进电脑里，就是为了自己可以更心安理得地记不住它们。 之所以说这种情况更可怕，是因为他们运用了不露痕迹的方式，把「并没有真的学到什么东西」这件事巧妙地敷衍过去：「反正随时可以再看我的笔记。」 这样的人也许会买很多书（然而不看），或者读很多文章（然而不想），或者整理出很多读书心得（然而并不用来改变自己）。这些事做得越卖力，他们陷入的幻觉可能就越深。有一些读者常常给我留言：「你说的没错，可是然后呢？」你看，他们关注的不是我说了什么，而是我没说的还有哪些。描述一种现象，他们首先会想到：可是也有例外吧？如果证明是一个普适的规律，他们又会说：原因是什么呢？假设提出了原因，他们很快又抱怨：说得头头是道，怎么不讲一讲解决办法？如果我有那么牛，连解决建议都提了，恐怕还是会说：道理都懂，然而并没有什么X用。 这种思维方式，就等于是说：「我们来聊一聊A吧。」 「好啊，我最近认识了A的朋友B，B是个好人，他还介绍我认识了C……」 你看，这种思维方式自有好处。话题已经从A的身上转开了，但是在说话的人看来，似乎仿佛，自己并不算是在跑题。正如「道理都懂，然而并没有什么X用」这句话，说出来很轻松，也就不会让人觉察到——其实道理也没有真的都懂。 我想，这里面大概也有一种安全感。一个人学东西之所以无法专注，可能就是因为他无法忍受专注在一个点上的感觉。学习一个东西（尤其是简单的，重复的）往往让人焦虑。因为这一刻你真的停在一个东西上，就会意识到自己有多渺小，而要学的东西似乎还无穷无尽。因此，用最快的速度跳跃式地前进，用摘要的形式纵观大概，存成照片或者写成笔记，或是把注意力投向这个东西之外，「然后呢，然后还有什么？」这样就可以说：「行了，这个我已经懂了。」这是回避焦虑的法宝。 一口一口地吃饭太慢了。恨不得一口吃下一百口，谁叫锅里还有那么多？ 所以重要的事情才要说三遍。可是你还记得上一段看了三遍的话是什么吗？ 对一个学习者来说，这个时代是最好的时代，但可能也是最坏的时代。今天的信息是整个地泛滥了，你很难让自己真的不去焦虑。如果你想用方便的方式解决这种焦虑，就只有不断吸收复杂的信息。并不缺这样的信息源，随便在网上找一找，就有数不清的「绝世武功的目录」，这辈子肯定练不完，只好先用脑子过一遍。 如果真的想学一点东西，就需要一种特别的能力。我把它叫做「忍受简单的能力」。我不知道是叫能力还是勇气更好，因为它涉及到了一种真正意义上的放弃。——当你在某一个点上停下来，打算认真下点工夫的时候，这意味着放弃想象中的其它可能。你得到的只是简单的一点点，失去的却是头脑中的整片汪洋。一个人守着这样一点，面对巨大的不确定也不逃避，他要么需要很勇敢，要么是很天真。 就像一个专注吃手的婴儿，他一旦意识到自己离长大还有多远，可能就急了。 这篇文章阐述的也不过是一个简单的道理而已，几句话就能说明白，并没有给出什么成体系的理论，方法，和建议。然后呢？——然后我就停在这里了。 原文地址：《忍受简单的能力》","updated":"2020-03-13T03:06:29.740Z","categories":[{"name":"生活","slug":"生活","permalink":"https://zouchanglin.cn/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"生活","slug":"生活","permalink":"https://zouchanglin.cn/tags/%E7%94%9F%E6%B4%BB/"}]},{"title":"下载中文文件乱码解决方式","date":"2017-11-04T13:51:00.000Z","path":"2017/11/04/下载中文文件乱码解决方式/","text":"关于编码的问题有几点需要说清楚：UTF-8国际编码，GBK中文编码。GBK包含GB2312，即如果通过GB2312编码后可以通过GBK解码，反之可能不成立;这个道理很简单，计算机存储的是010001010010…这种的数据，也只能存储这样的数据，通过二进制的规则可以解析为数字，如2二进制就是10，这也就意味着任意数字在有限存储位的情况下都可表示为010101…这样的数据，老外的文字就是26个字母，我们假想为分别对应1~26，但是却不是这样的，真正的对应关系就是ASCII码表中的关系，但是如何表示汉字呢？很显然也需要一套对应的码表，于是UTF-8、GBK、GB2312这些编码方式就是为了解决这个问题的。 下面看看正题：首先我的目录是：压缩包即是我要下载的文件 我的下载界面： 1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Insert title here&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h3&gt; 使用a标签指向服务器资源 &lt;/h3&gt; &lt;a href=\"/Web14_response/download/a.flv\"&gt;a.flv&lt;/a&gt;&lt;br&gt; &lt;a href=\"/Web14_response/download/a.jpg\"&gt;a.jpg&lt;/a&gt;&lt;br&gt; &lt;a href=\"/Web14_response/download/a.mp3\"&gt;a.mp3&lt;/a&gt;&lt;br&gt; &lt;a href=\"/Web14_response/download/a.mp4\"&gt;a.mp4&lt;/a&gt;&lt;br&gt; &lt;a href=\"/Web14_response/download/a.txt\"&gt;a.txt&lt;/a&gt;&lt;br&gt; &lt;a href=\"/Web14_response/download/a.zip\"&gt;a.zip&lt;/a&gt;&lt;br&gt; &lt;h3&gt; 使用服务器代码完成服务器资源下载 &lt;/h3&gt; &lt;a href=\"/Web14_response/DownloadServlet2?filename=a.flv\"&gt;a.flv&lt;/a&gt;&lt;br&gt; &lt;a href=\"/Web14_response/DownloadServlet2?filename=a.jpg\"&gt;a.jpg&lt;/a&gt;&lt;br&gt; &lt;a href=\"/Web14_response/DownloadServlet2?filename=a.mp3\"&gt;a.mp3&lt;/a&gt;&lt;br&gt; &lt;a href=\"/Web14_response/DownloadServlet2?filename=a.mp4\"&gt;a.mp4&lt;/a&gt;&lt;br&gt; &lt;a href=\"/Web14_response/DownloadServlet2?filename=a.txt\"&gt;a.txt&lt;/a&gt;&lt;br&gt; &lt;a href=\"/Web14_response/DownloadServlet2?filename=压缩包.zip\"&gt;压缩包.zip&lt;/a&gt;&lt;br&gt;&lt;/body&gt;&lt;/html&gt; 提供文件下载的Servlet: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.xpu.content; import java.io.FileInputStream;import java.io.IOException;import java.io.InputStream;import java.net.URLEncoder; import javax.servlet.ServletException;import javax.servlet.ServletOutputStream;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse; import sun.misc.BASE64Encoder; public class DownloadServlet extends HttpServlet &#123; public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; // 这个主要是下载中文的 // 获取要下载的文件的名称 String filename = request.getParameter(\"filename\"); // 解决获得中文参数的乱码 filename = new String(filename.getBytes(\"ISO8859-1\"), \"UTF-8\"); // 设置头信息 response.setContentType(getServletContext().getMimeType(filename)); String agent = request.getHeader(\"User-Agent\"); String filenameEncoder = \"\"; if (agent.contains(\"MSIE\")) &#123; // IE浏览器 filenameEncoder = URLEncoder.encode(filename, \"utf-8\"); filenameEncoder = filename.replace(\"+\", \" \"); &#125; else if (agent.contains(\"Firefox\")) &#123; // 火狐浏览器 BASE64Encoder base64Encoder = new BASE64Encoder(); filenameEncoder = \"=?utf-8?B?\" + base64Encoder.encode(filename.getBytes(\"utf-8\")) + \"?=\"; &#125; else &#123; // 其它浏览器 filenameEncoder = URLEncoder.encode(filename, \"utf-8\"); &#125; response.setHeader(\"Content-Disposition\", \"attachment;filename=\" + filenameEncoder); System.out.println(filename); // 获取文件的绝对路径 String path = getServletContext().getRealPath(\"/download/\" + filename); // 获取该文件的输入流 InputStream is = new FileInputStream(path); // 获取输出流 ServletOutputStream os = response.getOutputStream(); // 文件拷贝的模板代码 int len = 0; byte[] bys = new byte[1024]; while ((len = is.read(bys)) != -1) &#123; os.write(bys, 0, len); &#125; is.close(); &#125; public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; doGet(request, response); &#125;&#125; 下载效果：","updated":"2020-03-13T03:06:29.715Z","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://zouchanglin.cn/categories/Web%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://zouchanglin.cn/tags/JavaWeb/"},{"name":"文件下载","slug":"文件下载","permalink":"https://zouchanglin.cn/tags/%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD/"}]},{"title":"Linux权限与重要目录","date":"2017-10-02T03:51:00.000Z","path":"2017/10/02/Linux权限与重要目录/","text":"权限问题字母简写说明 简称 含义 u user (文件的拥有者) g group (文件所属组) o other (其他用户) a all (所有用户) Linux进入目录需要的权限对user需要rwx权限，对group需要rx权限，对other需要rx权限； 在目录中执行touch、ls、 rm、 mv需要的权限 touch: 对所有者需要rw权限，对所有者所在组需要r权限，对其他用户需要r权限；ls: 都需要r权限rm: 都需要w权限mv: 都需要w权限 结论 读权限：对文件具有读取文件内容的权限、对文件目录具有浏览目录信息的权限写权限：对文件具有修改文件内容的权限、对文件目录具有移动删除目录信息的权限执行权限：对文件具有执行文件的权限、对文件目录具有进入目录的权限 Linux的文件分类普通文件（regular file）就是一般存取的文件，由ls -al显示出来的属性中，第一个属性为 [-]，例如 [-rwxrwxrwx]。另外，依照文件的内容，又大致可以分为：纯文本文件这是Unix系统中最多的一种文件类型，之所以称为纯文本文件，是因为内容可以直接读到的数据，例如数字、字母等等。设 置文件几乎都属于这种文件类型。举例来说，使用命令“cat ~/.bashrc”就可以看到该文件的内容（cat是将文件内容读出来）。 二进制文件（binary）系统其实仅认识且可以执行二进制文件（binary file）。Linux中的可执行文件（脚本，文本方式的批处理文件不算）就是这种格式的。举例来说，命令cat就是一个二进制文件。 数据格式的文件（data）有些程序在运行过程中，会读取某些特定格式的文件，那些特定格式的文件可以称为数据文件（data file）。举例来说，Linux在用户登入时，都会将登录数据记录在 /var/log/wtmp文件内，该文件是一个数据文件，它能通过last命令读出来。但使用cat时，会读出乱码。因为它是属于一种特殊格式的文件。 目录文件（directory）就是目录，第一个属性为 [d]，例如 [drwxrwxrwx]。 连接文件（link）类似Windows下面的快捷方式。第一个属性为 [l]，例如 [lrwxrwxrwx]。 设备与设备文件（device）与系统外设及存储等相关的一些文件，通常都集中在 /dev目录。通常又分为两种： 块设备文件就是存储数据以供系统存取的接口设备，简单而言就是硬盘。例如一号硬盘的代码是 /dev/hda1等文件。第一个属性为 [b]，支持随机读取。 字符设备文件即串行端口的接口设备，例如键盘、鼠标等等。第一个属性为 [c]，只能按照顺序读写。 套接字（sockets）这类文件通常用在网络数据连接。可以启动一个程序来监听客户端的要求，客户端就可以通过套接字来进行数据通信。第一个属性为 [s]，最常在 /var/run目录中看到这种文件类型。 管道（FIFO,pipe）FIFO也是一种特殊的文件类型，它主要的目的是，解决多个程序同时存取一个文件所造成的错误。FIFO是first-in-first-out（先进先出）的缩写。第一个属性为 [p]。 [5] 重要目录/根目录，所有的目录、文件、设备都在/之下，/就是Linux文件系统的组织者，也是最上级的领导者。 /bin 就是二进制（binary）英文缩写。在一般的系统当中，都可以在这个目录下找到linux常用的命令。系统所需要的那些命令位于此目录。 /boot Linux的内核及引导系统程序所需要的文件目录，比如 vmlinuz initrd.img 文件都位于这个目录中。在一般情况下，GRUB或LILO系统引导管理器也位于这个目录。 /cdrom 这个目录在刚刚安装系统的时候是空的。可以将光驱文件系统挂在这个目录下。例如：mount /dev/cdrom /cdrom /dev 设备（device)的英文缩写。这个目录对所有的用户都十分重要。因为在这个目录中包含了所有linux系统中使用的外部设备。但是这里并不是放的外部设备的驱动程序。这一点和常用的windows,dos操作系统不一样。它实际上是一个访问这些外部设备的端口。可以非常方便地去访问这些外部设备，和访问一个文件，一个目录没有任何区别。 /etc 这个目录是linux系统中最重要的目录之一。在这个目录下存放了系统管理时要用到的各种配置文件和子目录。要用到的网络配置文件，文件系统，x系统配置文件，设备配置信息，设置用户信息等都在这个目录下。 /home 如果建立一个用户，用户名是”xx”,那么在/home目录下就有一个对应的/home/xx路径，用来存放用户的主目录。 /lib 是库（library）英文缩写。这个目录是用来存放系统动态连接共享库的。几乎所有的应用程序都会用到这个目录下的共享库。因此，千万不要轻易对这个目录进行什么操作，一旦发生问题，系统就不能工作了。 /lost+found 在ext2或ext3文件系统中，当系统意外崩溃或机器意外关机，而产生一些文件碎片放在这里。当系统启动的过程中fsck工具会检查这里，并修复已经损坏的文件系统。有时系统发生问题，有很多的文件被移到这个目录中，可能会用手工的方式来修复，或移到文件到原来的位置上。 /mnt 这个目录一般是用于存放挂载储存设备的挂载目录的，比如有cdrom等目录。可以参看/etc/fstab的定义。 /media 有些linux的发行版使用这个目录来挂载那些usb接口的移动硬盘（包括U盘）、CD/DVD驱动器等等。 /opt 这里主要存放那些可选的程序。 /proc 可以在这个目录下获取系统信息。这些信息是在内存中，由系统自己产生的。 /root Linux超级权限用户root的家目录。 /sbin 这个目录是用来存放系统管理员的系统管理程序。大多是涉及系统管理的命令的存放，是超级权限用户root的可执行命令存放地，普通用户无权限执行这个目录下的命令，这个目录和/usr/sbin; /usr/X11R6/sbin或/usr/local/sbin目录是相似的，凡是目录sbin中包含的都是root权限才能执行的。 /selinux 对SElinux的一些配置文件目录，SElinux可以让linux更加安全。 /srv服务启动后，所需访问的数据目录，举个例子来说，www服务启动读取的网页数据就可以放在/srv/www中 /tmp 临时文件目录，用来存放不同程序执行时产生的临时文件。有时用户运行程序的时候，会产生临时文件。/tmp就用来存放临时文件的。/var/tmp目录和这个目录相似。 /usr 这是linux系统中占用硬盘空间最大的目录。用户的很多应用程序和文件都存放在这个目录下。在这个目录下，可以找到那些不适合放在/bin或/etc目录下的额外的工具 /usr/local 这里主要存放那些手动安装的软件，即不是通过“新立得”或apt-get安装的软件。它和/usr目录具有相类似的目录结构。让软件包管理器来管理/usr目录，而把自定义的脚本（scripts)放到/usr/local目录下面、。 /usr/share系统共用的东西存放地，比如/usr/share/fonts 是字体目录，/usr/share/doc和/usr/share/man帮助文件。 /var这个目录的内容是经常变动的，看名字就知道，可以理解为vary的缩写，/var下有/var/log 这是用来存放系统日志的目录。/var/ www目录是定义Apache服务器站点存放目录；/var/lib 用来存放一些库文件，比如MySQL的，以及MySQL数据库的的存放地。 /sys目录结构如图所示： .├── block├── bus├── class├── dev├── devices├── firmware├── fs├── hypervisor├── kernel├── module└── power block 该目录下的所有子目录代表着系统中当前被发现的所有块设备。按照功能来说防止在/sys/class下会更合适，但由于历史遗留因素而一直存在于/sys/block，但从linux2.6.22内核开始这部分就已经标记为过去时，只有打开了CONFIG_SYSFS_DEPRECATED配置编译才会有这个目录存在，并且其中的内容在从linux2.6.26版本开始已经正式移到了/sys/class/block，旧的接口/sys/block为了向后兼容而保留存在，但其中的内容已经变为了指向它们在/sys/devices/中真实设备的符号链接文件。 bus该目录下的每个子目录都是kernel支持并且已经注册了的总线类型。这是内核设备按照总线类型分层放置的目录结构，/sys/devices中的所有设备都是连接于某种总线之下的，bus子目录下的每种具体总线之下可以找到每个具体设备的符号链接，一般来说每个子目录(总线类型)下包含两个子目录，一个是devices，另一个是drivers；其中devices下是这个总线类型下的所有设备，这些设备都是符号链接，它们分别指向真正的设备(/sys/devices/name/下)；而drivers下是所有注册在这个总线上的驱动，每个driver子目录下 是一些可以观察和修改的driver参数。(它也是构成linux统一设备模型的一部分) class 该目录下包含所有注册在kernel里面的设备类型，这是按照设备功能分类的设备模型，每个设备类型表达具有一种功能的设备。每个设备类型子目录下都是这种哦哦那个设备类型的各种具体设备的符号链接，这些链接指向/sys/devices/name下的具体设备。设备类型和设备并没有一一对应的关系，一个物理设备可能具备多种设备类型；一个设备类型只表达具有一种功能的设备，比如：系统所有输入设备都会出现在/sys/class/input之下，而不论它们是以何种总线连接到系统的。(/sys/class也是构成linux统一设备模型的一部分) dev该目录下维护一个按照字符设备和块设备的主次号码(major:minor)链接到真实设备(/sys/devices)的符号链接文件。 devices该目录下是全局设备结构体系，包含所有被发现的注册在各种总线上的各种物理设备。一般来说，所有的物理设备都按其在总线上的拓扑结构来显示，但有两个例外，即platform devices和system devices。platform devices一般是挂在芯片内部的高速或者低速总线上的各种控制器和外设，它们能被CPU直接寻址；system devices不是外设，而是芯片内部的核心结构，比如CPU，timer等，它们一般没有相关的驱动，但是会有一些体系结构相关的代码来配置它们。(sys/devices是内核对系统中所有设备的分层次表达模型，也是/sys文件系统管理设备的最重要的目录结构) fs按照设计，该目录使用来描述系统中所有的文件系统，包括文件系统本身和按照文件系统分类存放的已挂载点。 firmware该目录下包含对固件对象(firmware object)和属性进行操作和观察的接口，即这里是系统加载固件机制的对用户空间的接口.(关于固件有专用于固件加载的一套API) hypervisor该目录是与虚拟化Xen相关的装置。(Xen是一个开放源代码的虚拟机监视器) kernel这个目录下存放的是内核中所有可调整的参数 module 该目录下有系统中所有的模块信息，不论这些模块是以内联(inlined)方式编译到内核映像文件中还是编译为外模块(.ko文件)，都可能出现在/sys/module中。即module目录下包含了所有的被载入kernel的模块。 powe该目录是系统中的电源选项，对正在使用的power子系统的描述。这个目录下有几个属性文件可以用于控制整个机器的电源状态，如可以向其中写入控制命令让机器关机/重启等等。 在 /sys/devices 下是所有设备的真实对象，包括如视频卡和以太网卡等真实的设备，也包括ACPI 等不那么显而易见的真实设备、还有tty, bonding 等纯粹虚拟的设备；在其它目录如 class, bus 等中则在分类的目录中含有大量对 devices 中真实对象引用的符号链接文件；","updated":"2020-03-13T03:06:29.675Z","categories":[{"name":"操作系统理论","slug":"操作系统理论","permalink":"https://zouchanglin.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zouchanglin.cn/tags/Linux/"}]},{"title":"Windows与VS下载链接","date":"2017-09-17T03:20:29.000Z","path":"2017/09/17/Windows与VS下载链接/","text":"操作系统通过这些链接可以下载到正版的操作系统，就别去雨林某风，XXX之家下载了，而且迅雷还是超级快的 文件名 CN_WINXP_PRO_ISO.img SHA1 73800FE8959F40361BAE3A6553CC66D27D78722E 文件大小 505.63MB 发布时间 2001-10-30 1ed2k:&#x2F;&#x2F;|file|CN_WINXP_PRO_ISO.img|530186240|7855069CE4216615D761654E2B75A4F7|&#x2F; 文件名 cn_windows_7_enterprise_x64_dvd_x15-70741.iso SHA1 EE20DAF2CDEDD71C374E241340DEB651728A69C4 文件大小 2.98GB 发布时间 2009-08-23 1ed2k:&#x2F;&#x2F;|file|cn_windows_7_enterprise_x64_dvd_x15-70741.iso|3203516416|876DCF115C2EE28D74B178BE1A84AB3B|&#x2F; 文件名 cn_windows_7_ultimate_x64_dvd_x15-66043.iso SHA1 4A98A2F1ED794425674D04A37B70B9763522B0D4 文件大小 3.11GB 发布时间 2009-08-26 1ed2k:&#x2F;&#x2F;|file|cn_windows_7_ultimate_x64_dvd_x15-66043.iso|3341268992|7DD7FA757CE6D2DB78B6901F81A6907A|&#x2F; cn_windows_8_x64_dvd_915407.iso SHA1 A87C4AA85D55CD83BAE9160560D1CB3319DD675C 文件大小 3.4GB 发布时间 2012-08-15 1ed2k:&#x2F;&#x2F;|file|cn_windows_8_x64_dvd_915407.iso|3652950016|5C7F8C212BD3A1827866563773A431C2|&#x2F; 文件名 cn_windows_8_1_x64_dvd_2707237.iso SHA1 F79E0093DDEDD488F40D4AE6B6F0FA3C529051E1 文件大小 3.8GB 发布时间 2013-09-09 1ed2k:&#x2F;&#x2F;|file|cn_windows_8_1_x64_dvd_2707237.iso|4076017664|839CBE17F3CE8411E8206B92658A91FA|&#x2F; 文件名 cn_windows_10_multiple_editions_x64_dvd_6848463.iso SHA1 C71D49A6144772F352806201EF564951BE55EDD5 文件大小 4.01GB 发布时间 2015-07-29 1ed2k:&#x2F;&#x2F;|file|cn_windows_10_multiple_editions_x64_dvd_6848463.iso|4303300608|94FD861E82458005A9CA8E617379856A|&#x2F; 开发工具 文件名 cn_visual_studio_2010_ultimate_x86_dvd_532347.iso SHA1 44B73423A7BBCE38D06BA55ECD821946630BEA4D 文件大小 2.5GB 发布时间 2010-05-26 1ed2k:&#x2F;&#x2F;|file|cn_visual_studio_2010_ultimate_x86_dvd_532347.iso|2685982720|4AE6228933DDE49D9BFA4C3467C831C2|&#x2F; 文件名 cn_visual_studio_community_2015_x86_dvd_6847368.iso SHA1 1044F9F4E0EA1304AFECF6780BF599F1DA248DF8 文件大小 3.74GB 发布时间 2015-11-13 1ed2k:&#x2F;&#x2F;|file|cn_visual_studio_community_2015_x86_dvd_6847368.iso|4013920256|EB7F6605EDE67509E218E29173AC6574|&#x2F; 文件名 cn_sql_server_2016_developer_x64_dvd_8776722.iso SHA1 6DF281E15CC18F2D0D257DC02884A12BEB8A92B9 文件大小 2.28GB 发布时间 2016-06-01 1ed2k:&#x2F;&#x2F;|file|cn_sql_server_2016_developer_x64_dvd_8776722.iso|2452795392|EF6BAADFBCC9C647180B0F93FD0186D0|&#x2F; 设计工具 文件名 mu_office_home_and_business_2016_for_mac_mac_dvd_7027756.iso SHA1 4B228801760C32BFF3AD6C037E10826BD5539383 文件大小 1.13GB 发布时间 2015-09-22 1ed2k:&#x2F;&#x2F;|file|mu_office_home_and_business_2016_for_mac_mac_dvd_7027756.iso|1214924800|D6FA02597D30709949C4FEA6AA0F9D6B|&#x2F;","updated":"2020-03-13T03:06:29.713Z","categories":[{"name":"系统安装修复","slug":"系统安装修复","permalink":"https://zouchanglin.cn/categories/%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E4%BF%AE%E5%A4%8D/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://zouchanglin.cn/tags/Windows/"}]},{"title":"Mac下制作Windows启动盘","date":"2017-06-17T03:51:00.000Z","path":"2017/06/17/Mac下制作Windows启动盘/","text":"下载Windows镜像https://msdn.itellyou.cn/ ,在这个网站可以找到你要下载的镜像文件，当然还有其他的下载链接 双击ISO文件双击你下载的ISO文件，这时你相当于将镜像挂载到了自己的电脑上，应该出现如下所示的情况。 注意挂载后的文件夹的名字，如我的Win10镜像挂载之后的名字就是J_CCSA_X64FRE_ZH-CN_DV5。 将挂载后的文件拷贝入U盘1diskutil list 会返回当前所有Volume的列表，找到你的U盘，这里为disk2 将U盘格式化为MS-DOS格式输入下列命令并将disk#更改为你U盘的序号，比如上述列子就改为disk2： 1diskutil eraseDisk MS-DOS &quot;WINDOWS10&quot; MBR disk2 将镜像文件拷贝进U盘，输入下列命令并将VolumeName换成你的镜像的名字，比如我就换成J_CCSA_X64FRE_ZH-CN_DV5 1cp -rp &#x2F;Volumes&#x2F;VolumeName&#x2F;* &#x2F;Volumes&#x2F;WINDOWS10&#x2F; 当Terminal里出现新的一行带“~”的内容时，启动盘就制作成功了！","updated":"2020-03-13T03:06:29.684Z","categories":[{"name":"系统安装修复","slug":"系统安装修复","permalink":"https://zouchanglin.cn/categories/%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E4%BF%AE%E5%A4%8D/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"https://zouchanglin.cn/tags/Mac/"},{"name":"Windows","slug":"Windows","permalink":"https://zouchanglin.cn/tags/Windows/"}]}]